{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 13:39:40.840885: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-21 13:39:40.957680: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-21 13:39:41.368404: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nkspartan/miniconda3/envs/tf-gpu/lib/\n",
      "2022-11-21 13:39:41.368468: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nkspartan/miniconda3/envs/tf-gpu/lib/\n",
      "2022-11-21 13:39:41.368472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/tmp/ipykernel_51029/891238804.py:15: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tools.eval_measures import stde\n",
    "\n",
    "import kerastuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 13:39:41.829691: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-21 13:39:41.852867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:41.857474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:41.857672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:42.257011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:42.257218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:42.257376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:42.257507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 3989 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the etl info results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>remove_time_features</th>\n",
       "      <th>generic_features</th>\n",
       "      <th>remove_atypical_values</th>\n",
       "      <th>feature_combination</th>\n",
       "      <th>remove_feature_selection</th>\n",
       "      <th>remove_invalid_correlated_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   remove_time_features  generic_features  remove_atypical_values  \\\n",
       "0                 False             False                   False   \n",
       "\n",
       "   feature_combination  remove_feature_selection  \\\n",
       "0                False                     False   \n",
       "\n",
       "   remove_invalid_correlated_features  \n",
       "0                               False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info = pd.read_csv('../dataset_clean/options_csv_v1_etl.csv')\n",
    "df_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SensorTime</th>\n",
       "      <th>CaptureTime</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Agency</th>\n",
       "      <th>SiteNumber</th>\n",
       "      <th>TimeZone</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Discharge</th>\n",
       "      <th>...</th>\n",
       "      <th>WwRawLineMin</th>\n",
       "      <th>WwRawLineMax</th>\n",
       "      <th>WwRawLineMean</th>\n",
       "      <th>WwRawLineSigma</th>\n",
       "      <th>WwCurveLineMin</th>\n",
       "      <th>WwCurveLineMax</th>\n",
       "      <th>WwCurveLineMean</th>\n",
       "      <th>WwCurveLineSigma</th>\n",
       "      <th>RiverArea</th>\n",
       "      <th>RiverWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2012-06-09T13:09:07</td>\n",
       "      <td>statelineweir_20120609_farrell_001.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49975.0</td>\n",
       "      <td>207.508733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2012-06-09T13:10:29</td>\n",
       "      <td>statelineweir_20120609_farrell_002.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50184.0</td>\n",
       "      <td>208.663145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-06-09 13:45:00</td>\n",
       "      <td>2012-06-09T13:44:01</td>\n",
       "      <td>statelineweir_20120609_farrell_003.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.96</td>\n",
       "      <td>873.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50543.0</td>\n",
       "      <td>209.445067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2012-06-09 14:45:00</td>\n",
       "      <td>2012-06-09T14:44:30</td>\n",
       "      <td>statelineweir_20120609_farrell_004.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50856.0</td>\n",
       "      <td>211.265690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-06-09 15:45:00</td>\n",
       "      <td>2012-06-09T15:44:59</td>\n",
       "      <td>statelineweir_20120609_farrell_005.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51004.0</td>\n",
       "      <td>211.250274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42054</th>\n",
       "      <td>42054</td>\n",
       "      <td>42054</td>\n",
       "      <td>2019-10-11 09:00:00</td>\n",
       "      <td>2019-10-11T08:59:53</td>\n",
       "      <td>statelineweir_20191011_farrell_409.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9284.0</td>\n",
       "      <td>77521.0</td>\n",
       "      <td>38385.370066</td>\n",
       "      <td>15952.029728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70085.0</td>\n",
       "      <td>37550.894823</td>\n",
       "      <td>16444.401209</td>\n",
       "      <td>45842.0</td>\n",
       "      <td>194.934605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42055</th>\n",
       "      <td>42055</td>\n",
       "      <td>42055</td>\n",
       "      <td>2019-10-11 10:00:00</td>\n",
       "      <td>2019-10-11T09:59:52</td>\n",
       "      <td>statelineweir_20191011_farrell_410.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10092.0</td>\n",
       "      <td>74614.0</td>\n",
       "      <td>40162.989292</td>\n",
       "      <td>15467.708856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70061.0</td>\n",
       "      <td>39397.339095</td>\n",
       "      <td>16009.008049</td>\n",
       "      <td>42300.0</td>\n",
       "      <td>194.762264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42056</th>\n",
       "      <td>42056</td>\n",
       "      <td>42056</td>\n",
       "      <td>2019-10-11 11:00:00</td>\n",
       "      <td>2019-10-11T10:59:52</td>\n",
       "      <td>statelineweir_20191011_farrell_411.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7067.0</td>\n",
       "      <td>83260.0</td>\n",
       "      <td>42095.946590</td>\n",
       "      <td>16770.357949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76335.0</td>\n",
       "      <td>41350.006568</td>\n",
       "      <td>17489.374617</td>\n",
       "      <td>41080.0</td>\n",
       "      <td>196.480105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42057</th>\n",
       "      <td>42057</td>\n",
       "      <td>42057</td>\n",
       "      <td>2019-10-11 12:00:00</td>\n",
       "      <td>2019-10-11T11:59:53</td>\n",
       "      <td>statelineweir_20191011_farrell_412.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6283.0</td>\n",
       "      <td>83045.0</td>\n",
       "      <td>45345.490954</td>\n",
       "      <td>17498.432849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78882.0</td>\n",
       "      <td>44553.920296</td>\n",
       "      <td>18268.294896</td>\n",
       "      <td>40976.0</td>\n",
       "      <td>193.595245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42058</th>\n",
       "      <td>42058</td>\n",
       "      <td>42058</td>\n",
       "      <td>2019-10-11 12:45:00</td>\n",
       "      <td>2019-10-11T12:59:52</td>\n",
       "      <td>statelineweir_20191011_farrell_413.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7375.0</td>\n",
       "      <td>89813.0</td>\n",
       "      <td>47877.870782</td>\n",
       "      <td>19963.166359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82630.0</td>\n",
       "      <td>47280.270559</td>\n",
       "      <td>20559.358767</td>\n",
       "      <td>41435.0</td>\n",
       "      <td>196.801994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42059 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0           SensorTime          CaptureTime  \\\n",
       "0                 0           0  2012-06-09 13:15:00  2012-06-09T13:09:07   \n",
       "1                 1           1  2012-06-09 13:15:00  2012-06-09T13:10:29   \n",
       "2                 2           2  2012-06-09 13:45:00  2012-06-09T13:44:01   \n",
       "3                 3           3  2012-06-09 14:45:00  2012-06-09T14:44:30   \n",
       "4                 4           4  2012-06-09 15:45:00  2012-06-09T15:44:59   \n",
       "...             ...         ...                  ...                  ...   \n",
       "42054         42054       42054  2019-10-11 09:00:00  2019-10-11T08:59:53   \n",
       "42055         42055       42055  2019-10-11 10:00:00  2019-10-11T09:59:52   \n",
       "42056         42056       42056  2019-10-11 11:00:00  2019-10-11T10:59:52   \n",
       "42057         42057       42057  2019-10-11 12:00:00  2019-10-11T11:59:53   \n",
       "42058         42058       42058  2019-10-11 12:45:00  2019-10-11T12:59:52   \n",
       "\n",
       "                                     Filename Agency  SiteNumber TimeZone  \\\n",
       "0      statelineweir_20120609_farrell_001.jpg   USGS     6674500      MDT   \n",
       "1      statelineweir_20120609_farrell_002.jpg   USGS     6674500      MDT   \n",
       "2      statelineweir_20120609_farrell_003.jpg   USGS     6674500      MDT   \n",
       "3      statelineweir_20120609_farrell_004.jpg   USGS     6674500      MDT   \n",
       "4      statelineweir_20120609_farrell_005.jpg   USGS     6674500      MDT   \n",
       "...                                       ...    ...         ...      ...   \n",
       "42054  statelineweir_20191011_farrell_409.jpg   USGS     6674500      MDT   \n",
       "42055  statelineweir_20191011_farrell_410.jpg   USGS     6674500      MDT   \n",
       "42056  statelineweir_20191011_farrell_411.jpg   USGS     6674500      MDT   \n",
       "42057  statelineweir_20191011_farrell_412.jpg   USGS     6674500      MDT   \n",
       "42058  statelineweir_20191011_farrell_413.jpg   USGS     6674500      MDT   \n",
       "\n",
       "       Stage  Discharge  ... WwRawLineMin  WwRawLineMax  WwRawLineMean  \\\n",
       "0       2.99      916.0  ...          0.0           0.0       0.000000   \n",
       "1       2.99      916.0  ...          0.0           0.0       0.000000   \n",
       "2       2.96      873.0  ...          0.0           0.0       0.000000   \n",
       "3       2.94      846.0  ...          0.0           0.0       0.000000   \n",
       "4       2.94      846.0  ...          0.0           0.0       0.000000   \n",
       "...      ...        ...  ...          ...           ...            ...   \n",
       "42054   2.54      434.0  ...       9284.0       77521.0   38385.370066   \n",
       "42055   2.54      434.0  ...      10092.0       74614.0   40162.989292   \n",
       "42056   2.54      434.0  ...       7067.0       83260.0   42095.946590   \n",
       "42057   2.54      434.0  ...       6283.0       83045.0   45345.490954   \n",
       "42058   2.54      434.0  ...       7375.0       89813.0   47877.870782   \n",
       "\n",
       "       WwRawLineSigma  WwCurveLineMin  WwCurveLineMax  WwCurveLineMean  \\\n",
       "0            0.000000             0.0             0.0         0.000000   \n",
       "1            0.000000             0.0             0.0         0.000000   \n",
       "2            0.000000             0.0             0.0         0.000000   \n",
       "3            0.000000             0.0             0.0         0.000000   \n",
       "4            0.000000             0.0             0.0         0.000000   \n",
       "...               ...             ...             ...              ...   \n",
       "42054    15952.029728             0.0         70085.0     37550.894823   \n",
       "42055    15467.708856             0.0         70061.0     39397.339095   \n",
       "42056    16770.357949             0.0         76335.0     41350.006568   \n",
       "42057    17498.432849             0.0         78882.0     44553.920296   \n",
       "42058    19963.166359             0.0         82630.0     47280.270559   \n",
       "\n",
       "       WwCurveLineSigma  RiverArea  RiverWidth  \n",
       "0              0.000000    49975.0  207.508733  \n",
       "1              0.000000    50184.0  208.663145  \n",
       "2              0.000000    50543.0  209.445067  \n",
       "3              0.000000    50856.0  211.265690  \n",
       "4              0.000000    51004.0  211.250274  \n",
       "...                 ...        ...         ...  \n",
       "42054      16444.401209    45842.0  194.934605  \n",
       "42055      16009.008049    42300.0  194.762264  \n",
       "42056      17489.374617    41080.0  196.480105  \n",
       "42057      18268.294896    40976.0  193.595245  \n",
       "42058      20559.358767    41435.0  196.801994  \n",
       "\n",
       "[42059 rows x 63 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset/V2_PlatteRiverWeir_features_merged_all.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SensorTime'] = pd.to_datetime(df['SensorTime'])\n",
    "df['Year'] = df['SensorTime'].dt.year\n",
    "df['Month'] = df['SensorTime'].dt.month\n",
    "df['date_offset'] = (df.SensorTime.dt.month * 100 + df.SensorTime.dt.day - 320)%1300\n",
    "\n",
    "df['Season'] = pd.cut(df['date_offset'], [0, 300, 602, 900, 1300], \n",
    "                      labels=['spring', 'summer', 'autumn', 'winter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CaptureTime</th>\n",
       "      <th>SensorTime</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Discharge</th>\n",
       "      <th>RiverArea</th>\n",
       "      <th>RiverWidth</th>\n",
       "      <th>Month</th>\n",
       "      <th>Season</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-06-09T13:09:07</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>49975.0</td>\n",
       "      <td>207.508733</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-06-09T13:10:29</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>50184.0</td>\n",
       "      <td>208.663145</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-06-09T13:44:01</td>\n",
       "      <td>2012-06-09 13:45:00</td>\n",
       "      <td>2.96</td>\n",
       "      <td>873.0</td>\n",
       "      <td>50543.0</td>\n",
       "      <td>209.445067</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-06-09T14:44:30</td>\n",
       "      <td>2012-06-09 14:45:00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>50856.0</td>\n",
       "      <td>211.265690</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-06-09T15:44:59</td>\n",
       "      <td>2012-06-09 15:45:00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>51004.0</td>\n",
       "      <td>211.250274</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42054</th>\n",
       "      <td>2019-10-11T08:59:53</td>\n",
       "      <td>2019-10-11 09:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>45842.0</td>\n",
       "      <td>194.934605</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42055</th>\n",
       "      <td>2019-10-11T09:59:52</td>\n",
       "      <td>2019-10-11 10:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>42300.0</td>\n",
       "      <td>194.762264</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42056</th>\n",
       "      <td>2019-10-11T10:59:52</td>\n",
       "      <td>2019-10-11 11:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>41080.0</td>\n",
       "      <td>196.480105</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42057</th>\n",
       "      <td>2019-10-11T11:59:53</td>\n",
       "      <td>2019-10-11 12:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>40976.0</td>\n",
       "      <td>193.595245</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42058</th>\n",
       "      <td>2019-10-11T12:59:52</td>\n",
       "      <td>2019-10-11 12:45:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>41435.0</td>\n",
       "      <td>196.801994</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42059 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               CaptureTime          SensorTime  Stage  Discharge  RiverArea  \\\n",
       "0      2012-06-09T13:09:07 2012-06-09 13:15:00   2.99      916.0    49975.0   \n",
       "1      2012-06-09T13:10:29 2012-06-09 13:15:00   2.99      916.0    50184.0   \n",
       "2      2012-06-09T13:44:01 2012-06-09 13:45:00   2.96      873.0    50543.0   \n",
       "3      2012-06-09T14:44:30 2012-06-09 14:45:00   2.94      846.0    50856.0   \n",
       "4      2012-06-09T15:44:59 2012-06-09 15:45:00   2.94      846.0    51004.0   \n",
       "...                    ...                 ...    ...        ...        ...   \n",
       "42054  2019-10-11T08:59:53 2019-10-11 09:00:00   2.54      434.0    45842.0   \n",
       "42055  2019-10-11T09:59:52 2019-10-11 10:00:00   2.54      434.0    42300.0   \n",
       "42056  2019-10-11T10:59:52 2019-10-11 11:00:00   2.54      434.0    41080.0   \n",
       "42057  2019-10-11T11:59:53 2019-10-11 12:00:00   2.54      434.0    40976.0   \n",
       "42058  2019-10-11T12:59:52 2019-10-11 12:45:00   2.54      434.0    41435.0   \n",
       "\n",
       "       RiverWidth  Month  Season  Year  \n",
       "0      207.508733      6  spring  2012  \n",
       "1      208.663145      6  spring  2012  \n",
       "2      209.445067      6  spring  2012  \n",
       "3      211.265690      6  spring  2012  \n",
       "4      211.250274      6  spring  2012  \n",
       "...           ...    ...     ...   ...  \n",
       "42054  194.934605     10  autumn  2019  \n",
       "42055  194.762264     10  autumn  2019  \n",
       "42056  196.480105     10  autumn  2019  \n",
       "42057  193.595245     10  autumn  2019  \n",
       "42058  196.801994     10  autumn  2019  \n",
       "\n",
       "[42059 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"CaptureTime\", \"SensorTime\", \"Stage\", \"Discharge\", \"RiverArea\", \"RiverWidth\", \"Month\", \"Season\", \"Year\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaptureTime            object\n",
       "SensorTime     datetime64[ns]\n",
       "Stage                 float64\n",
       "Discharge             float64\n",
       "RiverArea             float64\n",
       "RiverWidth            float64\n",
       "Month                   int64\n",
       "Season               category\n",
       "Year                    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40148, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.Stage > 0]\n",
    "df = df[df.Discharge > 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40142, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.RiverWidth > 0]\n",
    "#df = df[df.Discharge > 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.Season != \"winter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaptureTime      0\n",
       "SensorTime       0\n",
       "Stage            0\n",
       "Discharge        0\n",
       "RiverArea        0\n",
       "RiverWidth       0\n",
       "Month            0\n",
       "Season         126\n",
       "Year             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide dataset to X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "df_train = df[(df.Year >= 2012) & (df.Year <= 2016)]\n",
    "df_train = df_train.iloc[np.random.permutation(len(df_train))]\n",
    "\n",
    "df_val = df[(df.Year >= 2017) & (df.Year <= 2017)]\n",
    "df_val = df_val.iloc[np.random.permutation(len(df_val))]\n",
    "\n",
    "df_test = df[(df.Year >= 2018) & (df.Year <= 2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])\n",
    "df_val = df_val.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])\n",
    "df_test = df_test.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = df_train[[\"Stage\"]].values\n",
    "X_train = df_train[[\"RiverWidth\", \"Month\"]].values\n",
    "\n",
    "y_val = df_train[[\"Stage\"]].values\n",
    "X_val = df_train[[\"RiverWidth\", \"Month\"]].values\n",
    "\n",
    "y_test = df_test[[\"Stage\"]].values\n",
    "X_test = df_test[[\"RiverWidth\", \"Month\"]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20304, 2)\n",
      "(20304, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "print(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(lr):\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.Input(shape=input_shape))\n",
    "  \n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(32, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(128, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"tanh\"))\n",
    "  \"\"\"model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(512, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(512, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(128, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(32, activation=\"tanh\"))\"\"\"\n",
    "\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(output_shape, activation = 'linear'))\n",
    "\n",
    "  \n",
    "  model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr), loss = 'mae', metrics = ['mse', tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mae', 'mape'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 13:39:43.144697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:43.145050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:43.145285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:43.145607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:43.145769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:43.145919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:43.146117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:43.146271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-21 13:39:43.146447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3989 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = model_builder(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date_actual = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/fit/\" + date_actual\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"model_weights/{date_actual}_mlp_best_weights.hdf5\",\n",
    "                               monitor='val_loss',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.5180 - mse: 0.7720 - rmse: 0.8786 - mae: 0.5180 - mape: 15.8458\n",
      "Epoch 1: val_loss improved from inf to 0.43083, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 2s 3ms/step - loss: 0.5155 - mse: 0.7675 - rmse: 0.8761 - mae: 0.5155 - mape: 15.7744 - val_loss: 0.4308 - val_mse: 0.6613 - val_rmse: 0.8132 - val_mae: 0.4308 - val_mape: 12.7185 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.4407 - mse: 0.6659 - rmse: 0.8160 - mae: 0.4407 - mape: 13.1070\n",
      "Epoch 2: val_loss improved from 0.43083 to 0.41631, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4390 - mse: 0.6651 - rmse: 0.8155 - mae: 0.4390 - mape: 13.0348 - val_loss: 0.4163 - val_mse: 0.6118 - val_rmse: 0.7822 - val_mae: 0.4163 - val_mape: 12.5033 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.4167 - mse: 0.6266 - rmse: 0.7916 - mae: 0.4167 - mape: 12.3363\n",
      "Epoch 3: val_loss improved from 0.41631 to 0.39662, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4155 - mse: 0.6244 - rmse: 0.7902 - mae: 0.4155 - mape: 12.2884 - val_loss: 0.3966 - val_mse: 0.5737 - val_rmse: 0.7574 - val_mae: 0.3966 - val_mape: 11.8778 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.4057 - mse: 0.6033 - rmse: 0.7767 - mae: 0.4057 - mape: 12.0676\n",
      "Epoch 4: val_loss did not improve from 0.39662\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4063 - mse: 0.6051 - rmse: 0.7779 - mae: 0.4063 - mape: 12.0797 - val_loss: 0.4208 - val_mse: 0.7388 - val_rmse: 0.8595 - val_mae: 0.4208 - val_mape: 11.6394 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.4010 - mse: 0.5926 - rmse: 0.7698 - mae: 0.4010 - mape: 12.0335\n",
      "Epoch 5: val_loss did not improve from 0.39662\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4026 - mse: 0.5964 - rmse: 0.7723 - mae: 0.4026 - mape: 12.0800 - val_loss: 0.3971 - val_mse: 0.5543 - val_rmse: 0.7445 - val_mae: 0.3971 - val_mape: 11.9191 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.3965 - mse: 0.5795 - rmse: 0.7613 - mae: 0.3965 - mape: 11.9924\n",
      "Epoch 6: val_loss improved from 0.39662 to 0.39064, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3951 - mse: 0.5756 - rmse: 0.7587 - mae: 0.3951 - mape: 11.9567 - val_loss: 0.3906 - val_mse: 0.5841 - val_rmse: 0.7642 - val_mae: 0.3906 - val_mape: 11.7939 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.4055 - mse: 0.6282 - rmse: 0.7926 - mae: 0.4055 - mape: 11.9970\n",
      "Epoch 7: val_loss improved from 0.39064 to 0.37484, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4036 - mse: 0.6213 - rmse: 0.7883 - mae: 0.4036 - mape: 11.9617 - val_loss: 0.3748 - val_mse: 0.5045 - val_rmse: 0.7103 - val_mae: 0.3748 - val_mape: 11.5338 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.3871 - mse: 0.5575 - rmse: 0.7466 - mae: 0.3871 - mape: 11.7435\n",
      "Epoch 8: val_loss did not improve from 0.37484\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3905 - mse: 0.5728 - rmse: 0.7568 - mae: 0.3905 - mape: 11.7726 - val_loss: 0.4153 - val_mse: 0.7338 - val_rmse: 0.8566 - val_mae: 0.4153 - val_mape: 11.4091 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.3902 - mse: 0.5773 - rmse: 0.7598 - mae: 0.3902 - mape: 11.6644\n",
      "Epoch 9: val_loss improved from 0.37484 to 0.36166, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3885 - mse: 0.5662 - rmse: 0.7525 - mae: 0.3885 - mape: 11.6358 - val_loss: 0.3617 - val_mse: 0.4479 - val_rmse: 0.6693 - val_mae: 0.3617 - val_mape: 11.3077 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.3928 - mse: 0.6045 - rmse: 0.7775 - mae: 0.3928 - mape: 11.5543\n",
      "Epoch 10: val_loss did not improve from 0.36166\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3937 - mse: 0.6084 - rmse: 0.7800 - mae: 0.3937 - mape: 11.5606 - val_loss: 0.4111 - val_mse: 0.7058 - val_rmse: 0.8401 - val_mae: 0.4111 - val_mape: 11.9146 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.3940 - mse: 0.5760 - rmse: 0.7589 - mae: 0.3940 - mape: 11.9641\n",
      "Epoch 11: val_loss did not improve from 0.36166\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3923 - mse: 0.5705 - rmse: 0.7553 - mae: 0.3923 - mape: 11.8861 - val_loss: 0.3689 - val_mse: 0.4872 - val_rmse: 0.6980 - val_mae: 0.3689 - val_mape: 11.3631 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.3632 - mse: 0.4692 - rmse: 0.6850 - mae: 0.3632 - mape: 11.0825\n",
      "Epoch 12: val_loss did not improve from 0.36166\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3629 - mse: 0.4687 - rmse: 0.6846 - mae: 0.3629 - mape: 11.0671 - val_loss: 0.4801 - val_mse: 0.7515 - val_rmse: 0.8669 - val_mae: 0.4801 - val_mape: 15.6492 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.3501 - mse: 0.4367 - rmse: 0.6608 - mae: 0.3501 - mape: 10.5328\n",
      "Epoch 13: val_loss did not improve from 0.36166\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3477 - mse: 0.4299 - rmse: 0.6557 - mae: 0.3477 - mape: 10.4699 - val_loss: 0.3681 - val_mse: 0.4440 - val_rmse: 0.6664 - val_mae: 0.3681 - val_mape: 11.5701 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.3727 - mse: 0.4990 - rmse: 0.7064 - mae: 0.3727 - mape: 11.4355\n",
      "Epoch 14: val_loss improved from 0.36166 to 0.33247, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3726 - mse: 0.4992 - rmse: 0.7066 - mae: 0.3726 - mape: 11.4367 - val_loss: 0.3325 - val_mse: 0.3769 - val_rmse: 0.6139 - val_mae: 0.3325 - val_mape: 10.6791 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.3985 - mse: 0.6018 - rmse: 0.7758 - mae: 0.3985 - mape: 11.8859\n",
      "Epoch 15: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3985 - mse: 0.6015 - rmse: 0.7755 - mae: 0.3985 - mape: 11.8858 - val_loss: 0.3767 - val_mse: 0.5323 - val_rmse: 0.7296 - val_mae: 0.3767 - val_mape: 11.3119 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3783 - mse: 0.5230 - rmse: 0.7232 - mae: 0.3783 - mape: 11.5140\n",
      "Epoch 16: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3779 - mse: 0.5220 - rmse: 0.7225 - mae: 0.3779 - mape: 11.5046 - val_loss: 0.3850 - val_mse: 0.5055 - val_rmse: 0.7110 - val_mae: 0.3850 - val_mape: 12.0192 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.3760 - mse: 0.5057 - rmse: 0.7111 - mae: 0.3760 - mape: 11.5158\n",
      "Epoch 17: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3746 - mse: 0.5036 - rmse: 0.7097 - mae: 0.3746 - mape: 11.4848 - val_loss: 0.3758 - val_mse: 0.5138 - val_rmse: 0.7168 - val_mae: 0.3758 - val_mape: 11.3080 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.3680 - mse: 0.4863 - rmse: 0.6973 - mae: 0.3680 - mape: 11.2962\n",
      "Epoch 18: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3673 - mse: 0.4854 - rmse: 0.6967 - mae: 0.3673 - mape: 11.2646 - val_loss: 0.3665 - val_mse: 0.4652 - val_rmse: 0.6821 - val_mae: 0.3665 - val_mape: 11.4668 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.3665 - mse: 0.4751 - rmse: 0.6893 - mae: 0.3665 - mape: 11.2370\n",
      "Epoch 19: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3649 - mse: 0.4711 - rmse: 0.6864 - mae: 0.3649 - mape: 11.2033 - val_loss: 0.3616 - val_mse: 0.4680 - val_rmse: 0.6841 - val_mae: 0.3616 - val_mape: 10.9098 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.3639 - mse: 0.4628 - rmse: 0.6803 - mae: 0.3639 - mape: 11.2085\n",
      "Epoch 20: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3638 - mse: 0.4624 - rmse: 0.6800 - mae: 0.3638 - mape: 11.2150 - val_loss: 0.3583 - val_mse: 0.4584 - val_rmse: 0.6771 - val_mae: 0.3583 - val_mape: 10.9886 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.3623 - mse: 0.4519 - rmse: 0.6722 - mae: 0.3623 - mape: 11.2367\n",
      "Epoch 21: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3623 - mse: 0.4526 - rmse: 0.6728 - mae: 0.3623 - mape: 11.2055 - val_loss: 0.3671 - val_mse: 0.4641 - val_rmse: 0.6812 - val_mae: 0.3671 - val_mape: 11.3960 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.4303 - mse: 0.6307 - rmse: 0.7942 - mae: 0.4303 - mape: 13.4590\n",
      "Epoch 22: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.4352 - mse: 0.6398 - rmse: 0.7999 - mae: 0.4352 - mape: 13.6186 - val_loss: 0.5338 - val_mse: 0.7577 - val_rmse: 0.8705 - val_mae: 0.5338 - val_mape: 17.1324 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.4309 - mse: 0.6715 - rmse: 0.8195 - mae: 0.4309 - mape: 12.8598\n",
      "Epoch 23: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.4309 - mse: 0.6721 - rmse: 0.8198 - mae: 0.4309 - mape: 12.8459 - val_loss: 0.4106 - val_mse: 0.6807 - val_rmse: 0.8251 - val_mae: 0.4106 - val_mape: 11.6871 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.3982 - mse: 0.5857 - rmse: 0.7653 - mae: 0.3982 - mape: 12.0427\n",
      "Epoch 24: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3984 - mse: 0.5850 - rmse: 0.7649 - mae: 0.3984 - mape: 12.0663 - val_loss: 0.3746 - val_mse: 0.5286 - val_rmse: 0.7271 - val_mae: 0.3746 - val_mape: 11.5597 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.3807 - mse: 0.5215 - rmse: 0.7222 - mae: 0.3807 - mape: 11.8252\n",
      "Epoch 25: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3808 - mse: 0.5218 - rmse: 0.7224 - mae: 0.3808 - mape: 11.8220 - val_loss: 0.3866 - val_mse: 0.4982 - val_rmse: 0.7058 - val_mae: 0.3866 - val_mape: 12.5929 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.3895 - mse: 0.5637 - rmse: 0.7508 - mae: 0.3895 - mape: 11.8037\n",
      "Epoch 26: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3894 - mse: 0.5641 - rmse: 0.7511 - mae: 0.3894 - mape: 11.7969 - val_loss: 0.3837 - val_mse: 0.5564 - val_rmse: 0.7459 - val_mae: 0.3837 - val_mape: 11.4270 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.3910 - mse: 0.5539 - rmse: 0.7442 - mae: 0.3910 - mape: 11.9328\n",
      "Epoch 27: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3912 - mse: 0.5547 - rmse: 0.7448 - mae: 0.3912 - mape: 11.9306 - val_loss: 0.3744 - val_mse: 0.5356 - val_rmse: 0.7318 - val_mae: 0.3744 - val_mape: 11.2673 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.3814 - mse: 0.5350 - rmse: 0.7314 - mae: 0.3814 - mape: 11.6305\n",
      "Epoch 28: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3811 - mse: 0.5339 - rmse: 0.7307 - mae: 0.3811 - mape: 11.6199 - val_loss: 0.3723 - val_mse: 0.5019 - val_rmse: 0.7084 - val_mae: 0.3723 - val_mape: 11.5323 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3933 - mse: 0.6052 - rmse: 0.7779 - mae: 0.3933 - mape: 11.5733\n",
      "Epoch 29: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3931 - mse: 0.6047 - rmse: 0.7777 - mae: 0.3931 - mape: 11.5685 - val_loss: 0.4008 - val_mse: 0.6897 - val_rmse: 0.8305 - val_mae: 0.4008 - val_mape: 11.0253 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.3895 - mse: 0.5648 - rmse: 0.7516 - mae: 0.3895 - mape: 11.7139\n",
      "Epoch 30: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3879 - mse: 0.5614 - rmse: 0.7493 - mae: 0.3879 - mape: 11.6573 - val_loss: 0.3827 - val_mse: 0.5179 - val_rmse: 0.7197 - val_mae: 0.3827 - val_mape: 11.5713 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.3741 - mse: 0.4993 - rmse: 0.7066 - mae: 0.3741 - mape: 11.5842\n",
      "Epoch 31: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3741 - mse: 0.4996 - rmse: 0.7068 - mae: 0.3741 - mape: 11.5722 - val_loss: 0.4041 - val_mse: 0.6934 - val_rmse: 0.8327 - val_mae: 0.4041 - val_mape: 11.9678 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.3762 - mse: 0.4959 - rmse: 0.7042 - mae: 0.3762 - mape: 11.6314\n",
      "Epoch 32: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3740 - mse: 0.4905 - rmse: 0.7004 - mae: 0.3740 - mape: 11.5477 - val_loss: 0.3664 - val_mse: 0.4546 - val_rmse: 0.6743 - val_mae: 0.3664 - val_mape: 11.4746 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.3648 - mse: 0.4559 - rmse: 0.6752 - mae: 0.3648 - mape: 11.3925\n",
      "Epoch 33: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3655 - mse: 0.4561 - rmse: 0.6753 - mae: 0.3655 - mape: 11.4270 - val_loss: 0.3667 - val_mse: 0.4116 - val_rmse: 0.6416 - val_mae: 0.3667 - val_mape: 11.9206 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.4045 - mse: 0.5583 - rmse: 0.7472 - mae: 0.4045 - mape: 12.5371\n",
      "Epoch 34: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.4043 - mse: 0.5576 - rmse: 0.7467 - mae: 0.4043 - mape: 12.5556 - val_loss: 0.3607 - val_mse: 0.4826 - val_rmse: 0.6947 - val_mae: 0.3607 - val_mape: 10.7220 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.3677 - mse: 0.4722 - rmse: 0.6872 - mae: 0.3677 - mape: 11.3459\n",
      "Epoch 35: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3709 - mse: 0.4854 - rmse: 0.6967 - mae: 0.3709 - mape: 11.3905 - val_loss: 0.5224 - val_mse: 1.0233 - val_rmse: 1.0116 - val_mae: 0.5224 - val_mape: 13.7041 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.3792 - mse: 0.5090 - rmse: 0.7134 - mae: 0.3792 - mape: 11.7373\n",
      "Epoch 36: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3788 - mse: 0.5064 - rmse: 0.7116 - mae: 0.3788 - mape: 11.7403 - val_loss: 0.3483 - val_mse: 0.3913 - val_rmse: 0.6256 - val_mae: 0.3483 - val_mape: 11.0921 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.3642 - mse: 0.4438 - rmse: 0.6661 - mae: 0.3642 - mape: 11.4605\n",
      "Epoch 37: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3636 - mse: 0.4427 - rmse: 0.6654 - mae: 0.3636 - mape: 11.4672 - val_loss: 0.3542 - val_mse: 0.3925 - val_rmse: 0.6265 - val_mae: 0.3542 - val_mape: 11.4387 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.3596 - mse: 0.4387 - rmse: 0.6623 - mae: 0.3596 - mape: 11.4406\n",
      "Epoch 38: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3603 - mse: 0.4386 - rmse: 0.6622 - mae: 0.3603 - mape: 11.4563 - val_loss: 0.3399 - val_mse: 0.3773 - val_rmse: 0.6142 - val_mae: 0.3399 - val_mape: 10.7767 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.4154 - mse: 0.6966 - rmse: 0.8346 - mae: 0.4154 - mape: 11.9173\n",
      "Epoch 39: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4154 - mse: 0.6948 - rmse: 0.8336 - mae: 0.4154 - mape: 11.9185 - val_loss: 0.3772 - val_mse: 0.5544 - val_rmse: 0.7446 - val_mae: 0.3772 - val_mape: 11.1088 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.3939 - mse: 0.5943 - rmse: 0.7709 - mae: 0.3939 - mape: 11.7548\n",
      "Epoch 40: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3936 - mse: 0.5927 - rmse: 0.7698 - mae: 0.3936 - mape: 11.7779 - val_loss: 0.3778 - val_mse: 0.5183 - val_rmse: 0.7200 - val_mae: 0.3778 - val_mape: 12.3691 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.3780 - mse: 0.5108 - rmse: 0.7147 - mae: 0.3780 - mape: 11.8563\n",
      "Epoch 41: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3774 - mse: 0.5083 - rmse: 0.7129 - mae: 0.3774 - mape: 11.8308 - val_loss: 0.3675 - val_mse: 0.4912 - val_rmse: 0.7009 - val_mae: 0.3675 - val_mape: 11.0051 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.3540 - mse: 0.4455 - rmse: 0.6674 - mae: 0.3540 - mape: 10.7799\n",
      "Epoch 42: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3583 - mse: 0.4537 - rmse: 0.6736 - mae: 0.3583 - mape: 10.9589 - val_loss: 0.4946 - val_mse: 0.6639 - val_rmse: 0.8148 - val_mae: 0.4946 - val_mape: 16.2500 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.3896 - mse: 0.5325 - rmse: 0.7297 - mae: 0.3896 - mape: 12.0373\n",
      "Epoch 43: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3884 - mse: 0.5303 - rmse: 0.7282 - mae: 0.3884 - mape: 12.0002 - val_loss: 0.3794 - val_mse: 0.4871 - val_rmse: 0.6980 - val_mae: 0.3794 - val_mape: 12.0282 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.3732 - mse: 0.4907 - rmse: 0.7005 - mae: 0.3732 - mape: 11.6546\n",
      "Epoch 44: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3730 - mse: 0.4903 - rmse: 0.7002 - mae: 0.3730 - mape: 11.6464 - val_loss: 0.3659 - val_mse: 0.4705 - val_rmse: 0.6859 - val_mae: 0.3659 - val_mape: 11.6607 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.3740 - mse: 0.5028 - rmse: 0.7091 - mae: 0.3740 - mape: 11.3659\n",
      "Epoch 45: val_loss did not improve from 0.33247\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3743 - mse: 0.5034 - rmse: 0.7095 - mae: 0.3743 - mape: 11.3641 - val_loss: 0.3450 - val_mse: 0.4203 - val_rmse: 0.6483 - val_mae: 0.3450 - val_mape: 10.2777 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.3462 - mse: 0.4011 - rmse: 0.6334 - mae: 0.3462 - mape: 10.5338\n",
      "Epoch 46: val_loss improved from 0.33247 to 0.33106, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3438 - mse: 0.3955 - rmse: 0.6289 - mae: 0.3438 - mape: 10.4563 - val_loss: 0.3311 - val_mse: 0.3211 - val_rmse: 0.5667 - val_mae: 0.3311 - val_mape: 10.5462 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.3736 - mse: 0.5378 - rmse: 0.7333 - mae: 0.3736 - mape: 11.0327\n",
      "Epoch 47: val_loss did not improve from 0.33106\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3745 - mse: 0.5440 - rmse: 0.7376 - mae: 0.3745 - mape: 11.0287 - val_loss: 0.3912 - val_mse: 0.6060 - val_rmse: 0.7784 - val_mae: 0.3912 - val_mape: 11.2807 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.3868 - mse: 0.5848 - rmse: 0.7647 - mae: 0.3868 - mape: 11.2529\n",
      "Epoch 48: val_loss did not improve from 0.33106\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3872 - mse: 0.5872 - rmse: 0.7663 - mae: 0.3872 - mape: 11.2553 - val_loss: 0.3865 - val_mse: 0.5488 - val_rmse: 0.7408 - val_mae: 0.3865 - val_mape: 11.8436 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.3807 - mse: 0.5451 - rmse: 0.7383 - mae: 0.3807 - mape: 11.4041\n",
      "Epoch 49: val_loss did not improve from 0.33106\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3794 - mse: 0.5409 - rmse: 0.7355 - mae: 0.3794 - mape: 11.3840 - val_loss: 0.3979 - val_mse: 0.5369 - val_rmse: 0.7327 - val_mae: 0.3979 - val_mape: 12.8296 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.3584 - mse: 0.4578 - rmse: 0.6766 - mae: 0.3584 - mape: 11.0949\n",
      "Epoch 50: val_loss did not improve from 0.33106\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3583 - mse: 0.4579 - rmse: 0.6767 - mae: 0.3583 - mape: 11.1143 - val_loss: 0.3567 - val_mse: 0.4418 - val_rmse: 0.6647 - val_mae: 0.3567 - val_mape: 11.0627 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3637 - mse: 0.4729 - rmse: 0.6877 - mae: 0.3637 - mape: 11.3179\n",
      "Epoch 51: val_loss did not improve from 0.33106\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3642 - mse: 0.4741 - rmse: 0.6886 - mae: 0.3642 - mape: 11.3258 - val_loss: 0.3862 - val_mse: 0.5032 - val_rmse: 0.7093 - val_mae: 0.3862 - val_mape: 11.9339 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.3355 - mse: 0.3836 - rmse: 0.6194 - mae: 0.3355 - mape: 10.0244\n",
      "Epoch 52: val_loss did not improve from 0.33106\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3356 - mse: 0.3817 - rmse: 0.6178 - mae: 0.3356 - mape: 10.0508 - val_loss: 0.3981 - val_mse: 0.4787 - val_rmse: 0.6919 - val_mae: 0.3981 - val_mape: 12.2921 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.3466 - mse: 0.3920 - rmse: 0.6261 - mae: 0.3466 - mape: 10.6021\n",
      "Epoch 53: val_loss improved from 0.33106 to 0.32233, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3445 - mse: 0.3871 - rmse: 0.6221 - mae: 0.3445 - mape: 10.5360 - val_loss: 0.3223 - val_mse: 0.3417 - val_rmse: 0.5845 - val_mae: 0.3223 - val_mape: 9.8676 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.3210 - mse: 0.3455 - rmse: 0.5878 - mae: 0.3210 - mape: 9.7622\n",
      "Epoch 54: val_loss improved from 0.32233 to 0.30593, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3204 - mse: 0.3438 - rmse: 0.5863 - mae: 0.3204 - mape: 9.7387 - val_loss: 0.3059 - val_mse: 0.2954 - val_rmse: 0.5435 - val_mae: 0.3059 - val_mape: 9.1635 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.3691 - mse: 0.5236 - rmse: 0.7236 - mae: 0.3691 - mape: 11.0794\n",
      "Epoch 55: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3731 - mse: 0.5400 - rmse: 0.7349 - mae: 0.3731 - mape: 11.1405 - val_loss: 0.4775 - val_mse: 0.9603 - val_rmse: 0.9799 - val_mae: 0.4775 - val_mape: 12.5638 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.4172 - mse: 0.7270 - rmse: 0.8526 - mae: 0.4172 - mape: 11.9916\n",
      "Epoch 56: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4131 - mse: 0.7040 - rmse: 0.8390 - mae: 0.4131 - mape: 11.9490 - val_loss: 0.3605 - val_mse: 0.4026 - val_rmse: 0.6345 - val_mae: 0.3605 - val_mape: 12.2431 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.3668 - mse: 0.4673 - rmse: 0.6836 - mae: 0.3668 - mape: 11.6889\n",
      "Epoch 57: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3723 - mse: 0.4940 - rmse: 0.7029 - mae: 0.3723 - mape: 11.6879 - val_loss: 0.4094 - val_mse: 0.7342 - val_rmse: 0.8569 - val_mae: 0.4094 - val_mape: 11.2486 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.3906 - mse: 0.5864 - rmse: 0.7658 - mae: 0.3906 - mape: 11.8855\n",
      "Epoch 58: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3942 - mse: 0.6013 - rmse: 0.7755 - mae: 0.3942 - mape: 11.8726 - val_loss: 0.4245 - val_mse: 0.7547 - val_rmse: 0.8687 - val_mae: 0.4245 - val_mape: 11.8828 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.4086 - mse: 0.7291 - rmse: 0.8538 - mae: 0.4086 - mape: 11.0269\n",
      "Epoch 59: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.4093 - mse: 0.7320 - rmse: 0.8556 - mae: 0.4093 - mape: 11.0464 - val_loss: 0.3967 - val_mse: 0.6997 - val_rmse: 0.8365 - val_mae: 0.3967 - val_mape: 10.5709 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.3994 - mse: 0.6333 - rmse: 0.7958 - mae: 0.3994 - mape: 11.7335\n",
      "Epoch 60: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4011 - mse: 0.6342 - rmse: 0.7964 - mae: 0.4011 - mape: 11.8244 - val_loss: 0.4523 - val_mse: 0.5509 - val_rmse: 0.7422 - val_mae: 0.4523 - val_mape: 13.9412 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.3963 - mse: 0.6066 - rmse: 0.7788 - mae: 0.3963 - mape: 11.8419\n",
      "Epoch 61: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3972 - mse: 0.6136 - rmse: 0.7833 - mae: 0.3972 - mape: 11.8266 - val_loss: 0.4781 - val_mse: 0.9575 - val_rmse: 0.9785 - val_mae: 0.4781 - val_mape: 12.7493 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.4154 - mse: 0.7287 - rmse: 0.8536 - mae: 0.4154 - mape: 11.6852\n",
      "Epoch 62: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.4151 - mse: 0.7263 - rmse: 0.8522 - mae: 0.4151 - mape: 11.6943 - val_loss: 0.4076 - val_mse: 0.6882 - val_rmse: 0.8296 - val_mae: 0.4076 - val_mape: 11.5273 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.3515 - mse: 0.4733 - rmse: 0.6880 - mae: 0.3515 - mape: 10.1769\n",
      "Epoch 63: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3509 - mse: 0.4695 - rmse: 0.6852 - mae: 0.3509 - mape: 10.1822 - val_loss: 0.3699 - val_mse: 0.5093 - val_rmse: 0.7137 - val_mae: 0.3699 - val_mape: 11.3763 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3297 - mse: 0.3902 - rmse: 0.6246 - mae: 0.3297 - mape: 9.8559\n",
      "Epoch 64: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3297 - mse: 0.3901 - rmse: 0.6246 - mae: 0.3297 - mape: 9.8559 - val_loss: 0.3140 - val_mse: 0.3568 - val_rmse: 0.5973 - val_mae: 0.3140 - val_mape: 9.0356 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3239 - mse: 0.3656 - rmse: 0.6046 - mae: 0.3239 - mape: 9.6935\n",
      "Epoch 65: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3240 - mse: 0.3655 - rmse: 0.6046 - mae: 0.3240 - mape: 9.6969 - val_loss: 0.3297 - val_mse: 0.3590 - val_rmse: 0.5992 - val_mae: 0.3297 - val_mape: 10.1737 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.3287 - mse: 0.3735 - rmse: 0.6112 - mae: 0.3287 - mape: 9.9691 \n",
      "Epoch 66: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3285 - mse: 0.3737 - rmse: 0.6113 - mae: 0.3285 - mape: 9.9656 - val_loss: 0.3130 - val_mse: 0.3208 - val_rmse: 0.5664 - val_mae: 0.3130 - val_mape: 9.5724 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.3216 - mse: 0.3537 - rmse: 0.5948 - mae: 0.3216 - mape: 9.7622\n",
      "Epoch 67: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3240 - mse: 0.3612 - rmse: 0.6010 - mae: 0.3240 - mape: 9.8633 - val_loss: 0.4154 - val_mse: 0.6676 - val_rmse: 0.8171 - val_mae: 0.4154 - val_mape: 13.1100 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.3149 - mse: 0.3304 - rmse: 0.5748 - mae: 0.3149 - mape: 9.6503\n",
      "Epoch 68: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3148 - mse: 0.3305 - rmse: 0.5749 - mae: 0.3148 - mape: 9.6517 - val_loss: 0.3113 - val_mse: 0.3092 - val_rmse: 0.5561 - val_mae: 0.3113 - val_mape: 9.9634 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.3348 - mse: 0.4035 - rmse: 0.6352 - mae: 0.3348 - mape: 10.1683\n",
      "Epoch 69: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3355 - mse: 0.4063 - rmse: 0.6374 - mae: 0.3355 - mape: 10.1495 - val_loss: 0.3281 - val_mse: 0.3944 - val_rmse: 0.6280 - val_mae: 0.3281 - val_mape: 9.5841 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.3338 - mse: 0.3888 - rmse: 0.6235 - mae: 0.3338 - mape: 9.9459\n",
      "Epoch 70: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3340 - mse: 0.3893 - rmse: 0.6239 - mae: 0.3340 - mape: 9.9499 - val_loss: 0.3160 - val_mse: 0.3647 - val_rmse: 0.6039 - val_mae: 0.3160 - val_mape: 9.3584 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.3416 - mse: 0.4141 - rmse: 0.6435 - mae: 0.3416 - mape: 10.3706\n",
      "Epoch 71: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3409 - mse: 0.4122 - rmse: 0.6420 - mae: 0.3409 - mape: 10.3519 - val_loss: 0.3752 - val_mse: 0.5501 - val_rmse: 0.7417 - val_mae: 0.3752 - val_mape: 11.3192 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.3327 - mse: 0.3901 - rmse: 0.6246 - mae: 0.3327 - mape: 10.1093\n",
      "Epoch 72: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3303 - mse: 0.3846 - rmse: 0.6201 - mae: 0.3303 - mape: 10.0475 - val_loss: 0.3157 - val_mse: 0.3661 - val_rmse: 0.6051 - val_mae: 0.3157 - val_mape: 9.6397 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.3234 - mse: 0.3640 - rmse: 0.6033 - mae: 0.3234 - mape: 9.8927\n",
      "Epoch 73: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3227 - mse: 0.3610 - rmse: 0.6008 - mae: 0.3227 - mape: 9.8813 - val_loss: 0.3081 - val_mse: 0.3217 - val_rmse: 0.5672 - val_mae: 0.3081 - val_mape: 9.2384 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.3273 - mse: 0.3583 - rmse: 0.5985 - mae: 0.3273 - mape: 10.0515\n",
      "Epoch 74: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3265 - mse: 0.3565 - rmse: 0.5971 - mae: 0.3265 - mape: 10.0179 - val_loss: 0.3156 - val_mse: 0.3074 - val_rmse: 0.5544 - val_mae: 0.3156 - val_mape: 9.7224 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3229 - mse: 0.3435 - rmse: 0.5861 - mae: 0.3229 - mape: 9.9833\n",
      "Epoch 75: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3228 - mse: 0.3433 - rmse: 0.5859 - mae: 0.3228 - mape: 9.9802 - val_loss: 0.3112 - val_mse: 0.3114 - val_rmse: 0.5580 - val_mae: 0.3112 - val_mape: 9.8376 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.3124 - mse: 0.3277 - rmse: 0.5725 - mae: 0.3124 - mape: 9.6159\n",
      "Epoch 76: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3121 - mse: 0.3271 - rmse: 0.5720 - mae: 0.3121 - mape: 9.6063 - val_loss: 0.3302 - val_mse: 0.3423 - val_rmse: 0.5851 - val_mae: 0.3302 - val_mape: 9.7015 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.3317 - mse: 0.3726 - rmse: 0.6104 - mae: 0.3317 - mape: 10.2480\n",
      "Epoch 77: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3311 - mse: 0.3717 - rmse: 0.6096 - mae: 0.3311 - mape: 10.2346 - val_loss: 0.3624 - val_mse: 0.4539 - val_rmse: 0.6737 - val_mae: 0.3624 - val_mape: 11.7762 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.3628 - mse: 0.4868 - rmse: 0.6977 - mae: 0.3628 - mape: 11.0890\n",
      "Epoch 78: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3628 - mse: 0.4863 - rmse: 0.6974 - mae: 0.3628 - mape: 11.0887 - val_loss: 0.3639 - val_mse: 0.4682 - val_rmse: 0.6842 - val_mae: 0.3639 - val_mape: 11.3936 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.3522 - mse: 0.4226 - rmse: 0.6501 - mae: 0.3522 - mape: 11.1666\n",
      "Epoch 79: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3523 - mse: 0.4203 - rmse: 0.6483 - mae: 0.3523 - mape: 11.1986 - val_loss: 0.3518 - val_mse: 0.4031 - val_rmse: 0.6349 - val_mae: 0.3518 - val_mape: 11.0076 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.3752 - mse: 0.4995 - rmse: 0.7068 - mae: 0.3752 - mape: 11.8927\n",
      "Epoch 80: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3756 - mse: 0.5016 - rmse: 0.7082 - mae: 0.3756 - mape: 11.8444 - val_loss: 0.3804 - val_mse: 0.5379 - val_rmse: 0.7334 - val_mae: 0.3804 - val_mape: 11.4994 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.3705 - mse: 0.4877 - rmse: 0.6984 - mae: 0.3705 - mape: 11.8050\n",
      "Epoch 81: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3754 - mse: 0.5109 - rmse: 0.7148 - mae: 0.3754 - mape: 11.7915 - val_loss: 0.4279 - val_mse: 0.8099 - val_rmse: 0.8999 - val_mae: 0.4279 - val_mape: 11.9092 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.4328 - mse: 0.7903 - rmse: 0.8890 - mae: 0.4328 - mape: 12.1937\n",
      "Epoch 82: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4315 - mse: 0.7830 - rmse: 0.8849 - mae: 0.4315 - mape: 12.1998 - val_loss: 0.4040 - val_mse: 0.6515 - val_rmse: 0.8071 - val_mae: 0.4040 - val_mape: 11.8297 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.4037 - mse: 0.6456 - rmse: 0.8035 - mae: 0.4037 - mape: 11.8259\n",
      "Epoch 83: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4040 - mse: 0.6442 - rmse: 0.8026 - mae: 0.4040 - mape: 11.8487 - val_loss: 0.4410 - val_mse: 0.6534 - val_rmse: 0.8083 - val_mae: 0.4410 - val_mape: 14.6484 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.4040 - mse: 0.5490 - rmse: 0.7410 - mae: 0.4040 - mape: 12.7009\n",
      "Epoch 84: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4035 - mse: 0.5489 - rmse: 0.7409 - mae: 0.4035 - mape: 12.6814 - val_loss: 0.3747 - val_mse: 0.5102 - val_rmse: 0.7143 - val_mae: 0.3747 - val_mape: 11.2532 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.4302 - mse: 0.7270 - rmse: 0.8527 - mae: 0.4302 - mape: 12.4186\n",
      "Epoch 85: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4304 - mse: 0.7271 - rmse: 0.8527 - mae: 0.4304 - mape: 12.4191 - val_loss: 0.4559 - val_mse: 0.7190 - val_rmse: 0.8480 - val_mae: 0.4559 - val_mape: 13.4373 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.3764 - mse: 0.4959 - rmse: 0.7042 - mae: 0.3764 - mape: 11.6634\n",
      "Epoch 86: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3763 - mse: 0.4950 - rmse: 0.7036 - mae: 0.3763 - mape: 11.6647 - val_loss: 0.3353 - val_mse: 0.3568 - val_rmse: 0.5973 - val_mae: 0.3353 - val_mape: 10.8905 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.4235 - mse: 0.5716 - rmse: 0.7560 - mae: 0.4235 - mape: 13.7328\n",
      "Epoch 87: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4266 - mse: 0.5723 - rmse: 0.7565 - mae: 0.4266 - mape: 13.8927 - val_loss: 0.4556 - val_mse: 0.6307 - val_rmse: 0.7942 - val_mae: 0.4556 - val_mape: 14.9107 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.4596 - mse: 0.6374 - rmse: 0.7984 - mae: 0.4596 - mape: 15.0796\n",
      "Epoch 88: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4592 - mse: 0.6372 - rmse: 0.7982 - mae: 0.4592 - mape: 15.0609 - val_loss: 0.4779 - val_mse: 0.6675 - val_rmse: 0.8170 - val_mae: 0.4779 - val_mape: 15.1959 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.4581 - mse: 0.6411 - rmse: 0.8007 - mae: 0.4581 - mape: 14.9409\n",
      "Epoch 89: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4589 - mse: 0.6432 - rmse: 0.8020 - mae: 0.4589 - mape: 14.9517 - val_loss: 0.4545 - val_mse: 0.6415 - val_rmse: 0.8009 - val_mae: 0.4545 - val_mape: 14.7993 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.4582 - mse: 0.6424 - rmse: 0.8015 - mae: 0.4582 - mape: 14.9284\n",
      "Epoch 90: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4584 - mse: 0.6432 - rmse: 0.8020 - mae: 0.4584 - mape: 14.9356 - val_loss: 0.4696 - val_mse: 0.6446 - val_rmse: 0.8028 - val_mae: 0.4696 - val_mape: 15.5505 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.4616 - mse: 0.6460 - rmse: 0.8037 - mae: 0.4616 - mape: 15.0205\n",
      "Epoch 91: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4608 - mse: 0.6449 - rmse: 0.8031 - mae: 0.4608 - mape: 15.0285 - val_loss: 0.4559 - val_mse: 0.6466 - val_rmse: 0.8041 - val_mae: 0.4559 - val_mape: 14.7245 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.4589 - mse: 0.6430 - rmse: 0.8019 - mae: 0.4589 - mape: 14.9603\n",
      "Epoch 92: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4593 - mse: 0.6437 - rmse: 0.8023 - mae: 0.4593 - mape: 14.9634 - val_loss: 0.4552 - val_mse: 0.6405 - val_rmse: 0.8003 - val_mae: 0.4552 - val_mape: 14.8659 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.4573 - mse: 0.6398 - rmse: 0.7999 - mae: 0.4573 - mape: 14.9264\n",
      "Epoch 93: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4590 - mse: 0.6438 - rmse: 0.8024 - mae: 0.4590 - mape: 14.9552 - val_loss: 0.4588 - val_mse: 0.6447 - val_rmse: 0.8029 - val_mae: 0.4588 - val_mape: 14.9343 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.3870 - mse: 0.5678 - rmse: 0.7535 - mae: 0.3870 - mape: 11.3599\n",
      "Epoch 94: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3835 - mse: 0.5597 - rmse: 0.7481 - mae: 0.3835 - mape: 11.2519 - val_loss: 0.3920 - val_mse: 0.5476 - val_rmse: 0.7400 - val_mae: 0.3920 - val_mape: 11.4766 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.3080 - mse: 0.3511 - rmse: 0.5926 - mae: 0.3080 - mape: 8.9184\n",
      "Epoch 95: val_loss did not improve from 0.30593\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3075 - mse: 0.3504 - rmse: 0.5919 - mae: 0.3075 - mape: 8.9030 - val_loss: 0.3085 - val_mse: 0.3516 - val_rmse: 0.5929 - val_mae: 0.3085 - val_mape: 9.0424 - lr: 1.0000e-04\n",
      "Epoch 96/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2970 - mse: 0.3330 - rmse: 0.5770 - mae: 0.2970 - mape: 8.6883\n",
      "Epoch 96: val_loss improved from 0.30593 to 0.28969, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2958 - mse: 0.3300 - rmse: 0.5745 - mae: 0.2958 - mape: 8.6673 - val_loss: 0.2897 - val_mse: 0.3185 - val_rmse: 0.5643 - val_mae: 0.2897 - val_mape: 8.5093 - lr: 1.0000e-04\n",
      "Epoch 97/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2921 - mse: 0.3231 - rmse: 0.5684 - mae: 0.2921 - mape: 8.5767\n",
      "Epoch 97: val_loss improved from 0.28969 to 0.28945, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2915 - mse: 0.3226 - rmse: 0.5680 - mae: 0.2915 - mape: 8.5665 - val_loss: 0.2894 - val_mse: 0.3222 - val_rmse: 0.5676 - val_mae: 0.2894 - val_mape: 8.4498 - lr: 1.0000e-04\n",
      "Epoch 98/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2884 - mse: 0.3208 - rmse: 0.5664 - mae: 0.2884 - mape: 8.4809\n",
      "Epoch 98: val_loss improved from 0.28945 to 0.28397, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2885 - mse: 0.3223 - rmse: 0.5677 - mae: 0.2885 - mape: 8.4933 - val_loss: 0.2840 - val_mse: 0.3159 - val_rmse: 0.5621 - val_mae: 0.2840 - val_mape: 8.3585 - lr: 1.0000e-04\n",
      "Epoch 99/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2868 - mse: 0.3217 - rmse: 0.5672 - mae: 0.2868 - mape: 8.4565\n",
      "Epoch 99: val_loss did not improve from 0.28397\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2866 - mse: 0.3200 - rmse: 0.5657 - mae: 0.2866 - mape: 8.4455 - val_loss: 0.2877 - val_mse: 0.3180 - val_rmse: 0.5639 - val_mae: 0.2877 - val_mape: 8.5849 - lr: 1.0000e-04\n",
      "Epoch 100/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2865 - mse: 0.3209 - rmse: 0.5665 - mae: 0.2865 - mape: 8.4544\n",
      "Epoch 100: val_loss did not improve from 0.28397\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2867 - mse: 0.3214 - rmse: 0.5669 - mae: 0.2867 - mape: 8.4510 - val_loss: 0.2881 - val_mse: 0.3207 - val_rmse: 0.5663 - val_mae: 0.2881 - val_mape: 8.4108 - lr: 1.0000e-04\n",
      "Epoch 101/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2858 - mse: 0.3176 - rmse: 0.5635 - mae: 0.2858 - mape: 8.4197\n",
      "Epoch 101: val_loss did not improve from 0.28397\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2859 - mse: 0.3172 - rmse: 0.5632 - mae: 0.2859 - mape: 8.4127 - val_loss: 0.2843 - val_mse: 0.3158 - val_rmse: 0.5620 - val_mae: 0.2843 - val_mape: 8.4017 - lr: 1.0000e-04\n",
      "Epoch 102/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2869 - mse: 0.3201 - rmse: 0.5658 - mae: 0.2869 - mape: 8.4471\n",
      "Epoch 102: val_loss did not improve from 0.28397\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2869 - mse: 0.3200 - rmse: 0.5657 - mae: 0.2869 - mape: 8.4468 - val_loss: 0.2850 - val_mse: 0.3140 - val_rmse: 0.5604 - val_mae: 0.2850 - val_mape: 8.3894 - lr: 1.0000e-04\n",
      "Epoch 103/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2874 - mse: 0.3217 - rmse: 0.5672 - mae: 0.2874 - mape: 8.4644\n",
      "Epoch 103: val_loss improved from 0.28397 to 0.28291, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2873 - mse: 0.3220 - rmse: 0.5674 - mae: 0.2873 - mape: 8.4560 - val_loss: 0.2829 - val_mse: 0.3132 - val_rmse: 0.5597 - val_mae: 0.2829 - val_mape: 8.3561 - lr: 1.0000e-04\n",
      "Epoch 104/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2845 - mse: 0.3152 - rmse: 0.5614 - mae: 0.2845 - mape: 8.3654\n",
      "Epoch 104: val_loss did not improve from 0.28291\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2854 - mse: 0.3173 - rmse: 0.5633 - mae: 0.2854 - mape: 8.3888 - val_loss: 0.2857 - val_mse: 0.3193 - val_rmse: 0.5651 - val_mae: 0.2857 - val_mape: 8.4638 - lr: 1.0000e-04\n",
      "Epoch 105/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2869 - mse: 0.3179 - rmse: 0.5639 - mae: 0.2869 - mape: 8.4579\n",
      "Epoch 105: val_loss improved from 0.28291 to 0.28278, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2860 - mse: 0.3165 - rmse: 0.5626 - mae: 0.2860 - mape: 8.4418 - val_loss: 0.2828 - val_mse: 0.3127 - val_rmse: 0.5592 - val_mae: 0.2828 - val_mape: 8.3326 - lr: 1.0000e-04\n",
      "Epoch 106/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2856 - mse: 0.3132 - rmse: 0.5596 - mae: 0.2856 - mape: 8.4102\n",
      "Epoch 106: val_loss did not improve from 0.28278\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2854 - mse: 0.3154 - rmse: 0.5616 - mae: 0.2854 - mape: 8.4093 - val_loss: 0.2830 - val_mse: 0.3167 - val_rmse: 0.5628 - val_mae: 0.2830 - val_mape: 8.2918 - lr: 1.0000e-04\n",
      "Epoch 107/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2860 - mse: 0.3201 - rmse: 0.5658 - mae: 0.2860 - mape: 8.4143\n",
      "Epoch 107: val_loss improved from 0.28278 to 0.28276, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2868 - mse: 0.3217 - rmse: 0.5672 - mae: 0.2868 - mape: 8.4359 - val_loss: 0.2828 - val_mse: 0.3142 - val_rmse: 0.5605 - val_mae: 0.2828 - val_mape: 8.2910 - lr: 1.0000e-04\n",
      "Epoch 108/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2859 - mse: 0.3149 - rmse: 0.5611 - mae: 0.2859 - mape: 8.4355\n",
      "Epoch 108: val_loss did not improve from 0.28276\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2856 - mse: 0.3164 - rmse: 0.5625 - mae: 0.2856 - mape: 8.4319 - val_loss: 0.2834 - val_mse: 0.3149 - val_rmse: 0.5611 - val_mae: 0.2834 - val_mape: 8.3083 - lr: 1.0000e-04\n",
      "Epoch 109/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2854 - mse: 0.3161 - rmse: 0.5623 - mae: 0.2854 - mape: 8.4216\n",
      "Epoch 109: val_loss did not improve from 0.28276\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2854 - mse: 0.3173 - rmse: 0.5633 - mae: 0.2854 - mape: 8.4071 - val_loss: 0.2834 - val_mse: 0.3114 - val_rmse: 0.5580 - val_mae: 0.2834 - val_mape: 8.3559 - lr: 1.0000e-04\n",
      "Epoch 110/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2850 - mse: 0.3172 - rmse: 0.5632 - mae: 0.2850 - mape: 8.4112\n",
      "Epoch 110: val_loss did not improve from 0.28276\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2859 - mse: 0.3182 - rmse: 0.5641 - mae: 0.2859 - mape: 8.4285 - val_loss: 0.2828 - val_mse: 0.3133 - val_rmse: 0.5598 - val_mae: 0.2828 - val_mape: 8.3070 - lr: 1.0000e-04\n",
      "Epoch 111/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2856 - mse: 0.3158 - rmse: 0.5620 - mae: 0.2856 - mape: 8.4106\n",
      "Epoch 111: val_loss did not improve from 0.28276\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2862 - mse: 0.3175 - rmse: 0.5634 - mae: 0.2862 - mape: 8.4372 - val_loss: 0.2846 - val_mse: 0.3154 - val_rmse: 0.5616 - val_mae: 0.2846 - val_mape: 8.3418 - lr: 1.0000e-04\n",
      "Epoch 112/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2842 - mse: 0.3146 - rmse: 0.5609 - mae: 0.2842 - mape: 8.3559\n",
      "Epoch 112: val_loss did not improve from 0.28276\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2853 - mse: 0.3161 - rmse: 0.5622 - mae: 0.2853 - mape: 8.4041 - val_loss: 0.2828 - val_mse: 0.3145 - val_rmse: 0.5608 - val_mae: 0.2828 - val_mape: 8.3244 - lr: 1.0000e-04\n",
      "Epoch 113/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2867 - mse: 0.3196 - rmse: 0.5653 - mae: 0.2867 - mape: 8.4348\n",
      "Epoch 113: val_loss did not improve from 0.28276\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2857 - mse: 0.3165 - rmse: 0.5626 - mae: 0.2857 - mape: 8.4187 - val_loss: 0.2842 - val_mse: 0.3131 - val_rmse: 0.5595 - val_mae: 0.2842 - val_mape: 8.4369 - lr: 1.0000e-04\n",
      "Epoch 114/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2840 - mse: 0.3139 - rmse: 0.5603 - mae: 0.2840 - mape: 8.3887\n",
      "Epoch 114: val_loss did not improve from 0.28276\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2842 - mse: 0.3144 - rmse: 0.5607 - mae: 0.2842 - mape: 8.3975 - val_loss: 0.2839 - val_mse: 0.3152 - val_rmse: 0.5614 - val_mae: 0.2839 - val_mape: 8.3542 - lr: 1.0000e-04\n",
      "Epoch 115/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2867 - mse: 0.3200 - rmse: 0.5657 - mae: 0.2867 - mape: 8.4526\n",
      "Epoch 115: val_loss did not improve from 0.28276\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2874 - mse: 0.3207 - rmse: 0.5663 - mae: 0.2874 - mape: 8.4605 - val_loss: 0.2841 - val_mse: 0.3150 - val_rmse: 0.5612 - val_mae: 0.2841 - val_mape: 8.4133 - lr: 1.0000e-04\n",
      "Epoch 116/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2843 - mse: 0.3129 - rmse: 0.5594 - mae: 0.2843 - mape: 8.3889\n",
      "Epoch 116: val_loss did not improve from 0.28276\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2844 - mse: 0.3131 - rmse: 0.5595 - mae: 0.2844 - mape: 8.3964 - val_loss: 0.2891 - val_mse: 0.3253 - val_rmse: 0.5704 - val_mae: 0.2891 - val_mape: 8.3929 - lr: 1.0000e-04\n",
      "Epoch 117/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2815 - mse: 0.2993 - rmse: 0.5471 - mae: 0.2815 - mape: 8.3650\n",
      "Epoch 117: val_loss did not improve from 0.28276\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2811 - mse: 0.2978 - rmse: 0.5457 - mae: 0.2811 - mape: 8.3851 - val_loss: 0.2836 - val_mse: 0.2862 - val_rmse: 0.5349 - val_mae: 0.2836 - val_mape: 8.4742 - lr: 1.0000e-04\n",
      "Epoch 118/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2762 - mse: 0.2699 - rmse: 0.5195 - mae: 0.2762 - mape: 8.4493\n",
      "Epoch 118: val_loss improved from 0.28276 to 0.27296, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2762 - mse: 0.2696 - rmse: 0.5192 - mae: 0.2762 - mape: 8.4503 - val_loss: 0.2730 - val_mse: 0.2634 - val_rmse: 0.5132 - val_mae: 0.2730 - val_mape: 8.3707 - lr: 1.0000e-04\n",
      "Epoch 119/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2755 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2755 - mape: 8.4542\n",
      "Epoch 119: val_loss did not improve from 0.27296\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2755 - mse: 0.2674 - rmse: 0.5172 - mae: 0.2755 - mape: 8.4528 - val_loss: 0.2739 - val_mse: 0.2660 - val_rmse: 0.5158 - val_mae: 0.2739 - val_mape: 8.3439 - lr: 1.0000e-04\n",
      "Epoch 120/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2743 - mse: 0.2664 - rmse: 0.5161 - mae: 0.2743 - mape: 8.4239\n",
      "Epoch 120: val_loss did not improve from 0.27296\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2743 - mse: 0.2657 - rmse: 0.5155 - mae: 0.2743 - mape: 8.4228 - val_loss: 0.2805 - val_mse: 0.2766 - val_rmse: 0.5260 - val_mae: 0.2805 - val_mape: 8.6528 - lr: 1.0000e-04\n",
      "Epoch 121/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2735 - mse: 0.2632 - rmse: 0.5130 - mae: 0.2735 - mape: 8.3974\n",
      "Epoch 121: val_loss improved from 0.27296 to 0.27287, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2748 - mse: 0.2668 - rmse: 0.5166 - mae: 0.2748 - mape: 8.4394 - val_loss: 0.2729 - val_mse: 0.2635 - val_rmse: 0.5133 - val_mae: 0.2729 - val_mape: 8.4189 - lr: 1.0000e-04\n",
      "Epoch 122/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2752 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2752 - mape: 8.4646\n",
      "Epoch 122: val_loss improved from 0.27287 to 0.27217, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2749 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2749 - mape: 8.4461 - val_loss: 0.2722 - val_mse: 0.2626 - val_rmse: 0.5124 - val_mae: 0.2722 - val_mape: 8.4016 - lr: 1.0000e-04\n",
      "Epoch 123/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2755 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2755 - mape: 8.4722\n",
      "Epoch 123: val_loss improved from 0.27217 to 0.27167, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2755 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2755 - mape: 8.4722 - val_loss: 0.2717 - val_mse: 0.2626 - val_rmse: 0.5125 - val_mae: 0.2717 - val_mape: 8.3471 - lr: 1.0000e-04\n",
      "Epoch 124/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2750 - mse: 0.2698 - rmse: 0.5194 - mae: 0.2750 - mape: 8.4672\n",
      "Epoch 124: val_loss improved from 0.27167 to 0.27123, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2749 - mse: 0.2696 - rmse: 0.5193 - mae: 0.2749 - mape: 8.4601 - val_loss: 0.2712 - val_mse: 0.2626 - val_rmse: 0.5124 - val_mae: 0.2712 - val_mape: 8.3292 - lr: 1.0000e-04\n",
      "Epoch 125/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2733 - mse: 0.2659 - rmse: 0.5156 - mae: 0.2733 - mape: 8.3601\n",
      "Epoch 125: val_loss did not improve from 0.27123\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2737 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2737 - mape: 8.4041 - val_loss: 0.2774 - val_mse: 0.2703 - val_rmse: 0.5199 - val_mae: 0.2774 - val_mape: 8.6623 - lr: 1.0000e-04\n",
      "Epoch 126/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2740 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2740 - mape: 8.4349\n",
      "Epoch 126: val_loss improved from 0.27123 to 0.27078, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2732 - mse: 0.2657 - rmse: 0.5155 - mae: 0.2732 - mape: 8.4124 - val_loss: 0.2708 - val_mse: 0.2611 - val_rmse: 0.5109 - val_mae: 0.2708 - val_mape: 8.3228 - lr: 1.0000e-04\n",
      "Epoch 127/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2738 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2738 - mape: 8.4731\n",
      "Epoch 127: val_loss improved from 0.27078 to 0.27063, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2741 - mse: 0.2652 - rmse: 0.5150 - mae: 0.2741 - mape: 8.4959 - val_loss: 0.2706 - val_mse: 0.2575 - val_rmse: 0.5075 - val_mae: 0.2706 - val_mape: 8.4903 - lr: 1.0000e-04\n",
      "Epoch 128/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2725 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2725 - mape: 8.5179\n",
      "Epoch 128: val_loss did not improve from 0.27063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2726 - mse: 0.2623 - rmse: 0.5122 - mae: 0.2726 - mape: 8.5191 - val_loss: 0.2731 - val_mse: 0.2648 - val_rmse: 0.5146 - val_mae: 0.2731 - val_mape: 8.4342 - lr: 1.0000e-04\n",
      "Epoch 129/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2742 - mse: 0.2661 - rmse: 0.5159 - mae: 0.2742 - mape: 8.5720\n",
      "Epoch 129: val_loss did not improve from 0.27063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2739 - mse: 0.2657 - rmse: 0.5154 - mae: 0.2739 - mape: 8.5736 - val_loss: 0.2711 - val_mse: 0.2624 - val_rmse: 0.5123 - val_mae: 0.2711 - val_mape: 8.4010 - lr: 1.0000e-04\n",
      "Epoch 130/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2715 - mse: 0.2616 - rmse: 0.5114 - mae: 0.2715 - mape: 8.4848\n",
      "Epoch 130: val_loss improved from 0.27063 to 0.26933, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2715 - mse: 0.2616 - rmse: 0.5114 - mae: 0.2715 - mape: 8.4848 - val_loss: 0.2693 - val_mse: 0.2550 - val_rmse: 0.5049 - val_mae: 0.2693 - val_mape: 8.4517 - lr: 1.0000e-04\n",
      "Epoch 131/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2739 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2739 - mape: 8.5628\n",
      "Epoch 131: val_loss improved from 0.26933 to 0.26873, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2720 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2720 - mape: 8.5125 - val_loss: 0.2687 - val_mse: 0.2566 - val_rmse: 0.5066 - val_mae: 0.2687 - val_mape: 8.4188 - lr: 1.0000e-04\n",
      "Epoch 132/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2706 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2706 - mape: 8.4842\n",
      "Epoch 132: val_loss did not improve from 0.26873\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2707 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2707 - mape: 8.4858 - val_loss: 0.2716 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2716 - val_mape: 8.6188 - lr: 1.0000e-04\n",
      "Epoch 133/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2740 - mse: 0.2658 - rmse: 0.5155 - mae: 0.2740 - mape: 8.5795\n",
      "Epoch 133: val_loss did not improve from 0.26873\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2741 - mse: 0.2655 - rmse: 0.5153 - mae: 0.2741 - mape: 8.5951 - val_loss: 0.2693 - val_mse: 0.2573 - val_rmse: 0.5072 - val_mae: 0.2693 - val_mape: 8.4220 - lr: 1.0000e-04\n",
      "Epoch 134/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2722 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2722 - mape: 8.5274\n",
      "Epoch 134: val_loss did not improve from 0.26873\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2718 - mse: 0.2617 - rmse: 0.5116 - mae: 0.2718 - mape: 8.5184 - val_loss: 0.2697 - val_mse: 0.2558 - val_rmse: 0.5058 - val_mae: 0.2697 - val_mape: 8.4814 - lr: 1.0000e-04\n",
      "Epoch 135/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2721 - mse: 0.2616 - rmse: 0.5114 - mae: 0.2721 - mape: 8.5129\n",
      "Epoch 135: val_loss improved from 0.26873 to 0.26850, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2720 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2720 - mape: 8.5118 - val_loss: 0.2685 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2685 - val_mape: 8.4036 - lr: 1.0000e-04\n",
      "Epoch 136/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2715 - mse: 0.2622 - rmse: 0.5121 - mae: 0.2715 - mape: 8.5070\n",
      "Epoch 136: val_loss did not improve from 0.26850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2715 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2715 - mape: 8.5008 - val_loss: 0.2746 - val_mse: 0.2666 - val_rmse: 0.5163 - val_mae: 0.2746 - val_mape: 8.6783 - lr: 1.0000e-04\n",
      "Epoch 137/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2735 - mse: 0.2638 - rmse: 0.5137 - mae: 0.2735 - mape: 8.5822\n",
      "Epoch 137: val_loss did not improve from 0.26850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2733 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2733 - mape: 8.5809 - val_loss: 0.2687 - val_mse: 0.2568 - val_rmse: 0.5068 - val_mae: 0.2687 - val_mape: 8.3730 - lr: 1.0000e-04\n",
      "Epoch 138/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2705 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2705 - mape: 8.4939\n",
      "Epoch 138: val_loss did not improve from 0.26850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2714 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2714 - mape: 8.5184 - val_loss: 0.2698 - val_mse: 0.2577 - val_rmse: 0.5077 - val_mae: 0.2698 - val_mape: 8.4962 - lr: 1.0000e-04\n",
      "Epoch 139/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2731 - mse: 0.2655 - rmse: 0.5153 - mae: 0.2731 - mape: 8.5624\n",
      "Epoch 139: val_loss did not improve from 0.26850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2728 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2728 - mape: 8.5540 - val_loss: 0.2697 - val_mse: 0.2557 - val_rmse: 0.5057 - val_mae: 0.2697 - val_mape: 8.5380 - lr: 1.0000e-04\n",
      "Epoch 140/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2716 - mse: 0.2626 - rmse: 0.5124 - mae: 0.2716 - mape: 8.5549\n",
      "Epoch 140: val_loss did not improve from 0.26850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2712 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2712 - mape: 8.5216 - val_loss: 0.2734 - val_mse: 0.2619 - val_rmse: 0.5118 - val_mae: 0.2734 - val_mape: 8.7520 - lr: 1.0000e-04\n",
      "Epoch 141/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2711 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2711 - mape: 8.5204\n",
      "Epoch 141: val_loss did not improve from 0.26850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2708 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2708 - mape: 8.5272 - val_loss: 0.2726 - val_mse: 0.2651 - val_rmse: 0.5149 - val_mae: 0.2726 - val_mape: 8.5335 - lr: 1.0000e-04\n",
      "Epoch 142/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2715 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2715 - mape: 8.5442\n",
      "Epoch 142: val_loss did not improve from 0.26850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2709 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2709 - mape: 8.5366 - val_loss: 0.2704 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2704 - val_mape: 8.5590 - lr: 1.0000e-04\n",
      "Epoch 143/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2723 - mse: 0.2630 - rmse: 0.5129 - mae: 0.2723 - mape: 8.5850\n",
      "Epoch 143: val_loss improved from 0.26850 to 0.26696, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2708 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2708 - mape: 8.5376 - val_loss: 0.2670 - val_mse: 0.2553 - val_rmse: 0.5053 - val_mae: 0.2670 - val_mape: 8.4702 - lr: 1.0000e-04\n",
      "Epoch 144/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2701 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2701 - mape: 8.5689\n",
      "Epoch 144: val_loss did not improve from 0.26696\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2720 - mse: 0.2661 - rmse: 0.5159 - mae: 0.2720 - mape: 8.6469 - val_loss: 0.2715 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2715 - val_mape: 8.4509 - lr: 1.0000e-04\n",
      "Epoch 145/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2684 - mse: 0.2624 - rmse: 0.5123 - mae: 0.2684 - mape: 8.5674\n",
      "Epoch 145: val_loss improved from 0.26696 to 0.26574, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2685 - mse: 0.2627 - rmse: 0.5125 - mae: 0.2685 - mape: 8.5777 - val_loss: 0.2657 - val_mse: 0.2611 - val_rmse: 0.5109 - val_mae: 0.2657 - val_mape: 8.5314 - lr: 1.0000e-04\n",
      "Epoch 146/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2695 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2695 - mape: 8.5987\n",
      "Epoch 146: val_loss did not improve from 0.26574\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2688 - mse: 0.2628 - rmse: 0.5127 - mae: 0.2688 - mape: 8.5914 - val_loss: 0.2678 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2678 - val_mape: 8.6006 - lr: 1.0000e-04\n",
      "Epoch 147/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2687 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2687 - mape: 8.5911\n",
      "Epoch 147: val_loss did not improve from 0.26574\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2691 - mse: 0.2637 - rmse: 0.5136 - mae: 0.2691 - mape: 8.6022 - val_loss: 0.2712 - val_mse: 0.2670 - val_rmse: 0.5168 - val_mae: 0.2712 - val_mape: 8.7256 - lr: 1.0000e-04\n",
      "Epoch 148/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2675 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2675 - mape: 8.5521\n",
      "Epoch 148: val_loss did not improve from 0.26574\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2681 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2681 - mape: 8.5707 - val_loss: 0.2691 - val_mse: 0.2628 - val_rmse: 0.5126 - val_mae: 0.2691 - val_mape: 8.4464 - lr: 1.0000e-04\n",
      "Epoch 149/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2690 - mse: 0.2659 - rmse: 0.5157 - mae: 0.2690 - mape: 8.5923\n",
      "Epoch 149: val_loss did not improve from 0.26574\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2687 - mse: 0.2655 - rmse: 0.5152 - mae: 0.2687 - mape: 8.5848 - val_loss: 0.2669 - val_mse: 0.2742 - val_rmse: 0.5236 - val_mae: 0.2669 - val_mape: 8.6287 - lr: 1.0000e-04\n",
      "Epoch 150/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2683 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2683 - mape: 8.5916\n",
      "Epoch 150: val_loss did not improve from 0.26574\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2685 - mse: 0.2666 - rmse: 0.5163 - mae: 0.2685 - mape: 8.5907 - val_loss: 0.2672 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2672 - val_mape: 8.5700 - lr: 1.0000e-04\n",
      "Epoch 151/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2714 - mse: 0.2685 - rmse: 0.5182 - mae: 0.2714 - mape: 8.6695\n",
      "Epoch 151: val_loss improved from 0.26574 to 0.26563, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2697 - mse: 0.2653 - rmse: 0.5150 - mae: 0.2697 - mape: 8.6188 - val_loss: 0.2656 - val_mse: 0.2570 - val_rmse: 0.5070 - val_mae: 0.2656 - val_mape: 8.5106 - lr: 1.0000e-04\n",
      "Epoch 152/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2673 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2673 - mape: 8.5311\n",
      "Epoch 152: val_loss did not improve from 0.26563\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2676 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2676 - mape: 8.5485 - val_loss: 0.2666 - val_mse: 0.2637 - val_rmse: 0.5135 - val_mae: 0.2666 - val_mape: 8.5296 - lr: 1.0000e-04\n",
      "Epoch 153/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2670 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2670 - mape: 8.5474\n",
      "Epoch 153: val_loss did not improve from 0.26563\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2673 - mse: 0.2617 - rmse: 0.5116 - mae: 0.2673 - mape: 8.5424 - val_loss: 0.2783 - val_mse: 0.2874 - val_rmse: 0.5361 - val_mae: 0.2783 - val_mape: 9.0476 - lr: 1.0000e-04\n",
      "Epoch 154/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2696 - mse: 0.2659 - rmse: 0.5157 - mae: 0.2696 - mape: 8.6256\n",
      "Epoch 154: val_loss did not improve from 0.26563\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2696 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2696 - mape: 8.6178 - val_loss: 0.2754 - val_mse: 0.2783 - val_rmse: 0.5275 - val_mae: 0.2754 - val_mape: 8.6820 - lr: 1.0000e-04\n",
      "Epoch 155/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2707 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2707 - mape: 8.6257\n",
      "Epoch 155: val_loss did not improve from 0.26563\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2709 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2709 - mape: 8.6320 - val_loss: 0.2671 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2671 - val_mape: 8.6191 - lr: 1.0000e-04\n",
      "Epoch 156/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2697 - mse: 0.2656 - rmse: 0.5153 - mae: 0.2697 - mape: 8.5775\n",
      "Epoch 156: val_loss improved from 0.26563 to 0.26516, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2687 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2687 - mape: 8.5638 - val_loss: 0.2652 - val_mse: 0.2544 - val_rmse: 0.5043 - val_mae: 0.2652 - val_mape: 8.4630 - lr: 1.0000e-04\n",
      "Epoch 157/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2682 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2682 - mape: 8.5570\n",
      "Epoch 157: val_loss did not improve from 0.26516\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2680 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2680 - mape: 8.5611 - val_loss: 0.2664 - val_mse: 0.2635 - val_rmse: 0.5133 - val_mae: 0.2664 - val_mape: 8.4442 - lr: 1.0000e-04\n",
      "Epoch 158/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2665 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2665 - mape: 8.5161\n",
      "Epoch 158: val_loss did not improve from 0.26516\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2671 - mse: 0.2614 - rmse: 0.5112 - mae: 0.2671 - mape: 8.5155 - val_loss: 0.2687 - val_mse: 0.2678 - val_rmse: 0.5175 - val_mae: 0.2687 - val_mape: 8.7271 - lr: 1.0000e-04\n",
      "Epoch 159/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2663 - mse: 0.2619 - rmse: 0.5117 - mae: 0.2663 - mape: 8.4968\n",
      "Epoch 159: val_loss improved from 0.26516 to 0.26448, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2670 - mse: 0.2628 - rmse: 0.5127 - mae: 0.2670 - mape: 8.5351 - val_loss: 0.2645 - val_mse: 0.2537 - val_rmse: 0.5037 - val_mae: 0.2645 - val_mape: 8.3389 - lr: 1.0000e-04\n",
      "Epoch 160/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2679 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2679 - mape: 8.5480\n",
      "Epoch 160: val_loss did not improve from 0.26448\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2677 - mse: 0.2626 - rmse: 0.5124 - mae: 0.2677 - mape: 8.5424 - val_loss: 0.2678 - val_mse: 0.2525 - val_rmse: 0.5025 - val_mae: 0.2678 - val_mape: 8.3910 - lr: 1.0000e-04\n",
      "Epoch 161/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2680 - mse: 0.2607 - rmse: 0.5105 - mae: 0.2680 - mape: 8.5158\n",
      "Epoch 161: val_loss did not improve from 0.26448\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2668 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2668 - mape: 8.4849 - val_loss: 0.2661 - val_mse: 0.2670 - val_rmse: 0.5167 - val_mae: 0.2661 - val_mape: 8.6326 - lr: 1.0000e-04\n",
      "Epoch 162/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2671 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2671 - mape: 8.5454\n",
      "Epoch 162: val_loss improved from 0.26448 to 0.26377, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2671 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2671 - mape: 8.5454 - val_loss: 0.2638 - val_mse: 0.2540 - val_rmse: 0.5040 - val_mae: 0.2638 - val_mape: 8.4288 - lr: 1.0000e-04\n",
      "Epoch 163/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2671 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2671 - mape: 8.5274\n",
      "Epoch 163: val_loss did not improve from 0.26377\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2674 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2674 - mape: 8.5334 - val_loss: 0.2644 - val_mse: 0.2563 - val_rmse: 0.5063 - val_mae: 0.2644 - val_mape: 8.5266 - lr: 1.0000e-04\n",
      "Epoch 164/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2657 - mse: 0.2561 - rmse: 0.5061 - mae: 0.2657 - mape: 8.4745\n",
      "Epoch 164: val_loss did not improve from 0.26377\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2661 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2661 - mape: 8.4806 - val_loss: 0.2646 - val_mse: 0.2636 - val_rmse: 0.5134 - val_mae: 0.2646 - val_mape: 8.5490 - lr: 1.0000e-04\n",
      "Epoch 165/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2679 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2679 - mape: 8.5370\n",
      "Epoch 165: val_loss did not improve from 0.26377\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2674 - mse: 0.2632 - rmse: 0.5131 - mae: 0.2674 - mape: 8.5255 - val_loss: 0.2676 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2676 - val_mape: 8.6047 - lr: 1.0000e-04\n",
      "Epoch 166/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2667 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2667 - mape: 8.5078\n",
      "Epoch 166: val_loss improved from 0.26377 to 0.26365, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2668 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2668 - mape: 8.5208 - val_loss: 0.2637 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2637 - val_mape: 8.4671 - lr: 1.0000e-04\n",
      "Epoch 167/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2680 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2680 - mape: 8.5475\n",
      "Epoch 167: val_loss did not improve from 0.26365\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2679 - mse: 0.2620 - rmse: 0.5119 - mae: 0.2679 - mape: 8.5448 - val_loss: 0.2846 - val_mse: 0.2866 - val_rmse: 0.5354 - val_mae: 0.2846 - val_mape: 8.8954 - lr: 1.0000e-04\n",
      "Epoch 168/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2668 - mse: 0.2615 - rmse: 0.5113 - mae: 0.2668 - mape: 8.4972\n",
      "Epoch 168: val_loss did not improve from 0.26365\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2670 - mse: 0.2617 - rmse: 0.5115 - mae: 0.2670 - mape: 8.5059 - val_loss: 0.2642 - val_mse: 0.2575 - val_rmse: 0.5075 - val_mae: 0.2642 - val_mape: 8.4699 - lr: 1.0000e-04\n",
      "Epoch 169/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2670 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2670 - mape: 8.5018\n",
      "Epoch 169: val_loss did not improve from 0.26365\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2667 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2667 - mape: 8.5026 - val_loss: 0.2649 - val_mse: 0.2595 - val_rmse: 0.5095 - val_mae: 0.2649 - val_mape: 8.5198 - lr: 1.0000e-04\n",
      "Epoch 170/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2649 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2649 - mape: 8.4392\n",
      "Epoch 170: val_loss improved from 0.26365 to 0.26349, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2657 - mse: 0.2589 - rmse: 0.5089 - mae: 0.2657 - mape: 8.4661 - val_loss: 0.2635 - val_mse: 0.2584 - val_rmse: 0.5084 - val_mae: 0.2635 - val_mape: 8.3523 - lr: 1.0000e-04\n",
      "Epoch 171/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2674 - mse: 0.2660 - rmse: 0.5158 - mae: 0.2674 - mape: 8.5245\n",
      "Epoch 171: val_loss did not improve from 0.26349\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2667 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2667 - mape: 8.4850 - val_loss: 0.2640 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2640 - val_mape: 8.3997 - lr: 1.0000e-04\n",
      "Epoch 172/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2655 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2655 - mape: 8.4608\n",
      "Epoch 172: val_loss did not improve from 0.26349\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2654 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2654 - mape: 8.4565 - val_loss: 0.2712 - val_mse: 0.2706 - val_rmse: 0.5202 - val_mae: 0.2712 - val_mape: 8.8153 - lr: 1.0000e-04\n",
      "Epoch 173/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2669 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2669 - mape: 8.5132\n",
      "Epoch 173: val_loss did not improve from 0.26349\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2669 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2669 - mape: 8.5132 - val_loss: 0.2673 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2673 - val_mape: 8.3624 - lr: 1.0000e-04\n",
      "Epoch 174/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2661 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2661 - mape: 8.4803\n",
      "Epoch 174: val_loss did not improve from 0.26349\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2657 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2657 - mape: 8.4654 - val_loss: 0.2659 - val_mse: 0.2630 - val_rmse: 0.5128 - val_mae: 0.2659 - val_mape: 8.6055 - lr: 1.0000e-04\n",
      "Epoch 175/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2653 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2653 - mape: 8.4491\n",
      "Epoch 175: val_loss did not improve from 0.26349\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2655 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2655 - mape: 8.4557 - val_loss: 0.2679 - val_mse: 0.2664 - val_rmse: 0.5162 - val_mae: 0.2679 - val_mape: 8.7365 - lr: 1.0000e-04\n",
      "Epoch 176/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2673 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2673 - mape: 8.5241\n",
      "Epoch 176: val_loss improved from 0.26349 to 0.26284, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2673 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2673 - mape: 8.5058 - val_loss: 0.2628 - val_mse: 0.2501 - val_rmse: 0.5001 - val_mae: 0.2628 - val_mape: 8.3070 - lr: 1.0000e-04\n",
      "Epoch 177/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2649 - mse: 0.2545 - rmse: 0.5045 - mae: 0.2649 - mape: 8.4352\n",
      "Epoch 177: val_loss improved from 0.26284 to 0.26247, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2661 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2661 - mape: 8.4585 - val_loss: 0.2625 - val_mse: 0.2537 - val_rmse: 0.5037 - val_mae: 0.2625 - val_mape: 8.3353 - lr: 1.0000e-04\n",
      "Epoch 178/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2685 - mse: 0.2643 - rmse: 0.5141 - mae: 0.2685 - mape: 8.5503\n",
      "Epoch 178: val_loss did not improve from 0.26247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2683 - mse: 0.2639 - rmse: 0.5137 - mae: 0.2683 - mape: 8.5441 - val_loss: 0.2667 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2667 - val_mape: 8.5582 - lr: 1.0000e-04\n",
      "Epoch 179/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2674 - mse: 0.2669 - rmse: 0.5167 - mae: 0.2674 - mape: 8.5283\n",
      "Epoch 179: val_loss did not improve from 0.26247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2674 - mse: 0.2653 - rmse: 0.5151 - mae: 0.2674 - mape: 8.5142 - val_loss: 0.2648 - val_mse: 0.2505 - val_rmse: 0.5005 - val_mae: 0.2648 - val_mape: 8.3737 - lr: 1.0000e-04\n",
      "Epoch 180/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2674 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2674 - mape: 8.5100\n",
      "Epoch 180: val_loss did not improve from 0.26247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2668 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2668 - mape: 8.4781 - val_loss: 0.2634 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2634 - val_mape: 8.3981 - lr: 1.0000e-04\n",
      "Epoch 181/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2671 - mse: 0.2617 - rmse: 0.5115 - mae: 0.2671 - mape: 8.5320\n",
      "Epoch 181: val_loss did not improve from 0.26247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2675 - mse: 0.2620 - rmse: 0.5119 - mae: 0.2675 - mape: 8.5306 - val_loss: 0.2634 - val_mse: 0.2584 - val_rmse: 0.5084 - val_mae: 0.2634 - val_mape: 8.3529 - lr: 1.0000e-04\n",
      "Epoch 182/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2650 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2650 - mape: 8.4526\n",
      "Epoch 182: val_loss did not improve from 0.26247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2651 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2651 - mape: 8.4498 - val_loss: 0.2663 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2663 - val_mape: 8.5481 - lr: 1.0000e-04\n",
      "Epoch 183/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2647 - mse: 0.2562 - rmse: 0.5062 - mae: 0.2647 - mape: 8.4361\n",
      "Epoch 183: val_loss did not improve from 0.26247\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2649 - mse: 0.2567 - rmse: 0.5066 - mae: 0.2649 - mape: 8.4421 - val_loss: 0.2625 - val_mse: 0.2518 - val_rmse: 0.5018 - val_mae: 0.2625 - val_mape: 8.3018 - lr: 1.0000e-04\n",
      "Epoch 184/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2652 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2652 - mape: 8.4567\n",
      "Epoch 184: val_loss improved from 0.26247 to 0.26238, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2658 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2658 - mape: 8.4620 - val_loss: 0.2624 - val_mse: 0.2502 - val_rmse: 0.5002 - val_mae: 0.2624 - val_mape: 8.3569 - lr: 1.0000e-04\n",
      "Epoch 185/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2653 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2653 - mape: 8.4481\n",
      "Epoch 185: val_loss did not improve from 0.26238\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2654 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2654 - mape: 8.4482 - val_loss: 0.2642 - val_mse: 0.2498 - val_rmse: 0.4998 - val_mae: 0.2642 - val_mape: 8.2974 - lr: 1.0000e-04\n",
      "Epoch 186/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2653 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2653 - mape: 8.4353\n",
      "Epoch 186: val_loss did not improve from 0.26238\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2651 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2651 - mape: 8.4284 - val_loss: 0.2626 - val_mse: 0.2555 - val_rmse: 0.5055 - val_mae: 0.2626 - val_mape: 8.3987 - lr: 1.0000e-04\n",
      "Epoch 187/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2659 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2659 - mape: 8.4526\n",
      "Epoch 187: val_loss did not improve from 0.26238\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2665 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2665 - mape: 8.4646 - val_loss: 0.2649 - val_mse: 0.2625 - val_rmse: 0.5124 - val_mae: 0.2649 - val_mape: 8.5673 - lr: 1.0000e-04\n",
      "Epoch 188/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2659 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2659 - mape: 8.4809\n",
      "Epoch 188: val_loss did not improve from 0.26238\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2657 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2657 - mape: 8.4768 - val_loss: 0.2649 - val_mse: 0.2649 - val_rmse: 0.5147 - val_mae: 0.2649 - val_mape: 8.5953 - lr: 1.0000e-04\n",
      "Epoch 189/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2633 - mse: 0.2553 - rmse: 0.5053 - mae: 0.2633 - mape: 8.4029\n",
      "Epoch 189: val_loss did not improve from 0.26238\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2641 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2641 - mape: 8.4309 - val_loss: 0.2628 - val_mse: 0.2497 - val_rmse: 0.4997 - val_mae: 0.2628 - val_mape: 8.2724 - lr: 1.0000e-04\n",
      "Epoch 190/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2631 - mse: 0.2531 - rmse: 0.5031 - mae: 0.2631 - mape: 8.3928\n",
      "Epoch 190: val_loss did not improve from 0.26238\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2648 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2648 - mape: 8.4363 - val_loss: 0.2632 - val_mse: 0.2568 - val_rmse: 0.5067 - val_mae: 0.2632 - val_mape: 8.4660 - lr: 1.0000e-04\n",
      "Epoch 191/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2657 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2657 - mape: 8.4942\n",
      "Epoch 191: val_loss did not improve from 0.26238\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2663 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2663 - mape: 8.4759 - val_loss: 0.2644 - val_mse: 0.2633 - val_rmse: 0.5132 - val_mae: 0.2644 - val_mape: 8.5924 - lr: 1.0000e-04\n",
      "Epoch 192/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2656 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2656 - mape: 8.4618\n",
      "Epoch 192: val_loss did not improve from 0.26238\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2656 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2656 - mape: 8.4469 - val_loss: 0.2656 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2656 - val_mape: 8.5531 - lr: 1.0000e-04\n",
      "Epoch 193/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2656 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2656 - mape: 8.4710\n",
      "Epoch 193: val_loss improved from 0.26238 to 0.26191, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2657 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2657 - mape: 8.4806 - val_loss: 0.2619 - val_mse: 0.2545 - val_rmse: 0.5045 - val_mae: 0.2619 - val_mape: 8.3530 - lr: 1.0000e-04\n",
      "Epoch 194/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2648 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2648 - mape: 8.4349\n",
      "Epoch 194: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2648 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2648 - mape: 8.4343 - val_loss: 0.2686 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2686 - val_mape: 8.4305 - lr: 1.0000e-04\n",
      "Epoch 195/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2670 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2670 - mape: 8.4722\n",
      "Epoch 195: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2670 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2670 - mape: 8.4722 - val_loss: 0.2653 - val_mse: 0.2680 - val_rmse: 0.5177 - val_mae: 0.2653 - val_mape: 8.6406 - lr: 1.0000e-04\n",
      "Epoch 196/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2661 - mse: 0.2624 - rmse: 0.5122 - mae: 0.2661 - mape: 8.4932\n",
      "Epoch 196: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2652 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2652 - mape: 8.4567 - val_loss: 0.2642 - val_mse: 0.2500 - val_rmse: 0.5000 - val_mae: 0.2642 - val_mape: 8.4181 - lr: 1.0000e-04\n",
      "Epoch 197/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2668 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2668 - mape: 8.4877\n",
      "Epoch 197: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2665 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2665 - mape: 8.4837 - val_loss: 0.2636 - val_mse: 0.2497 - val_rmse: 0.4997 - val_mae: 0.2636 - val_mape: 8.2326 - lr: 1.0000e-04\n",
      "Epoch 198/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2653 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2653 - mape: 8.4578\n",
      "Epoch 198: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2648 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2648 - mape: 8.4535 - val_loss: 0.2683 - val_mse: 0.2542 - val_rmse: 0.5042 - val_mae: 0.2683 - val_mape: 8.3054 - lr: 1.0000e-04\n",
      "Epoch 199/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2639 - mse: 0.2547 - rmse: 0.5047 - mae: 0.2639 - mape: 8.4064\n",
      "Epoch 199: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2646 - mse: 0.2557 - rmse: 0.5057 - mae: 0.2646 - mape: 8.4315 - val_loss: 0.2718 - val_mse: 0.2654 - val_rmse: 0.5152 - val_mae: 0.2718 - val_mape: 8.4259 - lr: 1.0000e-04\n",
      "Epoch 200/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2650 - mse: 0.2571 - rmse: 0.5071 - mae: 0.2650 - mape: 8.4266\n",
      "Epoch 200: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2648 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2648 - mape: 8.4300 - val_loss: 0.2690 - val_mse: 0.2634 - val_rmse: 0.5132 - val_mae: 0.2690 - val_mape: 8.3814 - lr: 1.0000e-04\n",
      "Epoch 201/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2653 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2653 - mape: 8.4339\n",
      "Epoch 201: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2652 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2652 - mape: 8.4273 - val_loss: 0.2621 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2621 - val_mape: 8.4544 - lr: 1.0000e-04\n",
      "Epoch 202/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2642 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2642 - mape: 8.4099\n",
      "Epoch 202: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2641 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2641 - mape: 8.4192 - val_loss: 0.2640 - val_mse: 0.2523 - val_rmse: 0.5023 - val_mae: 0.2640 - val_mape: 8.4015 - lr: 1.0000e-04\n",
      "Epoch 203/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2679 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2679 - mape: 8.4883\n",
      "Epoch 203: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2683 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2683 - mape: 8.5043 - val_loss: 0.2646 - val_mse: 0.2535 - val_rmse: 0.5035 - val_mae: 0.2646 - val_mape: 8.4808 - lr: 1.0000e-04\n",
      "Epoch 204/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2677 - mse: 0.2640 - rmse: 0.5138 - mae: 0.2677 - mape: 8.5206\n",
      "Epoch 204: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2672 - mse: 0.2617 - rmse: 0.5115 - mae: 0.2672 - mape: 8.5109 - val_loss: 0.2653 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2653 - val_mape: 8.2926 - lr: 1.0000e-04\n",
      "Epoch 205/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2652 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2652 - mape: 8.4411\n",
      "Epoch 205: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2653 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2653 - mape: 8.4410 - val_loss: 0.2646 - val_mse: 0.2620 - val_rmse: 0.5118 - val_mae: 0.2646 - val_mape: 8.5875 - lr: 1.0000e-04\n",
      "Epoch 206/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2667 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2667 - mape: 8.4871\n",
      "Epoch 206: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2666 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2666 - mape: 8.4831 - val_loss: 0.2636 - val_mse: 0.2532 - val_rmse: 0.5032 - val_mae: 0.2636 - val_mape: 8.3042 - lr: 1.0000e-04\n",
      "Epoch 207/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2640 - mse: 0.2553 - rmse: 0.5052 - mae: 0.2640 - mape: 8.3929\n",
      "Epoch 207: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2643 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2643 - mape: 8.3982 - val_loss: 0.2670 - val_mse: 0.2634 - val_rmse: 0.5132 - val_mae: 0.2670 - val_mape: 8.3435 - lr: 1.0000e-04\n",
      "Epoch 208/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2650 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2650 - mape: 8.4602\n",
      "Epoch 208: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2650 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2650 - mape: 8.4556 - val_loss: 0.2636 - val_mse: 0.2499 - val_rmse: 0.4999 - val_mae: 0.2636 - val_mape: 8.3252 - lr: 1.0000e-04\n",
      "Epoch 209/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2660 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2660 - mape: 8.4578\n",
      "Epoch 209: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2659 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2659 - mape: 8.4542 - val_loss: 0.2637 - val_mse: 0.2615 - val_rmse: 0.5114 - val_mae: 0.2637 - val_mape: 8.3682 - lr: 1.0000e-04\n",
      "Epoch 210/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2666 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2666 - mape: 8.5034\n",
      "Epoch 210: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2663 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2663 - mape: 8.4968 - val_loss: 0.2679 - val_mse: 0.2586 - val_rmse: 0.5086 - val_mae: 0.2679 - val_mape: 8.5350 - lr: 1.0000e-04\n",
      "Epoch 211/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2651 - mse: 0.2561 - rmse: 0.5061 - mae: 0.2651 - mape: 8.4748\n",
      "Epoch 211: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2647 - mse: 0.2561 - rmse: 0.5061 - mae: 0.2647 - mape: 8.4467 - val_loss: 0.2653 - val_mse: 0.2669 - val_rmse: 0.5166 - val_mae: 0.2653 - val_mape: 8.6167 - lr: 1.0000e-04\n",
      "Epoch 212/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2650 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2650 - mape: 8.4569\n",
      "Epoch 212: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2650 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2650 - mape: 8.4487 - val_loss: 0.2621 - val_mse: 0.2504 - val_rmse: 0.5004 - val_mae: 0.2621 - val_mape: 8.3092 - lr: 1.0000e-04\n",
      "Epoch 213/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2657 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2657 - mape: 8.4838\n",
      "Epoch 213: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2656 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2656 - mape: 8.4795 - val_loss: 0.2650 - val_mse: 0.2529 - val_rmse: 0.5029 - val_mae: 0.2650 - val_mape: 8.4202 - lr: 1.0000e-04\n",
      "Epoch 214/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2656 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2656 - mape: 8.4353\n",
      "Epoch 214: val_loss did not improve from 0.26191\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2655 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2655 - mape: 8.4237 - val_loss: 0.2671 - val_mse: 0.2659 - val_rmse: 0.5157 - val_mae: 0.2671 - val_mape: 8.6712 - lr: 1.0000e-04\n",
      "Epoch 215/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2630 - mse: 0.2566 - rmse: 0.5065 - mae: 0.2630 - mape: 8.4069\n",
      "Epoch 215: val_loss improved from 0.26191 to 0.26188, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2633 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2633 - mape: 8.4161 - val_loss: 0.2619 - val_mse: 0.2548 - val_rmse: 0.5048 - val_mae: 0.2619 - val_mape: 8.4464 - lr: 1.0000e-04\n",
      "Epoch 216/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2651 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2651 - mape: 8.4460\n",
      "Epoch 216: val_loss did not improve from 0.26188\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2645 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2645 - mape: 8.4450 - val_loss: 0.2620 - val_mse: 0.2509 - val_rmse: 0.5009 - val_mae: 0.2620 - val_mape: 8.2524 - lr: 1.0000e-04\n",
      "Epoch 217/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2654 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2654 - mape: 8.4493\n",
      "Epoch 217: val_loss did not improve from 0.26188\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2656 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2656 - mape: 8.4622 - val_loss: 0.2647 - val_mse: 0.2537 - val_rmse: 0.5037 - val_mae: 0.2647 - val_mape: 8.2864 - lr: 1.0000e-04\n",
      "Epoch 218/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2643 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2643 - mape: 8.4274\n",
      "Epoch 218: val_loss improved from 0.26188 to 0.26058, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2637 - mse: 0.2570 - rmse: 0.5070 - mae: 0.2637 - mape: 8.4175 - val_loss: 0.2606 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2606 - val_mape: 8.3676 - lr: 1.0000e-04\n",
      "Epoch 219/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2656 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2656 - mape: 8.4561\n",
      "Epoch 219: val_loss did not improve from 0.26058\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2650 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2650 - mape: 8.4393 - val_loss: 0.2606 - val_mse: 0.2551 - val_rmse: 0.5051 - val_mae: 0.2606 - val_mape: 8.3457 - lr: 1.0000e-04\n",
      "Epoch 220/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2660 - mse: 0.2629 - rmse: 0.5127 - mae: 0.2660 - mape: 8.4922\n",
      "Epoch 220: val_loss did not improve from 0.26058\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2656 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2656 - mape: 8.4796 - val_loss: 0.2634 - val_mse: 0.2506 - val_rmse: 0.5006 - val_mae: 0.2634 - val_mape: 8.2633 - lr: 1.0000e-04\n",
      "Epoch 221/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2641 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2641 - mape: 8.4398\n",
      "Epoch 221: val_loss did not improve from 0.26058\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2651 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2651 - mape: 8.4552 - val_loss: 0.2639 - val_mse: 0.2513 - val_rmse: 0.5013 - val_mae: 0.2639 - val_mape: 8.2590 - lr: 1.0000e-04\n",
      "Epoch 222/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2639 - mse: 0.2557 - rmse: 0.5056 - mae: 0.2639 - mape: 8.3974\n",
      "Epoch 222: val_loss improved from 0.26058 to 0.26029, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2641 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2641 - mape: 8.4046 - val_loss: 0.2603 - val_mse: 0.2532 - val_rmse: 0.5032 - val_mae: 0.2603 - val_mape: 8.2778 - lr: 1.0000e-04\n",
      "Epoch 223/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2677 - mse: 0.2654 - rmse: 0.5152 - mae: 0.2677 - mape: 8.5349\n",
      "Epoch 223: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2675 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2675 - mape: 8.5251 - val_loss: 0.2627 - val_mse: 0.2491 - val_rmse: 0.4991 - val_mae: 0.2627 - val_mape: 8.3128 - lr: 1.0000e-04\n",
      "Epoch 224/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2646 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2646 - mape: 8.4543\n",
      "Epoch 224: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2660 - mse: 0.2623 - rmse: 0.5122 - mae: 0.2660 - mape: 8.4862 - val_loss: 0.2654 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2654 - val_mape: 8.3271 - lr: 1.0000e-04\n",
      "Epoch 225/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2635 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2635 - mape: 8.4059\n",
      "Epoch 225: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2635 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2635 - mape: 8.4059 - val_loss: 0.2647 - val_mse: 0.2615 - val_rmse: 0.5113 - val_mae: 0.2647 - val_mape: 8.5791 - lr: 1.0000e-04\n",
      "Epoch 226/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2657 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2657 - mape: 8.4777\n",
      "Epoch 226: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2653 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2653 - mape: 8.4601 - val_loss: 0.2692 - val_mse: 0.2689 - val_rmse: 0.5185 - val_mae: 0.2692 - val_mape: 8.6801 - lr: 1.0000e-04\n",
      "Epoch 227/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2642 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2642 - mape: 8.4204\n",
      "Epoch 227: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2654 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2654 - mape: 8.4754 - val_loss: 0.2636 - val_mse: 0.2506 - val_rmse: 0.5006 - val_mae: 0.2636 - val_mape: 8.3010 - lr: 1.0000e-04\n",
      "Epoch 228/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2660 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2660 - mape: 8.4576\n",
      "Epoch 228: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2658 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2658 - mape: 8.4601 - val_loss: 0.2650 - val_mse: 0.2534 - val_rmse: 0.5034 - val_mae: 0.2650 - val_mape: 8.4648 - lr: 1.0000e-04\n",
      "Epoch 229/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2655 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2655 - mape: 8.4671\n",
      "Epoch 229: val_loss improved from 0.26029 to 0.26006, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2646 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2646 - mape: 8.4355 - val_loss: 0.2601 - val_mse: 0.2513 - val_rmse: 0.5013 - val_mae: 0.2601 - val_mape: 8.2684 - lr: 1.0000e-04\n",
      "Epoch 230/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2657 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2657 - mape: 8.4688\n",
      "Epoch 230: val_loss did not improve from 0.26006\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2643 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2643 - mape: 8.4233 - val_loss: 0.2631 - val_mse: 0.2631 - val_rmse: 0.5130 - val_mae: 0.2631 - val_mape: 8.4870 - lr: 1.0000e-04\n",
      "Epoch 231/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2645 - mse: 0.2565 - rmse: 0.5065 - mae: 0.2645 - mape: 8.4707\n",
      "Epoch 231: val_loss did not improve from 0.26006\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2644 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2644 - mape: 8.4590 - val_loss: 0.2626 - val_mse: 0.2612 - val_rmse: 0.5111 - val_mae: 0.2626 - val_mape: 8.3783 - lr: 1.0000e-04\n",
      "Epoch 232/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2644 - mse: 0.2564 - rmse: 0.5063 - mae: 0.2644 - mape: 8.4166\n",
      "Epoch 232: val_loss did not improve from 0.26006\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2644 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2644 - mape: 8.4218 - val_loss: 0.2750 - val_mse: 0.2876 - val_rmse: 0.5363 - val_mae: 0.2750 - val_mape: 9.0545 - lr: 1.0000e-04\n",
      "Epoch 233/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2661 - mse: 0.2621 - rmse: 0.5119 - mae: 0.2661 - mape: 8.5027\n",
      "Epoch 233: val_loss did not improve from 0.26006\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2656 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2656 - mape: 8.4924 - val_loss: 0.2671 - val_mse: 0.2554 - val_rmse: 0.5054 - val_mae: 0.2671 - val_mape: 8.4627 - lr: 1.0000e-04\n",
      "Epoch 234/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2655 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2655 - mape: 8.4778\n",
      "Epoch 234: val_loss did not improve from 0.26006\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2658 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2658 - mape: 8.4824 - val_loss: 0.2761 - val_mse: 0.2922 - val_rmse: 0.5406 - val_mae: 0.2761 - val_mape: 9.1675 - lr: 1.0000e-04\n",
      "Epoch 235/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2664 - mse: 0.2650 - rmse: 0.5148 - mae: 0.2664 - mape: 8.5100\n",
      "Epoch 235: val_loss did not improve from 0.26006\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2661 - mse: 0.2638 - rmse: 0.5136 - mae: 0.2661 - mape: 8.4953 - val_loss: 0.2621 - val_mse: 0.2529 - val_rmse: 0.5029 - val_mae: 0.2621 - val_mape: 8.4555 - lr: 1.0000e-04\n",
      "Epoch 236/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2639 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2639 - mape: 8.4395\n",
      "Epoch 236: val_loss did not improve from 0.26006\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2642 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2642 - mape: 8.4362 - val_loss: 0.2631 - val_mse: 0.2548 - val_rmse: 0.5048 - val_mae: 0.2631 - val_mape: 8.2834 - lr: 1.0000e-04\n",
      "Epoch 237/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2634 - mse: 0.2556 - rmse: 0.5055 - mae: 0.2634 - mape: 8.4056\n",
      "Epoch 237: val_loss did not improve from 0.26006\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2644 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2644 - mape: 8.4379 - val_loss: 0.2607 - val_mse: 0.2508 - val_rmse: 0.5008 - val_mae: 0.2607 - val_mape: 8.3621 - lr: 1.0000e-04\n",
      "Epoch 238/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2542 - rmse: 0.5042 - mae: 0.2616 - mape: 8.3376\n",
      "Epoch 238: val_loss improved from 0.26006 to 0.25994, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2622 - mse: 0.2546 - rmse: 0.5046 - mae: 0.2622 - mape: 8.3709 - val_loss: 0.2599 - val_mse: 0.2521 - val_rmse: 0.5021 - val_mae: 0.2599 - val_mape: 8.3084 - lr: 1.0000e-04\n",
      "Epoch 239/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2638 - mse: 0.2612 - rmse: 0.5110 - mae: 0.2638 - mape: 8.4285\n",
      "Epoch 239: val_loss did not improve from 0.25994\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2641 - mse: 0.2620 - rmse: 0.5118 - mae: 0.2641 - mape: 8.4469 - val_loss: 0.2714 - val_mse: 0.2639 - val_rmse: 0.5137 - val_mae: 0.2714 - val_mape: 8.3646 - lr: 1.0000e-04\n",
      "Epoch 240/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2640 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2640 - mape: 8.3878\n",
      "Epoch 240: val_loss did not improve from 0.25994\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2647 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2647 - mape: 8.4256 - val_loss: 0.2600 - val_mse: 0.2503 - val_rmse: 0.5003 - val_mae: 0.2600 - val_mape: 8.2450 - lr: 1.0000e-04\n",
      "Epoch 241/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2652 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2652 - mape: 8.4844\n",
      "Epoch 241: val_loss did not improve from 0.25994\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2641 - mse: 0.2616 - rmse: 0.5114 - mae: 0.2641 - mape: 8.4434 - val_loss: 0.2650 - val_mse: 0.2617 - val_rmse: 0.5116 - val_mae: 0.2650 - val_mape: 8.6274 - lr: 1.0000e-04\n",
      "Epoch 242/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2649 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2649 - mape: 8.4749\n",
      "Epoch 242: val_loss did not improve from 0.25994\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2648 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2648 - mape: 8.4658 - val_loss: 0.2626 - val_mse: 0.2554 - val_rmse: 0.5054 - val_mae: 0.2626 - val_mape: 8.3014 - lr: 1.0000e-04\n",
      "Epoch 243/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2626 - mse: 0.2563 - rmse: 0.5063 - mae: 0.2626 - mape: 8.3920\n",
      "Epoch 243: val_loss improved from 0.25994 to 0.25961, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2630 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2630 - mape: 8.3993 - val_loss: 0.2596 - val_mse: 0.2522 - val_rmse: 0.5022 - val_mae: 0.2596 - val_mape: 8.3384 - lr: 1.0000e-04\n",
      "Epoch 244/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2628 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2628 - mape: 8.3739\n",
      "Epoch 244: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2638 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2638 - mape: 8.4120 - val_loss: 0.2707 - val_mse: 0.2636 - val_rmse: 0.5134 - val_mae: 0.2707 - val_mape: 8.3770 - lr: 1.0000e-04\n",
      "Epoch 245/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2646 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2646 - mape: 8.4504\n",
      "Epoch 245: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2642 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2642 - mape: 8.4342 - val_loss: 0.2641 - val_mse: 0.2512 - val_rmse: 0.5012 - val_mae: 0.2641 - val_mape: 8.3712 - lr: 1.0000e-04\n",
      "Epoch 246/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2628 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2628 - mape: 8.3962\n",
      "Epoch 246: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2628 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2628 - mape: 8.3960 - val_loss: 0.2615 - val_mse: 0.2530 - val_rmse: 0.5030 - val_mae: 0.2615 - val_mape: 8.2457 - lr: 1.0000e-04\n",
      "Epoch 247/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2647 - mse: 0.2620 - rmse: 0.5119 - mae: 0.2647 - mape: 8.4654\n",
      "Epoch 247: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2627 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2627 - mape: 8.4015 - val_loss: 0.2613 - val_mse: 0.2481 - val_rmse: 0.4981 - val_mae: 0.2613 - val_mape: 8.2247 - lr: 1.0000e-04\n",
      "Epoch 248/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2638 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2638 - mape: 8.4481\n",
      "Epoch 248: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2626 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2626 - mape: 8.4066 - val_loss: 0.2909 - val_mse: 0.2989 - val_rmse: 0.5467 - val_mae: 0.2909 - val_mape: 8.7475 - lr: 1.0000e-04\n",
      "Epoch 249/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2657 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2657 - mape: 8.4535\n",
      "Epoch 249: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2653 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2653 - mape: 8.4438 - val_loss: 0.2614 - val_mse: 0.2543 - val_rmse: 0.5043 - val_mae: 0.2614 - val_mape: 8.2693 - lr: 1.0000e-04\n",
      "Epoch 250/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2644 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2644 - mape: 8.4388\n",
      "Epoch 250: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2643 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2643 - mape: 8.4310 - val_loss: 0.2671 - val_mse: 0.2628 - val_rmse: 0.5126 - val_mae: 0.2671 - val_mape: 8.6129 - lr: 1.0000e-04\n",
      "Epoch 251/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2634 - mse: 0.2549 - rmse: 0.5049 - mae: 0.2634 - mape: 8.4071\n",
      "Epoch 251: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2630 - mse: 0.2547 - rmse: 0.5047 - mae: 0.2630 - mape: 8.3876 - val_loss: 0.2596 - val_mse: 0.2499 - val_rmse: 0.4999 - val_mae: 0.2596 - val_mape: 8.3009 - lr: 1.0000e-04\n",
      "Epoch 252/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2640 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2640 - mape: 8.4374\n",
      "Epoch 252: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2639 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2639 - mape: 8.4268 - val_loss: 0.2678 - val_mse: 0.2632 - val_rmse: 0.5130 - val_mae: 0.2678 - val_mape: 8.3376 - lr: 1.0000e-04\n",
      "Epoch 253/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2663 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2663 - mape: 8.5064\n",
      "Epoch 253: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2670 - mse: 0.2657 - rmse: 0.5154 - mae: 0.2670 - mape: 8.5268 - val_loss: 0.2630 - val_mse: 0.2625 - val_rmse: 0.5123 - val_mae: 0.2630 - val_mape: 8.5562 - lr: 1.0000e-04\n",
      "Epoch 254/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2628 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2628 - mape: 8.4146\n",
      "Epoch 254: val_loss improved from 0.25961 to 0.25909, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2629 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2629 - mape: 8.4213 - val_loss: 0.2591 - val_mse: 0.2526 - val_rmse: 0.5026 - val_mae: 0.2591 - val_mape: 8.3127 - lr: 1.0000e-04\n",
      "Epoch 255/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2624 - mse: 0.2570 - rmse: 0.5070 - mae: 0.2624 - mape: 8.3885\n",
      "Epoch 255: val_loss did not improve from 0.25909\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2634 - mse: 0.2589 - rmse: 0.5089 - mae: 0.2634 - mape: 8.4271 - val_loss: 0.2626 - val_mse: 0.2507 - val_rmse: 0.5007 - val_mae: 0.2626 - val_mape: 8.3868 - lr: 1.0000e-04\n",
      "Epoch 256/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2632 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2632 - mape: 8.4331\n",
      "Epoch 256: val_loss did not improve from 0.25909\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2627 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2627 - mape: 8.4141 - val_loss: 0.2598 - val_mse: 0.2496 - val_rmse: 0.4996 - val_mae: 0.2598 - val_mape: 8.2790 - lr: 1.0000e-04\n",
      "Epoch 257/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2626 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2626 - mape: 8.4122\n",
      "Epoch 257: val_loss did not improve from 0.25909\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2621 - mape: 8.3987 - val_loss: 0.2686 - val_mse: 0.2660 - val_rmse: 0.5158 - val_mae: 0.2686 - val_mape: 8.7820 - lr: 1.0000e-04\n",
      "Epoch 258/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2618 - mse: 0.2562 - rmse: 0.5062 - mae: 0.2618 - mape: 8.3823\n",
      "Epoch 258: val_loss did not improve from 0.25909\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2563 - rmse: 0.5063 - mae: 0.2619 - mape: 8.3871 - val_loss: 0.2616 - val_mse: 0.2496 - val_rmse: 0.4996 - val_mae: 0.2616 - val_mape: 8.3779 - lr: 1.0000e-04\n",
      "Epoch 259/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2639 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2639 - mape: 8.4529\n",
      "Epoch 259: val_loss did not improve from 0.25909\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2641 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2641 - mape: 8.4576 - val_loss: 0.2603 - val_mse: 0.2565 - val_rmse: 0.5065 - val_mae: 0.2603 - val_mape: 8.3552 - lr: 1.0000e-04\n",
      "Epoch 260/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2622 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2622 - mape: 8.3868\n",
      "Epoch 260: val_loss did not improve from 0.25909\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2624 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2624 - mape: 8.4012 - val_loss: 0.2670 - val_mse: 0.2660 - val_rmse: 0.5157 - val_mae: 0.2670 - val_mape: 8.7267 - lr: 1.0000e-04\n",
      "Epoch 261/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2633 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2633 - mape: 8.4377\n",
      "Epoch 261: val_loss did not improve from 0.25909\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2632 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2632 - mape: 8.4358 - val_loss: 0.2630 - val_mse: 0.2477 - val_rmse: 0.4977 - val_mae: 0.2630 - val_mape: 8.3538 - lr: 1.0000e-04\n",
      "Epoch 262/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2645 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2645 - mape: 8.4606\n",
      "Epoch 262: val_loss did not improve from 0.25909\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2639 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2639 - mape: 8.4467 - val_loss: 0.2593 - val_mse: 0.2477 - val_rmse: 0.4977 - val_mae: 0.2593 - val_mape: 8.2931 - lr: 1.0000e-04\n",
      "Epoch 263/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2621 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2621 - mape: 8.3740\n",
      "Epoch 263: val_loss improved from 0.25909 to 0.25837, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2623 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2623 - mape: 8.3784 - val_loss: 0.2584 - val_mse: 0.2491 - val_rmse: 0.4991 - val_mae: 0.2584 - val_mape: 8.3070 - lr: 1.0000e-04\n",
      "Epoch 264/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2623 - mse: 0.2552 - rmse: 0.5052 - mae: 0.2623 - mape: 8.3909\n",
      "Epoch 264: val_loss did not improve from 0.25837\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2544 - rmse: 0.5043 - mae: 0.2619 - mape: 8.3782 - val_loss: 0.2625 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2625 - val_mape: 8.3053 - lr: 1.0000e-04\n",
      "Epoch 265/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2644 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2644 - mape: 8.4277\n",
      "Epoch 265: val_loss did not improve from 0.25837\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2641 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2641 - mape: 8.4323 - val_loss: 0.2639 - val_mse: 0.2711 - val_rmse: 0.5207 - val_mae: 0.2639 - val_mape: 8.4046 - lr: 1.0000e-04\n",
      "Epoch 266/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2607 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2607 - mape: 8.3877\n",
      "Epoch 266: val_loss did not improve from 0.25837\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2598 - rmse: 0.5098 - mae: 0.2618 - mape: 8.4246 - val_loss: 0.2648 - val_mse: 0.2617 - val_rmse: 0.5116 - val_mae: 0.2648 - val_mape: 8.6077 - lr: 1.0000e-04\n",
      "Epoch 267/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2547 - rmse: 0.5047 - mae: 0.2610 - mape: 8.3748\n",
      "Epoch 267: val_loss did not improve from 0.25837\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2546 - rmse: 0.5045 - mae: 0.2609 - mape: 8.3679 - val_loss: 0.2639 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2639 - val_mape: 8.5520 - lr: 1.0000e-04\n",
      "Epoch 268/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2612 - mse: 0.2530 - rmse: 0.5030 - mae: 0.2612 - mape: 8.3566\n",
      "Epoch 268: val_loss did not improve from 0.25837\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2523 - rmse: 0.5023 - mae: 0.2606 - mape: 8.3333 - val_loss: 0.2610 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2610 - val_mape: 8.5216 - lr: 1.0000e-04\n",
      "Epoch 269/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2628 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2628 - mape: 8.4206\n",
      "Epoch 269: val_loss did not improve from 0.25837\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2619 - mape: 8.3933 - val_loss: 0.2591 - val_mse: 0.2507 - val_rmse: 0.5007 - val_mae: 0.2591 - val_mape: 8.2260 - lr: 1.0000e-04\n",
      "Epoch 270/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2622 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2622 - mape: 8.3938\n",
      "Epoch 270: val_loss did not improve from 0.25837\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2616 - mape: 8.3678 - val_loss: 0.2587 - val_mse: 0.2530 - val_rmse: 0.5030 - val_mae: 0.2587 - val_mape: 8.3489 - lr: 1.0000e-04\n",
      "Epoch 271/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2604 - mse: 0.2558 - rmse: 0.5058 - mae: 0.2604 - mape: 8.3416\n",
      "Epoch 271: val_loss did not improve from 0.25837\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2558 - rmse: 0.5058 - mae: 0.2609 - mape: 8.3464 - val_loss: 0.2629 - val_mse: 0.2652 - val_rmse: 0.5149 - val_mae: 0.2629 - val_mape: 8.5727 - lr: 1.0000e-04\n",
      "Epoch 272/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2631 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2631 - mape: 8.4324\n",
      "Epoch 272: val_loss improved from 0.25837 to 0.25758, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2630 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2630 - mape: 8.4267 - val_loss: 0.2576 - val_mse: 0.2552 - val_rmse: 0.5052 - val_mae: 0.2576 - val_mape: 8.2838 - lr: 1.0000e-04\n",
      "Epoch 273/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2620 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2620 - mape: 8.3825\n",
      "Epoch 273: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2615 - mape: 8.3688 - val_loss: 0.2626 - val_mse: 0.2538 - val_rmse: 0.5038 - val_mae: 0.2626 - val_mape: 8.2176 - lr: 1.0000e-04\n",
      "Epoch 274/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2617 - mse: 0.2560 - rmse: 0.5059 - mae: 0.2617 - mape: 8.3740\n",
      "Epoch 274: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2555 - rmse: 0.5055 - mae: 0.2611 - mape: 8.3620 - val_loss: 0.2588 - val_mse: 0.2537 - val_rmse: 0.5037 - val_mae: 0.2588 - val_mape: 8.3750 - lr: 1.0000e-04\n",
      "Epoch 275/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2617 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2617 - mape: 8.3745\n",
      "Epoch 275: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2562 - rmse: 0.5061 - mae: 0.2615 - mape: 8.3660 - val_loss: 0.2599 - val_mse: 0.2549 - val_rmse: 0.5049 - val_mae: 0.2599 - val_mape: 8.3928 - lr: 1.0000e-04\n",
      "Epoch 276/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2615 - mse: 0.2563 - rmse: 0.5063 - mae: 0.2615 - mape: 8.3764\n",
      "Epoch 276: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2556 - rmse: 0.5055 - mae: 0.2617 - mape: 8.3885 - val_loss: 0.2682 - val_mse: 0.2705 - val_rmse: 0.5201 - val_mae: 0.2682 - val_mape: 8.6958 - lr: 1.0000e-04\n",
      "Epoch 277/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2626 - mse: 0.2571 - rmse: 0.5071 - mae: 0.2626 - mape: 8.4141\n",
      "Epoch 277: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2623 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2623 - mape: 8.3936 - val_loss: 0.2592 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2592 - val_mape: 8.4087 - lr: 1.0000e-04\n",
      "Epoch 278/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2605 - mse: 0.2536 - rmse: 0.5036 - mae: 0.2605 - mape: 8.3373\n",
      "Epoch 278: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2542 - rmse: 0.5042 - mae: 0.2609 - mape: 8.3541 - val_loss: 0.2724 - val_mse: 0.2542 - val_rmse: 0.5042 - val_mae: 0.2724 - val_mape: 8.4923 - lr: 1.0000e-04\n",
      "Epoch 279/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2616 - mse: 0.2551 - rmse: 0.5050 - mae: 0.2616 - mape: 8.3784\n",
      "Epoch 279: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2607 - mse: 0.2536 - rmse: 0.5036 - mae: 0.2607 - mape: 8.3445 - val_loss: 0.2593 - val_mse: 0.2620 - val_rmse: 0.5119 - val_mae: 0.2593 - val_mape: 8.4129 - lr: 1.0000e-04\n",
      "Epoch 280/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2550 - rmse: 0.5050 - mae: 0.2609 - mape: 8.3535\n",
      "Epoch 280: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2554 - rmse: 0.5053 - mae: 0.2608 - mape: 8.3483 - val_loss: 0.2609 - val_mse: 0.2544 - val_rmse: 0.5044 - val_mae: 0.2609 - val_mape: 8.4400 - lr: 1.0000e-04\n",
      "Epoch 281/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2600 - mse: 0.2534 - rmse: 0.5034 - mae: 0.2600 - mape: 8.3194\n",
      "Epoch 281: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2537 - rmse: 0.5036 - mae: 0.2600 - mape: 8.3167 - val_loss: 0.2617 - val_mse: 0.2528 - val_rmse: 0.5027 - val_mae: 0.2617 - val_mape: 8.3201 - lr: 1.0000e-04\n",
      "Epoch 282/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2615 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2615 - mape: 8.3721\n",
      "Epoch 282: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2564 - rmse: 0.5063 - mae: 0.2615 - mape: 8.3646 - val_loss: 0.2631 - val_mse: 0.2602 - val_rmse: 0.5101 - val_mae: 0.2631 - val_mape: 8.5536 - lr: 1.0000e-04\n",
      "Epoch 283/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2655 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2655 - mape: 8.4998\n",
      "Epoch 283: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2643 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2643 - mape: 8.4642 - val_loss: 0.2622 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2622 - val_mape: 8.2906 - lr: 1.0000e-04\n",
      "Epoch 284/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2601 - mse: 0.2543 - rmse: 0.5042 - mae: 0.2601 - mape: 8.3381\n",
      "Epoch 284: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2552 - rmse: 0.5052 - mae: 0.2604 - mape: 8.3579 - val_loss: 0.2668 - val_mse: 0.2539 - val_rmse: 0.5039 - val_mae: 0.2668 - val_mape: 8.3902 - lr: 1.0000e-04\n",
      "Epoch 285/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2602 - mse: 0.2561 - rmse: 0.5060 - mae: 0.2602 - mape: 8.3361\n",
      "Epoch 285: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2557 - rmse: 0.5057 - mae: 0.2602 - mape: 8.3380 - val_loss: 0.2631 - val_mse: 0.2571 - val_rmse: 0.5070 - val_mae: 0.2631 - val_mape: 8.4405 - lr: 1.0000e-04\n",
      "Epoch 286/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2609 - mse: 0.2549 - rmse: 0.5048 - mae: 0.2609 - mape: 8.3592\n",
      "Epoch 286: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2555 - rmse: 0.5055 - mae: 0.2612 - mape: 8.3622 - val_loss: 0.2591 - val_mse: 0.2534 - val_rmse: 0.5033 - val_mae: 0.2591 - val_mape: 8.1976 - lr: 1.0000e-04\n",
      "Epoch 287/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2615 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2615 - mape: 8.3729\n",
      "Epoch 287: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2611 - mape: 8.3692 - val_loss: 0.2592 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2592 - val_mape: 8.4414 - lr: 1.0000e-04\n",
      "Epoch 288/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2622 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2622 - mape: 8.4242\n",
      "Epoch 288: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2625 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2625 - mape: 8.4381 - val_loss: 0.2602 - val_mse: 0.2526 - val_rmse: 0.5026 - val_mae: 0.2602 - val_mape: 8.4393 - lr: 1.0000e-04\n",
      "Epoch 289/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2596 - mse: 0.2524 - rmse: 0.5024 - mae: 0.2596 - mape: 8.2997\n",
      "Epoch 289: val_loss did not improve from 0.25758\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2538 - rmse: 0.5038 - mae: 0.2599 - mape: 8.3187 - val_loss: 0.2594 - val_mse: 0.2510 - val_rmse: 0.5010 - val_mae: 0.2594 - val_mape: 8.3719 - lr: 1.0000e-04\n",
      "Epoch 290/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2605 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2605 - mape: 8.3860\n",
      "Epoch 290: val_loss improved from 0.25758 to 0.25735, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2560 - rmse: 0.5060 - mae: 0.2600 - mape: 8.3561 - val_loss: 0.2573 - val_mse: 0.2505 - val_rmse: 0.5005 - val_mae: 0.2573 - val_mape: 8.2663 - lr: 1.0000e-04\n",
      "Epoch 291/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2616 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2616 - mape: 8.4133\n",
      "Epoch 291: val_loss did not improve from 0.25735\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2615 - mape: 8.3889 - val_loss: 0.2587 - val_mse: 0.2515 - val_rmse: 0.5015 - val_mae: 0.2587 - val_mape: 8.2538 - lr: 1.0000e-04\n",
      "Epoch 292/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2556 - rmse: 0.5055 - mae: 0.2609 - mape: 8.3662\n",
      "Epoch 292: val_loss did not improve from 0.25735\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2615 - mape: 8.3720 - val_loss: 0.2578 - val_mse: 0.2486 - val_rmse: 0.4986 - val_mae: 0.2578 - val_mape: 8.2826 - lr: 1.0000e-04\n",
      "Epoch 293/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2601 - mse: 0.2547 - rmse: 0.5047 - mae: 0.2601 - mape: 8.3466\n",
      "Epoch 293: val_loss did not improve from 0.25735\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2529 - rmse: 0.5028 - mae: 0.2595 - mape: 8.3291 - val_loss: 0.2586 - val_mse: 0.2498 - val_rmse: 0.4998 - val_mae: 0.2586 - val_mape: 8.2855 - lr: 1.0000e-04\n",
      "Epoch 294/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2617 - mse: 0.2532 - rmse: 0.5032 - mae: 0.2617 - mape: 8.3797\n",
      "Epoch 294: val_loss improved from 0.25735 to 0.25623, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2542 - rmse: 0.5041 - mae: 0.2614 - mape: 8.3788 - val_loss: 0.2562 - val_mse: 0.2494 - val_rmse: 0.4994 - val_mae: 0.2562 - val_mape: 8.2218 - lr: 1.0000e-04\n",
      "Epoch 295/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2621 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2621 - mape: 8.3729\n",
      "Epoch 295: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2620 - mape: 8.3709 - val_loss: 0.2732 - val_mse: 0.2709 - val_rmse: 0.5205 - val_mae: 0.2732 - val_mape: 8.7474 - lr: 1.0000e-04\n",
      "Epoch 296/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2613 - mse: 0.2555 - rmse: 0.5055 - mae: 0.2613 - mape: 8.3457\n",
      "Epoch 296: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2555 - rmse: 0.5055 - mae: 0.2613 - mape: 8.3457 - val_loss: 0.2566 - val_mse: 0.2499 - val_rmse: 0.4999 - val_mae: 0.2566 - val_mape: 8.2220 - lr: 1.0000e-04\n",
      "Epoch 297/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2536 - rmse: 0.5035 - mae: 0.2610 - mape: 8.3360\n",
      "Epoch 297: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2542 - rmse: 0.5041 - mae: 0.2609 - mape: 8.3384 - val_loss: 0.2590 - val_mse: 0.2627 - val_rmse: 0.5125 - val_mae: 0.2590 - val_mape: 8.3763 - lr: 1.0000e-04\n",
      "Epoch 298/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2539 - rmse: 0.5039 - mae: 0.2598 - mape: 8.3382\n",
      "Epoch 298: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2557 - rmse: 0.5056 - mae: 0.2602 - mape: 8.3458 - val_loss: 0.2659 - val_mse: 0.2716 - val_rmse: 0.5211 - val_mae: 0.2659 - val_mape: 8.7069 - lr: 1.0000e-04\n",
      "Epoch 299/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2611 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2611 - mape: 8.3445\n",
      "Epoch 299: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2562 - rmse: 0.5062 - mae: 0.2611 - mape: 8.3513 - val_loss: 0.2597 - val_mse: 0.2507 - val_rmse: 0.5007 - val_mae: 0.2597 - val_mape: 8.4529 - lr: 1.0000e-04\n",
      "Epoch 300/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2603 - mse: 0.2546 - rmse: 0.5046 - mae: 0.2603 - mape: 8.3294\n",
      "Epoch 300: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2615 - mape: 8.3737 - val_loss: 0.2664 - val_mse: 0.2653 - val_rmse: 0.5150 - val_mae: 0.2664 - val_mape: 8.3420 - lr: 1.0000e-04\n",
      "Epoch 301/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2617 - mse: 0.2566 - rmse: 0.5065 - mae: 0.2617 - mape: 8.3971\n",
      "Epoch 301: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2568 - rmse: 0.5067 - mae: 0.2614 - mape: 8.3851 - val_loss: 0.2572 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2572 - val_mape: 8.1966 - lr: 1.0000e-04\n",
      "Epoch 302/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2625 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2625 - mape: 8.4006\n",
      "Epoch 302: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2620 - mape: 8.3729 - val_loss: 0.2578 - val_mse: 0.2542 - val_rmse: 0.5042 - val_mae: 0.2578 - val_mape: 8.3322 - lr: 1.0000e-04\n",
      "Epoch 303/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2624 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2624 - mape: 8.4281\n",
      "Epoch 303: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2617 - mape: 8.3969 - val_loss: 0.2626 - val_mse: 0.2510 - val_rmse: 0.5010 - val_mae: 0.2626 - val_mape: 8.2142 - lr: 1.0000e-04\n",
      "Epoch 304/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2626 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2626 - mape: 8.4017\n",
      "Epoch 304: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2560 - rmse: 0.5059 - mae: 0.2614 - mape: 8.3660 - val_loss: 0.2595 - val_mse: 0.2651 - val_rmse: 0.5149 - val_mae: 0.2595 - val_mape: 8.4203 - lr: 1.0000e-04\n",
      "Epoch 305/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2596 - mse: 0.2538 - rmse: 0.5038 - mae: 0.2596 - mape: 8.3063\n",
      "Epoch 305: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2534 - rmse: 0.5034 - mae: 0.2594 - mape: 8.3089 - val_loss: 0.2597 - val_mse: 0.2563 - val_rmse: 0.5062 - val_mae: 0.2597 - val_mape: 8.4051 - lr: 1.0000e-04\n",
      "Epoch 306/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2583 - mse: 0.2519 - rmse: 0.5019 - mae: 0.2583 - mape: 8.2846\n",
      "Epoch 306: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2527 - rmse: 0.5027 - mae: 0.2592 - mape: 8.3169 - val_loss: 0.2576 - val_mse: 0.2473 - val_rmse: 0.4973 - val_mae: 0.2576 - val_mape: 8.2137 - lr: 1.0000e-04\n",
      "Epoch 307/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2616 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2616 - mape: 8.3695\n",
      "Epoch 307: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2550 - rmse: 0.5050 - mae: 0.2608 - mape: 8.3422 - val_loss: 0.2583 - val_mse: 0.2492 - val_rmse: 0.4992 - val_mae: 0.2583 - val_mape: 8.1726 - lr: 1.0000e-04\n",
      "Epoch 308/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2551 - rmse: 0.5051 - mae: 0.2596 - mape: 8.3145\n",
      "Epoch 308: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2550 - rmse: 0.5049 - mae: 0.2596 - mape: 8.3227 - val_loss: 0.2603 - val_mse: 0.2511 - val_rmse: 0.5010 - val_mae: 0.2603 - val_mape: 8.1818 - lr: 1.0000e-04\n",
      "Epoch 309/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2631 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2631 - mape: 8.4375\n",
      "Epoch 309: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2628 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2628 - mape: 8.4184 - val_loss: 0.2576 - val_mse: 0.2513 - val_rmse: 0.5013 - val_mae: 0.2576 - val_mape: 8.2945 - lr: 1.0000e-04\n",
      "Epoch 310/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2606 - mse: 0.2563 - rmse: 0.5063 - mae: 0.2606 - mape: 8.3247\n",
      "Epoch 310: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2601 - mse: 0.2556 - rmse: 0.5055 - mae: 0.2601 - mape: 8.3164 - val_loss: 0.2591 - val_mse: 0.2495 - val_rmse: 0.4995 - val_mae: 0.2591 - val_mape: 8.3232 - lr: 1.0000e-04\n",
      "Epoch 311/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2626 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2626 - mape: 8.4302\n",
      "Epoch 311: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2619 - mape: 8.4112 - val_loss: 0.2600 - val_mse: 0.2532 - val_rmse: 0.5032 - val_mae: 0.2600 - val_mape: 8.3833 - lr: 1.0000e-04\n",
      "Epoch 312/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2606 - mse: 0.2551 - rmse: 0.5051 - mae: 0.2606 - mape: 8.3390\n",
      "Epoch 312: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2552 - rmse: 0.5052 - mae: 0.2607 - mape: 8.3427 - val_loss: 0.2675 - val_mse: 0.2638 - val_rmse: 0.5136 - val_mae: 0.2675 - val_mape: 8.3281 - lr: 1.0000e-04\n",
      "Epoch 313/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2610 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2610 - mape: 8.3572\n",
      "Epoch 313: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2547 - rmse: 0.5047 - mae: 0.2606 - mape: 8.3397 - val_loss: 0.2599 - val_mse: 0.2543 - val_rmse: 0.5043 - val_mae: 0.2599 - val_mape: 8.3711 - lr: 1.0000e-04\n",
      "Epoch 314/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2608 - mse: 0.2533 - rmse: 0.5033 - mae: 0.2608 - mape: 8.3322\n",
      "Epoch 314: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2537 - rmse: 0.5036 - mae: 0.2609 - mape: 8.3432 - val_loss: 0.2582 - val_mse: 0.2506 - val_rmse: 0.5006 - val_mae: 0.2582 - val_mape: 8.2821 - lr: 1.0000e-04\n",
      "Epoch 315/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2590 - mse: 0.2550 - rmse: 0.5050 - mae: 0.2590 - mape: 8.3025\n",
      "Epoch 315: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2560 - rmse: 0.5060 - mae: 0.2600 - mape: 8.3273 - val_loss: 0.2609 - val_mse: 0.2448 - val_rmse: 0.4948 - val_mae: 0.2609 - val_mape: 8.2481 - lr: 1.0000e-04\n",
      "Epoch 316/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2593 - mse: 0.2514 - rmse: 0.5014 - mae: 0.2593 - mape: 8.2815\n",
      "Epoch 316: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2612 - mape: 8.3373 - val_loss: 0.2632 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2632 - val_mape: 8.5520 - lr: 1.0000e-04\n",
      "Epoch 317/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2595 - mse: 0.2533 - rmse: 0.5033 - mae: 0.2595 - mape: 8.3051\n",
      "Epoch 317: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2539 - rmse: 0.5039 - mae: 0.2599 - mape: 8.3219 - val_loss: 0.2661 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2661 - val_mape: 8.2788 - lr: 1.0000e-04\n",
      "Epoch 318/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2547 - rmse: 0.5046 - mae: 0.2607 - mape: 8.3564\n",
      "Epoch 318: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2550 - rmse: 0.5050 - mae: 0.2606 - mape: 8.3496 - val_loss: 0.2572 - val_mse: 0.2491 - val_rmse: 0.4991 - val_mae: 0.2572 - val_mape: 8.2546 - lr: 1.0000e-04\n",
      "Epoch 319/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2613 - mse: 0.2552 - rmse: 0.5052 - mae: 0.2613 - mape: 8.3550\n",
      "Epoch 319: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2546 - rmse: 0.5046 - mae: 0.2609 - mape: 8.3468 - val_loss: 0.2571 - val_mse: 0.2469 - val_rmse: 0.4969 - val_mae: 0.2571 - val_mape: 8.2314 - lr: 1.0000e-04\n",
      "Epoch 320/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2598 - mse: 0.2545 - rmse: 0.5045 - mae: 0.2598 - mape: 8.3106\n",
      "Epoch 320: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2539 - rmse: 0.5039 - mae: 0.2595 - mape: 8.3024 - val_loss: 0.2567 - val_mse: 0.2514 - val_rmse: 0.5014 - val_mae: 0.2567 - val_mape: 8.2922 - lr: 1.0000e-04\n",
      "Epoch 321/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2609 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2609 - mape: 8.3586\n",
      "Epoch 321: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2610 - mape: 8.3589 - val_loss: 0.2597 - val_mse: 0.2479 - val_rmse: 0.4979 - val_mae: 0.2597 - val_mape: 8.3588 - lr: 1.0000e-04\n",
      "Epoch 322/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2609 - mse: 0.2536 - rmse: 0.5036 - mae: 0.2609 - mape: 8.3301\n",
      "Epoch 322: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2610 - mape: 8.3331 - val_loss: 0.2883 - val_mse: 0.3160 - val_rmse: 0.5621 - val_mae: 0.2883 - val_mape: 9.4762 - lr: 1.0000e-04\n",
      "Epoch 323/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2591 - mse: 0.2510 - rmse: 0.5009 - mae: 0.2591 - mape: 8.2750\n",
      "Epoch 323: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2608 - mape: 8.3173 - val_loss: 0.2604 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2604 - val_mape: 8.5002 - lr: 1.0000e-04\n",
      "Epoch 324/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2599 - mse: 0.2535 - rmse: 0.5035 - mae: 0.2599 - mape: 8.3127\n",
      "Epoch 324: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2533 - rmse: 0.5033 - mae: 0.2600 - mape: 8.3071 - val_loss: 0.2683 - val_mse: 0.2619 - val_rmse: 0.5117 - val_mae: 0.2683 - val_mape: 8.8350 - lr: 1.0000e-04\n",
      "Epoch 325/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2595 - mse: 0.2541 - rmse: 0.5041 - mae: 0.2595 - mape: 8.3022\n",
      "Epoch 325: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2535 - rmse: 0.5035 - mae: 0.2596 - mape: 8.3105 - val_loss: 0.2651 - val_mse: 0.2548 - val_rmse: 0.5048 - val_mae: 0.2651 - val_mape: 8.2396 - lr: 1.0000e-04\n",
      "Epoch 326/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2605 - mse: 0.2536 - rmse: 0.5036 - mae: 0.2605 - mape: 8.3223\n",
      "Epoch 326: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2601 - mse: 0.2536 - rmse: 0.5036 - mae: 0.2601 - mape: 8.3203 - val_loss: 0.2607 - val_mse: 0.2525 - val_rmse: 0.5025 - val_mae: 0.2607 - val_mape: 8.1999 - lr: 1.0000e-04\n",
      "Epoch 327/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2587 - mse: 0.2512 - rmse: 0.5012 - mae: 0.2587 - mape: 8.2620\n",
      "Epoch 327: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2514 - rmse: 0.5014 - mae: 0.2588 - mape: 8.2606 - val_loss: 0.2564 - val_mse: 0.2509 - val_rmse: 0.5009 - val_mae: 0.2564 - val_mape: 8.2797 - lr: 1.0000e-04\n",
      "Epoch 328/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2601 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2601 - mape: 8.3337\n",
      "Epoch 328: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2545 - rmse: 0.5045 - mae: 0.2602 - mape: 8.3362 - val_loss: 0.2672 - val_mse: 0.2458 - val_rmse: 0.4958 - val_mae: 0.2672 - val_mape: 8.4201 - lr: 1.0000e-04\n",
      "Epoch 329/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2613 - mse: 0.2559 - rmse: 0.5058 - mae: 0.2613 - mape: 8.3561\n",
      "Epoch 329: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2559 - rmse: 0.5058 - mae: 0.2613 - mape: 8.3561 - val_loss: 0.2638 - val_mse: 0.2669 - val_rmse: 0.5167 - val_mae: 0.2638 - val_mape: 8.6271 - lr: 1.0000e-04\n",
      "Epoch 330/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2606 - mse: 0.2536 - rmse: 0.5036 - mae: 0.2606 - mape: 8.3204\n",
      "Epoch 330: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2618 - mape: 8.3627 - val_loss: 0.2601 - val_mse: 0.2636 - val_rmse: 0.5134 - val_mae: 0.2601 - val_mape: 8.3996 - lr: 1.0000e-04\n",
      "Epoch 331/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2543 - rmse: 0.5043 - mae: 0.2607 - mape: 8.3402\n",
      "Epoch 331: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2543 - rmse: 0.5043 - mae: 0.2609 - mape: 8.3408 - val_loss: 0.2595 - val_mse: 0.2508 - val_rmse: 0.5008 - val_mae: 0.2595 - val_mape: 8.1856 - lr: 1.0000e-04\n",
      "Epoch 332/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2614 - mse: 0.2559 - rmse: 0.5058 - mae: 0.2614 - mape: 8.3434\n",
      "Epoch 332: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2558 - rmse: 0.5058 - mae: 0.2614 - mape: 8.3427 - val_loss: 0.2611 - val_mse: 0.2557 - val_rmse: 0.5057 - val_mae: 0.2611 - val_mape: 8.4281 - lr: 1.0000e-04\n",
      "Epoch 333/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2603 - mse: 0.2518 - rmse: 0.5018 - mae: 0.2603 - mape: 8.3080\n",
      "Epoch 333: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2558 - rmse: 0.5058 - mae: 0.2614 - mape: 8.3443 - val_loss: 0.2564 - val_mse: 0.2490 - val_rmse: 0.4990 - val_mae: 0.2564 - val_mape: 8.2580 - lr: 1.0000e-04\n",
      "Epoch 334/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2606 - mse: 0.2559 - rmse: 0.5059 - mae: 0.2606 - mape: 8.3538\n",
      "Epoch 334: val_loss did not improve from 0.25623\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2553 - rmse: 0.5053 - mae: 0.2604 - mape: 8.3154 - val_loss: 0.2577 - val_mse: 0.2441 - val_rmse: 0.4941 - val_mae: 0.2577 - val_mape: 8.1562 - lr: 1.0000e-04\n",
      "Epoch 335/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2561 - mse: 0.2449 - rmse: 0.4949 - mae: 0.2561 - mape: 8.1550\n",
      "Epoch 335: val_loss improved from 0.25623 to 0.25531, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2562 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2562 - mape: 8.1512 - val_loss: 0.2553 - val_mse: 0.2471 - val_rmse: 0.4971 - val_mae: 0.2553 - val_mape: 8.1671 - lr: 1.0000e-05\n",
      "Epoch 336/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2552 - mse: 0.2482 - rmse: 0.4981 - mae: 0.2552 - mape: 8.1610\n",
      "Epoch 336: val_loss improved from 0.25531 to 0.25518, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2558 - mse: 0.2481 - rmse: 0.4981 - mae: 0.2558 - mape: 8.1776 - val_loss: 0.2552 - val_mse: 0.2466 - val_rmse: 0.4966 - val_mae: 0.2552 - val_mape: 8.1463 - lr: 1.0000e-05\n",
      "Epoch 337/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2550 - mse: 0.2459 - rmse: 0.4959 - mae: 0.2550 - mape: 8.1661\n",
      "Epoch 337: val_loss did not improve from 0.25518\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2556 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2556 - mape: 8.1800 - val_loss: 0.2553 - val_mse: 0.2465 - val_rmse: 0.4965 - val_mae: 0.2553 - val_mape: 8.1452 - lr: 1.0000e-05\n",
      "Epoch 338/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2555 - mse: 0.2471 - rmse: 0.4971 - mae: 0.2555 - mape: 8.1685\n",
      "Epoch 338: val_loss did not improve from 0.25518\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2554 - mape: 8.1652 - val_loss: 0.2562 - val_mse: 0.2491 - val_rmse: 0.4991 - val_mae: 0.2562 - val_mape: 8.1424 - lr: 1.0000e-05\n",
      "Epoch 339/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2559 - mse: 0.2506 - rmse: 0.5006 - mae: 0.2559 - mape: 8.1843\n",
      "Epoch 339: val_loss did not improve from 0.25518\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2557 - mse: 0.2494 - rmse: 0.4994 - mae: 0.2557 - mape: 8.1933 - val_loss: 0.2568 - val_mse: 0.2479 - val_rmse: 0.4979 - val_mae: 0.2568 - val_mape: 8.1340 - lr: 1.0000e-05\n",
      "Epoch 340/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2554 - mse: 0.2483 - rmse: 0.4983 - mae: 0.2554 - mape: 8.1708\n",
      "Epoch 340: val_loss improved from 0.25518 to 0.25504, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2482 - rmse: 0.4982 - mae: 0.2554 - mape: 8.1701 - val_loss: 0.2550 - val_mse: 0.2471 - val_rmse: 0.4971 - val_mae: 0.2550 - val_mape: 8.1702 - lr: 1.0000e-05\n",
      "Epoch 341/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2558 - mse: 0.2479 - rmse: 0.4979 - mae: 0.2558 - mape: 8.1700\n",
      "Epoch 341: val_loss improved from 0.25504 to 0.25503, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2556 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2556 - mape: 8.1715 - val_loss: 0.2550 - val_mse: 0.2472 - val_rmse: 0.4972 - val_mae: 0.2550 - val_mape: 8.1624 - lr: 1.0000e-05\n",
      "Epoch 342/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2558 - mse: 0.2494 - rmse: 0.4994 - mae: 0.2558 - mape: 8.1947\n",
      "Epoch 342: val_loss improved from 0.25503 to 0.25500, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2477 - rmse: 0.4977 - mae: 0.2553 - mape: 8.1815 - val_loss: 0.2550 - val_mse: 0.2473 - val_rmse: 0.4973 - val_mae: 0.2550 - val_mape: 8.1754 - lr: 1.0000e-05\n",
      "Epoch 343/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2557 - mse: 0.2482 - rmse: 0.4982 - mae: 0.2557 - mape: 8.1849\n",
      "Epoch 343: val_loss did not improve from 0.25500\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2477 - rmse: 0.4977 - mae: 0.2554 - mape: 8.1825 - val_loss: 0.2551 - val_mse: 0.2465 - val_rmse: 0.4965 - val_mae: 0.2551 - val_mape: 8.1651 - lr: 1.0000e-05\n",
      "Epoch 344/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2563 - mse: 0.2490 - rmse: 0.4990 - mae: 0.2563 - mape: 8.1996\n",
      "Epoch 344: val_loss improved from 0.25500 to 0.25498, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2556 - mse: 0.2478 - rmse: 0.4978 - mae: 0.2556 - mape: 8.1780 - val_loss: 0.2550 - val_mse: 0.2472 - val_rmse: 0.4972 - val_mae: 0.2550 - val_mape: 8.1729 - lr: 1.0000e-05\n",
      "Epoch 345/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2553 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2553 - mape: 8.1802\n",
      "Epoch 345: val_loss did not improve from 0.25498\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2467 - rmse: 0.4966 - mae: 0.2553 - mape: 8.1766 - val_loss: 0.2551 - val_mse: 0.2475 - val_rmse: 0.4975 - val_mae: 0.2551 - val_mape: 8.1486 - lr: 1.0000e-05\n",
      "Epoch 346/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2555 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2555 - mape: 8.1786\n",
      "Epoch 346: val_loss did not improve from 0.25498\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2557 - mse: 0.2485 - rmse: 0.4985 - mae: 0.2557 - mape: 8.1807 - val_loss: 0.2553 - val_mse: 0.2478 - val_rmse: 0.4978 - val_mae: 0.2553 - val_mape: 8.2011 - lr: 1.0000e-05\n",
      "Epoch 347/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2547 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2547 - mape: 8.1438\n",
      "Epoch 347: val_loss did not improve from 0.25498\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2555 - mape: 8.1779 - val_loss: 0.2551 - val_mse: 0.2478 - val_rmse: 0.4978 - val_mae: 0.2551 - val_mape: 8.1880 - lr: 1.0000e-05\n",
      "Epoch 348/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2559 - mse: 0.2485 - rmse: 0.4985 - mae: 0.2559 - mape: 8.1924\n",
      "Epoch 348: val_loss did not improve from 0.25498\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2556 - mse: 0.2479 - rmse: 0.4979 - mae: 0.2556 - mape: 8.1801 - val_loss: 0.2553 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2553 - val_mape: 8.1353 - lr: 1.0000e-05\n",
      "Epoch 349/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2542 - mse: 0.2458 - rmse: 0.4958 - mae: 0.2542 - mape: 8.1162\n",
      "Epoch 349: val_loss did not improve from 0.25498\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2554 - mape: 8.1757 - val_loss: 0.2550 - val_mse: 0.2462 - val_rmse: 0.4962 - val_mae: 0.2550 - val_mape: 8.1596 - lr: 1.0000e-05\n",
      "Epoch 350/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2558 - mse: 0.2470 - rmse: 0.4969 - mae: 0.2558 - mape: 8.1681\n",
      "Epoch 350: val_loss did not improve from 0.25498\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2558 - mse: 0.2480 - rmse: 0.4980 - mae: 0.2558 - mape: 8.1734 - val_loss: 0.2553 - val_mse: 0.2470 - val_rmse: 0.4970 - val_mae: 0.2553 - val_mape: 8.1317 - lr: 1.0000e-05\n",
      "Epoch 351/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2554 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2554 - mape: 8.1718\n",
      "Epoch 351: val_loss did not improve from 0.25498\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2556 - mse: 0.2471 - rmse: 0.4970 - mae: 0.2556 - mape: 8.1725 - val_loss: 0.2551 - val_mse: 0.2477 - val_rmse: 0.4977 - val_mae: 0.2551 - val_mape: 8.1597 - lr: 1.0000e-05\n",
      "Epoch 352/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2552 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2552 - mape: 8.1831\n",
      "Epoch 352: val_loss did not improve from 0.25498\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2470 - rmse: 0.4969 - mae: 0.2554 - mape: 8.1754 - val_loss: 0.2551 - val_mse: 0.2472 - val_rmse: 0.4972 - val_mae: 0.2551 - val_mape: 8.1629 - lr: 1.0000e-05\n",
      "Epoch 353/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2560 - mse: 0.2480 - rmse: 0.4980 - mae: 0.2560 - mape: 8.1985\n",
      "Epoch 353: val_loss did not improve from 0.25498\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2557 - mse: 0.2480 - rmse: 0.4980 - mae: 0.2557 - mape: 8.1809 - val_loss: 0.2555 - val_mse: 0.2455 - val_rmse: 0.4955 - val_mae: 0.2555 - val_mape: 8.1381 - lr: 1.0000e-05\n",
      "Epoch 354/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2555 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2555 - mape: 8.1646\n",
      "Epoch 354: val_loss improved from 0.25498 to 0.25492, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2557 - mse: 0.2477 - rmse: 0.4977 - mae: 0.2557 - mape: 8.1689 - val_loss: 0.2549 - val_mse: 0.2472 - val_rmse: 0.4972 - val_mae: 0.2549 - val_mape: 8.1586 - lr: 1.0000e-05\n",
      "Epoch 355/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2552 - mse: 0.2476 - rmse: 0.4975 - mae: 0.2552 - mape: 8.1718\n",
      "Epoch 355: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2481 - rmse: 0.4981 - mae: 0.2555 - mape: 8.1787 - val_loss: 0.2555 - val_mse: 0.2489 - val_rmse: 0.4989 - val_mae: 0.2555 - val_mape: 8.2261 - lr: 1.0000e-05\n",
      "Epoch 356/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2552 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2552 - mape: 8.1710\n",
      "Epoch 356: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2479 - rmse: 0.4979 - mae: 0.2555 - mape: 8.1786 - val_loss: 0.2558 - val_mse: 0.2481 - val_rmse: 0.4981 - val_mae: 0.2558 - val_mape: 8.1350 - lr: 1.0000e-05\n",
      "Epoch 357/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2553 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2553 - mape: 8.1526\n",
      "Epoch 357: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2555 - mape: 8.1593 - val_loss: 0.2551 - val_mse: 0.2488 - val_rmse: 0.4988 - val_mae: 0.2551 - val_mape: 8.2057 - lr: 1.0000e-05\n",
      "Epoch 358/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2551 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2551 - mape: 8.1648\n",
      "Epoch 358: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2485 - rmse: 0.4985 - mae: 0.2553 - mape: 8.1801 - val_loss: 0.2556 - val_mse: 0.2475 - val_rmse: 0.4975 - val_mae: 0.2556 - val_mape: 8.1343 - lr: 1.0000e-05\n",
      "Epoch 359/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2552 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2552 - mape: 8.2055\n",
      "Epoch 359: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2553 - mape: 8.1816 - val_loss: 0.2553 - val_mse: 0.2453 - val_rmse: 0.4953 - val_mae: 0.2553 - val_mape: 8.1652 - lr: 1.0000e-05\n",
      "Epoch 360/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2555 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2555 - mape: 8.1526\n",
      "Epoch 360: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2557 - mse: 0.2470 - rmse: 0.4969 - mae: 0.2557 - mape: 8.1682 - val_loss: 0.2553 - val_mse: 0.2476 - val_rmse: 0.4976 - val_mae: 0.2553 - val_mape: 8.1988 - lr: 1.0000e-05\n",
      "Epoch 361/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2555 - mse: 0.2484 - rmse: 0.4984 - mae: 0.2555 - mape: 8.1788\n",
      "Epoch 361: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2481 - rmse: 0.4980 - mae: 0.2555 - mape: 8.1788 - val_loss: 0.2557 - val_mse: 0.2456 - val_rmse: 0.4956 - val_mae: 0.2557 - val_mape: 8.1176 - lr: 1.0000e-05\n",
      "Epoch 362/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2564 - mse: 0.2483 - rmse: 0.4983 - mae: 0.2564 - mape: 8.1779\n",
      "Epoch 362: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2556 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2556 - mape: 8.1580 - val_loss: 0.2553 - val_mse: 0.2477 - val_rmse: 0.4977 - val_mae: 0.2553 - val_mape: 8.1424 - lr: 1.0000e-05\n",
      "Epoch 363/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2558 - mse: 0.2480 - rmse: 0.4980 - mae: 0.2558 - mape: 8.1763\n",
      "Epoch 363: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2559 - mse: 0.2487 - rmse: 0.4987 - mae: 0.2559 - mape: 8.1774 - val_loss: 0.2556 - val_mse: 0.2473 - val_rmse: 0.4973 - val_mae: 0.2556 - val_mape: 8.2241 - lr: 1.0000e-05\n",
      "Epoch 364/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2559 - mse: 0.2482 - rmse: 0.4982 - mae: 0.2559 - mape: 8.1784\n",
      "Epoch 364: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2554 - mape: 8.1611 - val_loss: 0.2551 - val_mse: 0.2471 - val_rmse: 0.4971 - val_mae: 0.2551 - val_mape: 8.1363 - lr: 1.0000e-05\n",
      "Epoch 365/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2558 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2558 - mape: 8.1600\n",
      "Epoch 365: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2553 - mape: 8.1533 - val_loss: 0.2555 - val_mse: 0.2479 - val_rmse: 0.4979 - val_mae: 0.2555 - val_mape: 8.1314 - lr: 1.0000e-05\n",
      "Epoch 366/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2563 - mse: 0.2498 - rmse: 0.4998 - mae: 0.2563 - mape: 8.1873\n",
      "Epoch 366: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2555 - mape: 8.1642 - val_loss: 0.2549 - val_mse: 0.2470 - val_rmse: 0.4970 - val_mae: 0.2549 - val_mape: 8.1679 - lr: 1.0000e-05\n",
      "Epoch 367/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2554 - mse: 0.2478 - rmse: 0.4978 - mae: 0.2554 - mape: 8.1688\n",
      "Epoch 367: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2556 - mse: 0.2481 - rmse: 0.4981 - mae: 0.2556 - mape: 8.1763 - val_loss: 0.2550 - val_mse: 0.2472 - val_rmse: 0.4972 - val_mae: 0.2550 - val_mape: 8.1395 - lr: 1.0000e-05\n",
      "Epoch 368/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2556 - mse: 0.2481 - rmse: 0.4981 - mae: 0.2556 - mape: 8.1690\n",
      "Epoch 368: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2481 - rmse: 0.4981 - mae: 0.2554 - mape: 8.1745 - val_loss: 0.2552 - val_mse: 0.2469 - val_rmse: 0.4969 - val_mae: 0.2552 - val_mape: 8.1853 - lr: 1.0000e-05\n",
      "Epoch 369/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2560 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2560 - mape: 8.2085\n",
      "Epoch 369: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2554 - mape: 8.1630 - val_loss: 0.2550 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2550 - val_mape: 8.1330 - lr: 1.0000e-05\n",
      "Epoch 370/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2548 - mse: 0.2456 - rmse: 0.4956 - mae: 0.2548 - mape: 8.1544\n",
      "Epoch 370: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2551 - mse: 0.2467 - rmse: 0.4966 - mae: 0.2551 - mape: 8.1560 - val_loss: 0.2549 - val_mse: 0.2476 - val_rmse: 0.4976 - val_mae: 0.2549 - val_mape: 8.1801 - lr: 1.0000e-05\n",
      "Epoch 371/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2552 - mse: 0.2482 - rmse: 0.4982 - mae: 0.2552 - mape: 8.1708\n",
      "Epoch 371: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2555 - mape: 8.1683 - val_loss: 0.2551 - val_mse: 0.2471 - val_rmse: 0.4971 - val_mae: 0.2551 - val_mape: 8.1636 - lr: 1.0000e-05\n",
      "Epoch 372/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2548 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2548 - mape: 8.1499\n",
      "Epoch 372: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2555 - mape: 8.1738 - val_loss: 0.2549 - val_mse: 0.2455 - val_rmse: 0.4954 - val_mae: 0.2549 - val_mape: 8.1492 - lr: 1.0000e-05\n",
      "Epoch 373/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2562 - mse: 0.2498 - rmse: 0.4998 - mae: 0.2562 - mape: 8.2130\n",
      "Epoch 373: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2555 - mape: 8.1711 - val_loss: 0.2552 - val_mse: 0.2473 - val_rmse: 0.4973 - val_mae: 0.2552 - val_mape: 8.1996 - lr: 1.0000e-05\n",
      "Epoch 374/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2554 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2554 - mape: 8.1744\n",
      "Epoch 374: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2554 - mape: 8.1625 - val_loss: 0.2554 - val_mse: 0.2479 - val_rmse: 0.4979 - val_mae: 0.2554 - val_mape: 8.2211 - lr: 1.0000e-05\n",
      "Epoch 375/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2554 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2554 - mape: 8.1663\n",
      "Epoch 375: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2476 - rmse: 0.4975 - mae: 0.2553 - mape: 8.1644 - val_loss: 0.2554 - val_mse: 0.2452 - val_rmse: 0.4952 - val_mae: 0.2554 - val_mape: 8.1385 - lr: 1.0000e-05\n",
      "Epoch 376/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2564 - mse: 0.2501 - rmse: 0.5001 - mae: 0.2564 - mape: 8.2193\n",
      "Epoch 376: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2556 - mse: 0.2481 - rmse: 0.4981 - mae: 0.2556 - mape: 8.1773 - val_loss: 0.2552 - val_mse: 0.2458 - val_rmse: 0.4958 - val_mae: 0.2552 - val_mape: 8.1728 - lr: 1.0000e-05\n",
      "Epoch 377/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2561 - mse: 0.2478 - rmse: 0.4978 - mae: 0.2561 - mape: 8.1863\n",
      "Epoch 377: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2469 - rmse: 0.4968 - mae: 0.2554 - mape: 8.1621 - val_loss: 0.2552 - val_mse: 0.2482 - val_rmse: 0.4982 - val_mae: 0.2552 - val_mape: 8.1995 - lr: 1.0000e-05\n",
      "Epoch 378/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2555 - mse: 0.2469 - rmse: 0.4968 - mae: 0.2555 - mape: 8.1762\n",
      "Epoch 378: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2555 - mape: 8.1774 - val_loss: 0.2552 - val_mse: 0.2481 - val_rmse: 0.4981 - val_mae: 0.2552 - val_mape: 8.1457 - lr: 1.0000e-05\n",
      "Epoch 379/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2557 - mse: 0.2488 - rmse: 0.4988 - mae: 0.2557 - mape: 8.1348\n",
      "Epoch 379: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2554 - mape: 8.1552 - val_loss: 0.2552 - val_mse: 0.2489 - val_rmse: 0.4989 - val_mae: 0.2552 - val_mape: 8.1900 - lr: 1.0000e-05\n",
      "Epoch 380/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2546 - mse: 0.2456 - rmse: 0.4956 - mae: 0.2546 - mape: 8.1382\n",
      "Epoch 380: val_loss did not improve from 0.25492\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2478 - rmse: 0.4978 - mae: 0.2555 - mape: 8.1721 - val_loss: 0.2553 - val_mse: 0.2471 - val_rmse: 0.4971 - val_mae: 0.2553 - val_mape: 8.1220 - lr: 1.0000e-05\n",
      "Epoch 381/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2555 - mse: 0.2484 - rmse: 0.4984 - mae: 0.2555 - mape: 8.1720\n",
      "Epoch 381: val_loss improved from 0.25492 to 0.25489, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2557 - mse: 0.2483 - rmse: 0.4983 - mae: 0.2557 - mape: 8.1738 - val_loss: 0.2549 - val_mse: 0.2455 - val_rmse: 0.4955 - val_mae: 0.2549 - val_mape: 8.1491 - lr: 1.0000e-05\n",
      "Epoch 382/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2553 - mse: 0.2477 - rmse: 0.4977 - mae: 0.2553 - mape: 8.1607\n",
      "Epoch 382: val_loss did not improve from 0.25489\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2554 - mape: 8.1651 - val_loss: 0.2551 - val_mse: 0.2471 - val_rmse: 0.4971 - val_mae: 0.2551 - val_mape: 8.1910 - lr: 1.0000e-05\n",
      "Epoch 383/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2551 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2551 - mape: 8.1618\n",
      "Epoch 383: val_loss improved from 0.25489 to 0.25484, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2552 - mape: 8.1643 - val_loss: 0.2548 - val_mse: 0.2464 - val_rmse: 0.4964 - val_mae: 0.2548 - val_mape: 8.1624 - lr: 1.0000e-05\n",
      "Epoch 384/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2545 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2545 - mape: 8.1494\n",
      "Epoch 384: val_loss did not improve from 0.25484\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2555 - mape: 8.1788 - val_loss: 0.2550 - val_mse: 0.2454 - val_rmse: 0.4954 - val_mae: 0.2550 - val_mape: 8.1267 - lr: 1.0000e-05\n",
      "Epoch 385/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2554 - mse: 0.2486 - rmse: 0.4986 - mae: 0.2554 - mape: 8.1797\n",
      "Epoch 385: val_loss did not improve from 0.25484\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2478 - rmse: 0.4978 - mae: 0.2554 - mape: 8.1694 - val_loss: 0.2552 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2552 - val_mape: 8.1714 - lr: 1.0000e-05\n",
      "Epoch 386/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2548 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2548 - mape: 8.1497\n",
      "Epoch 386: val_loss did not improve from 0.25484\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2551 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2551 - mape: 8.1587 - val_loss: 0.2566 - val_mse: 0.2507 - val_rmse: 0.5007 - val_mae: 0.2566 - val_mape: 8.2768 - lr: 1.0000e-05\n",
      "Epoch 387/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2566 - mse: 0.2496 - rmse: 0.4996 - mae: 0.2566 - mape: 8.2078\n",
      "Epoch 387: val_loss did not improve from 0.25484\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2556 - mse: 0.2479 - rmse: 0.4979 - mae: 0.2556 - mape: 8.1764 - val_loss: 0.2549 - val_mse: 0.2462 - val_rmse: 0.4961 - val_mae: 0.2549 - val_mape: 8.1580 - lr: 1.0000e-05\n",
      "Epoch 388/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2554 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2554 - mape: 8.1610\n",
      "Epoch 388: val_loss did not improve from 0.25484\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2551 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2551 - mape: 8.1570 - val_loss: 0.2551 - val_mse: 0.2473 - val_rmse: 0.4973 - val_mae: 0.2551 - val_mape: 8.1322 - lr: 1.0000e-05\n",
      "Epoch 389/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2554 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2554 - mape: 8.1647\n",
      "Epoch 389: val_loss did not improve from 0.25484\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2467 - rmse: 0.4966 - mae: 0.2553 - mape: 8.1609 - val_loss: 0.2550 - val_mse: 0.2474 - val_rmse: 0.4974 - val_mae: 0.2550 - val_mape: 8.1466 - lr: 1.0000e-05\n",
      "Epoch 390/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2555 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2555 - mape: 8.1678\n",
      "Epoch 390: val_loss improved from 0.25484 to 0.25483, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2555 - mape: 8.1678 - val_loss: 0.2548 - val_mse: 0.2463 - val_rmse: 0.4963 - val_mae: 0.2548 - val_mape: 8.1382 - lr: 1.0000e-05\n",
      "Epoch 391/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2558 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2558 - mape: 8.1844\n",
      "Epoch 391: val_loss improved from 0.25483 to 0.25477, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2554 - mape: 8.1629 - val_loss: 0.2548 - val_mse: 0.2467 - val_rmse: 0.4967 - val_mae: 0.2548 - val_mape: 8.1538 - lr: 1.0000e-05\n",
      "Epoch 392/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2549 - mse: 0.2468 - rmse: 0.4967 - mae: 0.2549 - mape: 8.1518\n",
      "Epoch 392: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2479 - rmse: 0.4979 - mae: 0.2553 - mape: 8.1751 - val_loss: 0.2548 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2548 - val_mape: 8.1553 - lr: 1.0000e-05\n",
      "Epoch 393/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2550 - mse: 0.2461 - rmse: 0.4960 - mae: 0.2550 - mape: 8.1449\n",
      "Epoch 393: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2551 - mse: 0.2462 - rmse: 0.4961 - mae: 0.2551 - mape: 8.1537 - val_loss: 0.2554 - val_mse: 0.2473 - val_rmse: 0.4972 - val_mae: 0.2554 - val_mape: 8.1208 - lr: 1.0000e-05\n",
      "Epoch 394/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2566 - mse: 0.2498 - rmse: 0.4998 - mae: 0.2566 - mape: 8.1903\n",
      "Epoch 394: val_loss improved from 0.25477 to 0.25477, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2554 - mape: 8.1531 - val_loss: 0.2548 - val_mse: 0.2470 - val_rmse: 0.4970 - val_mae: 0.2548 - val_mape: 8.1632 - lr: 1.0000e-05\n",
      "Epoch 395/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2554 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2554 - mape: 8.1726\n",
      "Epoch 395: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2481 - rmse: 0.4981 - mae: 0.2554 - mape: 8.1699 - val_loss: 0.2553 - val_mse: 0.2465 - val_rmse: 0.4965 - val_mae: 0.2553 - val_mape: 8.1204 - lr: 1.0000e-05\n",
      "Epoch 396/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2552 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2552 - mape: 8.1530\n",
      "Epoch 396: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2553 - mape: 8.1558 - val_loss: 0.2554 - val_mse: 0.2488 - val_rmse: 0.4988 - val_mae: 0.2554 - val_mape: 8.2195 - lr: 1.0000e-05\n",
      "Epoch 397/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2550 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2550 - mape: 8.1505\n",
      "Epoch 397: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2482 - rmse: 0.4982 - mae: 0.2555 - mape: 8.1759 - val_loss: 0.2553 - val_mse: 0.2489 - val_rmse: 0.4989 - val_mae: 0.2553 - val_mape: 8.2057 - lr: 1.0000e-05\n",
      "Epoch 398/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2564 - mse: 0.2492 - rmse: 0.4992 - mae: 0.2564 - mape: 8.2067\n",
      "Epoch 398: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2555 - mape: 8.1649 - val_loss: 0.2549 - val_mse: 0.2476 - val_rmse: 0.4976 - val_mae: 0.2549 - val_mape: 8.1813 - lr: 1.0000e-05\n",
      "Epoch 399/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2549 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2549 - mape: 8.1556\n",
      "Epoch 399: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2552 - mape: 8.1626 - val_loss: 0.2558 - val_mse: 0.2470 - val_rmse: 0.4970 - val_mae: 0.2558 - val_mape: 8.1258 - lr: 1.0000e-05\n",
      "Epoch 400/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2551 - mse: 0.2458 - rmse: 0.4958 - mae: 0.2551 - mape: 8.1599\n",
      "Epoch 400: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2555 - mape: 8.1671 - val_loss: 0.2565 - val_mse: 0.2489 - val_rmse: 0.4989 - val_mae: 0.2565 - val_mape: 8.1441 - lr: 1.0000e-05\n",
      "Epoch 401/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2553 - mse: 0.2464 - rmse: 0.4963 - mae: 0.2553 - mape: 8.1469\n",
      "Epoch 401: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2477 - rmse: 0.4977 - mae: 0.2554 - mape: 8.1643 - val_loss: 0.2552 - val_mse: 0.2484 - val_rmse: 0.4984 - val_mae: 0.2552 - val_mape: 8.1598 - lr: 1.0000e-05\n",
      "Epoch 402/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2558 - mse: 0.2486 - rmse: 0.4986 - mae: 0.2558 - mape: 8.1959\n",
      "Epoch 402: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2480 - rmse: 0.4980 - mae: 0.2553 - mape: 8.1699 - val_loss: 0.2549 - val_mse: 0.2453 - val_rmse: 0.4953 - val_mae: 0.2549 - val_mape: 8.1519 - lr: 1.0000e-05\n",
      "Epoch 403/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2555 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2555 - mape: 8.1631\n",
      "Epoch 403: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2554 - mape: 8.1618 - val_loss: 0.2554 - val_mse: 0.2452 - val_rmse: 0.4952 - val_mae: 0.2554 - val_mape: 8.1460 - lr: 1.0000e-05\n",
      "Epoch 404/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2549 - mse: 0.2460 - rmse: 0.4959 - mae: 0.2549 - mape: 8.1383\n",
      "Epoch 404: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2552 - mape: 8.1476 - val_loss: 0.2549 - val_mse: 0.2472 - val_rmse: 0.4972 - val_mae: 0.2549 - val_mape: 8.1640 - lr: 1.0000e-05\n",
      "Epoch 405/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2552 - mse: 0.2471 - rmse: 0.4971 - mae: 0.2552 - mape: 8.1597\n",
      "Epoch 405: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2471 - rmse: 0.4971 - mae: 0.2552 - mape: 8.1655 - val_loss: 0.2551 - val_mse: 0.2464 - val_rmse: 0.4964 - val_mae: 0.2551 - val_mape: 8.1631 - lr: 1.0000e-05\n",
      "Epoch 406/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2555 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2555 - mape: 8.1622\n",
      "Epoch 406: val_loss did not improve from 0.25477\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2552 - mape: 8.1517 - val_loss: 0.2548 - val_mse: 0.2473 - val_rmse: 0.4973 - val_mae: 0.2548 - val_mape: 8.1428 - lr: 1.0000e-05\n",
      "Epoch 407/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2554 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2554 - mape: 8.1666\n",
      "Epoch 407: val_loss improved from 0.25477 to 0.25473, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2553 - mape: 8.1590 - val_loss: 0.2547 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2547 - val_mape: 8.1340 - lr: 1.0000e-05\n",
      "Epoch 408/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2545 - mse: 0.2457 - rmse: 0.4956 - mae: 0.2545 - mape: 8.1422\n",
      "Epoch 408: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2471 - rmse: 0.4971 - mae: 0.2554 - mape: 8.1601 - val_loss: 0.2554 - val_mse: 0.2489 - val_rmse: 0.4989 - val_mae: 0.2554 - val_mape: 8.2295 - lr: 1.0000e-05\n",
      "Epoch 409/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2553 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2553 - mape: 8.1693\n",
      "Epoch 409: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2553 - mape: 8.1714 - val_loss: 0.2548 - val_mse: 0.2458 - val_rmse: 0.4958 - val_mae: 0.2548 - val_mape: 8.1207 - lr: 1.0000e-05\n",
      "Epoch 410/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2545 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2545 - mape: 8.1125\n",
      "Epoch 410: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2554 - mape: 8.1509 - val_loss: 0.2548 - val_mse: 0.2470 - val_rmse: 0.4970 - val_mae: 0.2548 - val_mape: 8.1467 - lr: 1.0000e-05\n",
      "Epoch 411/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2544 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2544 - mape: 8.1495\n",
      "Epoch 411: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2551 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2551 - mape: 8.1666 - val_loss: 0.2550 - val_mse: 0.2444 - val_rmse: 0.4944 - val_mae: 0.2550 - val_mape: 8.1400 - lr: 1.0000e-05\n",
      "Epoch 412/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2547 - mse: 0.2452 - rmse: 0.4952 - mae: 0.2547 - mape: 8.1152\n",
      "Epoch 412: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2553 - mape: 8.1476 - val_loss: 0.2548 - val_mse: 0.2473 - val_rmse: 0.4973 - val_mae: 0.2548 - val_mape: 8.1802 - lr: 1.0000e-05\n",
      "Epoch 413/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2562 - mse: 0.2492 - rmse: 0.4992 - mae: 0.2562 - mape: 8.1895\n",
      "Epoch 413: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2552 - mape: 8.1571 - val_loss: 0.2554 - val_mse: 0.2486 - val_rmse: 0.4986 - val_mae: 0.2554 - val_mape: 8.2215 - lr: 1.0000e-05\n",
      "Epoch 414/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2556 - mse: 0.2485 - rmse: 0.4985 - mae: 0.2556 - mape: 8.1634\n",
      "Epoch 414: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2483 - rmse: 0.4983 - mae: 0.2555 - mape: 8.1587 - val_loss: 0.2548 - val_mse: 0.2457 - val_rmse: 0.4956 - val_mae: 0.2548 - val_mape: 8.1547 - lr: 1.0000e-05\n",
      "Epoch 415/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2552 - mse: 0.2471 - rmse: 0.4971 - mae: 0.2552 - mape: 8.1614\n",
      "Epoch 415: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2556 - mse: 0.2477 - rmse: 0.4977 - mae: 0.2556 - mape: 8.1604 - val_loss: 0.2557 - val_mse: 0.2484 - val_rmse: 0.4984 - val_mae: 0.2557 - val_mape: 8.2272 - lr: 1.0000e-05\n",
      "Epoch 416/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2557 - mse: 0.2489 - rmse: 0.4989 - mae: 0.2557 - mape: 8.1712\n",
      "Epoch 416: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2554 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2554 - mape: 8.1676 - val_loss: 0.2547 - val_mse: 0.2463 - val_rmse: 0.4963 - val_mae: 0.2547 - val_mape: 8.1388 - lr: 1.0000e-05\n",
      "Epoch 417/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2552 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2552 - mape: 8.1631\n",
      "Epoch 417: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2553 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2553 - mape: 8.1742 - val_loss: 0.2558 - val_mse: 0.2458 - val_rmse: 0.4958 - val_mae: 0.2558 - val_mape: 8.1032 - lr: 1.0000e-05\n",
      "Epoch 418/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2553 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2553 - mape: 8.1492\n",
      "Epoch 418: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2553 - mape: 8.1499 - val_loss: 0.2549 - val_mse: 0.2477 - val_rmse: 0.4977 - val_mae: 0.2549 - val_mape: 8.1909 - lr: 1.0000e-05\n",
      "Epoch 419/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2551 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2551 - mape: 8.1665\n",
      "Epoch 419: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2553 - mape: 8.1672 - val_loss: 0.2548 - val_mse: 0.2472 - val_rmse: 0.4972 - val_mae: 0.2548 - val_mape: 8.1743 - lr: 1.0000e-05\n",
      "Epoch 420/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2577 - mse: 0.2529 - rmse: 0.5029 - mae: 0.2577 - mape: 8.2421\n",
      "Epoch 420: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2479 - rmse: 0.4979 - mae: 0.2555 - mape: 8.1685 - val_loss: 0.2549 - val_mse: 0.2457 - val_rmse: 0.4957 - val_mae: 0.2549 - val_mape: 8.1248 - lr: 1.0000e-05\n",
      "Epoch 421/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2556 - mse: 0.2484 - rmse: 0.4984 - mae: 0.2556 - mape: 8.1686\n",
      "Epoch 421: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2477 - rmse: 0.4976 - mae: 0.2553 - mape: 8.1568 - val_loss: 0.2551 - val_mse: 0.2453 - val_rmse: 0.4953 - val_mae: 0.2551 - val_mape: 8.1183 - lr: 1.0000e-05\n",
      "Epoch 422/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2543 - mse: 0.2458 - rmse: 0.4957 - mae: 0.2543 - mape: 8.1490\n",
      "Epoch 422: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2552 - mape: 8.1635 - val_loss: 0.2550 - val_mse: 0.2476 - val_rmse: 0.4976 - val_mae: 0.2550 - val_mape: 8.1319 - lr: 1.0000e-05\n",
      "Epoch 423/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2549 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2549 - mape: 8.1435\n",
      "Epoch 423: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2552 - mape: 8.1640 - val_loss: 0.2548 - val_mse: 0.2458 - val_rmse: 0.4958 - val_mae: 0.2548 - val_mape: 8.1254 - lr: 1.0000e-05\n",
      "Epoch 424/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2554 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2554 - mape: 8.1425\n",
      "Epoch 424: val_loss did not improve from 0.25473\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2552 - mape: 8.1432 - val_loss: 0.2549 - val_mse: 0.2465 - val_rmse: 0.4964 - val_mae: 0.2549 - val_mape: 8.1252 - lr: 1.0000e-05\n",
      "Epoch 425/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2547 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2547 - mape: 8.1558\n",
      "Epoch 425: val_loss improved from 0.25473 to 0.25466, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2478 - rmse: 0.4978 - mae: 0.2552 - mape: 8.1547 - val_loss: 0.2547 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2547 - val_mape: 8.1286 - lr: 1.0000e-05\n",
      "Epoch 426/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2551 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2551 - mape: 8.1659\n",
      "Epoch 426: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2552 - mape: 8.1681 - val_loss: 0.2557 - val_mse: 0.2475 - val_rmse: 0.4975 - val_mae: 0.2557 - val_mape: 8.2294 - lr: 1.0000e-05\n",
      "Epoch 427/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2564 - mse: 0.2478 - rmse: 0.4978 - mae: 0.2564 - mape: 8.1839\n",
      "Epoch 427: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2459 - rmse: 0.4959 - mae: 0.2553 - mape: 8.1507 - val_loss: 0.2554 - val_mse: 0.2478 - val_rmse: 0.4978 - val_mae: 0.2554 - val_mape: 8.1291 - lr: 1.0000e-05\n",
      "Epoch 428/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2549 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2549 - mape: 8.1330\n",
      "Epoch 428: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2551 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2551 - mape: 8.1472 - val_loss: 0.2549 - val_mse: 0.2469 - val_rmse: 0.4969 - val_mae: 0.2549 - val_mape: 8.1367 - lr: 1.0000e-05\n",
      "Epoch 429/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2557 - mse: 0.2489 - rmse: 0.4989 - mae: 0.2557 - mape: 8.1811\n",
      "Epoch 429: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2479 - rmse: 0.4979 - mae: 0.2552 - mape: 8.1702 - val_loss: 0.2550 - val_mse: 0.2457 - val_rmse: 0.4957 - val_mae: 0.2550 - val_mape: 8.1115 - lr: 1.0000e-05\n",
      "Epoch 430/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2549 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2549 - mape: 8.1405\n",
      "Epoch 430: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2552 - mape: 8.1466 - val_loss: 0.2559 - val_mse: 0.2494 - val_rmse: 0.4994 - val_mae: 0.2559 - val_mape: 8.2539 - lr: 1.0000e-05\n",
      "Epoch 431/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2559 - mse: 0.2492 - rmse: 0.4992 - mae: 0.2559 - mape: 8.1704\n",
      "Epoch 431: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2553 - mape: 8.1543 - val_loss: 0.2550 - val_mse: 0.2474 - val_rmse: 0.4974 - val_mae: 0.2550 - val_mape: 8.1956 - lr: 1.0000e-05\n",
      "Epoch 432/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2549 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2549 - mape: 8.1355\n",
      "Epoch 432: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2552 - mape: 8.1532 - val_loss: 0.2551 - val_mse: 0.2457 - val_rmse: 0.4956 - val_mae: 0.2551 - val_mape: 8.1219 - lr: 1.0000e-05\n",
      "Epoch 433/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2557 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2557 - mape: 8.1582\n",
      "Epoch 433: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2555 - mape: 8.1566 - val_loss: 0.2547 - val_mse: 0.2450 - val_rmse: 0.4949 - val_mae: 0.2547 - val_mape: 8.1348 - lr: 1.0000e-05\n",
      "Epoch 434/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2556 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2556 - mape: 8.1585\n",
      "Epoch 434: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2553 - mape: 8.1506 - val_loss: 0.2549 - val_mse: 0.2473 - val_rmse: 0.4973 - val_mae: 0.2549 - val_mape: 8.1704 - lr: 1.0000e-05\n",
      "Epoch 435/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2560 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2560 - mape: 8.1625\n",
      "Epoch 435: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2553 - mape: 8.1433 - val_loss: 0.2554 - val_mse: 0.2482 - val_rmse: 0.4982 - val_mae: 0.2554 - val_mape: 8.2165 - lr: 1.0000e-05\n",
      "Epoch 436/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2553 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2553 - mape: 8.1720\n",
      "Epoch 436: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2551 - mse: 0.2479 - rmse: 0.4979 - mae: 0.2551 - mape: 8.1646 - val_loss: 0.2549 - val_mse: 0.2445 - val_rmse: 0.4945 - val_mae: 0.2549 - val_mape: 8.1023 - lr: 1.0000e-05\n",
      "Epoch 437/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2551 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2551 - mape: 8.1495\n",
      "Epoch 437: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2551 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2551 - mape: 8.1495 - val_loss: 0.2550 - val_mse: 0.2455 - val_rmse: 0.4955 - val_mae: 0.2550 - val_mape: 8.1602 - lr: 1.0000e-05\n",
      "Epoch 438/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2539 - mse: 0.2450 - rmse: 0.4950 - mae: 0.2539 - mape: 8.1251\n",
      "Epoch 438: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2550 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2550 - mape: 8.1493 - val_loss: 0.2547 - val_mse: 0.2453 - val_rmse: 0.4953 - val_mae: 0.2547 - val_mape: 8.1139 - lr: 1.0000e-05\n",
      "Epoch 439/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2544 - mse: 0.2446 - rmse: 0.4945 - mae: 0.2544 - mape: 8.1191\n",
      "Epoch 439: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2553 - mape: 8.1611 - val_loss: 0.2568 - val_mse: 0.2515 - val_rmse: 0.5015 - val_mae: 0.2568 - val_mape: 8.2954 - lr: 1.0000e-05\n",
      "Epoch 440/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2552 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2552 - mape: 8.1579\n",
      "Epoch 440: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2550 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2550 - mape: 8.1495 - val_loss: 0.2550 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2550 - val_mape: 8.1106 - lr: 1.0000e-05\n",
      "Epoch 441/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2552 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2552 - mape: 8.1527\n",
      "Epoch 441: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2552 - mape: 8.1527 - val_loss: 0.2549 - val_mse: 0.2445 - val_rmse: 0.4945 - val_mae: 0.2549 - val_mape: 8.1263 - lr: 1.0000e-05\n",
      "Epoch 442/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2548 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2548 - mape: 8.1281\n",
      "Epoch 442: val_loss did not improve from 0.25466\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2551 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2551 - mape: 8.1431 - val_loss: 0.2557 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2557 - val_mape: 8.0992 - lr: 1.0000e-05\n",
      "Epoch 443/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2558 - mse: 0.2482 - rmse: 0.4982 - mae: 0.2558 - mape: 8.1881\n",
      "Epoch 443: val_loss improved from 0.25466 to 0.25463, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2553 - mape: 8.1513 - val_loss: 0.2546 - val_mse: 0.2464 - val_rmse: 0.4964 - val_mae: 0.2546 - val_mape: 8.1488 - lr: 1.0000e-05\n",
      "Epoch 444/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2552 - mse: 0.2479 - rmse: 0.4979 - mae: 0.2552 - mape: 8.1490\n",
      "Epoch 444: val_loss did not improve from 0.25463\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2552 - mape: 8.1460 - val_loss: 0.2548 - val_mse: 0.2445 - val_rmse: 0.4945 - val_mae: 0.2548 - val_mape: 8.1313 - lr: 1.0000e-05\n",
      "Epoch 445/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2551 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2551 - mape: 8.1505\n",
      "Epoch 445: val_loss improved from 0.25463 to 0.25460, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2471 - rmse: 0.4971 - mae: 0.2553 - mape: 8.1595 - val_loss: 0.2546 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2546 - val_mape: 8.1340 - lr: 1.0000e-05\n",
      "Epoch 446/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2547 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2547 - mape: 8.1080\n",
      "Epoch 446: val_loss did not improve from 0.25460\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2551 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2551 - mape: 8.1475 - val_loss: 0.2548 - val_mse: 0.2453 - val_rmse: 0.4953 - val_mae: 0.2548 - val_mape: 8.1417 - lr: 1.0000e-05\n",
      "Epoch 447/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2554 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2554 - mape: 8.1671\n",
      "Epoch 447: val_loss improved from 0.25460 to 0.25459, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2552 - mape: 8.1567 - val_loss: 0.2546 - val_mse: 0.2456 - val_rmse: 0.4955 - val_mae: 0.2546 - val_mape: 8.1281 - lr: 1.0000e-05\n",
      "Epoch 448/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2560 - mse: 0.2484 - rmse: 0.4984 - mae: 0.2560 - mape: 8.1676\n",
      "Epoch 448: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2552 - mape: 8.1677 - val_loss: 0.2548 - val_mse: 0.2455 - val_rmse: 0.4955 - val_mae: 0.2548 - val_mape: 8.1174 - lr: 1.0000e-05\n",
      "Epoch 449/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2545 - mse: 0.2454 - rmse: 0.4954 - mae: 0.2545 - mape: 8.1258\n",
      "Epoch 449: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2551 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2551 - mape: 8.1428 - val_loss: 0.2557 - val_mse: 0.2475 - val_rmse: 0.4975 - val_mae: 0.2557 - val_mape: 8.2086 - lr: 1.0000e-05\n",
      "Epoch 450/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2558 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2558 - mape: 8.1716\n",
      "Epoch 450: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2552 - mape: 8.1500 - val_loss: 0.2546 - val_mse: 0.2464 - val_rmse: 0.4964 - val_mae: 0.2546 - val_mape: 8.1289 - lr: 1.0000e-05\n",
      "Epoch 451/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2551 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2551 - mape: 8.1514\n",
      "Epoch 451: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2551 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2551 - mape: 8.1514 - val_loss: 0.2547 - val_mse: 0.2469 - val_rmse: 0.4969 - val_mae: 0.2547 - val_mape: 8.1297 - lr: 1.0000e-05\n",
      "Epoch 452/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2555 - mse: 0.2495 - rmse: 0.4995 - mae: 0.2555 - mape: 8.1778\n",
      "Epoch 452: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2482 - rmse: 0.4982 - mae: 0.2553 - mape: 8.1758 - val_loss: 0.2553 - val_mse: 0.2458 - val_rmse: 0.4958 - val_mae: 0.2553 - val_mape: 8.1012 - lr: 1.0000e-05\n",
      "Epoch 453/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2549 - mse: 0.2461 - rmse: 0.4960 - mae: 0.2549 - mape: 8.1375\n",
      "Epoch 453: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2550 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2550 - mape: 8.1420 - val_loss: 0.2546 - val_mse: 0.2456 - val_rmse: 0.4956 - val_mae: 0.2546 - val_mape: 8.1379 - lr: 1.0000e-05\n",
      "Epoch 454/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2555 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2555 - mape: 8.1614\n",
      "Epoch 454: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2552 - mape: 8.1563 - val_loss: 0.2550 - val_mse: 0.2464 - val_rmse: 0.4964 - val_mae: 0.2550 - val_mape: 8.1673 - lr: 1.0000e-05\n",
      "Epoch 455/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2543 - mse: 0.2458 - rmse: 0.4958 - mae: 0.2543 - mape: 8.1330\n",
      "Epoch 455: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2550 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2550 - mape: 8.1554 - val_loss: 0.2548 - val_mse: 0.2457 - val_rmse: 0.4957 - val_mae: 0.2548 - val_mape: 8.1093 - lr: 1.0000e-05\n",
      "Epoch 456/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2552 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2552 - mape: 8.1634\n",
      "Epoch 456: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2550 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2550 - mape: 8.1534 - val_loss: 0.2564 - val_mse: 0.2486 - val_rmse: 0.4986 - val_mae: 0.2564 - val_mape: 8.1147 - lr: 1.0000e-05\n",
      "Epoch 457/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2552 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2552 - mape: 8.1491\n",
      "Epoch 457: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2553 - mape: 8.1539 - val_loss: 0.2548 - val_mse: 0.2469 - val_rmse: 0.4969 - val_mae: 0.2548 - val_mape: 8.1463 - lr: 1.0000e-05\n",
      "Epoch 458/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2551 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2551 - mape: 8.1634\n",
      "Epoch 458: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2553 - mape: 8.1668 - val_loss: 0.2554 - val_mse: 0.2477 - val_rmse: 0.4977 - val_mae: 0.2554 - val_mape: 8.1191 - lr: 1.0000e-05\n",
      "Epoch 459/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2550 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2550 - mape: 8.1642\n",
      "Epoch 459: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2552 - mape: 8.1599 - val_loss: 0.2558 - val_mse: 0.2484 - val_rmse: 0.4984 - val_mae: 0.2558 - val_mape: 8.2274 - lr: 1.0000e-05\n",
      "Epoch 460/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2550 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2550 - mape: 8.1465\n",
      "Epoch 460: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2550 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2550 - mape: 8.1465 - val_loss: 0.2552 - val_mse: 0.2481 - val_rmse: 0.4981 - val_mae: 0.2552 - val_mape: 8.1214 - lr: 1.0000e-05\n",
      "Epoch 461/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2568 - mse: 0.2512 - rmse: 0.5012 - mae: 0.2568 - mape: 8.1889\n",
      "Epoch 461: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2552 - mape: 8.1557 - val_loss: 0.2546 - val_mse: 0.2469 - val_rmse: 0.4968 - val_mae: 0.2546 - val_mape: 8.1475 - lr: 1.0000e-05\n",
      "Epoch 462/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2550 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2550 - mape: 8.1390\n",
      "Epoch 462: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2551 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2551 - mape: 8.1475 - val_loss: 0.2558 - val_mse: 0.2491 - val_rmse: 0.4991 - val_mae: 0.2558 - val_mape: 8.2409 - lr: 1.0000e-05\n",
      "Epoch 463/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2556 - mse: 0.2484 - rmse: 0.4984 - mae: 0.2556 - mape: 8.1690\n",
      "Epoch 463: val_loss improved from 0.25459 to 0.25459, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2552 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2552 - mape: 8.1577 - val_loss: 0.2546 - val_mse: 0.2474 - val_rmse: 0.4974 - val_mae: 0.2546 - val_mape: 8.1450 - lr: 1.0000e-05\n",
      "Epoch 464/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2556 - mse: 0.2482 - rmse: 0.4982 - mae: 0.2556 - mape: 8.1833\n",
      "Epoch 464: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2549 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2549 - mape: 8.1601 - val_loss: 0.2548 - val_mse: 0.2469 - val_rmse: 0.4969 - val_mae: 0.2548 - val_mape: 8.1196 - lr: 1.0000e-05\n",
      "Epoch 465/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2551 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2551 - mape: 8.1715\n",
      "Epoch 465: val_loss did not improve from 0.25459\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2549 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2549 - mape: 8.1560 - val_loss: 0.2550 - val_mse: 0.2466 - val_rmse: 0.4966 - val_mae: 0.2550 - val_mape: 8.1760 - lr: 1.0000e-05\n",
      "Epoch 466/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2542 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2542 - mape: 8.1427\n",
      "Epoch 466: val_loss improved from 0.25459 to 0.25454, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2548 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2548 - mape: 8.1468 - val_loss: 0.2545 - val_mse: 0.2459 - val_rmse: 0.4959 - val_mae: 0.2545 - val_mape: 8.1423 - lr: 1.0000e-06\n",
      "Epoch 467/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2541 - mse: 0.2453 - rmse: 0.4953 - mae: 0.2541 - mape: 8.1312\n",
      "Epoch 467: val_loss improved from 0.25454 to 0.25453, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2463 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1439 - val_loss: 0.2545 - val_mse: 0.2457 - val_rmse: 0.4957 - val_mae: 0.2545 - val_mape: 8.1334 - lr: 1.0000e-06\n",
      "Epoch 468/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2547 - mse: 0.2463 - rmse: 0.4962 - mae: 0.2547 - mape: 8.1391\n",
      "Epoch 468: val_loss improved from 0.25453 to 0.25452, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2546 - mape: 8.1378 - val_loss: 0.2545 - val_mse: 0.2457 - val_rmse: 0.4957 - val_mae: 0.2545 - val_mape: 8.1312 - lr: 1.0000e-06\n",
      "Epoch 469/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2542 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2542 - mape: 8.1217\n",
      "Epoch 469: val_loss did not improve from 0.25452\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2459 - rmse: 0.4959 - mae: 0.2546 - mape: 8.1375 - val_loss: 0.2545 - val_mse: 0.2457 - val_rmse: 0.4957 - val_mae: 0.2545 - val_mape: 8.1271 - lr: 1.0000e-06\n",
      "Epoch 470/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2546 - mse: 0.2461 - rmse: 0.4960 - mae: 0.2546 - mape: 8.1303\n",
      "Epoch 470: val_loss improved from 0.25452 to 0.25451, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2461 - rmse: 0.4960 - mae: 0.2546 - mape: 8.1303 - val_loss: 0.2545 - val_mse: 0.2459 - val_rmse: 0.4959 - val_mae: 0.2545 - val_mape: 8.1371 - lr: 1.0000e-06\n",
      "Epoch 471/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2546 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2546 - mape: 8.1298\n",
      "Epoch 471: val_loss did not improve from 0.25451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2459 - rmse: 0.4959 - mae: 0.2546 - mape: 8.1401 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1326 - lr: 1.0000e-06\n",
      "Epoch 472/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2551 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2551 - mape: 8.1365\n",
      "Epoch 472: val_loss improved from 0.25451 to 0.25451, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2463 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1335 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1370 - lr: 1.0000e-06\n",
      "Epoch 473/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2553 - mse: 0.2477 - rmse: 0.4977 - mae: 0.2553 - mape: 8.1520\n",
      "Epoch 473: val_loss improved from 0.25451 to 0.25451, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1408 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1317 - lr: 1.0000e-06\n",
      "Epoch 474/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2544 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2544 - mape: 8.1244\n",
      "Epoch 474: val_loss did not improve from 0.25451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2546 - mape: 8.1380 - val_loss: 0.2545 - val_mse: 0.2459 - val_rmse: 0.4958 - val_mae: 0.2545 - val_mape: 8.1262 - lr: 1.0000e-06\n",
      "Epoch 475/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2549 - mse: 0.2459 - rmse: 0.4959 - mae: 0.2549 - mape: 8.1295\n",
      "Epoch 475: val_loss did not improve from 0.25451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1363 - val_loss: 0.2545 - val_mse: 0.2459 - val_rmse: 0.4959 - val_mae: 0.2545 - val_mape: 8.1322 - lr: 1.0000e-06\n",
      "Epoch 476/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2542 - mse: 0.2454 - rmse: 0.4953 - mae: 0.2542 - mape: 8.1341\n",
      "Epoch 476: val_loss improved from 0.25451 to 0.25451, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2546 - mape: 8.1344 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1351 - lr: 1.0000e-06\n",
      "Epoch 477/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2552 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2552 - mape: 8.1561\n",
      "Epoch 477: val_loss did not improve from 0.25451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2546 - mape: 8.1419 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1329 - lr: 1.0000e-06\n",
      "Epoch 478/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2554 - mse: 0.2478 - rmse: 0.4978 - mae: 0.2554 - mape: 8.1686\n",
      "Epoch 478: val_loss did not improve from 0.25451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2546 - mape: 8.1361 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1233 - lr: 1.0000e-06\n",
      "Epoch 479/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2546 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1308\n",
      "Epoch 479: val_loss improved from 0.25451 to 0.25450, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1308 - val_loss: 0.2545 - val_mse: 0.2462 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1363 - lr: 1.0000e-06\n",
      "Epoch 480/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2542 - mse: 0.2455 - rmse: 0.4955 - mae: 0.2542 - mape: 8.1456\n",
      "Epoch 480: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2463 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1363 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1316 - lr: 1.0000e-06\n",
      "Epoch 481/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2543 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2543 - mape: 8.1233\n",
      "Epoch 481: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2546 - mape: 8.1403 - val_loss: 0.2545 - val_mse: 0.2462 - val_rmse: 0.4962 - val_mae: 0.2545 - val_mape: 8.1366 - lr: 1.0000e-06\n",
      "Epoch 482/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2541 - mse: 0.2454 - rmse: 0.4953 - mae: 0.2541 - mape: 8.1101\n",
      "Epoch 482: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1332 - val_loss: 0.2545 - val_mse: 0.2462 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1391 - lr: 1.0000e-06\n",
      "Epoch 483/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2547 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2547 - mape: 8.1485\n",
      "Epoch 483: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2546 - mape: 8.1475 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1324 - lr: 1.0000e-06\n",
      "Epoch 484/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2542 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2542 - mape: 8.1423\n",
      "Epoch 484: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2463 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1374 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1373 - lr: 1.0000e-06\n",
      "Epoch 485/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2536 - mse: 0.2441 - rmse: 0.4941 - mae: 0.2536 - mape: 8.1171\n",
      "Epoch 485: val_loss improved from 0.25450 to 0.25450, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2546 - mape: 8.1433 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1352 - lr: 1.0000e-06\n",
      "Epoch 486/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2548 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2548 - mape: 8.1496\n",
      "Epoch 486: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2546 - mape: 8.1405 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1371 - lr: 1.0000e-06\n",
      "Epoch 487/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2543 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2543 - mape: 8.1331\n",
      "Epoch 487: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2546 - mape: 8.1393 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1351 - lr: 1.0000e-06\n",
      "Epoch 488/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2558 - mse: 0.2481 - rmse: 0.4981 - mae: 0.2558 - mape: 8.1583\n",
      "Epoch 488: val_loss improved from 0.25450 to 0.25450, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1407 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1392 - lr: 1.0000e-06\n",
      "Epoch 489/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2551 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2551 - mape: 8.1378\n",
      "Epoch 489: val_loss improved from 0.25450 to 0.25450, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2462 - rmse: 0.4961 - mae: 0.2546 - mape: 8.1392 - val_loss: 0.2545 - val_mse: 0.2462 - val_rmse: 0.4962 - val_mae: 0.2545 - val_mape: 8.1379 - lr: 1.0000e-06\n",
      "Epoch 490/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2564 - mse: 0.2500 - rmse: 0.5000 - mae: 0.2564 - mape: 8.1796\n",
      "Epoch 490: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2546 - mape: 8.1312 - val_loss: 0.2545 - val_mse: 0.2463 - val_rmse: 0.4963 - val_mae: 0.2545 - val_mape: 8.1433 - lr: 1.0000e-06\n",
      "Epoch 491/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2541 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2541 - mape: 8.1366\n",
      "Epoch 491: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2546 - mape: 8.1464 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1346 - lr: 1.0000e-06\n",
      "Epoch 492/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2550 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2550 - mape: 8.1485\n",
      "Epoch 492: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2546 - mape: 8.1342 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1394 - lr: 1.0000e-06\n",
      "Epoch 493/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2544 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2544 - mape: 8.1362\n",
      "Epoch 493: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2546 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2546 - mape: 8.1421 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1339 - lr: 1.0000e-06\n",
      "Epoch 494/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2549 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2549 - mape: 8.1401\n",
      "Epoch 494: val_loss improved from 0.25450 to 0.25450, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2546 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2546 - mape: 8.1362 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1342 - lr: 1.0000e-06\n",
      "Epoch 495/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2555 - mse: 0.2469 - rmse: 0.4968 - mae: 0.2555 - mape: 8.1633\n",
      "Epoch 495: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2461 - rmse: 0.4960 - mae: 0.2546 - mape: 8.1330 - val_loss: 0.2545 - val_mse: 0.2464 - val_rmse: 0.4964 - val_mae: 0.2545 - val_mape: 8.1446 - lr: 1.0000e-06\n",
      "Epoch 496/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2547 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2547 - mape: 8.1485\n",
      "Epoch 496: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2546 - mape: 8.1456 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1358 - lr: 1.0000e-06\n",
      "Epoch 497/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2546 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2546 - mape: 8.1326\n",
      "Epoch 497: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2462 - rmse: 0.4961 - mae: 0.2546 - mape: 8.1338 - val_loss: 0.2545 - val_mse: 0.2463 - val_rmse: 0.4963 - val_mae: 0.2545 - val_mape: 8.1437 - lr: 1.0000e-06\n",
      "Epoch 498/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2552 - mse: 0.2481 - rmse: 0.4981 - mae: 0.2552 - mape: 8.1670\n",
      "Epoch 498: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2546 - mape: 8.1464 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1351 - lr: 1.0000e-06\n",
      "Epoch 499/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2550 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2550 - mape: 8.1523\n",
      "Epoch 499: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2461 - rmse: 0.4960 - mae: 0.2546 - mape: 8.1343 - val_loss: 0.2545 - val_mse: 0.2462 - val_rmse: 0.4962 - val_mae: 0.2545 - val_mape: 8.1389 - lr: 1.0000e-06\n",
      "Epoch 500/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2539 - mse: 0.2443 - rmse: 0.4942 - mae: 0.2539 - mape: 8.1372\n",
      "Epoch 500: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2546 - mape: 8.1437 - val_loss: 0.2545 - val_mse: 0.2459 - val_rmse: 0.4959 - val_mae: 0.2545 - val_mape: 8.1285 - lr: 1.0000e-06\n",
      "Epoch 501/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2559 - mse: 0.2494 - rmse: 0.4994 - mae: 0.2559 - mape: 8.1902\n",
      "Epoch 501: val_loss improved from 0.25450 to 0.25450, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1349 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-06\n",
      "Epoch 502/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2547 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2547 - mape: 8.1439\n",
      "Epoch 502: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2462 - rmse: 0.4961 - mae: 0.2546 - mape: 8.1413 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1342 - lr: 1.0000e-06\n",
      "Epoch 503/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2545 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2545 - mape: 8.1401\n",
      "Epoch 503: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2546 - mape: 8.1400 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1334 - lr: 1.0000e-06\n",
      "Epoch 504/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2553 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2553 - mape: 8.1526\n",
      "Epoch 504: val_loss did not improve from 0.25450\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1398 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1302 - lr: 1.0000e-06\n",
      "Epoch 505/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2538 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2538 - mape: 8.1339\n",
      "Epoch 505: val_loss improved from 0.25450 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1347 - val_loss: 0.2545 - val_mse: 0.2462 - val_rmse: 0.4962 - val_mae: 0.2545 - val_mape: 8.1383 - lr: 1.0000e-06\n",
      "Epoch 506/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2545 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2545 - mape: 8.1209\n",
      "Epoch 506: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2546 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2546 - mape: 8.1405 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1323 - lr: 1.0000e-06\n",
      "Epoch 507/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2547 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2547 - mape: 8.1193\n",
      "Epoch 507: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2545 - mape: 8.1329 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1335 - lr: 1.0000e-07\n",
      "Epoch 508/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2544 - mse: 0.2466 - rmse: 0.4965 - mae: 0.2544 - mape: 8.1330\n",
      "Epoch 508: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2545 - mape: 8.1336 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1340 - lr: 1.0000e-07\n",
      "Epoch 509/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2553 - mse: 0.2458 - rmse: 0.4958 - mae: 0.2553 - mape: 8.1415\n",
      "Epoch 509: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2545 - mape: 8.1343 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1343 - lr: 1.0000e-07\n",
      "Epoch 510/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2549 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2549 - mape: 8.1300\n",
      "Epoch 510: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2545 - mape: 8.1345 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1342 - lr: 1.0000e-07\n",
      "Epoch 511/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2540 - mse: 0.2443 - rmse: 0.4942 - mae: 0.2540 - mape: 8.1282\n",
      "Epoch 511: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1349 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1347 - lr: 1.0000e-07\n",
      "Epoch 512/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2538 - mse: 0.2448 - rmse: 0.4948 - mae: 0.2538 - mape: 8.1096\n",
      "Epoch 512: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2545 - mape: 8.1344 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1347 - lr: 1.0000e-07\n",
      "Epoch 513/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2534 - mse: 0.2440 - rmse: 0.4939 - mae: 0.2534 - mape: 8.0888\n",
      "Epoch 513: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2545 - mape: 8.1339 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1348 - lr: 1.0000e-07\n",
      "Epoch 514/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2540 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2540 - mape: 8.1373\n",
      "Epoch 514: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4960 - mae: 0.2545 - mape: 8.1344 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1350 - lr: 1.0000e-07\n",
      "Epoch 515/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2548 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2548 - mape: 8.1402\n",
      "Epoch 515: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4960 - mae: 0.2545 - mape: 8.1352 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-07\n",
      "Epoch 516/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2542 - mse: 0.2456 - rmse: 0.4956 - mae: 0.2542 - mape: 8.1280\n",
      "Epoch 516: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1364 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-07\n",
      "Epoch 517/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2545 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2545 - mape: 8.1340\n",
      "Epoch 517: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2545 - mape: 8.1340 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1348 - lr: 1.0000e-07\n",
      "Epoch 518/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2545 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2545 - mape: 8.1298\n",
      "Epoch 518: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2545 - mape: 8.1338 - val_loss: 0.2545 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2545 - val_mape: 8.1347 - lr: 1.0000e-07\n",
      "Epoch 519/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2546 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2546 - mape: 8.1380\n",
      "Epoch 519: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1360 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1356 - lr: 1.0000e-07\n",
      "Epoch 520/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2544 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2544 - mape: 8.1337\n",
      "Epoch 520: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1346 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-07\n",
      "Epoch 521/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2535 - mse: 0.2446 - rmse: 0.4946 - mae: 0.2535 - mape: 8.1020\n",
      "Epoch 521: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1359 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1358 - lr: 1.0000e-07\n",
      "Epoch 522/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1368\n",
      "Epoch 522: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1368 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1357 - lr: 1.0000e-07\n",
      "Epoch 523/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2560 - mse: 0.2485 - rmse: 0.4985 - mae: 0.2560 - mape: 8.1704\n",
      "Epoch 523: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1357 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-07\n",
      "Epoch 524/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2548 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2548 - mape: 8.1477\n",
      "Epoch 524: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1351 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1349 - lr: 1.0000e-07\n",
      "Epoch 525/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2546 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1346\n",
      "Epoch 525: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1343 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1356 - lr: 1.0000e-07\n",
      "Epoch 526/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2546 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2546 - mape: 8.1429\n",
      "Epoch 526: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1370 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1357 - lr: 1.0000e-07\n",
      "Epoch 527/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1360\n",
      "Epoch 527: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1360 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1358 - lr: 1.0000e-07\n",
      "Epoch 528/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2541 - mse: 0.2445 - rmse: 0.4945 - mae: 0.2541 - mape: 8.1264\n",
      "Epoch 528: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1366 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1360 - lr: 1.0000e-07\n",
      "Epoch 529/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2537 - mse: 0.2441 - rmse: 0.4941 - mae: 0.2537 - mape: 8.1119\n",
      "Epoch 529: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1358 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1362 - lr: 1.0000e-07\n",
      "Epoch 530/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2546 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1396\n",
      "Epoch 530: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1368 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1363 - lr: 1.0000e-07\n",
      "Epoch 531/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2537 - mse: 0.2453 - rmse: 0.4953 - mae: 0.2537 - mape: 8.1218\n",
      "Epoch 531: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1364 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1360 - lr: 1.0000e-07\n",
      "Epoch 532/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2545 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2545 - mape: 8.1361\n",
      "Epoch 532: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1370 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1358 - lr: 1.0000e-07\n",
      "Epoch 533/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2533 - mse: 0.2426 - rmse: 0.4925 - mae: 0.2533 - mape: 8.1107\n",
      "Epoch 533: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1347 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1359 - lr: 1.0000e-07\n",
      "Epoch 534/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2542 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2542 - mape: 8.1171\n",
      "Epoch 534: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1356 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1358 - lr: 1.0000e-07\n",
      "Epoch 535/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2537 - mse: 0.2447 - rmse: 0.4947 - mae: 0.2537 - mape: 8.1244\n",
      "Epoch 535: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1350 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1360 - lr: 1.0000e-07\n",
      "Epoch 536/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2544 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2544 - mape: 8.1266\n",
      "Epoch 536: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1359 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1349 - lr: 1.0000e-07\n",
      "Epoch 537/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2550 - mse: 0.2469 - rmse: 0.4968 - mae: 0.2550 - mape: 8.1622\n",
      "Epoch 537: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4960 - mae: 0.2545 - mape: 8.1350 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1350 - lr: 1.0000e-07\n",
      "Epoch 538/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2546 - mse: 0.2471 - rmse: 0.4971 - mae: 0.2546 - mape: 8.1337\n",
      "Epoch 538: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1366 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-07\n",
      "Epoch 539/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2544 - mse: 0.2459 - rmse: 0.4959 - mae: 0.2544 - mape: 8.1383\n",
      "Epoch 539: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1357 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1356 - lr: 1.0000e-07\n",
      "Epoch 540/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2551 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2551 - mape: 8.1536\n",
      "Epoch 540: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1359 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1356 - lr: 1.0000e-07\n",
      "Epoch 541/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2545 - mse: 0.2453 - rmse: 0.4953 - mae: 0.2545 - mape: 8.1344\n",
      "Epoch 541: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1368 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1357 - lr: 1.0000e-07\n",
      "Epoch 542/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1350\n",
      "Epoch 542: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1350 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1352 - lr: 1.0000e-07\n",
      "Epoch 543/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2547 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2547 - mape: 8.1315\n",
      "Epoch 543: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1372 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1357 - lr: 1.0000e-07\n",
      "Epoch 544/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2531 - mse: 0.2434 - rmse: 0.4933 - mae: 0.2531 - mape: 8.0859\n",
      "Epoch 544: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1352 - lr: 1.0000e-07\n",
      "Epoch 545/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2545 - mse: 0.2463 - rmse: 0.4962 - mae: 0.2545 - mape: 8.1221\n",
      "Epoch 545: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1367 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1357 - lr: 1.0000e-07\n",
      "Epoch 546/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353\n",
      "Epoch 546: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1352 - lr: 1.0000e-07\n",
      "Epoch 547/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2534 - mse: 0.2442 - rmse: 0.4942 - mae: 0.2534 - mape: 8.0749\n",
      "Epoch 547: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1352 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1352 - lr: 1.0000e-08\n",
      "Epoch 548/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2547 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2547 - mape: 8.1440\n",
      "Epoch 548: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1352 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 549/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2544 - mse: 0.2458 - rmse: 0.4958 - mae: 0.2544 - mape: 8.1394\n",
      "Epoch 549: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 550/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2544 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2544 - mape: 8.1343\n",
      "Epoch 550: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1352 - lr: 1.0000e-08\n",
      "Epoch 551/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2536 - mse: 0.2435 - rmse: 0.4935 - mae: 0.2536 - mape: 8.1090\n",
      "Epoch 551: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 552/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2542 - mse: 0.2451 - rmse: 0.4951 - mae: 0.2542 - mape: 8.1178\n",
      "Epoch 552: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1352 - lr: 1.0000e-08\n",
      "Epoch 553/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2549 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2549 - mape: 8.1444\n",
      "Epoch 553: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1352 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1352 - lr: 1.0000e-08\n",
      "Epoch 554/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1352\n",
      "Epoch 554: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1352 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1352 - lr: 1.0000e-08\n",
      "Epoch 555/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2541 - mse: 0.2444 - rmse: 0.4944 - mae: 0.2541 - mape: 8.1229\n",
      "Epoch 555: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1352 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 556/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2541 - mse: 0.2441 - rmse: 0.4940 - mae: 0.2541 - mape: 8.1104\n",
      "Epoch 556: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 557/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2547 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2547 - mape: 8.1392\n",
      "Epoch 557: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1352 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 558/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2550 - mse: 0.2485 - rmse: 0.4985 - mae: 0.2550 - mape: 8.1616\n",
      "Epoch 558: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1352 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1352 - lr: 1.0000e-08\n",
      "Epoch 559/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2534 - mse: 0.2448 - rmse: 0.4948 - mae: 0.2534 - mape: 8.0824\n",
      "Epoch 559: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1352 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 560/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2541 - mse: 0.2454 - rmse: 0.4953 - mae: 0.2541 - mape: 8.0859\n",
      "Epoch 560: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 561/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353\n",
      "Epoch 561: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 562/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2541 - mse: 0.2444 - rmse: 0.4944 - mae: 0.2541 - mape: 8.1155\n",
      "Epoch 562: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 563/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2545 - mse: 0.2467 - rmse: 0.4966 - mae: 0.2545 - mape: 8.1399\n",
      "Epoch 563: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 564/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2553 - mse: 0.2478 - rmse: 0.4978 - mae: 0.2553 - mape: 8.1541\n",
      "Epoch 564: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-08\n",
      "Epoch 565/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2543 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2543 - mape: 8.1233\n",
      "Epoch 565: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-08\n",
      "Epoch 566/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2543 - mse: 0.2451 - rmse: 0.4951 - mae: 0.2543 - mape: 8.1376\n",
      "Epoch 566: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-08\n",
      "Epoch 567/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2542 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2542 - mape: 8.1306\n",
      "Epoch 567: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 568/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2547 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2547 - mape: 8.1320\n",
      "Epoch 568: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 569/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2538 - mse: 0.2447 - rmse: 0.4946 - mae: 0.2538 - mape: 8.1117\n",
      "Epoch 569: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 570/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2553 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2553 - mape: 8.1651\n",
      "Epoch 570: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 571/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2547 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2547 - mape: 8.1479\n",
      "Epoch 571: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 572/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2542 - mse: 0.2455 - rmse: 0.4954 - mae: 0.2542 - mape: 8.1342\n",
      "Epoch 572: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 573/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2544 - mse: 0.2458 - rmse: 0.4957 - mae: 0.2544 - mape: 8.1397\n",
      "Epoch 573: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-08\n",
      "Epoch 574/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2548 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2548 - mape: 8.1395\n",
      "Epoch 574: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-08\n",
      "Epoch 575/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2542 - mse: 0.2460 - rmse: 0.4959 - mae: 0.2542 - mape: 8.1317\n",
      "Epoch 575: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 576/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2558 - mse: 0.2477 - rmse: 0.4977 - mae: 0.2558 - mape: 8.1868\n",
      "Epoch 576: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1353 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 577/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2541 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2541 - mape: 8.1216\n",
      "Epoch 577: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-08\n",
      "Epoch 578/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2547 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2547 - mape: 8.1403\n",
      "Epoch 578: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-08\n",
      "Epoch 579/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2541 - mse: 0.2449 - rmse: 0.4949 - mae: 0.2541 - mape: 8.1328\n",
      "Epoch 579: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1353 - lr: 1.0000e-08\n",
      "Epoch 580/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2544 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2544 - mape: 8.1457\n",
      "Epoch 580: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-08\n",
      "Epoch 581/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2549 - mse: 0.2476 - rmse: 0.4975 - mae: 0.2549 - mape: 8.1455\n",
      "Epoch 581: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-08\n",
      "Epoch 582/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2545 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2545 - mape: 8.1277\n",
      "Epoch 582: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-08\n",
      "Epoch 583/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2537 - mse: 0.2439 - rmse: 0.4939 - mae: 0.2537 - mape: 8.1065\n",
      "Epoch 583: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-08\n",
      "Epoch 584/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2544 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2544 - mape: 8.1429\n",
      "Epoch 584: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-08\n",
      "Epoch 585/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2545 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2545 - mape: 8.1350\n",
      "Epoch 585: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1354 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1354 - lr: 1.0000e-08\n",
      "Epoch 586/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2549 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2549 - mape: 8.1577\n",
      "Epoch 586: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1356 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-08\n",
      "Epoch 587/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2540 - mse: 0.2441 - rmse: 0.4940 - mae: 0.2540 - mape: 8.1226\n",
      "Epoch 587: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 588/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2546 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2546 - mape: 8.1370\n",
      "Epoch 588: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 589/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2548 - mse: 0.2478 - rmse: 0.4978 - mae: 0.2548 - mape: 8.1420\n",
      "Epoch 589: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 590/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2537 - mse: 0.2442 - rmse: 0.4942 - mae: 0.2537 - mape: 8.1187\n",
      "Epoch 590: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 591/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2535 - mse: 0.2432 - rmse: 0.4931 - mae: 0.2535 - mape: 8.1039\n",
      "Epoch 591: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 592/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355\n",
      "Epoch 592: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 593/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2540 - mse: 0.2446 - rmse: 0.4946 - mae: 0.2540 - mape: 8.1119\n",
      "Epoch 593: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 594/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2557 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2557 - mape: 8.1822\n",
      "Epoch 594: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 595/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2552 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2552 - mape: 8.1477\n",
      "Epoch 595: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 596/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2544 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2544 - mape: 8.1287\n",
      "Epoch 596: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 597/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2546 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2546 - mape: 8.1298\n",
      "Epoch 597: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 598/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1347\n",
      "Epoch 598: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 599/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2543 - mse: 0.2452 - rmse: 0.4951 - mae: 0.2543 - mape: 8.1314\n",
      "Epoch 599: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 600/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2539 - mse: 0.2444 - rmse: 0.4943 - mae: 0.2539 - mape: 8.1121\n",
      "Epoch 600: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 601/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2543 - mse: 0.2453 - rmse: 0.4953 - mae: 0.2543 - mape: 8.1296\n",
      "Epoch 601: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 602/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2547 - mse: 0.2463 - rmse: 0.4962 - mae: 0.2547 - mape: 8.1418\n",
      "Epoch 602: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 603/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2543 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2543 - mape: 8.1240\n",
      "Epoch 603: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 604/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2542 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2542 - mape: 8.1165\n",
      "Epoch 604: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 605/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2538 - mse: 0.2448 - rmse: 0.4948 - mae: 0.2538 - mape: 8.1148\n",
      "Epoch 605: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 606/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2532 - mse: 0.2428 - rmse: 0.4928 - mae: 0.2532 - mape: 8.1010\n",
      "Epoch 606: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 607/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2545 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2545 - mape: 8.1367\n",
      "Epoch 607: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 608/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2541 - mse: 0.2455 - rmse: 0.4955 - mae: 0.2541 - mape: 8.1182\n",
      "Epoch 608: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 609/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2542 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2542 - mape: 8.1289\n",
      "Epoch 609: val_loss improved from 0.25449 to 0.25449, saving model to model_weights/20221121-133943_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 610/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2549 - mse: 0.2474 - rmse: 0.4974 - mae: 0.2549 - mape: 8.1540\n",
      "Epoch 610: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 611/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2535 - mse: 0.2442 - rmse: 0.4942 - mae: 0.2535 - mape: 8.0959\n",
      "Epoch 611: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 612/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2534 - mse: 0.2451 - rmse: 0.4951 - mae: 0.2534 - mape: 8.0812\n",
      "Epoch 612: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 613/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2545 - mse: 0.2454 - rmse: 0.4953 - mae: 0.2545 - mape: 8.1221\n",
      "Epoch 613: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 614/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2548 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2548 - mape: 8.1496\n",
      "Epoch 614: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 615/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2547 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2547 - mape: 8.1386\n",
      "Epoch 615: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 616/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2551 - mse: 0.2477 - rmse: 0.4977 - mae: 0.2551 - mape: 8.1495\n",
      "Epoch 616: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 617/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2545 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2545 - mape: 8.1375\n",
      "Epoch 617: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 618/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2550 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2550 - mape: 8.1462\n",
      "Epoch 618: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 619/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2548 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2548 - mape: 8.1425\n",
      "Epoch 619: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 620/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2542 - mse: 0.2458 - rmse: 0.4958 - mae: 0.2542 - mape: 8.1315\n",
      "Epoch 620: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 621/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2554 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2554 - mape: 8.1451\n",
      "Epoch 621: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 622/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2547 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2547 - mape: 8.1343\n",
      "Epoch 622: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 623/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2549 - mse: 0.2458 - rmse: 0.4958 - mae: 0.2549 - mape: 8.1320\n",
      "Epoch 623: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 624/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2548 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2548 - mape: 8.1470\n",
      "Epoch 624: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 625/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2539 - mse: 0.2454 - rmse: 0.4954 - mae: 0.2539 - mape: 8.1206\n",
      "Epoch 625: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 626/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2544 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2544 - mape: 8.1421\n",
      "Epoch 626: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-09\n",
      "Epoch 627/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2550 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2550 - mape: 8.1533\n",
      "Epoch 627: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 628/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2539 - mse: 0.2443 - rmse: 0.4943 - mae: 0.2539 - mape: 8.1319\n",
      "Epoch 628: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 629/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2550 - mse: 0.2470 - rmse: 0.4969 - mae: 0.2550 - mape: 8.1482\n",
      "Epoch 629: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 630/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2540 - mse: 0.2451 - rmse: 0.4951 - mae: 0.2540 - mape: 8.1159\n",
      "Epoch 630: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 631/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2555 - mse: 0.2485 - rmse: 0.4985 - mae: 0.2555 - mape: 8.1577\n",
      "Epoch 631: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 632/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2556 - mse: 0.2479 - rmse: 0.4979 - mae: 0.2556 - mape: 8.1677\n",
      "Epoch 632: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 633/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2535 - mse: 0.2454 - rmse: 0.4953 - mae: 0.2535 - mape: 8.1226\n",
      "Epoch 633: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 634/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2545 - mse: 0.2469 - rmse: 0.4968 - mae: 0.2545 - mape: 8.1533\n",
      "Epoch 634: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 635/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2537 - mse: 0.2453 - rmse: 0.4952 - mae: 0.2537 - mape: 8.1172\n",
      "Epoch 635: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 636/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2545 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2545 - mape: 8.1322\n",
      "Epoch 636: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 637/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2549 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2549 - mape: 8.1488\n",
      "Epoch 637: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 638/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2545 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2545 - mape: 8.1267\n",
      "Epoch 638: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 639/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2547 - mse: 0.2451 - rmse: 0.4951 - mae: 0.2547 - mape: 8.1468\n",
      "Epoch 639: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 640/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2545 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2545 - mape: 8.1360\n",
      "Epoch 640: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 641/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2541 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2541 - mape: 8.1214\n",
      "Epoch 641: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 642/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2543 - mse: 0.2458 - rmse: 0.4958 - mae: 0.2543 - mape: 8.1300\n",
      "Epoch 642: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 643/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2546 - mse: 0.2463 - rmse: 0.4963 - mae: 0.2546 - mape: 8.1375\n",
      "Epoch 643: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 644/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2553 - mse: 0.2464 - rmse: 0.4963 - mae: 0.2553 - mape: 8.1608\n",
      "Epoch 644: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 645/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2547 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2547 - mape: 8.1428\n",
      "Epoch 645: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 646/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2534 - mse: 0.2443 - rmse: 0.4943 - mae: 0.2534 - mape: 8.1014\n",
      "Epoch 646: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 647/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2529 - mse: 0.2430 - rmse: 0.4930 - mae: 0.2529 - mape: 8.0998\n",
      "Epoch 647: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 648/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2536 - mse: 0.2450 - rmse: 0.4950 - mae: 0.2536 - mape: 8.1042\n",
      "Epoch 648: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 649/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2524 - mse: 0.2415 - rmse: 0.4915 - mae: 0.2524 - mape: 8.0601\n",
      "Epoch 649: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 650/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2544 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2544 - mape: 8.1254\n",
      "Epoch 650: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 651/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2539 - mse: 0.2451 - rmse: 0.4950 - mae: 0.2539 - mape: 8.1262\n",
      "Epoch 651: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 652/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2549 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2549 - mape: 8.1430\n",
      "Epoch 652: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 653/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2548 - mse: 0.2447 - rmse: 0.4946 - mae: 0.2548 - mape: 8.1244\n",
      "Epoch 653: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 654/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2543 - mse: 0.2464 - rmse: 0.4964 - mae: 0.2543 - mape: 8.1407\n",
      "Epoch 654: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 655/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2539 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2539 - mape: 8.1326\n",
      "Epoch 655: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 656/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2543 - mse: 0.2473 - rmse: 0.4972 - mae: 0.2543 - mape: 8.1404\n",
      "Epoch 656: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 657/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2544 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2544 - mape: 8.1407\n",
      "Epoch 657: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 658/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2547 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2547 - mape: 8.1519\n",
      "Epoch 658: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 659/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2544 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2544 - mape: 8.1319\n",
      "Epoch 659: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 660/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2542 - mse: 0.2455 - rmse: 0.4954 - mae: 0.2542 - mape: 8.1245\n",
      "Epoch 660: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 661/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2549 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2549 - mape: 8.1372\n",
      "Epoch 661: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 662/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2545 - mse: 0.2458 - rmse: 0.4958 - mae: 0.2545 - mape: 8.1311\n",
      "Epoch 662: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 663/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2524 - mse: 0.2411 - rmse: 0.4910 - mae: 0.2524 - mape: 8.0727\n",
      "Epoch 663: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 664/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2545 - mse: 0.2462 - rmse: 0.4962 - mae: 0.2545 - mape: 8.1369\n",
      "Epoch 664: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 665/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2544 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2544 - mape: 8.1342\n",
      "Epoch 665: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 666/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2532 - mse: 0.2437 - rmse: 0.4937 - mae: 0.2532 - mape: 8.0806\n",
      "Epoch 666: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-10\n",
      "Epoch 667/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2554 - mse: 0.2484 - rmse: 0.4984 - mae: 0.2554 - mape: 8.1617\n",
      "Epoch 667: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 668/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2539 - mse: 0.2444 - rmse: 0.4944 - mae: 0.2539 - mape: 8.1188\n",
      "Epoch 668: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 669/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2535 - mse: 0.2434 - rmse: 0.4934 - mae: 0.2535 - mape: 8.1071\n",
      "Epoch 669: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 670/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2530 - mse: 0.2434 - rmse: 0.4933 - mae: 0.2530 - mape: 8.0828\n",
      "Epoch 670: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 671/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355\n",
      "Epoch 671: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 672/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2552 - mse: 0.2481 - rmse: 0.4981 - mae: 0.2552 - mape: 8.1664\n",
      "Epoch 672: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 673/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2548 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2548 - mape: 8.1470\n",
      "Epoch 673: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 674/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2542 - mse: 0.2459 - rmse: 0.4959 - mae: 0.2542 - mape: 8.1357\n",
      "Epoch 674: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 675/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2545 - mse: 0.2458 - rmse: 0.4958 - mae: 0.2545 - mape: 8.1403\n",
      "Epoch 675: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 676/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2549 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2549 - mape: 8.1493\n",
      "Epoch 676: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 677/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2542 - mse: 0.2449 - rmse: 0.4948 - mae: 0.2542 - mape: 8.1283\n",
      "Epoch 677: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 678/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2540 - mse: 0.2451 - rmse: 0.4951 - mae: 0.2540 - mape: 8.1070\n",
      "Epoch 678: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 679/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2538 - mse: 0.2449 - rmse: 0.4949 - mae: 0.2538 - mape: 8.1253\n",
      "Epoch 679: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 680/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2545 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2545 - mape: 8.1323\n",
      "Epoch 680: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 681/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2546 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2546 - mape: 8.1441\n",
      "Epoch 681: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 682/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2547 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2547 - mape: 8.1421\n",
      "Epoch 682: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 683/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2541 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2541 - mape: 8.1278\n",
      "Epoch 683: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 684/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2550 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2550 - mape: 8.1490\n",
      "Epoch 684: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 685/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2547 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2547 - mape: 8.1415\n",
      "Epoch 685: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 686/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2552 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2552 - mape: 8.1409\n",
      "Epoch 686: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 687/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2542 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2542 - mape: 8.1413\n",
      "Epoch 687: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 688/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2544 - mse: 0.2459 - rmse: 0.4959 - mae: 0.2544 - mape: 8.1326\n",
      "Epoch 688: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 689/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2559 - mse: 0.2483 - rmse: 0.4983 - mae: 0.2559 - mape: 8.1957\n",
      "Epoch 689: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 690/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2536 - mse: 0.2444 - rmse: 0.4943 - mae: 0.2536 - mape: 8.1324\n",
      "Epoch 690: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 691/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2542 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2542 - mape: 8.1277\n",
      "Epoch 691: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 692/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2543 - mse: 0.2459 - rmse: 0.4958 - mae: 0.2543 - mape: 8.1143\n",
      "Epoch 692: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 693/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2549 - mse: 0.2466 - rmse: 0.4965 - mae: 0.2549 - mape: 8.1434\n",
      "Epoch 693: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 694/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2538 - mse: 0.2446 - rmse: 0.4946 - mae: 0.2538 - mape: 8.0933\n",
      "Epoch 694: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 695/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2546 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2546 - mape: 8.1339\n",
      "Epoch 695: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 696/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2551 - mse: 0.2473 - rmse: 0.4973 - mae: 0.2551 - mape: 8.1588\n",
      "Epoch 696: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 697/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2546 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2546 - mape: 8.1415\n",
      "Epoch 697: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 698/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2546 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2546 - mape: 8.1258\n",
      "Epoch 698: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 699/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2550 - mse: 0.2469 - rmse: 0.4969 - mae: 0.2550 - mape: 8.1503\n",
      "Epoch 699: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 700/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2546 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2546 - mape: 8.1476\n",
      "Epoch 700: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 701/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2546 - mse: 0.2467 - rmse: 0.4967 - mae: 0.2546 - mape: 8.1289\n",
      "Epoch 701: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 702/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2549 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2549 - mape: 8.1581\n",
      "Epoch 702: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 703/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2540 - mse: 0.2452 - rmse: 0.4952 - mae: 0.2540 - mape: 8.1090\n",
      "Epoch 703: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 704/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2550 - mse: 0.2479 - rmse: 0.4979 - mae: 0.2550 - mape: 8.1511\n",
      "Epoch 704: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 705/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2548 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2548 - mape: 8.1416\n",
      "Epoch 705: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 706/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2533 - mse: 0.2430 - rmse: 0.4929 - mae: 0.2533 - mape: 8.1068\n",
      "Epoch 706: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-11\n",
      "Epoch 707/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2549 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2549 - mape: 8.1422\n",
      "Epoch 707: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-12\n",
      "Epoch 708/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2547 - mse: 0.2465 - rmse: 0.4964 - mae: 0.2547 - mape: 8.1356\n",
      "Epoch 708: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-12\n",
      "Epoch 709/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2546 - mse: 0.2466 - rmse: 0.4966 - mae: 0.2546 - mape: 8.1321\n",
      "Epoch 709: val_loss did not improve from 0.25449\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2545 - mse: 0.2461 - rmse: 0.4961 - mae: 0.2545 - mape: 8.1355 - val_loss: 0.2545 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2545 - val_mape: 8.1355 - lr: 1.0000e-12\n",
      "Epoch 709: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f41707fbd00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 1000, batch_size = 64, validation_data = (X_val, y_val), callbacks=[checkpoint_callback, es_callback, tf.keras.callbacks.ReduceLROnPlateau(patience=40)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20221121-133943\n"
     ]
    }
   ],
   "source": [
    "print(date_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models.load_model(f'model_weights/{date_actual}_mlp_best_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 1s 1ms/step - loss: 0.3074 - mse: 0.2642 - rmse: 0.5140 - mae: 0.3074 - mape: 10.6937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3073977828025818,\n",
       " 0.26420512795448303,\n",
       " 0.5140088796615601,\n",
       " 0.3073977828025818,\n",
       " 10.693668365478516]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 683us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.3238037470474958\n",
      "mse:  0.2642050242898822\n",
      "rmse:  0.5140087784171417\n",
      "mae:  0.30739782162258417\n",
      "mape:  0.10693670950678061\n",
      "Error estandar:  0.5099678415178749\n"
     ]
    }
   ],
   "source": [
    "print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "print(\"mse: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"rmse: \", mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"mae: \", mean_absolute_error(y_test, y_pred))\n",
    "print(\"mape: \", mean_absolute_percentage_error(y_test, y_pred))\n",
    "print(\"Error estandar: \", stde(y_test.squeeze(),\n",
    "      y_pred.squeeze(), ddof=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x640 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residuals = y_test - y_pred\n",
    "residuals_std = residuals/residuals.std()\n",
    "\n",
    "y_real_stage = np.array([i[0] for i in y_test])\n",
    "residual_stage = np.array([i[0] for i in residuals])\n",
    "\n",
    "y_real_discharge = np.array([i[-1] for i in y_test])\n",
    "residual_discharge = np.array([i[-1] for i in residuals])\n",
    "\n",
    "\n",
    "figure, ax = plt.subplots(ncols=2, figsize=(20, 8), dpi=80)\n",
    "\n",
    "ax[0].scatter(y_real_stage, residual_stage / residual_stage.std(), label=\"stage residuals\")\n",
    "ax[1].scatter(y_real_discharge, residual_discharge / residual_discharge.std(), label=\"discharge residuals\")\n",
    "ax[0].axhline(y=0.0, color='r', linestyle='-')\n",
    "ax[1].axhline(y=0.0, color='r', linestyle='-')\n",
    "\n",
    "ax[0].set_title(\"Stage residuals\")\n",
    "ax[1].set_title(\"Discharge residuals\")\n",
    "\n",
    "ax[1].set_xlabel(\"Fitted values\")\n",
    "ax[0].set_xlabel(\"Fitted values\")\n",
    "ax[1].set_ylabel(\"Standarized residuals\")\n",
    "ax[0].set_ylabel(\"Standarized residuals\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import normal_ad\n",
    "\n",
    "#figure = sm.qqplot(residual_stage / residual_stage.std(), line ='45', label='stage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = sm.qqplot(residual_discharge / residual_discharge.std(), line='45', label='discharge')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(residual_stage / residual_stage.std(), density=True, bins = 60)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Residual stage');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(residual_discharge / residual_discharge.std(), density=True, bins = 60)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Residual discharge');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.0\n",
      "Hay evidencia de que los residuos no provienen de una distribución normal.\n"
     ]
    }
   ],
   "source": [
    "stat, pval = normal_ad(residual_discharge / residual_discharge.std())\n",
    "print(\"p-value:\", pval)\n",
    "\n",
    "if pval < 0.05:\n",
    "    print(\"Hay evidencia de que los residuos no provienen de una distribución normal.\")\n",
    "else:\n",
    "    print(\"No hay evidencia para rechazar la hipótesis de que los residuos vienen de una distribución normal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x640 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure, ax = plt.subplots(ncols=2, figsize=(20, 8), dpi=80)\n",
    "\n",
    "ax[0].plot(np.arange(len(y_test)), y_test, label=\"Stage real\")\n",
    "ax[0].plot(np.arange(len(y_test)), y_pred, label=\"Stage pred\")\n",
    "\n",
    "ax[0].set_title(\"Stage residuals\")\n",
    "ax[1].set_title(\"Discharge residuals\")\n",
    "\n",
    "ax[1].set_ylabel(\"Values\")\n",
    "ax[0].set_ylabel(\"Values\")\n",
    "ax[1].set_xlabel(\"Time\")\n",
    "ax[0].set_xlabel(\"Time\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79f576286c1276b480d6696ed40f6607e18214e4a2875a618cb5be817ff26007"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
