{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 14:28:23.794279: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 14:28:24.222681: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-23 14:28:24.959755: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nkspartan/miniconda3/envs/tf-gpu/lib/\n",
      "2022-11-23 14:28:24.959842: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nkspartan/miniconda3/envs/tf-gpu/lib/\n",
      "2022-11-23 14:28:24.959847: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/tmp/ipykernel_55335/891238804.py:15: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tools.eval_measures import stde\n",
    "\n",
    "import kerastuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 14:28:25.765798: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 14:28:25.791154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:25.847714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:25.847933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:26.750385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:26.751006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:26.751172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:26.751306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 3783 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the etl info results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>remove_time_features</th>\n",
       "      <th>generic_features</th>\n",
       "      <th>remove_atypical_values</th>\n",
       "      <th>feature_combination</th>\n",
       "      <th>remove_feature_selection</th>\n",
       "      <th>remove_invalid_correlated_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   remove_time_features  generic_features  remove_atypical_values  \\\n",
       "0                 False             False                   False   \n",
       "\n",
       "   feature_combination  remove_feature_selection  \\\n",
       "0                False                     False   \n",
       "\n",
       "   remove_invalid_correlated_features  \n",
       "0                               False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info = pd.read_csv('../dataset_clean/options_csv_v1_etl.csv')\n",
    "df_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SensorTime</th>\n",
       "      <th>CaptureTime</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Agency</th>\n",
       "      <th>SiteNumber</th>\n",
       "      <th>TimeZone</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Discharge</th>\n",
       "      <th>...</th>\n",
       "      <th>WwRawLineMin</th>\n",
       "      <th>WwRawLineMax</th>\n",
       "      <th>WwRawLineMean</th>\n",
       "      <th>WwRawLineSigma</th>\n",
       "      <th>WwCurveLineMin</th>\n",
       "      <th>WwCurveLineMax</th>\n",
       "      <th>WwCurveLineMean</th>\n",
       "      <th>WwCurveLineSigma</th>\n",
       "      <th>RiverArea</th>\n",
       "      <th>RiverWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2012-06-09T13:09:07</td>\n",
       "      <td>statelineweir_20120609_farrell_001.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49975.0</td>\n",
       "      <td>207.508733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2012-06-09T13:10:29</td>\n",
       "      <td>statelineweir_20120609_farrell_002.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50184.0</td>\n",
       "      <td>208.663145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-06-09 13:45:00</td>\n",
       "      <td>2012-06-09T13:44:01</td>\n",
       "      <td>statelineweir_20120609_farrell_003.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.96</td>\n",
       "      <td>873.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50543.0</td>\n",
       "      <td>209.445067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2012-06-09 14:45:00</td>\n",
       "      <td>2012-06-09T14:44:30</td>\n",
       "      <td>statelineweir_20120609_farrell_004.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50856.0</td>\n",
       "      <td>211.265690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-06-09 15:45:00</td>\n",
       "      <td>2012-06-09T15:44:59</td>\n",
       "      <td>statelineweir_20120609_farrell_005.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51004.0</td>\n",
       "      <td>211.250274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42054</th>\n",
       "      <td>42054</td>\n",
       "      <td>42054</td>\n",
       "      <td>2019-10-11 09:00:00</td>\n",
       "      <td>2019-10-11T08:59:53</td>\n",
       "      <td>statelineweir_20191011_farrell_409.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9284.0</td>\n",
       "      <td>77521.0</td>\n",
       "      <td>38385.370066</td>\n",
       "      <td>15952.029728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70085.0</td>\n",
       "      <td>37550.894823</td>\n",
       "      <td>16444.401209</td>\n",
       "      <td>45842.0</td>\n",
       "      <td>194.934605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42055</th>\n",
       "      <td>42055</td>\n",
       "      <td>42055</td>\n",
       "      <td>2019-10-11 10:00:00</td>\n",
       "      <td>2019-10-11T09:59:52</td>\n",
       "      <td>statelineweir_20191011_farrell_410.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10092.0</td>\n",
       "      <td>74614.0</td>\n",
       "      <td>40162.989292</td>\n",
       "      <td>15467.708856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70061.0</td>\n",
       "      <td>39397.339095</td>\n",
       "      <td>16009.008049</td>\n",
       "      <td>42300.0</td>\n",
       "      <td>194.762264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42056</th>\n",
       "      <td>42056</td>\n",
       "      <td>42056</td>\n",
       "      <td>2019-10-11 11:00:00</td>\n",
       "      <td>2019-10-11T10:59:52</td>\n",
       "      <td>statelineweir_20191011_farrell_411.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7067.0</td>\n",
       "      <td>83260.0</td>\n",
       "      <td>42095.946590</td>\n",
       "      <td>16770.357949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76335.0</td>\n",
       "      <td>41350.006568</td>\n",
       "      <td>17489.374617</td>\n",
       "      <td>41080.0</td>\n",
       "      <td>196.480105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42057</th>\n",
       "      <td>42057</td>\n",
       "      <td>42057</td>\n",
       "      <td>2019-10-11 12:00:00</td>\n",
       "      <td>2019-10-11T11:59:53</td>\n",
       "      <td>statelineweir_20191011_farrell_412.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6283.0</td>\n",
       "      <td>83045.0</td>\n",
       "      <td>45345.490954</td>\n",
       "      <td>17498.432849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78882.0</td>\n",
       "      <td>44553.920296</td>\n",
       "      <td>18268.294896</td>\n",
       "      <td>40976.0</td>\n",
       "      <td>193.595245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42058</th>\n",
       "      <td>42058</td>\n",
       "      <td>42058</td>\n",
       "      <td>2019-10-11 12:45:00</td>\n",
       "      <td>2019-10-11T12:59:52</td>\n",
       "      <td>statelineweir_20191011_farrell_413.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7375.0</td>\n",
       "      <td>89813.0</td>\n",
       "      <td>47877.870782</td>\n",
       "      <td>19963.166359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82630.0</td>\n",
       "      <td>47280.270559</td>\n",
       "      <td>20559.358767</td>\n",
       "      <td>41435.0</td>\n",
       "      <td>196.801994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42059 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0           SensorTime          CaptureTime  \\\n",
       "0                 0           0  2012-06-09 13:15:00  2012-06-09T13:09:07   \n",
       "1                 1           1  2012-06-09 13:15:00  2012-06-09T13:10:29   \n",
       "2                 2           2  2012-06-09 13:45:00  2012-06-09T13:44:01   \n",
       "3                 3           3  2012-06-09 14:45:00  2012-06-09T14:44:30   \n",
       "4                 4           4  2012-06-09 15:45:00  2012-06-09T15:44:59   \n",
       "...             ...         ...                  ...                  ...   \n",
       "42054         42054       42054  2019-10-11 09:00:00  2019-10-11T08:59:53   \n",
       "42055         42055       42055  2019-10-11 10:00:00  2019-10-11T09:59:52   \n",
       "42056         42056       42056  2019-10-11 11:00:00  2019-10-11T10:59:52   \n",
       "42057         42057       42057  2019-10-11 12:00:00  2019-10-11T11:59:53   \n",
       "42058         42058       42058  2019-10-11 12:45:00  2019-10-11T12:59:52   \n",
       "\n",
       "                                     Filename Agency  SiteNumber TimeZone  \\\n",
       "0      statelineweir_20120609_farrell_001.jpg   USGS     6674500      MDT   \n",
       "1      statelineweir_20120609_farrell_002.jpg   USGS     6674500      MDT   \n",
       "2      statelineweir_20120609_farrell_003.jpg   USGS     6674500      MDT   \n",
       "3      statelineweir_20120609_farrell_004.jpg   USGS     6674500      MDT   \n",
       "4      statelineweir_20120609_farrell_005.jpg   USGS     6674500      MDT   \n",
       "...                                       ...    ...         ...      ...   \n",
       "42054  statelineweir_20191011_farrell_409.jpg   USGS     6674500      MDT   \n",
       "42055  statelineweir_20191011_farrell_410.jpg   USGS     6674500      MDT   \n",
       "42056  statelineweir_20191011_farrell_411.jpg   USGS     6674500      MDT   \n",
       "42057  statelineweir_20191011_farrell_412.jpg   USGS     6674500      MDT   \n",
       "42058  statelineweir_20191011_farrell_413.jpg   USGS     6674500      MDT   \n",
       "\n",
       "       Stage  Discharge  ... WwRawLineMin  WwRawLineMax  WwRawLineMean  \\\n",
       "0       2.99      916.0  ...          0.0           0.0       0.000000   \n",
       "1       2.99      916.0  ...          0.0           0.0       0.000000   \n",
       "2       2.96      873.0  ...          0.0           0.0       0.000000   \n",
       "3       2.94      846.0  ...          0.0           0.0       0.000000   \n",
       "4       2.94      846.0  ...          0.0           0.0       0.000000   \n",
       "...      ...        ...  ...          ...           ...            ...   \n",
       "42054   2.54      434.0  ...       9284.0       77521.0   38385.370066   \n",
       "42055   2.54      434.0  ...      10092.0       74614.0   40162.989292   \n",
       "42056   2.54      434.0  ...       7067.0       83260.0   42095.946590   \n",
       "42057   2.54      434.0  ...       6283.0       83045.0   45345.490954   \n",
       "42058   2.54      434.0  ...       7375.0       89813.0   47877.870782   \n",
       "\n",
       "       WwRawLineSigma  WwCurveLineMin  WwCurveLineMax  WwCurveLineMean  \\\n",
       "0            0.000000             0.0             0.0         0.000000   \n",
       "1            0.000000             0.0             0.0         0.000000   \n",
       "2            0.000000             0.0             0.0         0.000000   \n",
       "3            0.000000             0.0             0.0         0.000000   \n",
       "4            0.000000             0.0             0.0         0.000000   \n",
       "...               ...             ...             ...              ...   \n",
       "42054    15952.029728             0.0         70085.0     37550.894823   \n",
       "42055    15467.708856             0.0         70061.0     39397.339095   \n",
       "42056    16770.357949             0.0         76335.0     41350.006568   \n",
       "42057    17498.432849             0.0         78882.0     44553.920296   \n",
       "42058    19963.166359             0.0         82630.0     47280.270559   \n",
       "\n",
       "       WwCurveLineSigma  RiverArea  RiverWidth  \n",
       "0              0.000000    49975.0  207.508733  \n",
       "1              0.000000    50184.0  208.663145  \n",
       "2              0.000000    50543.0  209.445067  \n",
       "3              0.000000    50856.0  211.265690  \n",
       "4              0.000000    51004.0  211.250274  \n",
       "...                 ...        ...         ...  \n",
       "42054      16444.401209    45842.0  194.934605  \n",
       "42055      16009.008049    42300.0  194.762264  \n",
       "42056      17489.374617    41080.0  196.480105  \n",
       "42057      18268.294896    40976.0  193.595245  \n",
       "42058      20559.358767    41435.0  196.801994  \n",
       "\n",
       "[42059 rows x 63 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset/V2_PlatteRiverWeir_features_merged_all.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SensorTime'] = pd.to_datetime(df['SensorTime'])\n",
    "df['Year'] = df['SensorTime'].dt.year\n",
    "df['Month'] = df['SensorTime'].dt.month\n",
    "df['date_offset'] = (df.SensorTime.dt.month * 100 + df.SensorTime.dt.day - 320)%1300\n",
    "\n",
    "df['Season'] = pd.cut(df['date_offset'], [0, 300, 602, 900, 1300], \n",
    "                      labels=['spring', 'summer', 'autumn', 'winter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CaptureTime</th>\n",
       "      <th>SensorTime</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Discharge</th>\n",
       "      <th>RiverArea</th>\n",
       "      <th>RiverWidth</th>\n",
       "      <th>Month</th>\n",
       "      <th>Season</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-06-09T13:09:07</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>49975.0</td>\n",
       "      <td>207.508733</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-06-09T13:10:29</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>50184.0</td>\n",
       "      <td>208.663145</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-06-09T13:44:01</td>\n",
       "      <td>2012-06-09 13:45:00</td>\n",
       "      <td>2.96</td>\n",
       "      <td>873.0</td>\n",
       "      <td>50543.0</td>\n",
       "      <td>209.445067</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-06-09T14:44:30</td>\n",
       "      <td>2012-06-09 14:45:00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>50856.0</td>\n",
       "      <td>211.265690</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-06-09T15:44:59</td>\n",
       "      <td>2012-06-09 15:45:00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>51004.0</td>\n",
       "      <td>211.250274</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42054</th>\n",
       "      <td>2019-10-11T08:59:53</td>\n",
       "      <td>2019-10-11 09:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>45842.0</td>\n",
       "      <td>194.934605</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42055</th>\n",
       "      <td>2019-10-11T09:59:52</td>\n",
       "      <td>2019-10-11 10:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>42300.0</td>\n",
       "      <td>194.762264</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42056</th>\n",
       "      <td>2019-10-11T10:59:52</td>\n",
       "      <td>2019-10-11 11:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>41080.0</td>\n",
       "      <td>196.480105</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42057</th>\n",
       "      <td>2019-10-11T11:59:53</td>\n",
       "      <td>2019-10-11 12:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>40976.0</td>\n",
       "      <td>193.595245</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42058</th>\n",
       "      <td>2019-10-11T12:59:52</td>\n",
       "      <td>2019-10-11 12:45:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>41435.0</td>\n",
       "      <td>196.801994</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42059 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               CaptureTime          SensorTime  Stage  Discharge  RiverArea  \\\n",
       "0      2012-06-09T13:09:07 2012-06-09 13:15:00   2.99      916.0    49975.0   \n",
       "1      2012-06-09T13:10:29 2012-06-09 13:15:00   2.99      916.0    50184.0   \n",
       "2      2012-06-09T13:44:01 2012-06-09 13:45:00   2.96      873.0    50543.0   \n",
       "3      2012-06-09T14:44:30 2012-06-09 14:45:00   2.94      846.0    50856.0   \n",
       "4      2012-06-09T15:44:59 2012-06-09 15:45:00   2.94      846.0    51004.0   \n",
       "...                    ...                 ...    ...        ...        ...   \n",
       "42054  2019-10-11T08:59:53 2019-10-11 09:00:00   2.54      434.0    45842.0   \n",
       "42055  2019-10-11T09:59:52 2019-10-11 10:00:00   2.54      434.0    42300.0   \n",
       "42056  2019-10-11T10:59:52 2019-10-11 11:00:00   2.54      434.0    41080.0   \n",
       "42057  2019-10-11T11:59:53 2019-10-11 12:00:00   2.54      434.0    40976.0   \n",
       "42058  2019-10-11T12:59:52 2019-10-11 12:45:00   2.54      434.0    41435.0   \n",
       "\n",
       "       RiverWidth  Month  Season  Year  \n",
       "0      207.508733      6  spring  2012  \n",
       "1      208.663145      6  spring  2012  \n",
       "2      209.445067      6  spring  2012  \n",
       "3      211.265690      6  spring  2012  \n",
       "4      211.250274      6  spring  2012  \n",
       "...           ...    ...     ...   ...  \n",
       "42054  194.934605     10  autumn  2019  \n",
       "42055  194.762264     10  autumn  2019  \n",
       "42056  196.480105     10  autumn  2019  \n",
       "42057  193.595245     10  autumn  2019  \n",
       "42058  196.801994     10  autumn  2019  \n",
       "\n",
       "[42059 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"CaptureTime\", \"SensorTime\", \"Stage\", \"Discharge\", \"RiverArea\", \"RiverWidth\", \"Month\", \"Season\", \"Year\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaptureTime            object\n",
       "SensorTime     datetime64[ns]\n",
       "Stage                 float64\n",
       "Discharge             float64\n",
       "RiverArea             float64\n",
       "RiverWidth            float64\n",
       "Month                   int64\n",
       "Season               category\n",
       "Year                    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40148, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.Stage > 0]\n",
    "df = df[df.Discharge > 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40142, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.RiverWidth > 0]\n",
    "#df = df[df.Discharge > 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.Season != \"winter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaptureTime      0\n",
       "SensorTime       0\n",
       "Stage            0\n",
       "Discharge        0\n",
       "RiverArea        0\n",
       "RiverWidth       0\n",
       "Month            0\n",
       "Season         126\n",
       "Year             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide dataset to X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "df_train = df[(df.Year >= 2012) & (df.Year <= 2016)]\n",
    "df_train = df_train.iloc[np.random.permutation(len(df_train))]\n",
    "\n",
    "df_val = df[(df.Year >= 2017) & (df.Year <= 2017)]\n",
    "df_val = df_val.iloc[np.random.permutation(len(df_val))]\n",
    "\n",
    "df_test = df[(df.Year >= 2018) & (df.Year <= 2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])\n",
    "df_val = df_val.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])\n",
    "df_test = df_test.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = df_train[[\"Stage\"]].values\n",
    "X_train = df_train[[\"RiverWidth\", \"Month\"]].values\n",
    "\n",
    "y_val = df_train[[\"Stage\"]].values\n",
    "X_val = df_train[[\"RiverWidth\", \"Month\"]].values\n",
    "\n",
    "y_test = df_test[[\"Stage\"]].values\n",
    "X_test = df_test[[\"RiverWidth\", \"Month\"]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20304, 2)\n",
      "(20304, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "print(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(lr):\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.Input(shape=input_shape))\n",
    "  \n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(32, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(128, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"tanh\"))\n",
    "  \"\"\"model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(512, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(512, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(128, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(32, activation=\"tanh\"))\"\"\"\n",
    "\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(output_shape, activation = 'linear'))\n",
    "\n",
    "  \n",
    "  model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr), loss = 'mae', metrics = ['mse', tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mae', 'mape'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 14:28:27.944937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:27.945222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:27.945480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:27.946121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:27.946345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:27.946554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:27.946816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:27.947041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 14:28:27.947204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3783 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = model_builder(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date_actual = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/fit/\" + date_actual\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"model_weights/{date_actual}_mlp_best_weights.hdf5\",\n",
    "                               monitor='val_loss',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.5293 - mse: 0.8020 - rmse: 0.8955 - mae: 0.5293 - mape: 16.3519\n",
      "Epoch 1: val_loss improved from inf to 0.51971, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 2s 3ms/step - loss: 0.5263 - mse: 0.7944 - rmse: 0.8913 - mae: 0.5263 - mape: 16.2632 - val_loss: 0.5197 - val_mse: 0.8677 - val_rmse: 0.9315 - val_mae: 0.5197 - val_mape: 14.5941 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.4342 - mse: 0.6550 - rmse: 0.8093 - mae: 0.4342 - mape: 12.8635\n",
      "Epoch 2: val_loss improved from 0.51971 to 0.42282, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4325 - mse: 0.6525 - rmse: 0.8078 - mae: 0.4325 - mape: 12.7992 - val_loss: 0.4228 - val_mse: 0.7012 - val_rmse: 0.8374 - val_mae: 0.4228 - val_mape: 11.7714 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.4133 - mse: 0.6213 - rmse: 0.7882 - mae: 0.4133 - mape: 12.1597\n",
      "Epoch 3: val_loss improved from 0.42282 to 0.39469, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4125 - mse: 0.6187 - rmse: 0.7866 - mae: 0.4125 - mape: 12.1492 - val_loss: 0.3947 - val_mse: 0.5526 - val_rmse: 0.7434 - val_mae: 0.3947 - val_mape: 11.8861 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.4014 - mse: 0.5877 - rmse: 0.7666 - mae: 0.4014 - mape: 11.9703\n",
      "Epoch 4: val_loss improved from 0.39469 to 0.38483, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4014 - mse: 0.5877 - rmse: 0.7666 - mae: 0.4014 - mape: 11.9703 - val_loss: 0.3848 - val_mse: 0.5479 - val_rmse: 0.7402 - val_mae: 0.3848 - val_mape: 11.5521 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.4056 - mse: 0.6091 - rmse: 0.7804 - mae: 0.4056 - mape: 12.0053\n",
      "Epoch 5: val_loss improved from 0.38483 to 0.38234, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4055 - mse: 0.6085 - rmse: 0.7801 - mae: 0.4055 - mape: 12.0188 - val_loss: 0.3823 - val_mse: 0.5244 - val_rmse: 0.7241 - val_mae: 0.3823 - val_mape: 11.6669 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3972 - mse: 0.5803 - rmse: 0.7618 - mae: 0.3972 - mape: 11.9578\n",
      "Epoch 6: val_loss did not improve from 0.38234\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3970 - mse: 0.5800 - rmse: 0.7616 - mae: 0.3970 - mape: 11.9545 - val_loss: 0.3959 - val_mse: 0.5958 - val_rmse: 0.7719 - val_mae: 0.3959 - val_mape: 11.7755 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.3952 - mse: 0.5722 - rmse: 0.7565 - mae: 0.3952 - mape: 11.9862\n",
      "Epoch 7: val_loss improved from 0.38234 to 0.37428, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3949 - mse: 0.5722 - rmse: 0.7564 - mae: 0.3949 - mape: 11.9537 - val_loss: 0.3743 - val_mse: 0.5109 - val_rmse: 0.7148 - val_mae: 0.3743 - val_mape: 11.5171 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.3786 - mse: 0.5143 - rmse: 0.7171 - mae: 0.3786 - mape: 11.7192\n",
      "Epoch 8: val_loss did not improve from 0.37428\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3784 - mse: 0.5136 - rmse: 0.7167 - mae: 0.3784 - mape: 11.7158 - val_loss: 0.3816 - val_mse: 0.4923 - val_rmse: 0.7016 - val_mae: 0.3816 - val_mape: 12.1807 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.3891 - mse: 0.5602 - rmse: 0.7484 - mae: 0.3891 - mape: 11.7948\n",
      "Epoch 9: val_loss improved from 0.37428 to 0.37404, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3901 - mse: 0.5601 - rmse: 0.7484 - mae: 0.3901 - mape: 11.8405 - val_loss: 0.3740 - val_mse: 0.4899 - val_rmse: 0.6999 - val_mae: 0.3740 - val_mape: 11.7615 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.3808 - mse: 0.5281 - rmse: 0.7267 - mae: 0.3808 - mape: 11.7569\n",
      "Epoch 10: val_loss did not improve from 0.37404\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3839 - mse: 0.5383 - rmse: 0.7337 - mae: 0.3839 - mape: 11.8171 - val_loss: 0.4190 - val_mse: 0.7119 - val_rmse: 0.8437 - val_mae: 0.4190 - val_mape: 11.6154 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.3832 - mse: 0.5412 - rmse: 0.7357 - mae: 0.3832 - mape: 11.7117\n",
      "Epoch 11: val_loss improved from 0.37404 to 0.37154, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.3819 - mse: 0.5367 - rmse: 0.7326 - mae: 0.3819 - mape: 11.6847 - val_loss: 0.3715 - val_mse: 0.4957 - val_rmse: 0.7040 - val_mae: 0.3715 - val_mape: 11.4715 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.4228 - mse: 0.6145 - rmse: 0.7839 - mae: 0.4228 - mape: 13.0252\n",
      "Epoch 12: val_loss did not improve from 0.37154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4221 - mse: 0.6134 - rmse: 0.7832 - mae: 0.4221 - mape: 12.9820 - val_loss: 0.3790 - val_mse: 0.5417 - val_rmse: 0.7360 - val_mae: 0.3790 - val_mape: 11.3954 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.3912 - mse: 0.5642 - rmse: 0.7512 - mae: 0.3912 - mape: 11.7718\n",
      "Epoch 13: val_loss did not improve from 0.37154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3903 - mse: 0.5609 - rmse: 0.7490 - mae: 0.3903 - mape: 11.7696 - val_loss: 0.3757 - val_mse: 0.5167 - val_rmse: 0.7188 - val_mae: 0.3757 - val_mape: 11.5731 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.3827 - mse: 0.5270 - rmse: 0.7259 - mae: 0.3827 - mape: 11.7767\n",
      "Epoch 14: val_loss did not improve from 0.37154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3826 - mse: 0.5283 - rmse: 0.7269 - mae: 0.3826 - mape: 11.7697 - val_loss: 0.3728 - val_mse: 0.5184 - val_rmse: 0.7200 - val_mae: 0.3728 - val_mape: 11.3667 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3933 - mse: 0.5764 - rmse: 0.7592 - mae: 0.3933 - mape: 11.4926\n",
      "Epoch 15: val_loss did not improve from 0.37154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3927 - mse: 0.5754 - rmse: 0.7586 - mae: 0.3927 - mape: 11.4786 - val_loss: 0.4065 - val_mse: 0.7127 - val_rmse: 0.8442 - val_mae: 0.4065 - val_mape: 11.2465 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.4156 - mse: 0.6889 - rmse: 0.8300 - mae: 0.4156 - mape: 11.9665\n",
      "Epoch 16: val_loss did not improve from 0.37154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4132 - mse: 0.6799 - rmse: 0.8245 - mae: 0.4132 - mape: 11.9677 - val_loss: 0.4039 - val_mse: 0.6739 - val_rmse: 0.8209 - val_mae: 0.4039 - val_mape: 11.9396 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.4002 - mse: 0.6176 - rmse: 0.7859 - mae: 0.4002 - mape: 11.9522\n",
      "Epoch 17: val_loss did not improve from 0.37154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4001 - mse: 0.6165 - rmse: 0.7852 - mae: 0.4001 - mape: 11.9516 - val_loss: 0.3943 - val_mse: 0.6279 - val_rmse: 0.7924 - val_mae: 0.3943 - val_mape: 11.5913 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.3732 - mse: 0.4953 - rmse: 0.7037 - mae: 0.3732 - mape: 11.5517\n",
      "Epoch 18: val_loss did not improve from 0.37154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3793 - mse: 0.5205 - rmse: 0.7215 - mae: 0.3793 - mape: 11.6208 - val_loss: 0.4230 - val_mse: 0.7502 - val_rmse: 0.8661 - val_mae: 0.4230 - val_mape: 11.9578 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.4147 - mse: 0.6974 - rmse: 0.8351 - mae: 0.4147 - mape: 11.8451\n",
      "Epoch 19: val_loss did not improve from 0.37154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4146 - mse: 0.6986 - rmse: 0.8358 - mae: 0.4146 - mape: 11.8470 - val_loss: 0.4370 - val_mse: 0.7593 - val_rmse: 0.8714 - val_mae: 0.4370 - val_mape: 12.2660 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.3933 - mse: 0.5900 - rmse: 0.7681 - mae: 0.3933 - mape: 11.5235\n",
      "Epoch 20: val_loss improved from 0.37154 to 0.37078, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3928 - mse: 0.5890 - rmse: 0.7675 - mae: 0.3928 - mape: 11.5132 - val_loss: 0.3708 - val_mse: 0.4930 - val_rmse: 0.7021 - val_mae: 0.3708 - val_mape: 11.2707 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.4124 - mse: 0.6640 - rmse: 0.8149 - mae: 0.4124 - mape: 11.9509\n",
      "Epoch 21: val_loss did not improve from 0.37078\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4113 - mse: 0.6608 - rmse: 0.8129 - mae: 0.4113 - mape: 11.9073 - val_loss: 0.3951 - val_mse: 0.5956 - val_rmse: 0.7718 - val_mae: 0.3951 - val_mape: 11.4313 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.3793 - mse: 0.5369 - rmse: 0.7328 - mae: 0.3793 - mape: 11.3844\n",
      "Epoch 22: val_loss did not improve from 0.37078\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3778 - mse: 0.5332 - rmse: 0.7302 - mae: 0.3778 - mape: 11.3591 - val_loss: 0.3999 - val_mse: 0.6385 - val_rmse: 0.7991 - val_mae: 0.3999 - val_mape: 12.1983 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.3412 - mse: 0.4167 - rmse: 0.6456 - mae: 0.3412 - mape: 10.3019\n",
      "Epoch 23: val_loss improved from 0.37078 to 0.31902, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3414 - mse: 0.4173 - rmse: 0.6460 - mae: 0.3414 - mape: 10.3350 - val_loss: 0.3190 - val_mse: 0.3447 - val_rmse: 0.5871 - val_mae: 0.3190 - val_mape: 9.5114 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.3534 - mse: 0.4509 - rmse: 0.6715 - mae: 0.3534 - mape: 10.8426\n",
      "Epoch 24: val_loss did not improve from 0.31902\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3545 - mse: 0.4534 - rmse: 0.6733 - mae: 0.3545 - mape: 10.8890 - val_loss: 0.3440 - val_mse: 0.4085 - val_rmse: 0.6391 - val_mae: 0.3440 - val_mape: 10.9050 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3717 - mse: 0.5273 - rmse: 0.7262 - mae: 0.3717 - mape: 11.1607\n",
      "Epoch 25: val_loss did not improve from 0.31902\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3716 - mse: 0.5272 - rmse: 0.7261 - mae: 0.3716 - mape: 11.1624 - val_loss: 0.3588 - val_mse: 0.4761 - val_rmse: 0.6900 - val_mae: 0.3588 - val_mape: 11.2654 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.3933 - mse: 0.6263 - rmse: 0.7914 - mae: 0.3933 - mape: 11.4496\n",
      "Epoch 26: val_loss did not improve from 0.31902\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3936 - mse: 0.6307 - rmse: 0.7942 - mae: 0.3936 - mape: 11.4595 - val_loss: 0.4454 - val_mse: 0.8725 - val_rmse: 0.9341 - val_mae: 0.4454 - val_mape: 11.9055 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.3511 - mse: 0.4285 - rmse: 0.6546 - mae: 0.3511 - mape: 10.5656\n",
      "Epoch 27: val_loss did not improve from 0.31902\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3484 - mse: 0.4195 - rmse: 0.6477 - mae: 0.3484 - mape: 10.4931 - val_loss: 0.3360 - val_mse: 0.3770 - val_rmse: 0.6140 - val_mae: 0.3360 - val_mape: 10.1166 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.3209 - mse: 0.3451 - rmse: 0.5875 - mae: 0.3209 - mape: 9.8014\n",
      "Epoch 28: val_loss improved from 0.31902 to 0.29737, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3202 - mse: 0.3436 - rmse: 0.5862 - mae: 0.3202 - mape: 9.7838 - val_loss: 0.2974 - val_mse: 0.2893 - val_rmse: 0.5379 - val_mae: 0.2974 - val_mape: 9.0330 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.3408 - mse: 0.3993 - rmse: 0.6319 - mae: 0.3408 - mape: 10.5198\n",
      "Epoch 29: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3405 - mse: 0.3989 - rmse: 0.6316 - mae: 0.3405 - mape: 10.4942 - val_loss: 0.3146 - val_mse: 0.3221 - val_rmse: 0.5676 - val_mae: 0.3146 - val_mape: 10.1328 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.3934 - mse: 0.6012 - rmse: 0.7754 - mae: 0.3934 - mape: 11.6922\n",
      "Epoch 30: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3903 - mse: 0.5876 - rmse: 0.7666 - mae: 0.3903 - mape: 11.6342 - val_loss: 0.3356 - val_mse: 0.3919 - val_rmse: 0.6260 - val_mae: 0.3356 - val_mape: 10.6620 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.3945 - mse: 0.6285 - rmse: 0.7928 - mae: 0.3945 - mape: 11.7731\n",
      "Epoch 31: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3942 - mse: 0.6274 - rmse: 0.7921 - mae: 0.3942 - mape: 11.7666 - val_loss: 0.3933 - val_mse: 0.6553 - val_rmse: 0.8095 - val_mae: 0.3933 - val_mape: 11.3290 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3984 - mse: 0.6199 - rmse: 0.7873 - mae: 0.3984 - mape: 11.9431\n",
      "Epoch 32: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3982 - mse: 0.6183 - rmse: 0.7863 - mae: 0.3982 - mape: 11.9411 - val_loss: 0.3818 - val_mse: 0.5342 - val_rmse: 0.7309 - val_mae: 0.3818 - val_mape: 11.8054 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.3932 - mse: 0.5998 - rmse: 0.7745 - mae: 0.3932 - mape: 11.6645\n",
      "Epoch 33: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3958 - mse: 0.6116 - rmse: 0.7820 - mae: 0.3958 - mape: 11.6642 - val_loss: 0.4071 - val_mse: 0.7088 - val_rmse: 0.8419 - val_mae: 0.4071 - val_mape: 11.2512 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.3896 - mse: 0.6117 - rmse: 0.7821 - mae: 0.3896 - mape: 11.0968\n",
      "Epoch 34: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3879 - mse: 0.6045 - rmse: 0.7775 - mae: 0.3879 - mape: 11.0660 - val_loss: 0.4181 - val_mse: 0.5510 - val_rmse: 0.7423 - val_mae: 0.4181 - val_mape: 12.8031 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.3420 - mse: 0.4034 - rmse: 0.6351 - mae: 0.3420 - mape: 10.2370\n",
      "Epoch 35: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3432 - mse: 0.4066 - rmse: 0.6376 - mae: 0.3432 - mape: 10.2655 - val_loss: 0.3556 - val_mse: 0.3919 - val_rmse: 0.6260 - val_mae: 0.3556 - val_mape: 11.3383 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.3355 - mse: 0.3718 - rmse: 0.6098 - mae: 0.3355 - mape: 10.1413\n",
      "Epoch 36: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3389 - mse: 0.3834 - rmse: 0.6192 - mae: 0.3389 - mape: 10.2352 - val_loss: 0.3336 - val_mse: 0.3778 - val_rmse: 0.6146 - val_mae: 0.3336 - val_mape: 10.3285 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.3271 - mse: 0.3559 - rmse: 0.5966 - mae: 0.3271 - mape: 10.0328\n",
      "Epoch 37: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3263 - mse: 0.3537 - rmse: 0.5947 - mae: 0.3263 - mape: 10.0061 - val_loss: 0.3016 - val_mse: 0.2969 - val_rmse: 0.5448 - val_mae: 0.3016 - val_mape: 9.2737 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.3338 - mse: 0.3856 - rmse: 0.6210 - mae: 0.3338 - mape: 10.1709\n",
      "Epoch 38: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3341 - mse: 0.3859 - rmse: 0.6212 - mae: 0.3341 - mape: 10.1782 - val_loss: 0.4054 - val_mse: 0.6209 - val_rmse: 0.7880 - val_mae: 0.4054 - val_mape: 12.5541 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.3466 - mse: 0.4199 - rmse: 0.6480 - mae: 0.3466 - mape: 10.7565\n",
      "Epoch 39: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3458 - mse: 0.4177 - rmse: 0.6463 - mae: 0.3458 - mape: 10.7295 - val_loss: 0.3546 - val_mse: 0.4056 - val_rmse: 0.6368 - val_mae: 0.3546 - val_mape: 11.0896 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.4179 - mse: 0.7325 - rmse: 0.8558 - mae: 0.4179 - mape: 12.0187\n",
      "Epoch 40: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4181 - mse: 0.7324 - rmse: 0.8558 - mae: 0.4181 - mape: 12.0303 - val_loss: 0.4234 - val_mse: 0.7517 - val_rmse: 0.8670 - val_mae: 0.4234 - val_mape: 12.3637 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.4262 - mse: 0.7657 - rmse: 0.8750 - mae: 0.4262 - mape: 12.0246\n",
      "Epoch 41: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4272 - mse: 0.7683 - rmse: 0.8765 - mae: 0.4272 - mape: 12.0718 - val_loss: 0.4381 - val_mse: 0.7220 - val_rmse: 0.8497 - val_mae: 0.4381 - val_mape: 13.5087 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.4109 - mse: 0.7191 - rmse: 0.8480 - mae: 0.4109 - mape: 11.8121\n",
      "Epoch 42: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4229 - mse: 0.7437 - rmse: 0.8624 - mae: 0.4229 - mape: 12.1043 - val_loss: 0.4956 - val_mse: 0.9180 - val_rmse: 0.9581 - val_mae: 0.4956 - val_mape: 13.6452 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.3688 - mse: 0.4837 - rmse: 0.6955 - mae: 0.3688 - mape: 11.2771\n",
      "Epoch 43: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3683 - mse: 0.4825 - rmse: 0.6947 - mae: 0.3683 - mape: 11.2684 - val_loss: 0.3581 - val_mse: 0.4410 - val_rmse: 0.6641 - val_mae: 0.3581 - val_mape: 11.1870 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.3648 - mse: 0.4653 - rmse: 0.6821 - mae: 0.3648 - mape: 11.2273\n",
      "Epoch 44: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3648 - mse: 0.4674 - rmse: 0.6837 - mae: 0.3648 - mape: 11.2535 - val_loss: 0.3673 - val_mse: 0.4403 - val_rmse: 0.6635 - val_mae: 0.3673 - val_mape: 11.5850 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.3653 - mse: 0.4712 - rmse: 0.6864 - mae: 0.3653 - mape: 11.2667\n",
      "Epoch 45: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3645 - mse: 0.4688 - rmse: 0.6847 - mae: 0.3645 - mape: 11.2485 - val_loss: 0.3457 - val_mse: 0.4106 - val_rmse: 0.6408 - val_mae: 0.3457 - val_mape: 10.8118 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.3737 - mse: 0.5020 - rmse: 0.7085 - mae: 0.3737 - mape: 11.4142\n",
      "Epoch 46: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3754 - mse: 0.5069 - rmse: 0.7119 - mae: 0.3754 - mape: 11.4863 - val_loss: 0.4396 - val_mse: 0.8202 - val_rmse: 0.9056 - val_mae: 0.4396 - val_mape: 12.2553 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.4094 - mse: 0.6492 - rmse: 0.8057 - mae: 0.4094 - mape: 11.7510\n",
      "Epoch 47: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4089 - mse: 0.6545 - rmse: 0.8090 - mae: 0.4089 - mape: 11.6854 - val_loss: 0.4260 - val_mse: 0.7715 - val_rmse: 0.8784 - val_mae: 0.4260 - val_mape: 11.6699 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.3965 - mse: 0.6164 - rmse: 0.7851 - mae: 0.3965 - mape: 11.6603\n",
      "Epoch 48: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3997 - mse: 0.6281 - rmse: 0.7925 - mae: 0.3997 - mape: 11.7007 - val_loss: 0.4202 - val_mse: 0.7564 - val_rmse: 0.8697 - val_mae: 0.4202 - val_mape: 11.5962 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.4051 - mse: 0.6701 - rmse: 0.8186 - mae: 0.4051 - mape: 11.7229\n",
      "Epoch 49: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4026 - mse: 0.6597 - rmse: 0.8122 - mae: 0.4026 - mape: 11.7017 - val_loss: 0.3590 - val_mse: 0.4438 - val_rmse: 0.6662 - val_mae: 0.3590 - val_mape: 11.0532 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.3426 - mse: 0.4136 - rmse: 0.6432 - mae: 0.3426 - mape: 10.3560\n",
      "Epoch 50: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3423 - mse: 0.4145 - rmse: 0.6438 - mae: 0.3423 - mape: 10.3258 - val_loss: 0.3095 - val_mse: 0.3561 - val_rmse: 0.5967 - val_mae: 0.3095 - val_mape: 9.1344 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.3854 - mse: 0.5519 - rmse: 0.7429 - mae: 0.3854 - mape: 11.4950\n",
      "Epoch 51: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3855 - mse: 0.5530 - rmse: 0.7436 - mae: 0.3855 - mape: 11.5001 - val_loss: 0.3960 - val_mse: 0.5331 - val_rmse: 0.7301 - val_mae: 0.3960 - val_mape: 12.3799 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.3801 - mse: 0.5348 - rmse: 0.7313 - mae: 0.3801 - mape: 11.4245\n",
      "Epoch 52: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3789 - mse: 0.5326 - rmse: 0.7298 - mae: 0.3789 - mape: 11.4303 - val_loss: 0.3763 - val_mse: 0.5251 - val_rmse: 0.7246 - val_mae: 0.3763 - val_mape: 11.4098 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.3772 - mse: 0.5225 - rmse: 0.7228 - mae: 0.3772 - mape: 11.4507\n",
      "Epoch 53: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3771 - mse: 0.5224 - rmse: 0.7228 - mae: 0.3771 - mape: 11.4514 - val_loss: 0.3694 - val_mse: 0.5111 - val_rmse: 0.7149 - val_mae: 0.3694 - val_mape: 11.2047 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.3736 - mse: 0.5056 - rmse: 0.7110 - mae: 0.3736 - mape: 11.4042\n",
      "Epoch 54: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3751 - mse: 0.5086 - rmse: 0.7132 - mae: 0.3751 - mape: 11.4737 - val_loss: 0.3698 - val_mse: 0.5184 - val_rmse: 0.7200 - val_mae: 0.3698 - val_mape: 11.0676 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.3716 - mse: 0.5029 - rmse: 0.7091 - mae: 0.3716 - mape: 11.3683\n",
      "Epoch 55: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3709 - mse: 0.5014 - rmse: 0.7081 - mae: 0.3709 - mape: 11.3519 - val_loss: 0.3968 - val_mse: 0.5076 - val_rmse: 0.7125 - val_mae: 0.3968 - val_mape: 12.5329 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.3702 - mse: 0.4906 - rmse: 0.7004 - mae: 0.3702 - mape: 11.3439\n",
      "Epoch 56: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3693 - mse: 0.4891 - rmse: 0.6993 - mae: 0.3693 - mape: 11.3131 - val_loss: 0.3654 - val_mse: 0.4837 - val_rmse: 0.6955 - val_mae: 0.3654 - val_mape: 11.1620 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3723 - mse: 0.4984 - rmse: 0.7059 - mae: 0.3723 - mape: 11.3861\n",
      "Epoch 57: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3722 - mse: 0.4982 - rmse: 0.7058 - mae: 0.3722 - mape: 11.3842 - val_loss: 0.3565 - val_mse: 0.4536 - val_rmse: 0.6735 - val_mae: 0.3565 - val_mape: 11.0232 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.3681 - mse: 0.4800 - rmse: 0.6928 - mae: 0.3681 - mape: 11.3246\n",
      "Epoch 58: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3703 - mse: 0.4893 - rmse: 0.6995 - mae: 0.3703 - mape: 11.3504 - val_loss: 0.3614 - val_mse: 0.4757 - val_rmse: 0.6897 - val_mae: 0.3614 - val_mape: 11.0651 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.3638 - mse: 0.4670 - rmse: 0.6834 - mae: 0.3638 - mape: 11.1715\n",
      "Epoch 59: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3642 - mse: 0.4682 - rmse: 0.6843 - mae: 0.3642 - mape: 11.1726 - val_loss: 0.3706 - val_mse: 0.5003 - val_rmse: 0.7073 - val_mae: 0.3706 - val_mape: 11.1073 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.3845 - mse: 0.5406 - rmse: 0.7352 - mae: 0.3845 - mape: 11.6670\n",
      "Epoch 60: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3829 - mse: 0.5360 - rmse: 0.7321 - mae: 0.3829 - mape: 11.6320 - val_loss: 0.3918 - val_mse: 0.5540 - val_rmse: 0.7443 - val_mae: 0.3918 - val_mape: 11.9674 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.4067 - mse: 0.5235 - rmse: 0.7236 - mae: 0.4067 - mape: 13.0102\n",
      "Epoch 61: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4128 - mse: 0.5344 - rmse: 0.7311 - mae: 0.4128 - mape: 13.2201 - val_loss: 0.5742 - val_mse: 0.7702 - val_rmse: 0.8776 - val_mae: 0.5742 - val_mape: 19.4896 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.5594 - mse: 0.7552 - rmse: 0.8690 - mae: 0.5594 - mape: 19.1682\n",
      "Epoch 62: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.5596 - mse: 0.7553 - rmse: 0.8691 - mae: 0.5596 - mape: 19.1743 - val_loss: 0.5591 - val_mse: 0.7547 - val_rmse: 0.8687 - val_mae: 0.5591 - val_mape: 19.2072 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.5579 - mse: 0.7508 - rmse: 0.8665 - mae: 0.5579 - mape: 19.1386\n",
      "Epoch 63: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.5591 - mse: 0.7545 - rmse: 0.8686 - mae: 0.5591 - mape: 19.1741 - val_loss: 0.5591 - val_mse: 0.7565 - val_rmse: 0.8698 - val_mae: 0.5591 - val_mape: 18.7964 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.4917 - mse: 0.6884 - rmse: 0.8297 - mae: 0.4917 - mape: 15.9918\n",
      "Epoch 64: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4913 - mse: 0.6891 - rmse: 0.8301 - mae: 0.4913 - mape: 15.9685 - val_loss: 0.4174 - val_mse: 0.5622 - val_rmse: 0.7498 - val_mae: 0.4174 - val_mape: 13.1542 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.5295 - mse: 0.7239 - rmse: 0.8508 - mae: 0.5295 - mape: 17.7606\n",
      "Epoch 65: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.5215 - mse: 0.7181 - rmse: 0.8474 - mae: 0.5215 - mape: 17.3837 - val_loss: 0.4323 - val_mse: 0.6977 - val_rmse: 0.8353 - val_mae: 0.4323 - val_mape: 12.0359 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.3847 - mse: 0.5235 - rmse: 0.7235 - mae: 0.3847 - mape: 11.5876\n",
      "Epoch 66: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3848 - mse: 0.5231 - rmse: 0.7232 - mae: 0.3848 - mape: 11.5958 - val_loss: 0.3977 - val_mse: 0.5315 - val_rmse: 0.7290 - val_mae: 0.3977 - val_mape: 11.9081 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.3827 - mse: 0.5182 - rmse: 0.7199 - mae: 0.3827 - mape: 11.6402\n",
      "Epoch 67: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3827 - mse: 0.5182 - rmse: 0.7199 - mae: 0.3827 - mape: 11.6402 - val_loss: 0.3640 - val_mse: 0.4214 - val_rmse: 0.6492 - val_mae: 0.3640 - val_mape: 11.4936 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.4222 - mse: 0.6805 - rmse: 0.8249 - mae: 0.4222 - mape: 12.3103\n",
      "Epoch 68: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4222 - mse: 0.6805 - rmse: 0.8249 - mae: 0.4222 - mape: 12.3103 - val_loss: 0.4009 - val_mse: 0.6718 - val_rmse: 0.8197 - val_mae: 0.4009 - val_mape: 11.6293 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3372 - mse: 0.4288 - rmse: 0.6548 - mae: 0.3372 - mape: 9.9684\n",
      "Epoch 69: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3372 - mse: 0.4287 - rmse: 0.6548 - mae: 0.3372 - mape: 9.9668 - val_loss: 0.3195 - val_mse: 0.3800 - val_rmse: 0.6164 - val_mae: 0.3195 - val_mape: 9.4381 - lr: 1.0000e-04\n",
      "Epoch 70/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.3096 - mse: 0.3579 - rmse: 0.5982 - mae: 0.3096 - mape: 9.2445\n",
      "Epoch 70: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3092 - mse: 0.3583 - rmse: 0.5986 - mae: 0.3092 - mape: 9.2315 - val_loss: 0.3043 - val_mse: 0.3490 - val_rmse: 0.5907 - val_mae: 0.3043 - val_mape: 9.1210 - lr: 1.0000e-04\n",
      "Epoch 71/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.3014 - mse: 0.3437 - rmse: 0.5863 - mae: 0.3014 - mape: 9.0528\n",
      "Epoch 71: val_loss did not improve from 0.29737\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3007 - mse: 0.3429 - rmse: 0.5856 - mae: 0.3007 - mape: 9.0230 - val_loss: 0.2978 - val_mse: 0.3409 - val_rmse: 0.5839 - val_mae: 0.2978 - val_mape: 8.8664 - lr: 1.0000e-04\n",
      "Epoch 72/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2980 - mse: 0.3401 - rmse: 0.5832 - mae: 0.2980 - mape: 8.8956\n",
      "Epoch 72: val_loss improved from 0.29737 to 0.29308, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2969 - mse: 0.3384 - rmse: 0.5817 - mae: 0.2969 - mape: 8.8682 - val_loss: 0.2931 - val_mse: 0.3382 - val_rmse: 0.5816 - val_mae: 0.2931 - val_mape: 8.6552 - lr: 1.0000e-04\n",
      "Epoch 73/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2945 - mse: 0.3366 - rmse: 0.5802 - mae: 0.2945 - mape: 8.7740\n",
      "Epoch 73: val_loss improved from 0.29308 to 0.29273, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2964 - mse: 0.3417 - rmse: 0.5846 - mae: 0.2964 - mape: 8.7928 - val_loss: 0.2927 - val_mse: 0.3376 - val_rmse: 0.5810 - val_mae: 0.2927 - val_mape: 8.6885 - lr: 1.0000e-04\n",
      "Epoch 74/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2935 - mse: 0.3374 - rmse: 0.5808 - mae: 0.2935 - mape: 8.6821\n",
      "Epoch 74: val_loss improved from 0.29273 to 0.28951, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2935 - mse: 0.3375 - rmse: 0.5810 - mae: 0.2935 - mape: 8.6794 - val_loss: 0.2895 - val_mse: 0.3301 - val_rmse: 0.5745 - val_mae: 0.2895 - val_mape: 8.5223 - lr: 1.0000e-04\n",
      "Epoch 75/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2904 - mse: 0.3285 - rmse: 0.5731 - mae: 0.2904 - mape: 8.6059\n",
      "Epoch 75: val_loss improved from 0.28951 to 0.28779, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2905 - mse: 0.3294 - rmse: 0.5740 - mae: 0.2905 - mape: 8.6087 - val_loss: 0.2878 - val_mse: 0.3258 - val_rmse: 0.5708 - val_mae: 0.2878 - val_mape: 8.5445 - lr: 1.0000e-04\n",
      "Epoch 76/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2889 - mse: 0.3245 - rmse: 0.5697 - mae: 0.2889 - mape: 8.5929\n",
      "Epoch 76: val_loss improved from 0.28779 to 0.28749, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2894 - mse: 0.3261 - rmse: 0.5710 - mae: 0.2894 - mape: 8.5952 - val_loss: 0.2875 - val_mse: 0.3228 - val_rmse: 0.5681 - val_mae: 0.2875 - val_mape: 8.4946 - lr: 1.0000e-04\n",
      "Epoch 77/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2908 - mse: 0.3283 - rmse: 0.5730 - mae: 0.2908 - mape: 8.6238\n",
      "Epoch 77: val_loss did not improve from 0.28749\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2909 - mse: 0.3285 - rmse: 0.5731 - mae: 0.2909 - mape: 8.6269 - val_loss: 0.2892 - val_mse: 0.3229 - val_rmse: 0.5682 - val_mae: 0.2892 - val_mape: 8.6643 - lr: 1.0000e-04\n",
      "Epoch 78/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2867 - mse: 0.3193 - rmse: 0.5650 - mae: 0.2867 - mape: 8.5403\n",
      "Epoch 78: val_loss did not improve from 0.28749\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2894 - mse: 0.3258 - rmse: 0.5708 - mae: 0.2894 - mape: 8.5834 - val_loss: 0.2904 - val_mse: 0.3234 - val_rmse: 0.5687 - val_mae: 0.2904 - val_mape: 8.7189 - lr: 1.0000e-04\n",
      "Epoch 79/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2877 - mse: 0.3221 - rmse: 0.5675 - mae: 0.2877 - mape: 8.5017\n",
      "Epoch 79: val_loss did not improve from 0.28749\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2883 - mse: 0.3227 - rmse: 0.5681 - mae: 0.2883 - mape: 8.5518 - val_loss: 0.2930 - val_mse: 0.3356 - val_rmse: 0.5794 - val_mae: 0.2930 - val_mape: 8.5566 - lr: 1.0000e-04\n",
      "Epoch 80/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2866 - mse: 0.3196 - rmse: 0.5654 - mae: 0.2866 - mape: 8.4967\n",
      "Epoch 80: val_loss improved from 0.28749 to 0.28554, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2872 - mse: 0.3211 - rmse: 0.5667 - mae: 0.2872 - mape: 8.5111 - val_loss: 0.2855 - val_mse: 0.3156 - val_rmse: 0.5618 - val_mae: 0.2855 - val_mape: 8.4823 - lr: 1.0000e-04\n",
      "Epoch 81/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2876 - mse: 0.3239 - rmse: 0.5691 - mae: 0.2876 - mape: 8.5150\n",
      "Epoch 81: val_loss improved from 0.28554 to 0.28398, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2876 - mse: 0.3231 - rmse: 0.5684 - mae: 0.2876 - mape: 8.5138 - val_loss: 0.2840 - val_mse: 0.3166 - val_rmse: 0.5627 - val_mae: 0.2840 - val_mape: 8.4063 - lr: 1.0000e-04\n",
      "Epoch 82/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2854 - mse: 0.3166 - rmse: 0.5627 - mae: 0.2854 - mape: 8.4677\n",
      "Epoch 82: val_loss did not improve from 0.28398\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2867 - mse: 0.3182 - rmse: 0.5641 - mae: 0.2867 - mape: 8.5086 - val_loss: 0.2844 - val_mse: 0.3108 - val_rmse: 0.5575 - val_mae: 0.2844 - val_mape: 8.4863 - lr: 1.0000e-04\n",
      "Epoch 83/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2873 - mse: 0.3200 - rmse: 0.5657 - mae: 0.2873 - mape: 8.5260\n",
      "Epoch 83: val_loss improved from 0.28398 to 0.28385, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2881 - mse: 0.3218 - rmse: 0.5672 - mae: 0.2881 - mape: 8.5463 - val_loss: 0.2838 - val_mse: 0.3153 - val_rmse: 0.5615 - val_mae: 0.2838 - val_mape: 8.4063 - lr: 1.0000e-04\n",
      "Epoch 84/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2859 - mse: 0.3201 - rmse: 0.5658 - mae: 0.2859 - mape: 8.4767\n",
      "Epoch 84: val_loss improved from 0.28385 to 0.28229, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2851 - mse: 0.3187 - rmse: 0.5645 - mae: 0.2851 - mape: 8.4450 - val_loss: 0.2823 - val_mse: 0.3101 - val_rmse: 0.5569 - val_mae: 0.2823 - val_mape: 8.3747 - lr: 1.0000e-04\n",
      "Epoch 85/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2854 - mse: 0.3154 - rmse: 0.5616 - mae: 0.2854 - mape: 8.4763\n",
      "Epoch 85: val_loss did not improve from 0.28229\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2849 - mse: 0.3139 - rmse: 0.5603 - mae: 0.2849 - mape: 8.4617 - val_loss: 0.2845 - val_mse: 0.3138 - val_rmse: 0.5602 - val_mae: 0.2845 - val_mape: 8.4185 - lr: 1.0000e-04\n",
      "Epoch 86/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2842 - mse: 0.3129 - rmse: 0.5593 - mae: 0.2842 - mape: 8.4522\n",
      "Epoch 86: val_loss did not improve from 0.28229\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2845 - mse: 0.3129 - rmse: 0.5594 - mae: 0.2845 - mape: 8.4592 - val_loss: 0.2857 - val_mse: 0.3151 - val_rmse: 0.5614 - val_mae: 0.2857 - val_mape: 8.5705 - lr: 1.0000e-04\n",
      "Epoch 87/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2845 - mse: 0.3116 - rmse: 0.5582 - mae: 0.2845 - mape: 8.4773\n",
      "Epoch 87: val_loss improved from 0.28229 to 0.28102, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2847 - mse: 0.3126 - rmse: 0.5591 - mae: 0.2847 - mape: 8.4838 - val_loss: 0.2810 - val_mse: 0.3049 - val_rmse: 0.5522 - val_mae: 0.2810 - val_mape: 8.3379 - lr: 1.0000e-04\n",
      "Epoch 88/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2843 - mse: 0.3099 - rmse: 0.5567 - mae: 0.2843 - mape: 8.4758\n",
      "Epoch 88: val_loss did not improve from 0.28102\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2843 - mse: 0.3099 - rmse: 0.5567 - mae: 0.2843 - mape: 8.4758 - val_loss: 0.2829 - val_mse: 0.3090 - val_rmse: 0.5559 - val_mae: 0.2829 - val_mape: 8.3452 - lr: 1.0000e-04\n",
      "Epoch 89/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2843 - mse: 0.3090 - rmse: 0.5559 - mae: 0.2843 - mape: 8.4803\n",
      "Epoch 89: val_loss improved from 0.28102 to 0.27914, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2846 - mse: 0.3087 - rmse: 0.5556 - mae: 0.2846 - mape: 8.4947 - val_loss: 0.2791 - val_mse: 0.2978 - val_rmse: 0.5457 - val_mae: 0.2791 - val_mape: 8.3419 - lr: 1.0000e-04\n",
      "Epoch 90/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2823 - mse: 0.3036 - rmse: 0.5510 - mae: 0.2823 - mape: 8.4371\n",
      "Epoch 90: val_loss did not improve from 0.27914\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2827 - mse: 0.3039 - rmse: 0.5513 - mae: 0.2827 - mape: 8.4518 - val_loss: 0.2854 - val_mse: 0.3076 - val_rmse: 0.5546 - val_mae: 0.2854 - val_mape: 8.6250 - lr: 1.0000e-04\n",
      "Epoch 91/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2813 - mse: 0.2980 - rmse: 0.5459 - mae: 0.2813 - mape: 8.4299\n",
      "Epoch 91: val_loss improved from 0.27914 to 0.27750, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2808 - mse: 0.2963 - rmse: 0.5443 - mae: 0.2808 - mape: 8.4275 - val_loss: 0.2775 - val_mse: 0.2913 - val_rmse: 0.5398 - val_mae: 0.2775 - val_mape: 8.3493 - lr: 1.0000e-04\n",
      "Epoch 92/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2790 - mse: 0.2885 - rmse: 0.5371 - mae: 0.2790 - mape: 8.4336\n",
      "Epoch 92: val_loss did not improve from 0.27750\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2793 - mse: 0.2900 - rmse: 0.5386 - mae: 0.2793 - mape: 8.4323 - val_loss: 0.2803 - val_mse: 0.2927 - val_rmse: 0.5410 - val_mae: 0.2803 - val_mape: 8.3791 - lr: 1.0000e-04\n",
      "Epoch 93/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2812 - mse: 0.2916 - rmse: 0.5400 - mae: 0.2812 - mape: 8.5228\n",
      "Epoch 93: val_loss did not improve from 0.27750\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2811 - mse: 0.2910 - rmse: 0.5395 - mae: 0.2811 - mape: 8.5251 - val_loss: 0.2788 - val_mse: 0.2863 - val_rmse: 0.5351 - val_mae: 0.2788 - val_mape: 8.5319 - lr: 1.0000e-04\n",
      "Epoch 94/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2803 - mse: 0.2888 - rmse: 0.5374 - mae: 0.2803 - mape: 8.5310\n",
      "Epoch 94: val_loss did not improve from 0.27750\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2799 - mse: 0.2866 - rmse: 0.5354 - mae: 0.2799 - mape: 8.5132 - val_loss: 0.2839 - val_mse: 0.2956 - val_rmse: 0.5437 - val_mae: 0.2839 - val_mape: 8.4997 - lr: 1.0000e-04\n",
      "Epoch 95/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2777 - mse: 0.2793 - rmse: 0.5284 - mae: 0.2777 - mape: 8.4837\n",
      "Epoch 95: val_loss did not improve from 0.27750\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2777 - mse: 0.2792 - rmse: 0.5284 - mae: 0.2777 - mape: 8.4757 - val_loss: 0.2787 - val_mse: 0.2817 - val_rmse: 0.5308 - val_mae: 0.2787 - val_mape: 8.4403 - lr: 1.0000e-04\n",
      "Epoch 96/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2801 - mse: 0.2845 - rmse: 0.5334 - mae: 0.2801 - mape: 8.6064\n",
      "Epoch 96: val_loss improved from 0.27750 to 0.27348, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2793 - mse: 0.2827 - rmse: 0.5317 - mae: 0.2793 - mape: 8.5671 - val_loss: 0.2735 - val_mse: 0.2691 - val_rmse: 0.5187 - val_mae: 0.2735 - val_mape: 8.4080 - lr: 1.0000e-04\n",
      "Epoch 97/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2760 - mse: 0.2749 - rmse: 0.5243 - mae: 0.2760 - mape: 8.5019\n",
      "Epoch 97: val_loss did not improve from 0.27348\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2760 - mse: 0.2749 - rmse: 0.5243 - mae: 0.2760 - mape: 8.5019 - val_loss: 0.2776 - val_mse: 0.2730 - val_rmse: 0.5225 - val_mae: 0.2776 - val_mape: 8.6783 - lr: 1.0000e-04\n",
      "Epoch 98/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2749 - mse: 0.2719 - rmse: 0.5214 - mae: 0.2749 - mape: 8.4957\n",
      "Epoch 98: val_loss did not improve from 0.27348\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2748 - mse: 0.2711 - rmse: 0.5207 - mae: 0.2748 - mape: 8.4959 - val_loss: 0.2752 - val_mse: 0.2685 - val_rmse: 0.5181 - val_mae: 0.2752 - val_mape: 8.5001 - lr: 1.0000e-04\n",
      "Epoch 99/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2778 - mse: 0.2754 - rmse: 0.5248 - mae: 0.2778 - mape: 8.6087\n",
      "Epoch 99: val_loss improved from 0.27348 to 0.27238, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2769 - mse: 0.2738 - rmse: 0.5232 - mae: 0.2769 - mape: 8.5794 - val_loss: 0.2724 - val_mse: 0.2644 - val_rmse: 0.5142 - val_mae: 0.2724 - val_mape: 8.4474 - lr: 1.0000e-04\n",
      "Epoch 100/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2736 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2736 - mape: 8.5129\n",
      "Epoch 100: val_loss did not improve from 0.27238\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2732 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2732 - mape: 8.4827 - val_loss: 0.2729 - val_mse: 0.2689 - val_rmse: 0.5186 - val_mae: 0.2729 - val_mape: 8.5049 - lr: 1.0000e-04\n",
      "Epoch 101/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2755 - mse: 0.2723 - rmse: 0.5218 - mae: 0.2755 - mape: 8.5652\n",
      "Epoch 101: val_loss improved from 0.27238 to 0.27198, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2756 - mse: 0.2718 - rmse: 0.5214 - mae: 0.2756 - mape: 8.5670 - val_loss: 0.2720 - val_mse: 0.2661 - val_rmse: 0.5159 - val_mae: 0.2720 - val_mape: 8.4861 - lr: 1.0000e-04\n",
      "Epoch 102/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2753 - mse: 0.2723 - rmse: 0.5218 - mae: 0.2753 - mape: 8.5462\n",
      "Epoch 102: val_loss did not improve from 0.27198\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2751 - mse: 0.2715 - rmse: 0.5211 - mae: 0.2751 - mape: 8.5618 - val_loss: 0.2891 - val_mse: 0.2956 - val_rmse: 0.5437 - val_mae: 0.2891 - val_mape: 8.7915 - lr: 1.0000e-04\n",
      "Epoch 103/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2754 - mse: 0.2698 - rmse: 0.5195 - mae: 0.2754 - mape: 8.6022\n",
      "Epoch 103: val_loss improved from 0.27198 to 0.27022, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2751 - mse: 0.2699 - rmse: 0.5195 - mae: 0.2751 - mape: 8.5796 - val_loss: 0.2702 - val_mse: 0.2629 - val_rmse: 0.5127 - val_mae: 0.2702 - val_mape: 8.4315 - lr: 1.0000e-04\n",
      "Epoch 104/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2735 - mse: 0.2668 - rmse: 0.5166 - mae: 0.2735 - mape: 8.5593\n",
      "Epoch 104: val_loss did not improve from 0.27022\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2731 - mse: 0.2660 - rmse: 0.5157 - mae: 0.2731 - mape: 8.5451 - val_loss: 0.2775 - val_mse: 0.2737 - val_rmse: 0.5231 - val_mae: 0.2775 - val_mape: 8.8200 - lr: 1.0000e-04\n",
      "Epoch 105/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2741 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2741 - mape: 8.6191\n",
      "Epoch 105: val_loss did not improve from 0.27022\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2730 - mse: 0.2660 - rmse: 0.5158 - mae: 0.2730 - mape: 8.5600 - val_loss: 0.2775 - val_mse: 0.2741 - val_rmse: 0.5235 - val_mae: 0.2775 - val_mape: 8.8582 - lr: 1.0000e-04\n",
      "Epoch 106/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2741 - mse: 0.2685 - rmse: 0.5182 - mae: 0.2741 - mape: 8.5930\n",
      "Epoch 106: val_loss did not improve from 0.27022\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2740 - mse: 0.2682 - rmse: 0.5179 - mae: 0.2740 - mape: 8.5926 - val_loss: 0.2706 - val_mse: 0.2613 - val_rmse: 0.5111 - val_mae: 0.2706 - val_mape: 8.5504 - lr: 1.0000e-04\n",
      "Epoch 107/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2719 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2719 - mape: 8.5329\n",
      "Epoch 107: val_loss did not improve from 0.27022\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2733 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2733 - mape: 8.5746 - val_loss: 0.2711 - val_mse: 0.2629 - val_rmse: 0.5128 - val_mae: 0.2711 - val_mape: 8.4796 - lr: 1.0000e-04\n",
      "Epoch 108/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2746 - mse: 0.2694 - rmse: 0.5190 - mae: 0.2746 - mape: 8.6149\n",
      "Epoch 108: val_loss did not improve from 0.27022\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2746 - mse: 0.2696 - rmse: 0.5192 - mae: 0.2746 - mape: 8.6129 - val_loss: 0.2708 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2708 - val_mape: 8.5433 - lr: 1.0000e-04\n",
      "Epoch 109/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2736 - mse: 0.2685 - rmse: 0.5182 - mae: 0.2736 - mape: 8.6147\n",
      "Epoch 109: val_loss did not improve from 0.27022\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2735 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2735 - mape: 8.5958 - val_loss: 0.2750 - val_mse: 0.2619 - val_rmse: 0.5117 - val_mae: 0.2750 - val_mape: 8.7100 - lr: 1.0000e-04\n",
      "Epoch 110/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2727 - mse: 0.2653 - rmse: 0.5150 - mae: 0.2727 - mape: 8.5993\n",
      "Epoch 110: val_loss did not improve from 0.27022\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2728 - mse: 0.2660 - rmse: 0.5157 - mae: 0.2728 - mape: 8.5993 - val_loss: 0.2748 - val_mse: 0.2656 - val_rmse: 0.5153 - val_mae: 0.2748 - val_mape: 8.6380 - lr: 1.0000e-04\n",
      "Epoch 111/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2738 - mse: 0.2663 - rmse: 0.5161 - mae: 0.2738 - mape: 8.6135\n",
      "Epoch 111: val_loss did not improve from 0.27022\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2727 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2727 - mape: 8.5847 - val_loss: 0.2778 - val_mse: 0.2759 - val_rmse: 0.5253 - val_mae: 0.2778 - val_mape: 8.5308 - lr: 1.0000e-04\n",
      "Epoch 112/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2730 - mse: 0.2660 - rmse: 0.5157 - mae: 0.2730 - mape: 8.5980\n",
      "Epoch 112: val_loss did not improve from 0.27022\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2729 - mse: 0.2661 - rmse: 0.5159 - mae: 0.2729 - mape: 8.5915 - val_loss: 0.2744 - val_mse: 0.2703 - val_rmse: 0.5199 - val_mae: 0.2744 - val_mape: 8.4990 - lr: 1.0000e-04\n",
      "Epoch 113/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2720 - mse: 0.2660 - rmse: 0.5158 - mae: 0.2720 - mape: 8.5472\n",
      "Epoch 113: val_loss did not improve from 0.27022\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2732 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2732 - mape: 8.5906 - val_loss: 0.2872 - val_mse: 0.2925 - val_rmse: 0.5408 - val_mae: 0.2872 - val_mape: 8.7670 - lr: 1.0000e-04\n",
      "Epoch 114/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2751 - mse: 0.2711 - rmse: 0.5207 - mae: 0.2751 - mape: 8.6693\n",
      "Epoch 114: val_loss did not improve from 0.27022\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2749 - mse: 0.2715 - rmse: 0.5211 - mae: 0.2749 - mape: 8.6710 - val_loss: 0.2714 - val_mse: 0.2621 - val_rmse: 0.5119 - val_mae: 0.2714 - val_mape: 8.4407 - lr: 1.0000e-04\n",
      "Epoch 115/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2718 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2718 - mape: 8.5549\n",
      "Epoch 115: val_loss did not improve from 0.27022\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2717 - mse: 0.2628 - rmse: 0.5127 - mae: 0.2717 - mape: 8.5581 - val_loss: 0.2714 - val_mse: 0.2637 - val_rmse: 0.5135 - val_mae: 0.2714 - val_mape: 8.4286 - lr: 1.0000e-04\n",
      "Epoch 116/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2715 - mse: 0.2631 - rmse: 0.5129 - mae: 0.2715 - mape: 8.5572\n",
      "Epoch 116: val_loss improved from 0.27022 to 0.26833, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2718 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2718 - mape: 8.5619 - val_loss: 0.2683 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2683 - val_mape: 8.4208 - lr: 1.0000e-04\n",
      "Epoch 117/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2735 - mse: 0.2679 - rmse: 0.5176 - mae: 0.2735 - mape: 8.5891\n",
      "Epoch 117: val_loss did not improve from 0.26833\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2737 - mse: 0.2688 - rmse: 0.5184 - mae: 0.2737 - mape: 8.5984 - val_loss: 0.2696 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2696 - val_mape: 8.4394 - lr: 1.0000e-04\n",
      "Epoch 118/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2724 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2724 - mape: 8.6064\n",
      "Epoch 118: val_loss did not improve from 0.26833\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2719 - mse: 0.2639 - rmse: 0.5137 - mae: 0.2719 - mape: 8.5823 - val_loss: 0.2702 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2702 - val_mape: 8.6159 - lr: 1.0000e-04\n",
      "Epoch 119/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2726 - mse: 0.2663 - rmse: 0.5160 - mae: 0.2726 - mape: 8.5929\n",
      "Epoch 119: val_loss did not improve from 0.26833\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2730 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2730 - mape: 8.6143 - val_loss: 0.2754 - val_mse: 0.2698 - val_rmse: 0.5194 - val_mae: 0.2754 - val_mape: 8.7594 - lr: 1.0000e-04\n",
      "Epoch 120/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2716 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2716 - mape: 8.5921\n",
      "Epoch 120: val_loss did not improve from 0.26833\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2721 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2721 - mape: 8.5893 - val_loss: 0.2894 - val_mse: 0.3025 - val_rmse: 0.5500 - val_mae: 0.2894 - val_mape: 9.3321 - lr: 1.0000e-04\n",
      "Epoch 121/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2721 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2721 - mape: 8.5795\n",
      "Epoch 121: val_loss improved from 0.26833 to 0.26784, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2723 - mse: 0.2642 - rmse: 0.5140 - mae: 0.2723 - mape: 8.5897 - val_loss: 0.2678 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2678 - val_mape: 8.4375 - lr: 1.0000e-04\n",
      "Epoch 122/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2731 - mse: 0.2662 - rmse: 0.5160 - mae: 0.2731 - mape: 8.6140\n",
      "Epoch 122: val_loss did not improve from 0.26784\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2732 - mse: 0.2664 - rmse: 0.5161 - mae: 0.2732 - mape: 8.6128 - val_loss: 0.2683 - val_mse: 0.2578 - val_rmse: 0.5078 - val_mae: 0.2683 - val_mape: 8.4994 - lr: 1.0000e-04\n",
      "Epoch 123/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2725 - mse: 0.2657 - rmse: 0.5154 - mae: 0.2725 - mape: 8.6056\n",
      "Epoch 123: val_loss did not improve from 0.26784\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2723 - mse: 0.2660 - rmse: 0.5157 - mae: 0.2723 - mape: 8.5993 - val_loss: 0.2685 - val_mse: 0.2563 - val_rmse: 0.5062 - val_mae: 0.2685 - val_mape: 8.4476 - lr: 1.0000e-04\n",
      "Epoch 124/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2736 - mse: 0.2688 - rmse: 0.5184 - mae: 0.2736 - mape: 8.6224\n",
      "Epoch 124: val_loss did not improve from 0.26784\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2734 - mse: 0.2682 - rmse: 0.5179 - mae: 0.2734 - mape: 8.6162 - val_loss: 0.2704 - val_mse: 0.2617 - val_rmse: 0.5116 - val_mae: 0.2704 - val_mape: 8.4491 - lr: 1.0000e-04\n",
      "Epoch 125/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2746 - mse: 0.2688 - rmse: 0.5184 - mae: 0.2746 - mape: 8.6677\n",
      "Epoch 125: val_loss improved from 0.26784 to 0.26746, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2736 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2736 - mape: 8.6343 - val_loss: 0.2675 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2675 - val_mape: 8.4768 - lr: 1.0000e-04\n",
      "Epoch 126/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2723 - mse: 0.2663 - rmse: 0.5160 - mae: 0.2723 - mape: 8.5914\n",
      "Epoch 126: val_loss did not improve from 0.26746\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2713 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2713 - mape: 8.5613 - val_loss: 0.2771 - val_mse: 0.2720 - val_rmse: 0.5215 - val_mae: 0.2771 - val_mape: 8.6895 - lr: 1.0000e-04\n",
      "Epoch 127/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2723 - mse: 0.2654 - rmse: 0.5152 - mae: 0.2723 - mape: 8.6139\n",
      "Epoch 127: val_loss did not improve from 0.26746\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2714 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2714 - mape: 8.5828 - val_loss: 0.2699 - val_mse: 0.2606 - val_rmse: 0.5104 - val_mae: 0.2699 - val_mape: 8.4025 - lr: 1.0000e-04\n",
      "Epoch 128/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2728 - mse: 0.2677 - rmse: 0.5174 - mae: 0.2728 - mape: 8.6236\n",
      "Epoch 128: val_loss did not improve from 0.26746\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2731 - mse: 0.2683 - rmse: 0.5180 - mae: 0.2731 - mape: 8.6352 - val_loss: 0.2951 - val_mse: 0.3164 - val_rmse: 0.5625 - val_mae: 0.2951 - val_mape: 9.6089 - lr: 1.0000e-04\n",
      "Epoch 129/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2727 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2727 - mape: 8.6327\n",
      "Epoch 129: val_loss did not improve from 0.26746\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2725 - mse: 0.2663 - rmse: 0.5160 - mae: 0.2725 - mape: 8.6205 - val_loss: 0.2731 - val_mse: 0.2672 - val_rmse: 0.5169 - val_mae: 0.2731 - val_mape: 8.4831 - lr: 1.0000e-04\n",
      "Epoch 130/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2705 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2705 - mape: 8.5501\n",
      "Epoch 130: val_loss did not improve from 0.26746\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2705 - mse: 0.2629 - rmse: 0.5127 - mae: 0.2705 - mape: 8.5630 - val_loss: 0.2691 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2691 - val_mape: 8.4296 - lr: 1.0000e-04\n",
      "Epoch 131/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2706 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2706 - mape: 8.5659\n",
      "Epoch 131: val_loss did not improve from 0.26746\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2709 - mse: 0.2652 - rmse: 0.5149 - mae: 0.2709 - mape: 8.5619 - val_loss: 0.2734 - val_mse: 0.2674 - val_rmse: 0.5171 - val_mae: 0.2734 - val_mape: 8.7628 - lr: 1.0000e-04\n",
      "Epoch 132/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2699 - mse: 0.2597 - rmse: 0.5097 - mae: 0.2699 - mape: 8.5552\n",
      "Epoch 132: val_loss improved from 0.26746 to 0.26711, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2705 - mse: 0.2614 - rmse: 0.5112 - mae: 0.2705 - mape: 8.5616 - val_loss: 0.2671 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2671 - val_mape: 8.4298 - lr: 1.0000e-04\n",
      "Epoch 133/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2736 - mse: 0.2683 - rmse: 0.5180 - mae: 0.2736 - mape: 8.6646\n",
      "Epoch 133: val_loss did not improve from 0.26711\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2735 - mse: 0.2681 - rmse: 0.5178 - mae: 0.2735 - mape: 8.6618 - val_loss: 0.2692 - val_mse: 0.2586 - val_rmse: 0.5086 - val_mae: 0.2692 - val_mape: 8.3886 - lr: 1.0000e-04\n",
      "Epoch 134/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2740 - mse: 0.2701 - rmse: 0.5197 - mae: 0.2740 - mape: 8.6281\n",
      "Epoch 134: val_loss did not improve from 0.26711\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2741 - mse: 0.2704 - rmse: 0.5200 - mae: 0.2741 - mape: 8.6374 - val_loss: 0.2891 - val_mse: 0.3005 - val_rmse: 0.5482 - val_mae: 0.2891 - val_mape: 9.4753 - lr: 1.0000e-04\n",
      "Epoch 135/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2696 - mse: 0.2617 - rmse: 0.5115 - mae: 0.2696 - mape: 8.5305\n",
      "Epoch 135: val_loss improved from 0.26711 to 0.26701, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2696 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2696 - mape: 8.5441 - val_loss: 0.2670 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2670 - val_mape: 8.4447 - lr: 1.0000e-04\n",
      "Epoch 136/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2715 - mse: 0.2670 - rmse: 0.5168 - mae: 0.2715 - mape: 8.5963\n",
      "Epoch 136: val_loss improved from 0.26701 to 0.26677, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2714 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2714 - mape: 8.5881 - val_loss: 0.2668 - val_mse: 0.2579 - val_rmse: 0.5079 - val_mae: 0.2668 - val_mape: 8.3953 - lr: 1.0000e-04\n",
      "Epoch 137/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2726 - mse: 0.2658 - rmse: 0.5155 - mae: 0.2726 - mape: 8.6224\n",
      "Epoch 137: val_loss did not improve from 0.26677\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2722 - mse: 0.2655 - rmse: 0.5153 - mae: 0.2722 - mape: 8.6190 - val_loss: 0.2677 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2677 - val_mape: 8.5313 - lr: 1.0000e-04\n",
      "Epoch 138/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2710 - mse: 0.2660 - rmse: 0.5157 - mae: 0.2710 - mape: 8.5791\n",
      "Epoch 138: val_loss did not improve from 0.26677\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2706 - mse: 0.2650 - rmse: 0.5148 - mae: 0.2706 - mape: 8.5695 - val_loss: 0.2743 - val_mse: 0.2744 - val_rmse: 0.5239 - val_mae: 0.2743 - val_mape: 8.8750 - lr: 1.0000e-04\n",
      "Epoch 139/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2710 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2710 - mape: 8.5947\n",
      "Epoch 139: val_loss improved from 0.26677 to 0.26645, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2702 - mse: 0.2621 - rmse: 0.5119 - mae: 0.2702 - mape: 8.5783 - val_loss: 0.2665 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2665 - val_mape: 8.4134 - lr: 1.0000e-04\n",
      "Epoch 140/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2724 - mse: 0.2682 - rmse: 0.5179 - mae: 0.2724 - mape: 8.6097\n",
      "Epoch 140: val_loss did not improve from 0.26645\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2726 - mse: 0.2685 - rmse: 0.5181 - mae: 0.2726 - mape: 8.6167 - val_loss: 0.2681 - val_mse: 0.2608 - val_rmse: 0.5107 - val_mae: 0.2681 - val_mape: 8.6028 - lr: 1.0000e-04\n",
      "Epoch 141/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2711 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2711 - mape: 8.6265\n",
      "Epoch 141: val_loss did not improve from 0.26645\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2710 - mse: 0.2659 - rmse: 0.5156 - mae: 0.2710 - mape: 8.6139 - val_loss: 0.2701 - val_mse: 0.2667 - val_rmse: 0.5164 - val_mae: 0.2701 - val_mape: 8.6975 - lr: 1.0000e-04\n",
      "Epoch 142/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2728 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2728 - mape: 8.6289\n",
      "Epoch 142: val_loss did not improve from 0.26645\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2706 - mse: 0.2636 - rmse: 0.5135 - mae: 0.2706 - mape: 8.5599 - val_loss: 0.2667 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2667 - val_mape: 8.4512 - lr: 1.0000e-04\n",
      "Epoch 143/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2718 - mse: 0.2687 - rmse: 0.5184 - mae: 0.2718 - mape: 8.6259\n",
      "Epoch 143: val_loss did not improve from 0.26645\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2714 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2714 - mape: 8.6173 - val_loss: 0.2804 - val_mse: 0.2822 - val_rmse: 0.5312 - val_mae: 0.2804 - val_mape: 8.6582 - lr: 1.0000e-04\n",
      "Epoch 144/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2710 - mse: 0.2682 - rmse: 0.5179 - mae: 0.2710 - mape: 8.6169\n",
      "Epoch 144: val_loss improved from 0.26645 to 0.26639, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2715 - mse: 0.2692 - rmse: 0.5188 - mae: 0.2715 - mape: 8.6407 - val_loss: 0.2664 - val_mse: 0.2562 - val_rmse: 0.5062 - val_mae: 0.2664 - val_mape: 8.3554 - lr: 1.0000e-04\n",
      "Epoch 145/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2723 - mse: 0.2694 - rmse: 0.5191 - mae: 0.2723 - mape: 8.6177\n",
      "Epoch 145: val_loss did not improve from 0.26639\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2718 - mse: 0.2684 - rmse: 0.5181 - mae: 0.2718 - mape: 8.6111 - val_loss: 0.2676 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2676 - val_mape: 8.4932 - lr: 1.0000e-04\n",
      "Epoch 146/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2730 - mse: 0.2689 - rmse: 0.5186 - mae: 0.2730 - mape: 8.6561\n",
      "Epoch 146: val_loss did not improve from 0.26639\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2730 - mse: 0.2678 - rmse: 0.5174 - mae: 0.2730 - mape: 8.6399 - val_loss: 0.2732 - val_mse: 0.2677 - val_rmse: 0.5174 - val_mae: 0.2732 - val_mape: 8.5764 - lr: 1.0000e-04\n",
      "Epoch 147/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2708 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2708 - mape: 8.5966\n",
      "Epoch 147: val_loss did not improve from 0.26639\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2705 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2705 - mape: 8.5884 - val_loss: 0.2832 - val_mse: 0.2835 - val_rmse: 0.5324 - val_mae: 0.2832 - val_mape: 8.7882 - lr: 1.0000e-04\n",
      "Epoch 148/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2706 - mse: 0.2635 - rmse: 0.5134 - mae: 0.2706 - mape: 8.5792\n",
      "Epoch 148: val_loss did not improve from 0.26639\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2704 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2704 - mape: 8.5733 - val_loss: 0.2695 - val_mse: 0.2655 - val_rmse: 0.5152 - val_mae: 0.2695 - val_mape: 8.7023 - lr: 1.0000e-04\n",
      "Epoch 149/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2690 - mse: 0.2640 - rmse: 0.5138 - mae: 0.2690 - mape: 8.5203\n",
      "Epoch 149: val_loss did not improve from 0.26639\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2695 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2695 - mape: 8.5395 - val_loss: 0.2790 - val_mse: 0.2776 - val_rmse: 0.5269 - val_mae: 0.2790 - val_mape: 8.5746 - lr: 1.0000e-04\n",
      "Epoch 150/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2717 - mse: 0.2683 - rmse: 0.5180 - mae: 0.2717 - mape: 8.6011\n",
      "Epoch 150: val_loss did not improve from 0.26639\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2718 - mse: 0.2684 - rmse: 0.5181 - mae: 0.2718 - mape: 8.6065 - val_loss: 0.2688 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2688 - val_mape: 8.5486 - lr: 1.0000e-04\n",
      "Epoch 151/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2709 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2709 - mape: 8.5945\n",
      "Epoch 151: val_loss did not improve from 0.26639\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2703 - mse: 0.2663 - rmse: 0.5160 - mae: 0.2703 - mape: 8.5799 - val_loss: 0.2684 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2684 - val_mape: 8.4274 - lr: 1.0000e-04\n",
      "Epoch 152/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2707 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2707 - mape: 8.6012\n",
      "Epoch 152: val_loss improved from 0.26639 to 0.26630, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2709 - mse: 0.2677 - rmse: 0.5173 - mae: 0.2709 - mape: 8.6008 - val_loss: 0.2663 - val_mse: 0.2578 - val_rmse: 0.5077 - val_mae: 0.2663 - val_mape: 8.5011 - lr: 1.0000e-04\n",
      "Epoch 153/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2694 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2694 - mape: 8.5227\n",
      "Epoch 153: val_loss did not improve from 0.26630\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2694 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2694 - mape: 8.5227 - val_loss: 0.2772 - val_mse: 0.2764 - val_rmse: 0.5257 - val_mae: 0.2772 - val_mape: 8.5359 - lr: 1.0000e-04\n",
      "Epoch 154/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2742 - mse: 0.2722 - rmse: 0.5217 - mae: 0.2742 - mape: 8.6991\n",
      "Epoch 154: val_loss did not improve from 0.26630\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2732 - mse: 0.2706 - rmse: 0.5201 - mae: 0.2732 - mape: 8.6567 - val_loss: 0.2748 - val_mse: 0.2735 - val_rmse: 0.5230 - val_mae: 0.2748 - val_mape: 8.5400 - lr: 1.0000e-04\n",
      "Epoch 155/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2694 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2694 - mape: 8.5886\n",
      "Epoch 155: val_loss did not improve from 0.26630\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2692 - mse: 0.2640 - rmse: 0.5138 - mae: 0.2692 - mape: 8.5783 - val_loss: 0.2721 - val_mse: 0.2688 - val_rmse: 0.5185 - val_mae: 0.2721 - val_mape: 8.5185 - lr: 1.0000e-04\n",
      "Epoch 156/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2714 - mse: 0.2688 - rmse: 0.5184 - mae: 0.2714 - mape: 8.6314\n",
      "Epoch 156: val_loss did not improve from 0.26630\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2707 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2707 - mape: 8.6116 - val_loss: 0.2746 - val_mse: 0.2735 - val_rmse: 0.5229 - val_mae: 0.2746 - val_mape: 8.5550 - lr: 1.0000e-04\n",
      "Epoch 157/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2727 - mse: 0.2724 - rmse: 0.5219 - mae: 0.2727 - mape: 8.6583\n",
      "Epoch 157: val_loss did not improve from 0.26630\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2726 - mse: 0.2722 - rmse: 0.5218 - mae: 0.2726 - mape: 8.6630 - val_loss: 0.2737 - val_mse: 0.2711 - val_rmse: 0.5207 - val_mae: 0.2737 - val_mape: 8.4426 - lr: 1.0000e-04\n",
      "Epoch 158/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2688 - mse: 0.2618 - rmse: 0.5116 - mae: 0.2688 - mape: 8.5520\n",
      "Epoch 158: val_loss improved from 0.26630 to 0.26596, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2689 - mse: 0.2624 - rmse: 0.5122 - mae: 0.2689 - mape: 8.5522 - val_loss: 0.2660 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2660 - val_mape: 8.4365 - lr: 1.0000e-04\n",
      "Epoch 159/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2700 - mse: 0.2657 - rmse: 0.5155 - mae: 0.2700 - mape: 8.5699\n",
      "Epoch 159: val_loss improved from 0.26596 to 0.26527, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2700 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2700 - mape: 8.5675 - val_loss: 0.2653 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2653 - val_mape: 8.4996 - lr: 1.0000e-04\n",
      "Epoch 160/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2688 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2688 - mape: 8.5723\n",
      "Epoch 160: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2691 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2691 - mape: 8.5727 - val_loss: 0.2724 - val_mse: 0.2676 - val_rmse: 0.5173 - val_mae: 0.2724 - val_mape: 8.4245 - lr: 1.0000e-04\n",
      "Epoch 161/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2692 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2692 - mape: 8.5555\n",
      "Epoch 161: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2691 - mse: 0.2643 - rmse: 0.5141 - mae: 0.2691 - mape: 8.5522 - val_loss: 0.2767 - val_mse: 0.2829 - val_rmse: 0.5319 - val_mae: 0.2767 - val_mape: 8.9594 - lr: 1.0000e-04\n",
      "Epoch 162/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2703 - mse: 0.2657 - rmse: 0.5155 - mae: 0.2703 - mape: 8.5666\n",
      "Epoch 162: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2706 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2706 - mape: 8.5887 - val_loss: 0.2761 - val_mse: 0.2835 - val_rmse: 0.5324 - val_mae: 0.2761 - val_mape: 8.9425 - lr: 1.0000e-04\n",
      "Epoch 163/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2700 - mse: 0.2666 - rmse: 0.5163 - mae: 0.2700 - mape: 8.5719\n",
      "Epoch 163: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2710 - mse: 0.2682 - rmse: 0.5179 - mae: 0.2710 - mape: 8.6035 - val_loss: 0.2762 - val_mse: 0.2745 - val_rmse: 0.5239 - val_mae: 0.2762 - val_mape: 8.5256 - lr: 1.0000e-04\n",
      "Epoch 164/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2705 - mse: 0.2687 - rmse: 0.5184 - mae: 0.2705 - mape: 8.5963\n",
      "Epoch 164: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2712 - mse: 0.2700 - rmse: 0.5196 - mae: 0.2712 - mape: 8.6027 - val_loss: 0.2656 - val_mse: 0.2587 - val_rmse: 0.5087 - val_mae: 0.2656 - val_mape: 8.5174 - lr: 1.0000e-04\n",
      "Epoch 165/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2684 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2684 - mape: 8.5407\n",
      "Epoch 165: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2682 - mse: 0.2631 - rmse: 0.5129 - mae: 0.2682 - mape: 8.5353 - val_loss: 0.2664 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2664 - val_mape: 8.3516 - lr: 1.0000e-04\n",
      "Epoch 166/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2732 - mse: 0.2727 - rmse: 0.5222 - mae: 0.2732 - mape: 8.6796\n",
      "Epoch 166: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2724 - mse: 0.2714 - rmse: 0.5209 - mae: 0.2724 - mape: 8.6463 - val_loss: 0.2698 - val_mse: 0.2716 - val_rmse: 0.5212 - val_mae: 0.2698 - val_mape: 8.6955 - lr: 1.0000e-04\n",
      "Epoch 167/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2689 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2689 - mape: 8.5309\n",
      "Epoch 167: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2702 - mse: 0.2681 - rmse: 0.5178 - mae: 0.2702 - mape: 8.5761 - val_loss: 0.2671 - val_mse: 0.2594 - val_rmse: 0.5094 - val_mae: 0.2671 - val_mape: 8.5467 - lr: 1.0000e-04\n",
      "Epoch 168/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2704 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2704 - mape: 8.6197\n",
      "Epoch 168: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2707 - mse: 0.2688 - rmse: 0.5184 - mae: 0.2707 - mape: 8.6260 - val_loss: 0.2786 - val_mse: 0.2858 - val_rmse: 0.5346 - val_mae: 0.2786 - val_mape: 9.0471 - lr: 1.0000e-04\n",
      "Epoch 169/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2698 - mse: 0.2663 - rmse: 0.5160 - mae: 0.2698 - mape: 8.5650\n",
      "Epoch 169: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2698 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2698 - mape: 8.5658 - val_loss: 0.2760 - val_mse: 0.2787 - val_rmse: 0.5279 - val_mae: 0.2760 - val_mape: 8.9389 - lr: 1.0000e-04\n",
      "Epoch 170/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2709 - mse: 0.2669 - rmse: 0.5166 - mae: 0.2709 - mape: 8.6113\n",
      "Epoch 170: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2713 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2713 - mape: 8.6121 - val_loss: 0.2872 - val_mse: 0.3014 - val_rmse: 0.5490 - val_mae: 0.2872 - val_mape: 9.4682 - lr: 1.0000e-04\n",
      "Epoch 171/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2704 - mse: 0.2671 - rmse: 0.5169 - mae: 0.2704 - mape: 8.5868\n",
      "Epoch 171: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2700 - mse: 0.2669 - rmse: 0.5166 - mae: 0.2700 - mape: 8.5777 - val_loss: 0.2672 - val_mse: 0.2599 - val_rmse: 0.5098 - val_mae: 0.2672 - val_mape: 8.4482 - lr: 1.0000e-04\n",
      "Epoch 172/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2705 - mse: 0.2679 - rmse: 0.5176 - mae: 0.2705 - mape: 8.6142\n",
      "Epoch 172: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2705 - mse: 0.2677 - rmse: 0.5174 - mae: 0.2705 - mape: 8.6119 - val_loss: 0.2668 - val_mse: 0.2560 - val_rmse: 0.5059 - val_mae: 0.2668 - val_mape: 8.4394 - lr: 1.0000e-04\n",
      "Epoch 173/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2696 - mse: 0.2656 - rmse: 0.5153 - mae: 0.2696 - mape: 8.5665\n",
      "Epoch 173: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2698 - mse: 0.2669 - rmse: 0.5167 - mae: 0.2698 - mape: 8.5872 - val_loss: 0.2678 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2678 - val_mape: 8.4168 - lr: 1.0000e-04\n",
      "Epoch 174/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2711 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2711 - mape: 8.6133\n",
      "Epoch 174: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2713 - mse: 0.2684 - rmse: 0.5181 - mae: 0.2713 - mape: 8.6055 - val_loss: 0.2659 - val_mse: 0.2606 - val_rmse: 0.5104 - val_mae: 0.2659 - val_mape: 8.4969 - lr: 1.0000e-04\n",
      "Epoch 175/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2682 - mse: 0.2650 - rmse: 0.5147 - mae: 0.2682 - mape: 8.5305\n",
      "Epoch 175: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2688 - mse: 0.2664 - rmse: 0.5161 - mae: 0.2688 - mape: 8.5461 - val_loss: 0.2669 - val_mse: 0.2641 - val_rmse: 0.5139 - val_mae: 0.2669 - val_mape: 8.5562 - lr: 1.0000e-04\n",
      "Epoch 176/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2688 - mse: 0.2652 - rmse: 0.5150 - mae: 0.2688 - mape: 8.5376\n",
      "Epoch 176: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2684 - mse: 0.2648 - rmse: 0.5145 - mae: 0.2684 - mape: 8.5383 - val_loss: 0.2659 - val_mse: 0.2568 - val_rmse: 0.5068 - val_mae: 0.2659 - val_mape: 8.4784 - lr: 1.0000e-04\n",
      "Epoch 177/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2708 - mse: 0.2686 - rmse: 0.5183 - mae: 0.2708 - mape: 8.6100\n",
      "Epoch 177: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2707 - mse: 0.2689 - rmse: 0.5186 - mae: 0.2707 - mape: 8.6117 - val_loss: 0.2672 - val_mse: 0.2599 - val_rmse: 0.5098 - val_mae: 0.2672 - val_mape: 8.3225 - lr: 1.0000e-04\n",
      "Epoch 178/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2695 - mse: 0.2662 - rmse: 0.5159 - mae: 0.2695 - mape: 8.5785\n",
      "Epoch 178: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2696 - mse: 0.2666 - rmse: 0.5163 - mae: 0.2696 - mape: 8.5825 - val_loss: 0.3040 - val_mse: 0.3314 - val_rmse: 0.5757 - val_mae: 0.3040 - val_mape: 9.1419 - lr: 1.0000e-04\n",
      "Epoch 179/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2700 - mse: 0.2704 - rmse: 0.5200 - mae: 0.2700 - mape: 8.6040\n",
      "Epoch 179: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2701 - mse: 0.2704 - rmse: 0.5200 - mae: 0.2701 - mape: 8.5884 - val_loss: 0.2667 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2667 - val_mape: 8.3612 - lr: 1.0000e-04\n",
      "Epoch 180/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2674 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2674 - mape: 8.5087\n",
      "Epoch 180: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2684 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2684 - mape: 8.5413 - val_loss: 0.2663 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2663 - val_mape: 8.5563 - lr: 1.0000e-04\n",
      "Epoch 181/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2692 - mse: 0.2661 - rmse: 0.5159 - mae: 0.2692 - mape: 8.5483\n",
      "Epoch 181: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2692 - mse: 0.2660 - rmse: 0.5158 - mae: 0.2692 - mape: 8.5503 - val_loss: 0.2733 - val_mse: 0.2697 - val_rmse: 0.5194 - val_mae: 0.2733 - val_mape: 8.4410 - lr: 1.0000e-04\n",
      "Epoch 182/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2716 - mse: 0.2706 - rmse: 0.5202 - mae: 0.2716 - mape: 8.6182\n",
      "Epoch 182: val_loss did not improve from 0.26527\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2714 - mse: 0.2702 - rmse: 0.5198 - mae: 0.2714 - mape: 8.6068 - val_loss: 0.2678 - val_mse: 0.2628 - val_rmse: 0.5126 - val_mae: 0.2678 - val_mape: 8.3145 - lr: 1.0000e-04\n",
      "Epoch 183/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2706 - mse: 0.2664 - rmse: 0.5162 - mae: 0.2706 - mape: 8.5553\n",
      "Epoch 183: val_loss improved from 0.26527 to 0.26451, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2702 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2702 - mape: 8.5609 - val_loss: 0.2645 - val_mse: 0.2604 - val_rmse: 0.5103 - val_mae: 0.2645 - val_mape: 8.4438 - lr: 1.0000e-04\n",
      "Epoch 184/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2710 - mse: 0.2692 - rmse: 0.5188 - mae: 0.2710 - mape: 8.5994\n",
      "Epoch 184: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2703 - mse: 0.2679 - rmse: 0.5176 - mae: 0.2703 - mape: 8.5793 - val_loss: 0.2665 - val_mse: 0.2583 - val_rmse: 0.5083 - val_mae: 0.2665 - val_mape: 8.3900 - lr: 1.0000e-04\n",
      "Epoch 185/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2682 - mse: 0.2638 - rmse: 0.5136 - mae: 0.2682 - mape: 8.5579\n",
      "Epoch 185: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2682 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2682 - mape: 8.5504 - val_loss: 0.2654 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2654 - val_mape: 8.5080 - lr: 1.0000e-04\n",
      "Epoch 186/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2680 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2680 - mape: 8.4971\n",
      "Epoch 186: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2687 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2687 - mape: 8.5375 - val_loss: 0.2654 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2654 - val_mape: 8.4368 - lr: 1.0000e-04\n",
      "Epoch 187/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2685 - mse: 0.2679 - rmse: 0.5176 - mae: 0.2685 - mape: 8.5405\n",
      "Epoch 187: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2690 - mse: 0.2686 - rmse: 0.5183 - mae: 0.2690 - mape: 8.5676 - val_loss: 0.2680 - val_mse: 0.2656 - val_rmse: 0.5153 - val_mae: 0.2680 - val_mape: 8.6028 - lr: 1.0000e-04\n",
      "Epoch 188/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2705 - mse: 0.2688 - rmse: 0.5184 - mae: 0.2705 - mape: 8.5934\n",
      "Epoch 188: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2707 - mse: 0.2690 - rmse: 0.5186 - mae: 0.2707 - mape: 8.5906 - val_loss: 0.2736 - val_mse: 0.2724 - val_rmse: 0.5220 - val_mae: 0.2736 - val_mape: 8.5119 - lr: 1.0000e-04\n",
      "Epoch 189/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2686 - mse: 0.2663 - rmse: 0.5161 - mae: 0.2686 - mape: 8.5497\n",
      "Epoch 189: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2690 - mse: 0.2666 - rmse: 0.5164 - mae: 0.2690 - mape: 8.5594 - val_loss: 0.2649 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2649 - val_mape: 8.3484 - lr: 1.0000e-04\n",
      "Epoch 190/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2681 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2681 - mape: 8.5286\n",
      "Epoch 190: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2686 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2686 - mape: 8.5474 - val_loss: 0.2658 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2658 - val_mape: 8.4273 - lr: 1.0000e-04\n",
      "Epoch 191/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2705 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2705 - mape: 8.6289\n",
      "Epoch 191: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2703 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2703 - mape: 8.6275 - val_loss: 0.2659 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2659 - val_mape: 8.5665 - lr: 1.0000e-04\n",
      "Epoch 192/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2670 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2670 - mape: 8.4937\n",
      "Epoch 192: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2675 - mse: 0.2625 - rmse: 0.5123 - mae: 0.2675 - mape: 8.5085 - val_loss: 0.2683 - val_mse: 0.2658 - val_rmse: 0.5156 - val_mae: 0.2683 - val_mape: 8.4430 - lr: 1.0000e-04\n",
      "Epoch 193/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2679 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2679 - mape: 8.5417\n",
      "Epoch 193: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2690 - mse: 0.2688 - rmse: 0.5185 - mae: 0.2690 - mape: 8.5702 - val_loss: 0.2688 - val_mse: 0.2673 - val_rmse: 0.5170 - val_mae: 0.2688 - val_mape: 8.6168 - lr: 1.0000e-04\n",
      "Epoch 194/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2691 - mse: 0.2698 - rmse: 0.5194 - mae: 0.2691 - mape: 8.5997\n",
      "Epoch 194: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2684 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2684 - mape: 8.5741 - val_loss: 0.2696 - val_mse: 0.2704 - val_rmse: 0.5200 - val_mae: 0.2696 - val_mape: 8.7131 - lr: 1.0000e-04\n",
      "Epoch 195/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2712 - mse: 0.2718 - rmse: 0.5213 - mae: 0.2712 - mape: 8.6153\n",
      "Epoch 195: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2719 - mse: 0.2728 - rmse: 0.5223 - mae: 0.2719 - mape: 8.6477 - val_loss: 0.2818 - val_mse: 0.2875 - val_rmse: 0.5362 - val_mae: 0.2818 - val_mape: 8.6550 - lr: 1.0000e-04\n",
      "Epoch 196/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2682 - mse: 0.2662 - rmse: 0.5160 - mae: 0.2682 - mape: 8.5612\n",
      "Epoch 196: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2684 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2684 - mape: 8.5612 - val_loss: 0.2723 - val_mse: 0.2702 - val_rmse: 0.5199 - val_mae: 0.2723 - val_mape: 8.4225 - lr: 1.0000e-04\n",
      "Epoch 197/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2682 - mse: 0.2650 - rmse: 0.5148 - mae: 0.2682 - mape: 8.5395\n",
      "Epoch 197: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2689 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2689 - mape: 8.5523 - val_loss: 0.2657 - val_mse: 0.2617 - val_rmse: 0.5116 - val_mae: 0.2657 - val_mape: 8.4179 - lr: 1.0000e-04\n",
      "Epoch 198/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2712 - mse: 0.2698 - rmse: 0.5194 - mae: 0.2712 - mape: 8.5898\n",
      "Epoch 198: val_loss did not improve from 0.26451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2706 - mse: 0.2690 - rmse: 0.5187 - mae: 0.2706 - mape: 8.5774 - val_loss: 0.2660 - val_mse: 0.2653 - val_rmse: 0.5151 - val_mae: 0.2660 - val_mape: 8.5895 - lr: 1.0000e-04\n",
      "Epoch 199/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2701 - mse: 0.2720 - rmse: 0.5215 - mae: 0.2701 - mape: 8.6195\n",
      "Epoch 199: val_loss improved from 0.26451 to 0.26448, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2699 - mse: 0.2720 - rmse: 0.5215 - mae: 0.2699 - mape: 8.6080 - val_loss: 0.2645 - val_mse: 0.2573 - val_rmse: 0.5073 - val_mae: 0.2645 - val_mape: 8.3084 - lr: 1.0000e-04\n",
      "Epoch 200/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2694 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2694 - mape: 8.5594\n",
      "Epoch 200: val_loss did not improve from 0.26448\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2689 - mse: 0.2663 - rmse: 0.5161 - mae: 0.2689 - mape: 8.5336 - val_loss: 0.2662 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2662 - val_mape: 8.3529 - lr: 1.0000e-04\n",
      "Epoch 201/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2707 - mse: 0.2707 - rmse: 0.5203 - mae: 0.2707 - mape: 8.6251\n",
      "Epoch 201: val_loss did not improve from 0.26448\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2699 - mse: 0.2697 - rmse: 0.5193 - mae: 0.2699 - mape: 8.6011 - val_loss: 0.2748 - val_mse: 0.2830 - val_rmse: 0.5319 - val_mae: 0.2748 - val_mape: 8.9435 - lr: 1.0000e-04\n",
      "Epoch 202/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2700 - mse: 0.2682 - rmse: 0.5179 - mae: 0.2700 - mape: 8.5969\n",
      "Epoch 202: val_loss did not improve from 0.26448\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2699 - mse: 0.2697 - rmse: 0.5193 - mae: 0.2699 - mape: 8.5930 - val_loss: 0.2653 - val_mse: 0.2648 - val_rmse: 0.5146 - val_mae: 0.2653 - val_mape: 8.5485 - lr: 1.0000e-04\n",
      "Epoch 203/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2685 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2685 - mape: 8.5861\n",
      "Epoch 203: val_loss did not improve from 0.26448\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2685 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2685 - mape: 8.5861 - val_loss: 0.2680 - val_mse: 0.2633 - val_rmse: 0.5131 - val_mae: 0.2680 - val_mape: 8.3662 - lr: 1.0000e-04\n",
      "Epoch 204/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2680 - mse: 0.2658 - rmse: 0.5155 - mae: 0.2680 - mape: 8.5384\n",
      "Epoch 204: val_loss did not improve from 0.26448\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2679 - mse: 0.2655 - rmse: 0.5153 - mae: 0.2679 - mape: 8.5378 - val_loss: 0.2651 - val_mse: 0.2599 - val_rmse: 0.5098 - val_mae: 0.2651 - val_mape: 8.3742 - lr: 1.0000e-04\n",
      "Epoch 205/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2688 - mse: 0.2652 - rmse: 0.5150 - mae: 0.2688 - mape: 8.5534\n",
      "Epoch 205: val_loss did not improve from 0.26448\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2701 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2701 - mape: 8.6080 - val_loss: 0.2719 - val_mse: 0.2832 - val_rmse: 0.5322 - val_mae: 0.2719 - val_mape: 8.8655 - lr: 1.0000e-04\n",
      "Epoch 206/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2667 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2667 - mape: 8.4762\n",
      "Epoch 206: val_loss did not improve from 0.26448\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2674 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2674 - mape: 8.5018 - val_loss: 0.2790 - val_mse: 0.2949 - val_rmse: 0.5431 - val_mae: 0.2790 - val_mape: 9.1809 - lr: 1.0000e-04\n",
      "Epoch 207/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2702 - mse: 0.2712 - rmse: 0.5208 - mae: 0.2702 - mape: 8.6009\n",
      "Epoch 207: val_loss did not improve from 0.26448\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2704 - mse: 0.2719 - rmse: 0.5215 - mae: 0.2704 - mape: 8.6093 - val_loss: 0.2750 - val_mse: 0.2816 - val_rmse: 0.5306 - val_mae: 0.2750 - val_mape: 8.8756 - lr: 1.0000e-04\n",
      "Epoch 208/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2728 - mse: 0.2741 - rmse: 0.5235 - mae: 0.2728 - mape: 8.6521\n",
      "Epoch 208: val_loss did not improve from 0.26448\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2720 - mse: 0.2731 - rmse: 0.5226 - mae: 0.2720 - mape: 8.6274 - val_loss: 0.2657 - val_mse: 0.2638 - val_rmse: 0.5136 - val_mae: 0.2657 - val_mape: 8.5744 - lr: 1.0000e-04\n",
      "Epoch 209/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2669 - mse: 0.2630 - rmse: 0.5129 - mae: 0.2669 - mape: 8.4837\n",
      "Epoch 209: val_loss did not improve from 0.26448\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2681 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2681 - mape: 8.5597 - val_loss: 0.2733 - val_mse: 0.2757 - val_rmse: 0.5251 - val_mae: 0.2733 - val_mape: 8.8216 - lr: 1.0000e-04\n",
      "Epoch 210/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2712 - mse: 0.2733 - rmse: 0.5227 - mae: 0.2712 - mape: 8.6692\n",
      "Epoch 210: val_loss improved from 0.26448 to 0.26384, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2701 - mse: 0.2721 - rmse: 0.5217 - mae: 0.2701 - mape: 8.6290 - val_loss: 0.2638 - val_mse: 0.2567 - val_rmse: 0.5067 - val_mae: 0.2638 - val_mape: 8.4036 - lr: 1.0000e-04\n",
      "Epoch 211/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2687 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2687 - mape: 8.5983\n",
      "Epoch 211: val_loss did not improve from 0.26384\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2677 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2677 - mape: 8.5441 - val_loss: 0.2675 - val_mse: 0.2652 - val_rmse: 0.5149 - val_mae: 0.2675 - val_mape: 8.4091 - lr: 1.0000e-04\n",
      "Epoch 212/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2687 - mse: 0.2677 - rmse: 0.5174 - mae: 0.2687 - mape: 8.5501\n",
      "Epoch 212: val_loss improved from 0.26384 to 0.26381, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2685 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2685 - mape: 8.5347 - val_loss: 0.2638 - val_mse: 0.2585 - val_rmse: 0.5084 - val_mae: 0.2638 - val_mape: 8.3871 - lr: 1.0000e-04\n",
      "Epoch 213/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2676 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2676 - mape: 8.5081\n",
      "Epoch 213: val_loss did not improve from 0.26381\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2682 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2682 - mape: 8.5258 - val_loss: 0.2674 - val_mse: 0.2645 - val_rmse: 0.5143 - val_mae: 0.2674 - val_mape: 8.4185 - lr: 1.0000e-04\n",
      "Epoch 214/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2706 - mse: 0.2710 - rmse: 0.5206 - mae: 0.2706 - mape: 8.6230\n",
      "Epoch 214: val_loss did not improve from 0.26381\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2709 - mse: 0.2723 - rmse: 0.5218 - mae: 0.2709 - mape: 8.6308 - val_loss: 0.2653 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2653 - val_mape: 8.3825 - lr: 1.0000e-04\n",
      "Epoch 215/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2705 - mse: 0.2698 - rmse: 0.5194 - mae: 0.2705 - mape: 8.5848\n",
      "Epoch 215: val_loss did not improve from 0.26381\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2704 - mse: 0.2695 - rmse: 0.5192 - mae: 0.2704 - mape: 8.5858 - val_loss: 0.2655 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2655 - val_mape: 8.3952 - lr: 1.0000e-04\n",
      "Epoch 216/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2701 - mse: 0.2696 - rmse: 0.5193 - mae: 0.2701 - mape: 8.5830\n",
      "Epoch 216: val_loss did not improve from 0.26381\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2699 - mse: 0.2694 - rmse: 0.5190 - mae: 0.2699 - mape: 8.5731 - val_loss: 0.2694 - val_mse: 0.2682 - val_rmse: 0.5179 - val_mae: 0.2694 - val_mape: 8.4195 - lr: 1.0000e-04\n",
      "Epoch 217/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2703 - mse: 0.2690 - rmse: 0.5186 - mae: 0.2703 - mape: 8.5810\n",
      "Epoch 217: val_loss did not improve from 0.26381\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2703 - mse: 0.2698 - rmse: 0.5194 - mae: 0.2703 - mape: 8.5878 - val_loss: 0.2780 - val_mse: 0.2837 - val_rmse: 0.5326 - val_mae: 0.2780 - val_mape: 8.6010 - lr: 1.0000e-04\n",
      "Epoch 218/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2674 - mse: 0.2685 - rmse: 0.5181 - mae: 0.2674 - mape: 8.5308\n",
      "Epoch 218: val_loss did not improve from 0.26381\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2679 - mse: 0.2699 - rmse: 0.5195 - mae: 0.2679 - mape: 8.5387 - val_loss: 0.2689 - val_mse: 0.2674 - val_rmse: 0.5171 - val_mae: 0.2689 - val_mape: 8.3540 - lr: 1.0000e-04\n",
      "Epoch 219/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2672 - mse: 0.2684 - rmse: 0.5181 - mae: 0.2672 - mape: 8.5259\n",
      "Epoch 219: val_loss did not improve from 0.26381\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2674 - mse: 0.2685 - rmse: 0.5181 - mae: 0.2674 - mape: 8.5231 - val_loss: 0.2723 - val_mse: 0.2678 - val_rmse: 0.5175 - val_mae: 0.2723 - val_mape: 8.4426 - lr: 1.0000e-04\n",
      "Epoch 220/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2700 - mse: 0.2708 - rmse: 0.5204 - mae: 0.2700 - mape: 8.5935\n",
      "Epoch 220: val_loss did not improve from 0.26381\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2697 - mse: 0.2699 - rmse: 0.5195 - mae: 0.2697 - mape: 8.5842 - val_loss: 0.2658 - val_mse: 0.2682 - val_rmse: 0.5179 - val_mae: 0.2658 - val_mape: 8.5952 - lr: 1.0000e-04\n",
      "Epoch 221/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2700 - mse: 0.2757 - rmse: 0.5251 - mae: 0.2700 - mape: 8.6021\n",
      "Epoch 221: val_loss did not improve from 0.26381\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2688 - mse: 0.2719 - rmse: 0.5214 - mae: 0.2688 - mape: 8.5722 - val_loss: 0.2647 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2647 - val_mape: 8.3311 - lr: 1.0000e-04\n",
      "Epoch 222/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2711 - mse: 0.2726 - rmse: 0.5221 - mae: 0.2711 - mape: 8.6418\n",
      "Epoch 222: val_loss improved from 0.26381 to 0.26360, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2697 - mse: 0.2697 - rmse: 0.5194 - mae: 0.2697 - mape: 8.5877 - val_loss: 0.2636 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2636 - val_mape: 8.3428 - lr: 1.0000e-04\n",
      "Epoch 223/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2686 - mse: 0.2695 - rmse: 0.5192 - mae: 0.2686 - mape: 8.5726\n",
      "Epoch 223: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2683 - mse: 0.2690 - rmse: 0.5186 - mae: 0.2683 - mape: 8.5546 - val_loss: 0.2711 - val_mse: 0.2695 - val_rmse: 0.5191 - val_mae: 0.2711 - val_mape: 8.4195 - lr: 1.0000e-04\n",
      "Epoch 224/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2680 - mse: 0.2718 - rmse: 0.5213 - mae: 0.2680 - mape: 8.5560\n",
      "Epoch 224: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2689 - mse: 0.2723 - rmse: 0.5218 - mae: 0.2689 - mape: 8.5666 - val_loss: 0.2690 - val_mse: 0.2637 - val_rmse: 0.5135 - val_mae: 0.2690 - val_mape: 8.3919 - lr: 1.0000e-04\n",
      "Epoch 225/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2701 - mse: 0.2719 - rmse: 0.5215 - mae: 0.2701 - mape: 8.6026\n",
      "Epoch 225: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2693 - mse: 0.2703 - rmse: 0.5199 - mae: 0.2693 - mape: 8.5835 - val_loss: 0.2677 - val_mse: 0.2630 - val_rmse: 0.5128 - val_mae: 0.2677 - val_mape: 8.2881 - lr: 1.0000e-04\n",
      "Epoch 226/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2706 - mse: 0.2714 - rmse: 0.5209 - mae: 0.2706 - mape: 8.5997\n",
      "Epoch 226: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2701 - mse: 0.2710 - rmse: 0.5206 - mae: 0.2701 - mape: 8.5889 - val_loss: 0.2665 - val_mse: 0.2631 - val_rmse: 0.5130 - val_mae: 0.2665 - val_mape: 8.4580 - lr: 1.0000e-04\n",
      "Epoch 227/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2705 - mse: 0.2718 - rmse: 0.5214 - mae: 0.2705 - mape: 8.5890\n",
      "Epoch 227: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2699 - mse: 0.2704 - rmse: 0.5200 - mae: 0.2699 - mape: 8.5677 - val_loss: 0.2656 - val_mse: 0.2681 - val_rmse: 0.5178 - val_mae: 0.2656 - val_mape: 8.5166 - lr: 1.0000e-04\n",
      "Epoch 228/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2685 - mse: 0.2723 - rmse: 0.5218 - mae: 0.2685 - mape: 8.5525\n",
      "Epoch 228: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2683 - mse: 0.2702 - rmse: 0.5198 - mae: 0.2683 - mape: 8.5482 - val_loss: 0.2673 - val_mse: 0.2620 - val_rmse: 0.5118 - val_mae: 0.2673 - val_mape: 8.4189 - lr: 1.0000e-04\n",
      "Epoch 229/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2679 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2679 - mape: 8.5446\n",
      "Epoch 229: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2691 - mse: 0.2699 - rmse: 0.5195 - mae: 0.2691 - mape: 8.5944 - val_loss: 0.2868 - val_mse: 0.2985 - val_rmse: 0.5464 - val_mae: 0.2868 - val_mape: 8.6713 - lr: 1.0000e-04\n",
      "Epoch 230/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2701 - mse: 0.2734 - rmse: 0.5229 - mae: 0.2701 - mape: 8.5986\n",
      "Epoch 230: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2701 - mse: 0.2735 - rmse: 0.5229 - mae: 0.2701 - mape: 8.5994 - val_loss: 0.2637 - val_mse: 0.2613 - val_rmse: 0.5112 - val_mae: 0.2637 - val_mape: 8.4350 - lr: 1.0000e-04\n",
      "Epoch 231/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2685 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2685 - mape: 8.5359\n",
      "Epoch 231: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2677 - mse: 0.2658 - rmse: 0.5156 - mae: 0.2677 - mape: 8.5117 - val_loss: 0.2670 - val_mse: 0.2727 - val_rmse: 0.5223 - val_mae: 0.2670 - val_mape: 8.6743 - lr: 1.0000e-04\n",
      "Epoch 232/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2728 - mse: 0.2775 - rmse: 0.5268 - mae: 0.2728 - mape: 8.6919\n",
      "Epoch 232: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2721 - mse: 0.2752 - rmse: 0.5246 - mae: 0.2721 - mape: 8.6775 - val_loss: 0.2687 - val_mse: 0.2667 - val_rmse: 0.5164 - val_mae: 0.2687 - val_mape: 8.3806 - lr: 1.0000e-04\n",
      "Epoch 233/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2665 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2665 - mape: 8.5170\n",
      "Epoch 233: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2675 - mse: 0.2685 - rmse: 0.5181 - mae: 0.2675 - mape: 8.5528 - val_loss: 0.2654 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2654 - val_mape: 8.3907 - lr: 1.0000e-04\n",
      "Epoch 234/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2710 - mse: 0.2706 - rmse: 0.5202 - mae: 0.2710 - mape: 8.6126\n",
      "Epoch 234: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2709 - mse: 0.2704 - rmse: 0.5200 - mae: 0.2709 - mape: 8.6015 - val_loss: 0.2651 - val_mse: 0.2681 - val_rmse: 0.5178 - val_mae: 0.2651 - val_mape: 8.5722 - lr: 1.0000e-04\n",
      "Epoch 235/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2681 - mse: 0.2697 - rmse: 0.5193 - mae: 0.2681 - mape: 8.5466\n",
      "Epoch 235: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2681 - mse: 0.2697 - rmse: 0.5193 - mae: 0.2681 - mape: 8.5466 - val_loss: 0.2795 - val_mse: 0.2837 - val_rmse: 0.5327 - val_mae: 0.2795 - val_mape: 8.5513 - lr: 1.0000e-04\n",
      "Epoch 236/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2704 - mse: 0.2714 - rmse: 0.5210 - mae: 0.2704 - mape: 8.5883\n",
      "Epoch 236: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2705 - mse: 0.2714 - rmse: 0.5210 - mae: 0.2705 - mape: 8.5945 - val_loss: 0.2649 - val_mse: 0.2665 - val_rmse: 0.5162 - val_mae: 0.2649 - val_mape: 8.5946 - lr: 1.0000e-04\n",
      "Epoch 237/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2705 - mse: 0.2746 - rmse: 0.5240 - mae: 0.2705 - mape: 8.6165\n",
      "Epoch 237: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2710 - mse: 0.2752 - rmse: 0.5246 - mae: 0.2710 - mape: 8.6285 - val_loss: 0.2653 - val_mse: 0.2629 - val_rmse: 0.5128 - val_mae: 0.2653 - val_mape: 8.4686 - lr: 1.0000e-04\n",
      "Epoch 238/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2691 - mse: 0.2706 - rmse: 0.5202 - mae: 0.2691 - mape: 8.5660\n",
      "Epoch 238: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2698 - mse: 0.2728 - rmse: 0.5223 - mae: 0.2698 - mape: 8.5931 - val_loss: 0.2793 - val_mse: 0.2859 - val_rmse: 0.5347 - val_mae: 0.2793 - val_mape: 8.5891 - lr: 1.0000e-04\n",
      "Epoch 239/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2737 - mse: 0.2822 - rmse: 0.5313 - mae: 0.2737 - mape: 8.7387\n",
      "Epoch 239: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2715 - mse: 0.2768 - rmse: 0.5261 - mae: 0.2715 - mape: 8.6574 - val_loss: 0.2760 - val_mse: 0.2864 - val_rmse: 0.5351 - val_mae: 0.2760 - val_mape: 8.9471 - lr: 1.0000e-04\n",
      "Epoch 240/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2696 - mse: 0.2689 - rmse: 0.5185 - mae: 0.2696 - mape: 8.5600\n",
      "Epoch 240: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2687 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2687 - mape: 8.5364 - val_loss: 0.2793 - val_mse: 0.2970 - val_rmse: 0.5450 - val_mae: 0.2793 - val_mape: 9.1796 - lr: 1.0000e-04\n",
      "Epoch 241/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2678 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2678 - mape: 8.5304\n",
      "Epoch 241: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2680 - mse: 0.2685 - rmse: 0.5182 - mae: 0.2680 - mape: 8.5360 - val_loss: 0.2648 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2648 - val_mape: 8.3997 - lr: 1.0000e-04\n",
      "Epoch 242/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2679 - mse: 0.2677 - rmse: 0.5174 - mae: 0.2679 - mape: 8.5122\n",
      "Epoch 242: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2694 - mse: 0.2691 - rmse: 0.5187 - mae: 0.2694 - mape: 8.5583 - val_loss: 0.2663 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2663 - val_mape: 8.5108 - lr: 1.0000e-04\n",
      "Epoch 243/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2701 - mse: 0.2703 - rmse: 0.5199 - mae: 0.2701 - mape: 8.5718\n",
      "Epoch 243: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2703 - mse: 0.2707 - rmse: 0.5203 - mae: 0.2703 - mape: 8.5762 - val_loss: 0.2651 - val_mse: 0.2622 - val_rmse: 0.5120 - val_mae: 0.2651 - val_mape: 8.5038 - lr: 1.0000e-04\n",
      "Epoch 244/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2711 - mse: 0.2739 - rmse: 0.5234 - mae: 0.2711 - mape: 8.6393\n",
      "Epoch 244: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2710 - mse: 0.2737 - rmse: 0.5232 - mae: 0.2710 - mape: 8.6289 - val_loss: 0.2657 - val_mse: 0.2613 - val_rmse: 0.5112 - val_mae: 0.2657 - val_mape: 8.3862 - lr: 1.0000e-04\n",
      "Epoch 245/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2677 - mse: 0.2686 - rmse: 0.5182 - mae: 0.2677 - mape: 8.5340\n",
      "Epoch 245: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2682 - mse: 0.2687 - rmse: 0.5184 - mae: 0.2682 - mape: 8.5539 - val_loss: 0.2726 - val_mse: 0.2733 - val_rmse: 0.5228 - val_mae: 0.2726 - val_mape: 8.3608 - lr: 1.0000e-04\n",
      "Epoch 246/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2688 - mse: 0.2693 - rmse: 0.5189 - mae: 0.2688 - mape: 8.5420\n",
      "Epoch 246: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2684 - mse: 0.2681 - rmse: 0.5178 - mae: 0.2684 - mape: 8.5406 - val_loss: 0.2719 - val_mse: 0.2713 - val_rmse: 0.5209 - val_mae: 0.2719 - val_mape: 8.4993 - lr: 1.0000e-04\n",
      "Epoch 247/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2691 - mse: 0.2716 - rmse: 0.5212 - mae: 0.2691 - mape: 8.5846\n",
      "Epoch 247: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2696 - mse: 0.2716 - rmse: 0.5211 - mae: 0.2696 - mape: 8.6015 - val_loss: 0.2659 - val_mse: 0.2611 - val_rmse: 0.5110 - val_mae: 0.2659 - val_mape: 8.4912 - lr: 1.0000e-04\n",
      "Epoch 248/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2668 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2668 - mape: 8.5380\n",
      "Epoch 248: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2674 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2674 - mape: 8.5473 - val_loss: 0.2651 - val_mse: 0.2662 - val_rmse: 0.5159 - val_mae: 0.2651 - val_mape: 8.5549 - lr: 1.0000e-04\n",
      "Epoch 249/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2677 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2677 - mape: 8.5457\n",
      "Epoch 249: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2679 - mse: 0.2677 - rmse: 0.5174 - mae: 0.2679 - mape: 8.5368 - val_loss: 0.2658 - val_mse: 0.2622 - val_rmse: 0.5121 - val_mae: 0.2658 - val_mape: 8.4185 - lr: 1.0000e-04\n",
      "Epoch 250/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2702 - mse: 0.2709 - rmse: 0.5205 - mae: 0.2702 - mape: 8.6346\n",
      "Epoch 250: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2689 - mse: 0.2693 - rmse: 0.5189 - mae: 0.2689 - mape: 8.5856 - val_loss: 0.2666 - val_mse: 0.2678 - val_rmse: 0.5175 - val_mae: 0.2666 - val_mape: 8.4495 - lr: 1.0000e-04\n",
      "Epoch 251/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2668 - mse: 0.2684 - rmse: 0.5181 - mae: 0.2668 - mape: 8.5347\n",
      "Epoch 251: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2670 - mse: 0.2683 - rmse: 0.5180 - mae: 0.2670 - mape: 8.5356 - val_loss: 0.2644 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2644 - val_mape: 8.3498 - lr: 1.0000e-04\n",
      "Epoch 252/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2689 - mse: 0.2696 - rmse: 0.5192 - mae: 0.2689 - mape: 8.5608\n",
      "Epoch 252: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2694 - mse: 0.2705 - rmse: 0.5201 - mae: 0.2694 - mape: 8.5742 - val_loss: 0.2661 - val_mse: 0.2688 - val_rmse: 0.5184 - val_mae: 0.2661 - val_mape: 8.6448 - lr: 1.0000e-04\n",
      "Epoch 253/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2680 - mse: 0.2700 - rmse: 0.5196 - mae: 0.2680 - mape: 8.5647\n",
      "Epoch 253: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2679 - mse: 0.2700 - rmse: 0.5196 - mae: 0.2679 - mape: 8.5633 - val_loss: 0.2698 - val_mse: 0.2710 - val_rmse: 0.5206 - val_mae: 0.2698 - val_mape: 8.3908 - lr: 1.0000e-04\n",
      "Epoch 254/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2700 - mse: 0.2723 - rmse: 0.5218 - mae: 0.2700 - mape: 8.6214\n",
      "Epoch 254: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2694 - mse: 0.2703 - rmse: 0.5199 - mae: 0.2694 - mape: 8.5854 - val_loss: 0.2731 - val_mse: 0.2716 - val_rmse: 0.5212 - val_mae: 0.2731 - val_mape: 8.5929 - lr: 1.0000e-04\n",
      "Epoch 255/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2697 - mse: 0.2722 - rmse: 0.5217 - mae: 0.2697 - mape: 8.5855\n",
      "Epoch 255: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2699 - mse: 0.2727 - rmse: 0.5222 - mae: 0.2699 - mape: 8.5862 - val_loss: 0.2653 - val_mse: 0.2604 - val_rmse: 0.5103 - val_mae: 0.2653 - val_mape: 8.3056 - lr: 1.0000e-04\n",
      "Epoch 256/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2687 - mse: 0.2694 - rmse: 0.5190 - mae: 0.2687 - mape: 8.5499\n",
      "Epoch 256: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2686 - mse: 0.2696 - rmse: 0.5192 - mae: 0.2686 - mape: 8.5492 - val_loss: 0.2668 - val_mse: 0.2660 - val_rmse: 0.5158 - val_mae: 0.2668 - val_mape: 8.6236 - lr: 1.0000e-04\n",
      "Epoch 257/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2725 - mse: 0.2764 - rmse: 0.5258 - mae: 0.2725 - mape: 8.6468\n",
      "Epoch 257: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2723 - mse: 0.2764 - rmse: 0.5257 - mae: 0.2723 - mape: 8.6448 - val_loss: 0.2662 - val_mse: 0.2733 - val_rmse: 0.5228 - val_mae: 0.2662 - val_mape: 8.6594 - lr: 1.0000e-04\n",
      "Epoch 258/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2689 - mse: 0.2694 - rmse: 0.5191 - mae: 0.2689 - mape: 8.5824\n",
      "Epoch 258: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2679 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2679 - mape: 8.5553 - val_loss: 0.2701 - val_mse: 0.2629 - val_rmse: 0.5127 - val_mae: 0.2701 - val_mape: 8.3593 - lr: 1.0000e-04\n",
      "Epoch 259/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2694 - mse: 0.2717 - rmse: 0.5213 - mae: 0.2694 - mape: 8.5542\n",
      "Epoch 259: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2693 - mse: 0.2715 - rmse: 0.5211 - mae: 0.2693 - mape: 8.5514 - val_loss: 0.2678 - val_mse: 0.2711 - val_rmse: 0.5207 - val_mae: 0.2678 - val_mape: 8.4673 - lr: 1.0000e-04\n",
      "Epoch 260/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2673 - mse: 0.2702 - rmse: 0.5198 - mae: 0.2673 - mape: 8.5323\n",
      "Epoch 260: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2671 - mse: 0.2698 - rmse: 0.5194 - mae: 0.2671 - mape: 8.5385 - val_loss: 0.2687 - val_mse: 0.2674 - val_rmse: 0.5171 - val_mae: 0.2687 - val_mape: 8.4077 - lr: 1.0000e-04\n",
      "Epoch 261/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2703 - mse: 0.2753 - rmse: 0.5247 - mae: 0.2703 - mape: 8.6329\n",
      "Epoch 261: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2708 - mse: 0.2754 - rmse: 0.5248 - mae: 0.2708 - mape: 8.6382 - val_loss: 0.2707 - val_mse: 0.2721 - val_rmse: 0.5216 - val_mae: 0.2707 - val_mape: 8.8141 - lr: 1.0000e-04\n",
      "Epoch 262/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2688 - mse: 0.2725 - rmse: 0.5220 - mae: 0.2688 - mape: 8.5940\n",
      "Epoch 262: val_loss did not improve from 0.26360\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2679 - mse: 0.2710 - rmse: 0.5206 - mae: 0.2679 - mape: 8.5676 - val_loss: 0.2658 - val_mse: 0.2642 - val_rmse: 0.5140 - val_mae: 0.2658 - val_mape: 8.4114 - lr: 1.0000e-04\n",
      "Epoch 263/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2625 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2625 - mape: 8.3983\n",
      "Epoch 263: val_loss improved from 0.26360 to 0.26238, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2632 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2632 - mape: 8.4135 - val_loss: 0.2624 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2624 - val_mape: 8.3781 - lr: 1.0000e-05\n",
      "Epoch 264/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2635 - mse: 0.2628 - rmse: 0.5127 - mae: 0.2635 - mape: 8.4200\n",
      "Epoch 264: val_loss improved from 0.26238 to 0.26224, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2630 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2630 - mape: 8.4101 - val_loss: 0.2622 - val_mse: 0.2599 - val_rmse: 0.5098 - val_mae: 0.2622 - val_mape: 8.4006 - lr: 1.0000e-05\n",
      "Epoch 265/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2621 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2621 - mape: 8.3964\n",
      "Epoch 265: val_loss did not improve from 0.26224\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2629 - mse: 0.2618 - rmse: 0.5116 - mae: 0.2629 - mape: 8.4050 - val_loss: 0.2641 - val_mse: 0.2657 - val_rmse: 0.5155 - val_mae: 0.2641 - val_mape: 8.5277 - lr: 1.0000e-05\n",
      "Epoch 266/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2625 - mse: 0.2614 - rmse: 0.5112 - mae: 0.2625 - mape: 8.3885\n",
      "Epoch 266: val_loss did not improve from 0.26224\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2626 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2626 - mape: 8.4010 - val_loss: 0.2633 - val_mse: 0.2608 - val_rmse: 0.5107 - val_mae: 0.2633 - val_mape: 8.3541 - lr: 1.0000e-05\n",
      "Epoch 267/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2626 - mse: 0.2607 - rmse: 0.5105 - mae: 0.2626 - mape: 8.3876\n",
      "Epoch 267: val_loss did not improve from 0.26224\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2626 - mse: 0.2605 - rmse: 0.5103 - mae: 0.2626 - mape: 8.3902 - val_loss: 0.2624 - val_mse: 0.2616 - val_rmse: 0.5115 - val_mae: 0.2624 - val_mape: 8.4296 - lr: 1.0000e-05\n",
      "Epoch 268/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2648 - mse: 0.2650 - rmse: 0.5148 - mae: 0.2648 - mape: 8.4554\n",
      "Epoch 268: val_loss did not improve from 0.26224\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2626 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2626 - mape: 8.3913 - val_loss: 0.2635 - val_mse: 0.2653 - val_rmse: 0.5151 - val_mae: 0.2635 - val_mape: 8.5118 - lr: 1.0000e-05\n",
      "Epoch 269/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2627 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2627 - mape: 8.3957\n",
      "Epoch 269: val_loss improved from 0.26224 to 0.26205, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2626 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2626 - mape: 8.4042 - val_loss: 0.2620 - val_mse: 0.2611 - val_rmse: 0.5110 - val_mae: 0.2620 - val_mape: 8.4105 - lr: 1.0000e-05\n",
      "Epoch 270/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2631 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2631 - mape: 8.4114\n",
      "Epoch 270: val_loss did not improve from 0.26205\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2626 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2626 - mape: 8.3960 - val_loss: 0.2621 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2621 - val_mape: 8.3768 - lr: 1.0000e-05\n",
      "Epoch 271/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2627 - mse: 0.2625 - rmse: 0.5123 - mae: 0.2627 - mape: 8.4132\n",
      "Epoch 271: val_loss did not improve from 0.26205\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2622 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2622 - mape: 8.3944 - val_loss: 0.2625 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2625 - val_mape: 8.3425 - lr: 1.0000e-05\n",
      "Epoch 272/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2632 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2632 - mape: 8.4247\n",
      "Epoch 272: val_loss did not improve from 0.26205\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2624 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2624 - mape: 8.3927 - val_loss: 0.2622 - val_mse: 0.2612 - val_rmse: 0.5110 - val_mae: 0.2622 - val_mape: 8.4045 - lr: 1.0000e-05\n",
      "Epoch 273/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2624 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2624 - mape: 8.3758\n",
      "Epoch 273: val_loss improved from 0.26205 to 0.26192, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2625 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2625 - mape: 8.3890 - val_loss: 0.2619 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2619 - val_mape: 8.3680 - lr: 1.0000e-05\n",
      "Epoch 274/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2619 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2619 - mape: 8.3803\n",
      "Epoch 274: val_loss improved from 0.26192 to 0.26184, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2623 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2623 - mape: 8.3961 - val_loss: 0.2618 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2618 - val_mape: 8.3634 - lr: 1.0000e-05\n",
      "Epoch 275/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2634 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2634 - mape: 8.4241\n",
      "Epoch 275: val_loss did not improve from 0.26184\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2626 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2626 - mape: 8.3910 - val_loss: 0.2626 - val_mse: 0.2624 - val_rmse: 0.5123 - val_mae: 0.2626 - val_mape: 8.4388 - lr: 1.0000e-05\n",
      "Epoch 276/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2628 - mse: 0.2628 - rmse: 0.5126 - mae: 0.2628 - mape: 8.4006\n",
      "Epoch 276: val_loss did not improve from 0.26184\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2624 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2624 - mape: 8.3847 - val_loss: 0.2643 - val_mse: 0.2617 - val_rmse: 0.5116 - val_mae: 0.2643 - val_mape: 8.3544 - lr: 1.0000e-05\n",
      "Epoch 277/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2630 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2630 - mape: 8.4058\n",
      "Epoch 277: val_loss did not improve from 0.26184\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2626 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2626 - mape: 8.3856 - val_loss: 0.2622 - val_mse: 0.2621 - val_rmse: 0.5120 - val_mae: 0.2622 - val_mape: 8.4290 - lr: 1.0000e-05\n",
      "Epoch 278/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2621 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2621 - mape: 8.3927\n",
      "Epoch 278: val_loss did not improve from 0.26184\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2621 - mape: 8.3959 - val_loss: 0.2649 - val_mse: 0.2611 - val_rmse: 0.5110 - val_mae: 0.2649 - val_mape: 8.3203 - lr: 1.0000e-05\n",
      "Epoch 279/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2626 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2626 - mape: 8.3725\n",
      "Epoch 279: val_loss did not improve from 0.26184\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2624 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2624 - mape: 8.3696 - val_loss: 0.2621 - val_mse: 0.2616 - val_rmse: 0.5114 - val_mae: 0.2621 - val_mape: 8.4175 - lr: 1.0000e-05\n",
      "Epoch 280/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2632 - mse: 0.2612 - rmse: 0.5110 - mae: 0.2632 - mape: 8.3996\n",
      "Epoch 280: val_loss did not improve from 0.26184\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2624 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2624 - mape: 8.3905 - val_loss: 0.2620 - val_mse: 0.2609 - val_rmse: 0.5108 - val_mae: 0.2620 - val_mape: 8.4081 - lr: 1.0000e-05\n",
      "Epoch 281/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2626 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2626 - mape: 8.3977\n",
      "Epoch 281: val_loss did not improve from 0.26184\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2625 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2625 - mape: 8.3805 - val_loss: 0.2620 - val_mse: 0.2612 - val_rmse: 0.5111 - val_mae: 0.2620 - val_mape: 8.4059 - lr: 1.0000e-05\n",
      "Epoch 282/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2623 - mse: 0.2612 - rmse: 0.5110 - mae: 0.2623 - mape: 8.3858\n",
      "Epoch 282: val_loss improved from 0.26184 to 0.26167, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2621 - mape: 8.3933 - val_loss: 0.2617 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2617 - val_mape: 8.3683 - lr: 1.0000e-05\n",
      "Epoch 283/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2623 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2623 - mape: 8.3965\n",
      "Epoch 283: val_loss did not improve from 0.26167\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2623 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2623 - mape: 8.3965 - val_loss: 0.2617 - val_mse: 0.2602 - val_rmse: 0.5101 - val_mae: 0.2617 - val_mape: 8.3860 - lr: 1.0000e-05\n",
      "Epoch 284/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2627 - mse: 0.2629 - rmse: 0.5128 - mae: 0.2627 - mape: 8.4032\n",
      "Epoch 284: val_loss did not improve from 0.26167\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2623 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2623 - mape: 8.3841 - val_loss: 0.2618 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2618 - val_mape: 8.3481 - lr: 1.0000e-05\n",
      "Epoch 285/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2625 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2625 - mape: 8.3643\n",
      "Epoch 285: val_loss did not improve from 0.26167\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2626 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2626 - mape: 8.3845 - val_loss: 0.2626 - val_mse: 0.2628 - val_rmse: 0.5126 - val_mae: 0.2626 - val_mape: 8.4630 - lr: 1.0000e-05\n",
      "Epoch 286/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2622 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2622 - mape: 8.3897\n",
      "Epoch 286: val_loss improved from 0.26167 to 0.26167, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2628 - mse: 0.2620 - rmse: 0.5118 - mae: 0.2628 - mape: 8.4183 - val_loss: 0.2617 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2617 - val_mape: 8.3893 - lr: 1.0000e-05\n",
      "Epoch 287/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2631 - mse: 0.2629 - rmse: 0.5127 - mae: 0.2631 - mape: 8.4206\n",
      "Epoch 287: val_loss did not improve from 0.26167\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2623 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2623 - mape: 8.3982 - val_loss: 0.2619 - val_mse: 0.2599 - val_rmse: 0.5098 - val_mae: 0.2619 - val_mape: 8.3543 - lr: 1.0000e-05\n",
      "Epoch 288/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2621 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2621 - mape: 8.3879\n",
      "Epoch 288: val_loss improved from 0.26167 to 0.26165, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2622 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2622 - mape: 8.3866 - val_loss: 0.2617 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2617 - val_mape: 8.3509 - lr: 1.0000e-05\n",
      "Epoch 289/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2623 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2623 - mape: 8.3811\n",
      "Epoch 289: val_loss did not improve from 0.26165\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2620 - mape: 8.3749 - val_loss: 0.2617 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2617 - val_mape: 8.3625 - lr: 1.0000e-05\n",
      "Epoch 290/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2614 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2614 - mape: 8.3689\n",
      "Epoch 290: val_loss improved from 0.26165 to 0.26161, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2620 - mape: 8.3919 - val_loss: 0.2616 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2616 - val_mape: 8.3626 - lr: 1.0000e-05\n",
      "Epoch 291/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2621 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2621 - mape: 8.3809\n",
      "Epoch 291: val_loss did not improve from 0.26161\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2621 - mape: 8.3809 - val_loss: 0.2629 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2629 - val_mape: 8.3407 - lr: 1.0000e-05\n",
      "Epoch 292/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2623 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2623 - mape: 8.3857\n",
      "Epoch 292: val_loss did not improve from 0.26161\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2625 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2625 - mape: 8.3962 - val_loss: 0.2616 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2616 - val_mape: 8.3560 - lr: 1.0000e-05\n",
      "Epoch 293/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2625 - mse: 0.2611 - rmse: 0.5109 - mae: 0.2625 - mape: 8.3991\n",
      "Epoch 293: val_loss did not improve from 0.26161\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2622 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2622 - mape: 8.3821 - val_loss: 0.2616 - val_mse: 0.2604 - val_rmse: 0.5103 - val_mae: 0.2616 - val_mape: 8.3824 - lr: 1.0000e-05\n",
      "Epoch 294/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2633 - mse: 0.2624 - rmse: 0.5123 - mae: 0.2633 - mape: 8.4332\n",
      "Epoch 294: val_loss did not improve from 0.26161\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2622 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2622 - mape: 8.3791 - val_loss: 0.2617 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2617 - val_mape: 8.3890 - lr: 1.0000e-05\n",
      "Epoch 295/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2633 - mse: 0.2625 - rmse: 0.5123 - mae: 0.2633 - mape: 8.4112\n",
      "Epoch 295: val_loss improved from 0.26161 to 0.26157, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2623 - mse: 0.2613 - rmse: 0.5111 - mae: 0.2623 - mape: 8.3931 - val_loss: 0.2616 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2616 - val_mape: 8.3645 - lr: 1.0000e-05\n",
      "Epoch 296/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2622 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2622 - mape: 8.3827\n",
      "Epoch 296: val_loss did not improve from 0.26157\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2623 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2623 - mape: 8.3898 - val_loss: 0.2617 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2617 - val_mape: 8.3637 - lr: 1.0000e-05\n",
      "Epoch 297/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2620 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2620 - mape: 8.3638\n",
      "Epoch 297: val_loss did not improve from 0.26157\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2624 - mse: 0.2605 - rmse: 0.5103 - mae: 0.2624 - mape: 8.3818 - val_loss: 0.2617 - val_mse: 0.2604 - val_rmse: 0.5103 - val_mae: 0.2617 - val_mape: 8.3979 - lr: 1.0000e-05\n",
      "Epoch 298/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2627 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2627 - mape: 8.4039\n",
      "Epoch 298: val_loss did not improve from 0.26157\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2625 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2625 - mape: 8.3859 - val_loss: 0.2617 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2617 - val_mape: 8.3953 - lr: 1.0000e-05\n",
      "Epoch 299/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2618 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2618 - mape: 8.3785\n",
      "Epoch 299: val_loss did not improve from 0.26157\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2623 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2623 - mape: 8.3919 - val_loss: 0.2616 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2616 - val_mape: 8.3731 - lr: 1.0000e-05\n",
      "Epoch 300/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2627 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2627 - mape: 8.3971\n",
      "Epoch 300: val_loss did not improve from 0.26157\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2630 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2630 - mape: 8.4112 - val_loss: 0.2620 - val_mse: 0.2615 - val_rmse: 0.5114 - val_mae: 0.2620 - val_mape: 8.4318 - lr: 1.0000e-05\n",
      "Epoch 301/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2629 - mse: 0.2621 - rmse: 0.5119 - mae: 0.2629 - mape: 8.4177\n",
      "Epoch 301: val_loss did not improve from 0.26157\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2622 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2622 - mape: 8.3916 - val_loss: 0.2617 - val_mse: 0.2613 - val_rmse: 0.5112 - val_mae: 0.2617 - val_mape: 8.4052 - lr: 1.0000e-05\n",
      "Epoch 302/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2631 - mse: 0.2626 - rmse: 0.5124 - mae: 0.2631 - mape: 8.4116\n",
      "Epoch 302: val_loss did not improve from 0.26157\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2626 - mse: 0.2620 - rmse: 0.5119 - mae: 0.2626 - mape: 8.4009 - val_loss: 0.2617 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2617 - val_mape: 8.3431 - lr: 1.0000e-05\n",
      "Epoch 303/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2626 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2626 - mape: 8.4197\n",
      "Epoch 303: val_loss did not improve from 0.26157\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2621 - mape: 8.3813 - val_loss: 0.2620 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2620 - val_mape: 8.3304 - lr: 1.0000e-05\n",
      "Epoch 304/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2628 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2628 - mape: 8.4024\n",
      "Epoch 304: val_loss did not improve from 0.26157\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2620 - mape: 8.3827 - val_loss: 0.2616 - val_mse: 0.2589 - val_rmse: 0.5089 - val_mae: 0.2616 - val_mape: 8.3458 - lr: 1.0000e-05\n",
      "Epoch 305/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2625 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2625 - mape: 8.3946\n",
      "Epoch 305: val_loss did not improve from 0.26157\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2620 - mape: 8.3745 - val_loss: 0.2622 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2622 - val_mape: 8.3416 - lr: 1.0000e-05\n",
      "Epoch 306/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2629 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2629 - mape: 8.4182\n",
      "Epoch 306: val_loss did not improve from 0.26157\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2621 - mape: 8.3871 - val_loss: 0.2620 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2620 - val_mape: 8.3675 - lr: 1.0000e-05\n",
      "Epoch 307/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2606 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2606 - mape: 8.3457\n",
      "Epoch 307: val_loss improved from 0.26157 to 0.26150, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2622 - mse: 0.2616 - rmse: 0.5114 - mae: 0.2622 - mape: 8.3934 - val_loss: 0.2615 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2615 - val_mape: 8.3488 - lr: 1.0000e-05\n",
      "Epoch 308/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2629 - mse: 0.2610 - rmse: 0.5108 - mae: 0.2629 - mape: 8.3840\n",
      "Epoch 308: val_loss improved from 0.26150 to 0.26145, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2621 - mape: 8.3751 - val_loss: 0.2615 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2615 - val_mape: 8.3710 - lr: 1.0000e-05\n",
      "Epoch 309/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2624 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2624 - mape: 8.3861\n",
      "Epoch 309: val_loss did not improve from 0.26145\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2623 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2623 - mape: 8.3812 - val_loss: 0.2620 - val_mse: 0.2618 - val_rmse: 0.5116 - val_mae: 0.2620 - val_mape: 8.4361 - lr: 1.0000e-05\n",
      "Epoch 310/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2630 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2630 - mape: 8.4246\n",
      "Epoch 310: val_loss did not improve from 0.26145\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2622 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2622 - mape: 8.3945 - val_loss: 0.2630 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2630 - val_mape: 8.3118 - lr: 1.0000e-05\n",
      "Epoch 311/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2615 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2615 - mape: 8.3449\n",
      "Epoch 311: val_loss did not improve from 0.26145\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2621 - mape: 8.3734 - val_loss: 0.2617 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2617 - val_mape: 8.3420 - lr: 1.0000e-05\n",
      "Epoch 312/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2621 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2621 - mape: 8.3597\n",
      "Epoch 312: val_loss did not improve from 0.26145\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2620 - mape: 8.3774 - val_loss: 0.2624 - val_mse: 0.2630 - val_rmse: 0.5129 - val_mae: 0.2624 - val_mape: 8.4457 - lr: 1.0000e-05\n",
      "Epoch 313/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2614 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2614 - mape: 8.3677\n",
      "Epoch 313: val_loss did not improve from 0.26145\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2619 - mape: 8.3866 - val_loss: 0.2625 - val_mse: 0.2592 - val_rmse: 0.5092 - val_mae: 0.2625 - val_mape: 8.3225 - lr: 1.0000e-05\n",
      "Epoch 314/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2625 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2625 - mape: 8.3952\n",
      "Epoch 314: val_loss did not improve from 0.26145\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2623 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2623 - mape: 8.3660 - val_loss: 0.2616 - val_mse: 0.2612 - val_rmse: 0.5111 - val_mae: 0.2616 - val_mape: 8.3906 - lr: 1.0000e-05\n",
      "Epoch 315/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2618 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2618 - mape: 8.3455\n",
      "Epoch 315: val_loss improved from 0.26145 to 0.26141, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2607 - rmse: 0.5105 - mae: 0.2621 - mape: 8.3748 - val_loss: 0.2614 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2614 - val_mape: 8.3658 - lr: 1.0000e-05\n",
      "Epoch 316/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2619 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2619 - mape: 8.3592\n",
      "Epoch 316: val_loss did not improve from 0.26141\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2621 - mape: 8.3675 - val_loss: 0.2622 - val_mse: 0.2633 - val_rmse: 0.5132 - val_mae: 0.2622 - val_mape: 8.4518 - lr: 1.0000e-05\n",
      "Epoch 317/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2624 - mse: 0.2618 - rmse: 0.5116 - mae: 0.2624 - mape: 8.3970\n",
      "Epoch 317: val_loss did not improve from 0.26141\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2612 - rmse: 0.5110 - mae: 0.2620 - mape: 8.3849 - val_loss: 0.2618 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2618 - val_mape: 8.4115 - lr: 1.0000e-05\n",
      "Epoch 318/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2628 - mse: 0.2627 - rmse: 0.5126 - mae: 0.2628 - mape: 8.3924\n",
      "Epoch 318: val_loss did not improve from 0.26141\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2621 - mape: 8.3719 - val_loss: 0.2614 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2614 - val_mape: 8.3460 - lr: 1.0000e-05\n",
      "Epoch 319/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2624 - mse: 0.2611 - rmse: 0.5109 - mae: 0.2624 - mape: 8.3846\n",
      "Epoch 319: val_loss did not improve from 0.26141\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2620 - mape: 8.3755 - val_loss: 0.2633 - val_mse: 0.2652 - val_rmse: 0.5150 - val_mae: 0.2633 - val_mape: 8.4923 - lr: 1.0000e-05\n",
      "Epoch 320/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2615 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2615 - mape: 8.3499\n",
      "Epoch 320: val_loss did not improve from 0.26141\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2620 - mape: 8.3779 - val_loss: 0.2616 - val_mse: 0.2608 - val_rmse: 0.5106 - val_mae: 0.2616 - val_mape: 8.3920 - lr: 1.0000e-05\n",
      "Epoch 321/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2613 - mape: 8.3479\n",
      "Epoch 321: val_loss did not improve from 0.26141\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2617 - mape: 8.3706 - val_loss: 0.2617 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2617 - val_mape: 8.4014 - lr: 1.0000e-05\n",
      "Epoch 322/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2619 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2619 - mape: 8.3646\n",
      "Epoch 322: val_loss did not improve from 0.26141\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2621 - mape: 8.3681 - val_loss: 0.2617 - val_mse: 0.2614 - val_rmse: 0.5112 - val_mae: 0.2617 - val_mape: 8.4065 - lr: 1.0000e-05\n",
      "Epoch 323/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2625 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2625 - mape: 8.4005\n",
      "Epoch 323: val_loss did not improve from 0.26141\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2621 - mape: 8.3887 - val_loss: 0.2614 - val_mse: 0.2588 - val_rmse: 0.5088 - val_mae: 0.2614 - val_mape: 8.3427 - lr: 1.0000e-05\n",
      "Epoch 324/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2617 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2617 - mape: 8.3627\n",
      "Epoch 324: val_loss did not improve from 0.26141\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2623 - mse: 0.2617 - rmse: 0.5116 - mae: 0.2623 - mape: 8.3811 - val_loss: 0.2615 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2615 - val_mape: 8.3426 - lr: 1.0000e-05\n",
      "Epoch 325/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2617 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2617 - mape: 8.3697\n",
      "Epoch 325: val_loss did not improve from 0.26141\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2618 - mape: 8.3683 - val_loss: 0.2618 - val_mse: 0.2619 - val_rmse: 0.5117 - val_mae: 0.2618 - val_mape: 8.4277 - lr: 1.0000e-05\n",
      "Epoch 326/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2629 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2629 - mape: 8.3967\n",
      "Epoch 326: val_loss did not improve from 0.26141\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2620 - mape: 8.3723 - val_loss: 0.2618 - val_mse: 0.2595 - val_rmse: 0.5095 - val_mae: 0.2618 - val_mape: 8.3396 - lr: 1.0000e-05\n",
      "Epoch 327/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2636 - mse: 0.2648 - rmse: 0.5146 - mae: 0.2636 - mape: 8.4233\n",
      "Epoch 327: val_loss improved from 0.26141 to 0.26130, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2618 - mape: 8.3699 - val_loss: 0.2613 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2613 - val_mape: 8.3658 - lr: 1.0000e-05\n",
      "Epoch 328/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2619 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2619 - mape: 8.3969\n",
      "Epoch 328: val_loss did not improve from 0.26130\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2618 - mape: 8.3867 - val_loss: 0.2634 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2634 - val_mape: 8.3128 - lr: 1.0000e-05\n",
      "Epoch 329/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2616 - mape: 8.3514\n",
      "Epoch 329: val_loss did not improve from 0.26130\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2619 - mape: 8.3625 - val_loss: 0.2613 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2613 - val_mape: 8.3489 - lr: 1.0000e-05\n",
      "Epoch 330/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2622 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2622 - mape: 8.3802\n",
      "Epoch 330: val_loss did not improve from 0.26130\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2618 - mape: 8.3718 - val_loss: 0.2613 - val_mse: 0.2604 - val_rmse: 0.5103 - val_mae: 0.2613 - val_mape: 8.3644 - lr: 1.0000e-05\n",
      "Epoch 331/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2612 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2612 - mape: 8.3568\n",
      "Epoch 331: val_loss did not improve from 0.26130\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2621 - mape: 8.3759 - val_loss: 0.2614 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2614 - val_mape: 8.3653 - lr: 1.0000e-05\n",
      "Epoch 332/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2631 - mse: 0.2628 - rmse: 0.5127 - mae: 0.2631 - mape: 8.4043\n",
      "Epoch 332: val_loss improved from 0.26130 to 0.26127, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2619 - mape: 8.3747 - val_loss: 0.2613 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2613 - val_mape: 8.3605 - lr: 1.0000e-05\n",
      "Epoch 333/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2617 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2617 - mape: 8.3668\n",
      "Epoch 333: val_loss did not improve from 0.26127\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2616 - mape: 8.3647 - val_loss: 0.2621 - val_mse: 0.2623 - val_rmse: 0.5122 - val_mae: 0.2621 - val_mape: 8.4419 - lr: 1.0000e-05\n",
      "Epoch 334/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2634 - mse: 0.2631 - rmse: 0.5129 - mae: 0.2634 - mape: 8.4233\n",
      "Epoch 334: val_loss did not improve from 0.26127\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2622 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2622 - mape: 8.3807 - val_loss: 0.2624 - val_mse: 0.2631 - val_rmse: 0.5130 - val_mae: 0.2624 - val_mape: 8.4585 - lr: 1.0000e-05\n",
      "Epoch 335/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2609 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2609 - mape: 8.3474\n",
      "Epoch 335: val_loss did not improve from 0.26127\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2622 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2622 - mape: 8.3690 - val_loss: 0.2619 - val_mse: 0.2619 - val_rmse: 0.5117 - val_mae: 0.2619 - val_mape: 8.4214 - lr: 1.0000e-05\n",
      "Epoch 336/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2630 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2630 - mape: 8.3971\n",
      "Epoch 336: val_loss did not improve from 0.26127\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2619 - mape: 8.3748 - val_loss: 0.2613 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2613 - val_mape: 8.3415 - lr: 1.0000e-05\n",
      "Epoch 337/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2612 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2612 - mape: 8.3827\n",
      "Epoch 337: val_loss did not improve from 0.26127\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2617 - mape: 8.3744 - val_loss: 0.2613 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2613 - val_mape: 8.3329 - lr: 1.0000e-05\n",
      "Epoch 338/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2625 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2625 - mape: 8.3975\n",
      "Epoch 338: val_loss did not improve from 0.26127\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2621 - mape: 8.3854 - val_loss: 0.2613 - val_mse: 0.2585 - val_rmse: 0.5085 - val_mae: 0.2613 - val_mape: 8.3351 - lr: 1.0000e-05\n",
      "Epoch 339/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2616 - mape: 8.3566\n",
      "Epoch 339: val_loss did not improve from 0.26127\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2621 - mape: 8.3668 - val_loss: 0.2623 - val_mse: 0.2633 - val_rmse: 0.5131 - val_mae: 0.2623 - val_mape: 8.4512 - lr: 1.0000e-05\n",
      "Epoch 340/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2619 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2619 - mape: 8.3675\n",
      "Epoch 340: val_loss did not improve from 0.26127\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2619 - mape: 8.3634 - val_loss: 0.2616 - val_mse: 0.2599 - val_rmse: 0.5098 - val_mae: 0.2616 - val_mape: 8.3472 - lr: 1.0000e-05\n",
      "Epoch 341/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2620 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2620 - mape: 8.3655\n",
      "Epoch 341: val_loss did not improve from 0.26127\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2620 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2620 - mape: 8.3839 - val_loss: 0.2613 - val_mse: 0.2593 - val_rmse: 0.5093 - val_mae: 0.2613 - val_mape: 8.3388 - lr: 1.0000e-05\n",
      "Epoch 342/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2585 - rmse: 0.5085 - mae: 0.2608 - mape: 8.3446\n",
      "Epoch 342: val_loss did not improve from 0.26127\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2621 - mape: 8.3799 - val_loss: 0.2616 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2616 - val_mape: 8.3903 - lr: 1.0000e-05\n",
      "Epoch 343/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2610 - mape: 8.3518\n",
      "Epoch 343: val_loss did not improve from 0.26127\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2618 - mape: 8.3777 - val_loss: 0.2619 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2619 - val_mape: 8.3193 - lr: 1.0000e-05\n",
      "Epoch 344/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2625 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2625 - mape: 8.3846\n",
      "Epoch 344: val_loss did not improve from 0.26127\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2619 - mape: 8.3657 - val_loss: 0.2615 - val_mse: 0.2586 - val_rmse: 0.5086 - val_mae: 0.2615 - val_mape: 8.3230 - lr: 1.0000e-05\n",
      "Epoch 345/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2622 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2622 - mape: 8.3806\n",
      "Epoch 345: val_loss did not improve from 0.26127\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2619 - mape: 8.3773 - val_loss: 0.2613 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2613 - val_mape: 8.3485 - lr: 1.0000e-05\n",
      "Epoch 346/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2635 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2635 - mape: 8.4369\n",
      "Epoch 346: val_loss improved from 0.26127 to 0.26126, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2619 - mape: 8.3811 - val_loss: 0.2613 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2613 - val_mape: 8.3441 - lr: 1.0000e-05\n",
      "Epoch 347/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2621 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2621 - mape: 8.3775\n",
      "Epoch 347: val_loss improved from 0.26126 to 0.26124, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2620 - mape: 8.3790 - val_loss: 0.2612 - val_mse: 0.2590 - val_rmse: 0.5090 - val_mae: 0.2612 - val_mape: 8.3478 - lr: 1.0000e-05\n",
      "Epoch 348/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2606 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2606 - mape: 8.3395\n",
      "Epoch 348: val_loss did not improve from 0.26124\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2616 - mape: 8.3683 - val_loss: 0.2613 - val_mse: 0.2584 - val_rmse: 0.5084 - val_mae: 0.2613 - val_mape: 8.3200 - lr: 1.0000e-05\n",
      "Epoch 349/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2631 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2631 - mape: 8.3888\n",
      "Epoch 349: val_loss did not improve from 0.26124\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2619 - mape: 8.3656 - val_loss: 0.2614 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2614 - val_mape: 8.4010 - lr: 1.0000e-05\n",
      "Epoch 350/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2620 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2620 - mape: 8.3857\n",
      "Epoch 350: val_loss improved from 0.26124 to 0.26120, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2618 - mape: 8.3795 - val_loss: 0.2612 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2612 - val_mape: 8.3696 - lr: 1.0000e-05\n",
      "Epoch 351/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2629 - mse: 0.2631 - rmse: 0.5129 - mae: 0.2629 - mape: 8.4077\n",
      "Epoch 351: val_loss did not improve from 0.26120\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2617 - mape: 8.3715 - val_loss: 0.2615 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2615 - val_mape: 8.3465 - lr: 1.0000e-05\n",
      "Epoch 352/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2620 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2620 - mape: 8.3831\n",
      "Epoch 352: val_loss improved from 0.26120 to 0.26117, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2618 - mape: 8.3775 - val_loss: 0.2612 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2612 - val_mape: 8.3503 - lr: 1.0000e-05\n",
      "Epoch 353/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2613 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2613 - mape: 8.3597\n",
      "Epoch 353: val_loss did not improve from 0.26117\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2618 - mape: 8.3715 - val_loss: 0.2621 - val_mse: 0.2632 - val_rmse: 0.5130 - val_mae: 0.2621 - val_mape: 8.4447 - lr: 1.0000e-05\n",
      "Epoch 354/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2622 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2622 - mape: 8.3751\n",
      "Epoch 354: val_loss did not improve from 0.26117\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2621 - mape: 8.3794 - val_loss: 0.2614 - val_mse: 0.2599 - val_rmse: 0.5098 - val_mae: 0.2614 - val_mape: 8.3775 - lr: 1.0000e-05\n",
      "Epoch 355/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2612 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2612 - mape: 8.3443\n",
      "Epoch 355: val_loss did not improve from 0.26117\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2619 - mape: 8.3806 - val_loss: 0.2612 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2612 - val_mape: 8.3568 - lr: 1.0000e-05\n",
      "Epoch 356/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2615 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2615 - mape: 8.3614\n",
      "Epoch 356: val_loss did not improve from 0.26117\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2616 - mape: 8.3678 - val_loss: 0.2615 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2615 - val_mape: 8.3849 - lr: 1.0000e-05\n",
      "Epoch 357/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2619 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2619 - mape: 8.3674\n",
      "Epoch 357: val_loss did not improve from 0.26117\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2619 - mape: 8.3689 - val_loss: 0.2612 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2612 - val_mape: 8.3663 - lr: 1.0000e-05\n",
      "Epoch 358/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2620 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2620 - mape: 8.3764\n",
      "Epoch 358: val_loss improved from 0.26117 to 0.26112, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2620 - mape: 8.3764 - val_loss: 0.2611 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2611 - val_mape: 8.3488 - lr: 1.0000e-05\n",
      "Epoch 359/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2617 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2617 - mape: 8.3496\n",
      "Epoch 359: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2617 - mape: 8.3540 - val_loss: 0.2614 - val_mse: 0.2614 - val_rmse: 0.5113 - val_mae: 0.2614 - val_mape: 8.4001 - lr: 1.0000e-05\n",
      "Epoch 360/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2620 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2620 - mape: 8.3653\n",
      "Epoch 360: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2620 - mape: 8.3689 - val_loss: 0.2612 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2612 - val_mape: 8.3550 - lr: 1.0000e-05\n",
      "Epoch 361/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2626 - mse: 0.2627 - rmse: 0.5126 - mae: 0.2626 - mape: 8.4010\n",
      "Epoch 361: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2617 - mape: 8.3770 - val_loss: 0.2615 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2615 - val_mape: 8.3689 - lr: 1.0000e-05\n",
      "Epoch 362/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2616 - mape: 8.3533\n",
      "Epoch 362: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2604 - rmse: 0.5102 - mae: 0.2621 - mape: 8.3731 - val_loss: 0.2613 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2613 - val_mape: 8.3624 - lr: 1.0000e-05\n",
      "Epoch 363/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2618 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2618 - mape: 8.3610\n",
      "Epoch 363: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2619 - mape: 8.3699 - val_loss: 0.2617 - val_mse: 0.2588 - val_rmse: 0.5088 - val_mae: 0.2617 - val_mape: 8.3181 - lr: 1.0000e-05\n",
      "Epoch 364/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2607 - mape: 8.3300\n",
      "Epoch 364: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2617 - mape: 8.3705 - val_loss: 0.2611 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2611 - val_mape: 8.3773 - lr: 1.0000e-05\n",
      "Epoch 365/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2622 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2622 - mape: 8.3664\n",
      "Epoch 365: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2622 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2622 - mape: 8.3840 - val_loss: 0.2611 - val_mse: 0.2594 - val_rmse: 0.5094 - val_mae: 0.2611 - val_mape: 8.3590 - lr: 1.0000e-05\n",
      "Epoch 366/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2620 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2620 - mape: 8.3829\n",
      "Epoch 366: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2617 - mape: 8.3687 - val_loss: 0.2612 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2612 - val_mape: 8.3545 - lr: 1.0000e-05\n",
      "Epoch 367/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2619 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2619 - mape: 8.3830\n",
      "Epoch 367: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2619 - mape: 8.3833 - val_loss: 0.2623 - val_mse: 0.2583 - val_rmse: 0.5083 - val_mae: 0.2623 - val_mape: 8.2995 - lr: 1.0000e-05\n",
      "Epoch 368/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2619 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2619 - mape: 8.3670\n",
      "Epoch 368: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2619 - mape: 8.3670 - val_loss: 0.2611 - val_mse: 0.2591 - val_rmse: 0.5091 - val_mae: 0.2611 - val_mape: 8.3602 - lr: 1.0000e-05\n",
      "Epoch 369/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2621 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2621 - mape: 8.3817\n",
      "Epoch 369: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2617 - mape: 8.3693 - val_loss: 0.2630 - val_mse: 0.2653 - val_rmse: 0.5150 - val_mae: 0.2630 - val_mape: 8.5013 - lr: 1.0000e-05\n",
      "Epoch 370/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2622 - mse: 0.2616 - rmse: 0.5114 - mae: 0.2622 - mape: 8.3872\n",
      "Epoch 370: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2620 - mape: 8.3833 - val_loss: 0.2615 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2615 - val_mape: 8.3380 - lr: 1.0000e-05\n",
      "Epoch 371/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2629 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2629 - mape: 8.4217\n",
      "Epoch 371: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2619 - mape: 8.3659 - val_loss: 0.2613 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2613 - val_mape: 8.3340 - lr: 1.0000e-05\n",
      "Epoch 372/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2631 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2631 - mape: 8.4222\n",
      "Epoch 372: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2622 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2622 - mape: 8.3957 - val_loss: 0.2640 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2640 - val_mape: 8.3283 - lr: 1.0000e-05\n",
      "Epoch 373/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2611 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2611 - mape: 8.3323\n",
      "Epoch 373: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2616 - mape: 8.3503 - val_loss: 0.2620 - val_mse: 0.2632 - val_rmse: 0.5130 - val_mae: 0.2620 - val_mape: 8.4537 - lr: 1.0000e-05\n",
      "Epoch 374/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2625 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2625 - mape: 8.3853\n",
      "Epoch 374: val_loss did not improve from 0.26112\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2597 - rmse: 0.5097 - mae: 0.2618 - mape: 8.3664 - val_loss: 0.2629 - val_mse: 0.2645 - val_rmse: 0.5143 - val_mae: 0.2629 - val_mape: 8.4752 - lr: 1.0000e-05\n",
      "Epoch 375/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2619 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2619 - mape: 8.3736\n",
      "Epoch 375: val_loss improved from 0.26112 to 0.26107, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2620 - mape: 8.3754 - val_loss: 0.2611 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2611 - val_mape: 8.3557 - lr: 1.0000e-05\n",
      "Epoch 376/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2614 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2614 - mape: 8.3593\n",
      "Epoch 376: val_loss did not improve from 0.26107\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2616 - mape: 8.3717 - val_loss: 0.2612 - val_mse: 0.2583 - val_rmse: 0.5083 - val_mae: 0.2612 - val_mape: 8.3282 - lr: 1.0000e-05\n",
      "Epoch 377/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2618 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2618 - mape: 8.3773\n",
      "Epoch 377: val_loss did not improve from 0.26107\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2617 - mape: 8.3700 - val_loss: 0.2613 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2613 - val_mape: 8.3100 - lr: 1.0000e-05\n",
      "Epoch 378/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2613 - mape: 8.3715\n",
      "Epoch 378: val_loss did not improve from 0.26107\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2589 - rmse: 0.5089 - mae: 0.2616 - mape: 8.3538 - val_loss: 0.2617 - val_mse: 0.2615 - val_rmse: 0.5113 - val_mae: 0.2617 - val_mape: 8.4226 - lr: 1.0000e-05\n",
      "Epoch 379/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2622 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2622 - mape: 8.3801\n",
      "Epoch 379: val_loss did not improve from 0.26107\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2618 - mape: 8.3726 - val_loss: 0.2615 - val_mse: 0.2604 - val_rmse: 0.5103 - val_mae: 0.2615 - val_mape: 8.3813 - lr: 1.0000e-05\n",
      "Epoch 380/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2624 - mse: 0.2614 - rmse: 0.5112 - mae: 0.2624 - mape: 8.3815\n",
      "Epoch 380: val_loss did not improve from 0.26107\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2618 - mape: 8.3635 - val_loss: 0.2613 - val_mse: 0.2610 - val_rmse: 0.5108 - val_mae: 0.2613 - val_mape: 8.3912 - lr: 1.0000e-05\n",
      "Epoch 381/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2620 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2620 - mape: 8.3874\n",
      "Epoch 381: val_loss improved from 0.26107 to 0.26106, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2618 - mape: 8.3734 - val_loss: 0.2611 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2611 - val_mape: 8.3630 - lr: 1.0000e-05\n",
      "Epoch 382/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2626 - mse: 0.2617 - rmse: 0.5116 - mae: 0.2626 - mape: 8.4052\n",
      "Epoch 382: val_loss did not improve from 0.26106\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2617 - mape: 8.3664 - val_loss: 0.2613 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2613 - val_mape: 8.3391 - lr: 1.0000e-05\n",
      "Epoch 383/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2620 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2620 - mape: 8.3700\n",
      "Epoch 383: val_loss did not improve from 0.26106\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2616 - mape: 8.3659 - val_loss: 0.2611 - val_mse: 0.2585 - val_rmse: 0.5085 - val_mae: 0.2611 - val_mape: 8.3243 - lr: 1.0000e-05\n",
      "Epoch 384/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2612 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2612 - mape: 8.3485\n",
      "Epoch 384: val_loss did not improve from 0.26106\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2614 - mape: 8.3521 - val_loss: 0.2616 - val_mse: 0.2611 - val_rmse: 0.5110 - val_mae: 0.2616 - val_mape: 8.4089 - lr: 1.0000e-05\n",
      "Epoch 385/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2631 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2631 - mape: 8.4137\n",
      "Epoch 385: val_loss improved from 0.26106 to 0.26103, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2617 - mape: 8.3683 - val_loss: 0.2610 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2610 - val_mape: 8.3261 - lr: 1.0000e-05\n",
      "Epoch 386/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2607 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2607 - mape: 8.3489\n",
      "Epoch 386: val_loss did not improve from 0.26103\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2616 - mape: 8.3660 - val_loss: 0.2623 - val_mse: 0.2620 - val_rmse: 0.5119 - val_mae: 0.2623 - val_mape: 8.4323 - lr: 1.0000e-05\n",
      "Epoch 387/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2606 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2606 - mape: 8.3132\n",
      "Epoch 387: val_loss did not improve from 0.26103\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2613 - mape: 8.3488 - val_loss: 0.2618 - val_mse: 0.2592 - val_rmse: 0.5092 - val_mae: 0.2618 - val_mape: 8.3219 - lr: 1.0000e-05\n",
      "Epoch 388/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2621 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2621 - mape: 8.3796\n",
      "Epoch 388: val_loss did not improve from 0.26103\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2598 - rmse: 0.5098 - mae: 0.2616 - mape: 8.3685 - val_loss: 0.2624 - val_mse: 0.2586 - val_rmse: 0.5086 - val_mae: 0.2624 - val_mape: 8.2972 - lr: 1.0000e-05\n",
      "Epoch 389/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2616 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2616 - mape: 8.3586\n",
      "Epoch 389: val_loss did not improve from 0.26103\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2616 - mape: 8.3588 - val_loss: 0.2619 - val_mse: 0.2612 - val_rmse: 0.5111 - val_mae: 0.2619 - val_mape: 8.4133 - lr: 1.0000e-05\n",
      "Epoch 390/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2609 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2609 - mape: 8.3504\n",
      "Epoch 390: val_loss did not improve from 0.26103\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2614 - mape: 8.3707 - val_loss: 0.2610 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2610 - val_mape: 8.3321 - lr: 1.0000e-05\n",
      "Epoch 391/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2627 - mse: 0.2627 - rmse: 0.5125 - mae: 0.2627 - mape: 8.3829\n",
      "Epoch 391: val_loss did not improve from 0.26103\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2617 - mape: 8.3537 - val_loss: 0.2612 - val_mse: 0.2595 - val_rmse: 0.5095 - val_mae: 0.2612 - val_mape: 8.3396 - lr: 1.0000e-05\n",
      "Epoch 392/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2610 - mape: 8.3539\n",
      "Epoch 392: val_loss did not improve from 0.26103\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2614 - mape: 8.3727 - val_loss: 0.2613 - val_mse: 0.2578 - val_rmse: 0.5078 - val_mae: 0.2613 - val_mape: 8.3068 - lr: 1.0000e-05\n",
      "Epoch 393/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2629 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2629 - mape: 8.3714\n",
      "Epoch 393: val_loss improved from 0.26103 to 0.26098, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2620 - mape: 8.3661 - val_loss: 0.2610 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2610 - val_mape: 8.3426 - lr: 1.0000e-05\n",
      "Epoch 394/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2615 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2615 - mape: 8.3626\n",
      "Epoch 394: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2618 - mape: 8.3653 - val_loss: 0.2619 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2619 - val_mape: 8.3072 - lr: 1.0000e-05\n",
      "Epoch 395/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2619 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2619 - mape: 8.3528\n",
      "Epoch 395: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2619 - mape: 8.3528 - val_loss: 0.2614 - val_mse: 0.2620 - val_rmse: 0.5118 - val_mae: 0.2614 - val_mape: 8.4213 - lr: 1.0000e-05\n",
      "Epoch 396/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2608 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2608 - mape: 8.3141\n",
      "Epoch 396: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2617 - mape: 8.3770 - val_loss: 0.2613 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2613 - val_mape: 8.3817 - lr: 1.0000e-05\n",
      "Epoch 397/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2601 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2601 - mape: 8.3032\n",
      "Epoch 397: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2616 - mape: 8.3767 - val_loss: 0.2617 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2617 - val_mape: 8.2940 - lr: 1.0000e-05\n",
      "Epoch 398/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2618 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2618 - mape: 8.3612\n",
      "Epoch 398: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2616 - mape: 8.3541 - val_loss: 0.2617 - val_mse: 0.2608 - val_rmse: 0.5107 - val_mae: 0.2617 - val_mape: 8.4037 - lr: 1.0000e-05\n",
      "Epoch 399/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2627 - mse: 0.2629 - rmse: 0.5128 - mae: 0.2627 - mape: 8.4333\n",
      "Epoch 399: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2619 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2619 - mape: 8.3829 - val_loss: 0.2615 - val_mse: 0.2609 - val_rmse: 0.5108 - val_mae: 0.2615 - val_mape: 8.4023 - lr: 1.0000e-05\n",
      "Epoch 400/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2624 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2624 - mape: 8.4125\n",
      "Epoch 400: val_loss improved from 0.26098 to 0.26090, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2614 - mape: 8.3694 - val_loss: 0.2609 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2609 - val_mape: 8.3398 - lr: 1.0000e-05\n",
      "Epoch 401/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2610 - mape: 8.3192\n",
      "Epoch 401: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2613 - mape: 8.3491 - val_loss: 0.2613 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2613 - val_mape: 8.3618 - lr: 1.0000e-05\n",
      "Epoch 402/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2620 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2620 - mape: 8.3730\n",
      "Epoch 402: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2615 - mape: 8.3506 - val_loss: 0.2612 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2612 - val_mape: 8.3942 - lr: 1.0000e-05\n",
      "Epoch 403/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2618 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2618 - mape: 8.3653\n",
      "Epoch 403: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2615 - mape: 8.3568 - val_loss: 0.2655 - val_mse: 0.2724 - val_rmse: 0.5220 - val_mae: 0.2655 - val_mape: 8.6387 - lr: 1.0000e-05\n",
      "Epoch 404/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2621 - mse: 0.2628 - rmse: 0.5126 - mae: 0.2621 - mape: 8.4150\n",
      "Epoch 404: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2617 - mape: 8.3806 - val_loss: 0.2615 - val_mse: 0.2612 - val_rmse: 0.5110 - val_mae: 0.2615 - val_mape: 8.3934 - lr: 1.0000e-05\n",
      "Epoch 405/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2615 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2615 - mape: 8.3469\n",
      "Epoch 405: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2617 - mape: 8.3500 - val_loss: 0.2621 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2621 - val_mape: 8.3231 - lr: 1.0000e-05\n",
      "Epoch 406/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2615 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2615 - mape: 8.3576\n",
      "Epoch 406: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2615 - mape: 8.3576 - val_loss: 0.2627 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2627 - val_mape: 8.3086 - lr: 1.0000e-05\n",
      "Epoch 407/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2614 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2614 - mape: 8.3559\n",
      "Epoch 407: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2620 - mape: 8.3772 - val_loss: 0.2612 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2612 - val_mape: 8.3826 - lr: 1.0000e-05\n",
      "Epoch 408/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2608 - mape: 8.3441\n",
      "Epoch 408: val_loss improved from 0.26090 to 0.26088, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2614 - mape: 8.3614 - val_loss: 0.2609 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2609 - val_mape: 8.3339 - lr: 1.0000e-05\n",
      "Epoch 409/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2621 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2621 - mape: 8.3634\n",
      "Epoch 409: val_loss did not improve from 0.26088\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2605 - rmse: 0.5103 - mae: 0.2615 - mape: 8.3567 - val_loss: 0.2609 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2609 - val_mape: 8.3476 - lr: 1.0000e-05\n",
      "Epoch 410/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2621 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2621 - mape: 8.3632\n",
      "Epoch 410: val_loss did not improve from 0.26088\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2614 - mape: 8.3572 - val_loss: 0.2610 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2610 - val_mape: 8.3651 - lr: 1.0000e-05\n",
      "Epoch 411/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2618 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2618 - mape: 8.3695\n",
      "Epoch 411: val_loss did not improve from 0.26088\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2618 - mape: 8.3678 - val_loss: 0.2613 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2613 - val_mape: 8.3951 - lr: 1.0000e-05\n",
      "Epoch 412/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2613 - mape: 8.3751\n",
      "Epoch 412: val_loss did not improve from 0.26088\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2615 - mape: 8.3651 - val_loss: 0.2609 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2609 - val_mape: 8.3575 - lr: 1.0000e-05\n",
      "Epoch 413/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2626 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2626 - mape: 8.3720\n",
      "Epoch 413: val_loss did not improve from 0.26088\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2621 - mse: 0.2617 - rmse: 0.5115 - mae: 0.2621 - mape: 8.3728 - val_loss: 0.2610 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2610 - val_mape: 8.3292 - lr: 1.0000e-05\n",
      "Epoch 414/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2619 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2619 - mape: 8.3672\n",
      "Epoch 414: val_loss improved from 0.26088 to 0.26086, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2617 - mape: 8.3638 - val_loss: 0.2609 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2609 - val_mape: 8.3517 - lr: 1.0000e-05\n",
      "Epoch 415/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2613 - mape: 8.3435\n",
      "Epoch 415: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2615 - mape: 8.3682 - val_loss: 0.2610 - val_mse: 0.2593 - val_rmse: 0.5093 - val_mae: 0.2610 - val_mape: 8.3499 - lr: 1.0000e-05\n",
      "Epoch 416/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2604 - mape: 8.3219\n",
      "Epoch 416: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2613 - mape: 8.3514 - val_loss: 0.2609 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2609 - val_mape: 8.3529 - lr: 1.0000e-05\n",
      "Epoch 417/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2613 - mape: 8.3459\n",
      "Epoch 417: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2615 - mape: 8.3743 - val_loss: 0.2609 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2609 - val_mape: 8.3174 - lr: 1.0000e-05\n",
      "Epoch 418/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2610 - mape: 8.3709\n",
      "Epoch 418: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2612 - mape: 8.3446 - val_loss: 0.2612 - val_mse: 0.2609 - val_rmse: 0.5108 - val_mae: 0.2612 - val_mape: 8.3877 - lr: 1.0000e-05\n",
      "Epoch 419/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2617 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2617 - mape: 8.3669\n",
      "Epoch 419: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2603 - rmse: 0.5101 - mae: 0.2618 - mape: 8.3712 - val_loss: 0.2618 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2618 - val_mape: 8.3135 - lr: 1.0000e-05\n",
      "Epoch 420/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2640 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2640 - mape: 8.4489\n",
      "Epoch 420: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2620 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2620 - mape: 8.3683 - val_loss: 0.2613 - val_mse: 0.2619 - val_rmse: 0.5118 - val_mae: 0.2613 - val_mape: 8.4132 - lr: 1.0000e-05\n",
      "Epoch 421/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2608 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2608 - mape: 8.3321\n",
      "Epoch 421: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2617 - mape: 8.3603 - val_loss: 0.2610 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2610 - val_mape: 8.3115 - lr: 1.0000e-05\n",
      "Epoch 422/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2609 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2609 - mape: 8.3377\n",
      "Epoch 422: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2613 - mape: 8.3501 - val_loss: 0.2610 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2610 - val_mape: 8.3350 - lr: 1.0000e-05\n",
      "Epoch 423/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2613 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2613 - mape: 8.3508\n",
      "Epoch 423: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2613 - mape: 8.3463 - val_loss: 0.2622 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2622 - val_mape: 8.3129 - lr: 1.0000e-05\n",
      "Epoch 424/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2607 - mse: 0.2594 - rmse: 0.5094 - mae: 0.2607 - mape: 8.3422\n",
      "Epoch 424: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2614 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2614 - mape: 8.3587 - val_loss: 0.2614 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2614 - val_mape: 8.3409 - lr: 1.0000e-05\n",
      "Epoch 425/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2604 - mse: 0.2590 - rmse: 0.5090 - mae: 0.2604 - mape: 8.3401\n",
      "Epoch 425: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2618 - mape: 8.3684 - val_loss: 0.2611 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2611 - val_mape: 8.3255 - lr: 1.0000e-05\n",
      "Epoch 426/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2618 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2618 - mape: 8.3571\n",
      "Epoch 426: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2617 - mape: 8.3598 - val_loss: 0.2611 - val_mse: 0.2611 - val_rmse: 0.5110 - val_mae: 0.2611 - val_mape: 8.3849 - lr: 1.0000e-05\n",
      "Epoch 427/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2610 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2610 - mape: 8.3415\n",
      "Epoch 427: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2613 - mape: 8.3492 - val_loss: 0.2609 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2609 - val_mape: 8.3530 - lr: 1.0000e-05\n",
      "Epoch 428/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2609 - mape: 8.3244\n",
      "Epoch 428: val_loss did not improve from 0.26086\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2615 - mape: 8.3534 - val_loss: 0.2610 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2610 - val_mape: 8.3678 - lr: 1.0000e-05\n",
      "Epoch 429/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2616 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2616 - mape: 8.3600\n",
      "Epoch 429: val_loss improved from 0.26086 to 0.26076, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2616 - mape: 8.3584 - val_loss: 0.2608 - val_mse: 0.2595 - val_rmse: 0.5095 - val_mae: 0.2608 - val_mape: 8.3438 - lr: 1.0000e-05\n",
      "Epoch 430/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2626 - mse: 0.2620 - rmse: 0.5119 - mae: 0.2626 - mape: 8.3830\n",
      "Epoch 430: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2617 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2617 - mape: 8.3703 - val_loss: 0.2610 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2610 - val_mape: 8.3728 - lr: 1.0000e-05\n",
      "Epoch 431/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2617 - mse: 0.2617 - rmse: 0.5115 - mae: 0.2617 - mape: 8.3637\n",
      "Epoch 431: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2615 - mape: 8.3671 - val_loss: 0.2609 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2609 - val_mape: 8.3262 - lr: 1.0000e-05\n",
      "Epoch 432/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2616 - mape: 8.3697\n",
      "Epoch 432: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2616 - mape: 8.3525 - val_loss: 0.2610 - val_mse: 0.2604 - val_rmse: 0.5103 - val_mae: 0.2610 - val_mape: 8.3836 - lr: 1.0000e-05\n",
      "Epoch 433/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2618 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2618 - mape: 8.3741\n",
      "Epoch 433: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2614 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2614 - mape: 8.3555 - val_loss: 0.2617 - val_mse: 0.2631 - val_rmse: 0.5129 - val_mae: 0.2617 - val_mape: 8.4457 - lr: 1.0000e-05\n",
      "Epoch 434/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2611 - mse: 0.2593 - rmse: 0.5093 - mae: 0.2611 - mape: 8.3634\n",
      "Epoch 434: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2612 - mape: 8.3608 - val_loss: 0.2618 - val_mse: 0.2629 - val_rmse: 0.5127 - val_mae: 0.2618 - val_mape: 8.4319 - lr: 1.0000e-05\n",
      "Epoch 435/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2613 - mape: 8.3480\n",
      "Epoch 435: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2614 - mape: 8.3486 - val_loss: 0.2609 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2609 - val_mape: 8.3757 - lr: 1.0000e-05\n",
      "Epoch 436/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2618 - mse: 0.2607 - rmse: 0.5105 - mae: 0.2618 - mape: 8.3820\n",
      "Epoch 436: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2615 - mape: 8.3689 - val_loss: 0.2613 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2613 - val_mape: 8.3176 - lr: 1.0000e-05\n",
      "Epoch 437/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2611 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2611 - mape: 8.3329\n",
      "Epoch 437: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2611 - mape: 8.3404 - val_loss: 0.2611 - val_mse: 0.2609 - val_rmse: 0.5108 - val_mae: 0.2611 - val_mape: 8.3814 - lr: 1.0000e-05\n",
      "Epoch 438/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2617 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2617 - mape: 8.3925\n",
      "Epoch 438: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2616 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2616 - mape: 8.3854 - val_loss: 0.2608 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2608 - val_mape: 8.3405 - lr: 1.0000e-05\n",
      "Epoch 439/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2604 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2604 - mape: 8.3217\n",
      "Epoch 439: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2616 - mape: 8.3539 - val_loss: 0.2608 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2608 - val_mape: 8.3169 - lr: 1.0000e-05\n",
      "Epoch 440/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2621 - mse: 0.2621 - rmse: 0.5119 - mae: 0.2621 - mape: 8.3866\n",
      "Epoch 440: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2615 - mape: 8.3572 - val_loss: 0.2615 - val_mse: 0.2623 - val_rmse: 0.5121 - val_mae: 0.2615 - val_mape: 8.4250 - lr: 1.0000e-05\n",
      "Epoch 441/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2621 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2621 - mape: 8.3788\n",
      "Epoch 441: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2612 - mape: 8.3509 - val_loss: 0.2617 - val_mse: 0.2619 - val_rmse: 0.5117 - val_mae: 0.2617 - val_mape: 8.4082 - lr: 1.0000e-05\n",
      "Epoch 442/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2622 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2622 - mape: 8.3653\n",
      "Epoch 442: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2606 - rmse: 0.5104 - mae: 0.2616 - mape: 8.3616 - val_loss: 0.2612 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2612 - val_mape: 8.2872 - lr: 1.0000e-05\n",
      "Epoch 443/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2615 - mse: 0.2590 - rmse: 0.5090 - mae: 0.2615 - mape: 8.3373\n",
      "Epoch 443: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2616 - mape: 8.3382 - val_loss: 0.2624 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2624 - val_mape: 8.3111 - lr: 1.0000e-05\n",
      "Epoch 444/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2614 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2614 - mape: 8.3297\n",
      "Epoch 444: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2614 - mape: 8.3389 - val_loss: 0.2616 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2616 - val_mape: 8.3025 - lr: 1.0000e-05\n",
      "Epoch 445/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2622 - mse: 0.2598 - rmse: 0.5098 - mae: 0.2622 - mape: 8.3769\n",
      "Epoch 445: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2616 - mape: 8.3537 - val_loss: 0.2609 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2609 - val_mape: 8.3348 - lr: 1.0000e-05\n",
      "Epoch 446/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2621 - mse: 0.2608 - rmse: 0.5106 - mae: 0.2621 - mape: 8.3424\n",
      "Epoch 446: val_loss improved from 0.26076 to 0.26076, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2618 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2618 - mape: 8.3502 - val_loss: 0.2608 - val_mse: 0.2588 - val_rmse: 0.5088 - val_mae: 0.2608 - val_mape: 8.3255 - lr: 1.0000e-05\n",
      "Epoch 447/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2628 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2628 - mape: 8.3803\n",
      "Epoch 447: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2615 - mape: 8.3553 - val_loss: 0.2610 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2610 - val_mape: 8.3007 - lr: 1.0000e-05\n",
      "Epoch 448/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2610 - mape: 8.3294\n",
      "Epoch 448: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2616 - mape: 8.3554 - val_loss: 0.2610 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2610 - val_mape: 8.3209 - lr: 1.0000e-05\n",
      "Epoch 449/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2619 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2619 - mape: 8.3616\n",
      "Epoch 449: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2613 - mape: 8.3488 - val_loss: 0.2608 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2608 - val_mape: 8.3671 - lr: 1.0000e-05\n",
      "Epoch 450/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2594 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2594 - mape: 8.3130\n",
      "Epoch 450: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2612 - mape: 8.3608 - val_loss: 0.2610 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2610 - val_mape: 8.3136 - lr: 1.0000e-05\n",
      "Epoch 451/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2610 - mape: 8.3393\n",
      "Epoch 451: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2611 - mape: 8.3494 - val_loss: 0.2616 - val_mse: 0.2612 - val_rmse: 0.5111 - val_mae: 0.2616 - val_mape: 8.4066 - lr: 1.0000e-05\n",
      "Epoch 452/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2601 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2601 - mape: 8.2934\n",
      "Epoch 452: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2615 - mape: 8.3522 - val_loss: 0.2620 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2620 - val_mape: 8.2902 - lr: 1.0000e-05\n",
      "Epoch 453/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2610 - mse: 0.2592 - rmse: 0.5092 - mae: 0.2610 - mape: 8.3312\n",
      "Epoch 453: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2614 - mape: 8.3466 - val_loss: 0.2612 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2612 - val_mape: 8.3862 - lr: 1.0000e-05\n",
      "Epoch 454/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2609 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2609 - mape: 8.3514\n",
      "Epoch 454: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2613 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2613 - mape: 8.3565 - val_loss: 0.2611 - val_mse: 0.2608 - val_rmse: 0.5107 - val_mae: 0.2611 - val_mape: 8.3874 - lr: 1.0000e-05\n",
      "Epoch 455/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2625 - mse: 0.2623 - rmse: 0.5122 - mae: 0.2625 - mape: 8.3866\n",
      "Epoch 455: val_loss did not improve from 0.26076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2616 - mape: 8.3571 - val_loss: 0.2613 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2613 - val_mape: 8.3812 - lr: 1.0000e-05\n",
      "Epoch 456/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2612 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2612 - mape: 8.3355\n",
      "Epoch 456: val_loss improved from 0.26076 to 0.26069, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2615 - mape: 8.3534 - val_loss: 0.2607 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2607 - val_mape: 8.3171 - lr: 1.0000e-05\n",
      "Epoch 457/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2624 - mse: 0.2617 - rmse: 0.5116 - mae: 0.2624 - mape: 8.3575\n",
      "Epoch 457: val_loss did not improve from 0.26069\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2617 - mape: 8.3396 - val_loss: 0.2617 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2617 - val_mape: 8.3124 - lr: 1.0000e-05\n",
      "Epoch 458/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2626 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2626 - mape: 8.3980\n",
      "Epoch 458: val_loss did not improve from 0.26069\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2613 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2613 - mape: 8.3535 - val_loss: 0.2611 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2611 - val_mape: 8.3227 - lr: 1.0000e-05\n",
      "Epoch 459/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2616 - mape: 8.3544\n",
      "Epoch 459: val_loss did not improve from 0.26069\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2613 - mape: 8.3404 - val_loss: 0.2615 - val_mse: 0.2620 - val_rmse: 0.5119 - val_mae: 0.2615 - val_mape: 8.4179 - lr: 1.0000e-05\n",
      "Epoch 460/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2607 - mape: 8.3374\n",
      "Epoch 460: val_loss did not improve from 0.26069\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2615 - mape: 8.3606 - val_loss: 0.2609 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2609 - val_mape: 8.3156 - lr: 1.0000e-05\n",
      "Epoch 461/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2613 - mape: 8.3284\n",
      "Epoch 461: val_loss improved from 0.26069 to 0.26063, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2613 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2613 - mape: 8.3433 - val_loss: 0.2606 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2606 - val_mape: 8.3269 - lr: 1.0000e-05\n",
      "Epoch 462/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2616 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2616 - mape: 8.3626\n",
      "Epoch 462: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2613 - mape: 8.3558 - val_loss: 0.2617 - val_mse: 0.2616 - val_rmse: 0.5114 - val_mae: 0.2617 - val_mape: 8.4176 - lr: 1.0000e-05\n",
      "Epoch 463/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2603 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2603 - mape: 8.3114\n",
      "Epoch 463: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2611 - mape: 8.3489 - val_loss: 0.2612 - val_mse: 0.2613 - val_rmse: 0.5112 - val_mae: 0.2612 - val_mape: 8.3987 - lr: 1.0000e-05\n",
      "Epoch 464/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2616 - mape: 8.3606\n",
      "Epoch 464: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2613 - mape: 8.3574 - val_loss: 0.2607 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2607 - val_mape: 8.3171 - lr: 1.0000e-05\n",
      "Epoch 465/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2616 - mape: 8.3524\n",
      "Epoch 465: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2614 - mape: 8.3535 - val_loss: 0.2607 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2607 - val_mape: 8.3514 - lr: 1.0000e-05\n",
      "Epoch 466/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2613 - mape: 8.3577\n",
      "Epoch 466: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2612 - mape: 8.3484 - val_loss: 0.2622 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2622 - val_mape: 8.2954 - lr: 1.0000e-05\n",
      "Epoch 467/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2616 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2616 - mape: 8.3555\n",
      "Epoch 467: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2616 - mape: 8.3538 - val_loss: 0.2607 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2607 - val_mape: 8.3229 - lr: 1.0000e-05\n",
      "Epoch 468/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2616 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2616 - mape: 8.3678\n",
      "Epoch 468: val_loss improved from 0.26063 to 0.26063, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2617 - mape: 8.3696 - val_loss: 0.2606 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2606 - val_mape: 8.3217 - lr: 1.0000e-05\n",
      "Epoch 469/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2600 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2600 - mape: 8.3052\n",
      "Epoch 469: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2612 - mape: 8.3453 - val_loss: 0.2610 - val_mse: 0.2589 - val_rmse: 0.5089 - val_mae: 0.2610 - val_mape: 8.3173 - lr: 1.0000e-05\n",
      "Epoch 470/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2610 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2610 - mape: 8.3312\n",
      "Epoch 470: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2597 - rmse: 0.5097 - mae: 0.2613 - mape: 8.3437 - val_loss: 0.2609 - val_mse: 0.2605 - val_rmse: 0.5103 - val_mae: 0.2609 - val_mape: 8.3718 - lr: 1.0000e-05\n",
      "Epoch 471/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2617 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2617 - mape: 8.3734\n",
      "Epoch 471: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2613 - mape: 8.3571 - val_loss: 0.2607 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2607 - val_mape: 8.3550 - lr: 1.0000e-05\n",
      "Epoch 472/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2615 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2615 - mape: 8.3794\n",
      "Epoch 472: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2613 - mape: 8.3592 - val_loss: 0.2607 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2607 - val_mape: 8.3138 - lr: 1.0000e-05\n",
      "Epoch 473/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2619 - mse: 0.2615 - rmse: 0.5113 - mae: 0.2619 - mape: 8.3586\n",
      "Epoch 473: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2607 - rmse: 0.5105 - mae: 0.2614 - mape: 8.3538 - val_loss: 0.2619 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2619 - val_mape: 8.2922 - lr: 1.0000e-05\n",
      "Epoch 474/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2617 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2617 - mape: 8.3544\n",
      "Epoch 474: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2616 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2616 - mape: 8.3589 - val_loss: 0.2615 - val_mse: 0.2626 - val_rmse: 0.5125 - val_mae: 0.2615 - val_mape: 8.4249 - lr: 1.0000e-05\n",
      "Epoch 475/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2613 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2613 - mape: 8.3361\n",
      "Epoch 475: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2613 - mape: 8.3361 - val_loss: 0.2617 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2617 - val_mape: 8.3049 - lr: 1.0000e-05\n",
      "Epoch 476/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2617 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2617 - mape: 8.3703\n",
      "Epoch 476: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2613 - mape: 8.3590 - val_loss: 0.2625 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2625 - val_mape: 8.2807 - lr: 1.0000e-05\n",
      "Epoch 477/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2611 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2611 - mape: 8.3246\n",
      "Epoch 477: val_loss improved from 0.26063 to 0.26063, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2613 - mape: 8.3364 - val_loss: 0.2606 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2606 - val_mape: 8.3080 - lr: 1.0000e-05\n",
      "Epoch 478/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2591 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2591 - mape: 8.2729\n",
      "Epoch 478: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2612 - mape: 8.3466 - val_loss: 0.2607 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2607 - val_mape: 8.3153 - lr: 1.0000e-05\n",
      "Epoch 479/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2612 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2612 - mape: 8.3245\n",
      "Epoch 479: val_loss did not improve from 0.26063\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2611 - mape: 8.3485 - val_loss: 0.2608 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2608 - val_mape: 8.3102 - lr: 1.0000e-05\n",
      "Epoch 480/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2609 - mape: 8.3259\n",
      "Epoch 480: val_loss improved from 0.26063 to 0.26055, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2609 - mape: 8.3354 - val_loss: 0.2606 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2606 - val_mape: 8.3168 - lr: 1.0000e-05\n",
      "Epoch 481/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2614 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2614 - mape: 8.3573\n",
      "Epoch 481: val_loss did not improve from 0.26055\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2613 - mape: 8.3483 - val_loss: 0.2609 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2609 - val_mape: 8.3802 - lr: 1.0000e-05\n",
      "Epoch 482/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2624 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2624 - mape: 8.3879\n",
      "Epoch 482: val_loss did not improve from 0.26055\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2615 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2615 - mape: 8.3538 - val_loss: 0.2617 - val_mse: 0.2629 - val_rmse: 0.5127 - val_mae: 0.2617 - val_mape: 8.4223 - lr: 1.0000e-05\n",
      "Epoch 483/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2600 - mse: 0.2548 - rmse: 0.5048 - mae: 0.2600 - mape: 8.2760\n",
      "Epoch 483: val_loss did not improve from 0.26055\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2594 - rmse: 0.5094 - mae: 0.2611 - mape: 8.3340 - val_loss: 0.2607 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2607 - val_mape: 8.3531 - lr: 1.0000e-05\n",
      "Epoch 484/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2605 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2605 - mape: 8.3155\n",
      "Epoch 484: val_loss did not improve from 0.26055\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2611 - mape: 8.3415 - val_loss: 0.2609 - val_mse: 0.2585 - val_rmse: 0.5084 - val_mae: 0.2609 - val_mape: 8.2977 - lr: 1.0000e-05\n",
      "Epoch 485/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2612 - mse: 0.2595 - rmse: 0.5095 - mae: 0.2612 - mape: 8.3433\n",
      "Epoch 485: val_loss did not improve from 0.26055\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2612 - mape: 8.3441 - val_loss: 0.2614 - val_mse: 0.2619 - val_rmse: 0.5117 - val_mae: 0.2614 - val_mape: 8.4092 - lr: 1.0000e-05\n",
      "Epoch 486/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2611 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2611 - mape: 8.3394\n",
      "Epoch 486: val_loss did not improve from 0.26055\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2613 - mape: 8.3444 - val_loss: 0.2615 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2615 - val_mape: 8.3039 - lr: 1.0000e-05\n",
      "Epoch 487/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2614 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2614 - mape: 8.3555\n",
      "Epoch 487: val_loss did not improve from 0.26055\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2614 - mape: 8.3568 - val_loss: 0.2614 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2614 - val_mape: 8.2929 - lr: 1.0000e-05\n",
      "Epoch 488/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2601 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2601 - mape: 8.3184\n",
      "Epoch 488: val_loss did not improve from 0.26055\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2614 - mape: 8.3523 - val_loss: 0.2613 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2613 - val_mape: 8.2639 - lr: 1.0000e-05\n",
      "Epoch 489/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2612 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2612 - mape: 8.3344\n",
      "Epoch 489: val_loss did not improve from 0.26055\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2612 - mape: 8.3344 - val_loss: 0.2611 - val_mse: 0.2577 - val_rmse: 0.5076 - val_mae: 0.2611 - val_mape: 8.2947 - lr: 1.0000e-05\n",
      "Epoch 490/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2619 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2619 - mape: 8.3493\n",
      "Epoch 490: val_loss did not improve from 0.26055\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2612 - mape: 8.3441 - val_loss: 0.2608 - val_mse: 0.2577 - val_rmse: 0.5077 - val_mae: 0.2608 - val_mape: 8.2882 - lr: 1.0000e-05\n",
      "Epoch 491/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2610 - mape: 8.3502\n",
      "Epoch 491: val_loss improved from 0.26055 to 0.26054, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2612 - mape: 8.3378 - val_loss: 0.2605 - val_mse: 0.2584 - val_rmse: 0.5084 - val_mae: 0.2605 - val_mape: 8.3171 - lr: 1.0000e-05\n",
      "Epoch 492/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2609 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2609 - mape: 8.3340\n",
      "Epoch 492: val_loss did not improve from 0.26054\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2612 - mape: 8.3441 - val_loss: 0.2606 - val_mse: 0.2588 - val_rmse: 0.5088 - val_mae: 0.2606 - val_mape: 8.3224 - lr: 1.0000e-05\n",
      "Epoch 493/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2603 - mse: 0.2554 - rmse: 0.5054 - mae: 0.2603 - mape: 8.3009\n",
      "Epoch 493: val_loss did not improve from 0.26054\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2610 - mape: 8.3187 - val_loss: 0.2615 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2615 - val_mape: 8.3064 - lr: 1.0000e-05\n",
      "Epoch 494/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2608 - mape: 8.3303\n",
      "Epoch 494: val_loss did not improve from 0.26054\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2613 - mape: 8.3450 - val_loss: 0.2610 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2610 - val_mape: 8.2911 - lr: 1.0000e-05\n",
      "Epoch 495/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2603 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2603 - mape: 8.2866\n",
      "Epoch 495: val_loss did not improve from 0.26054\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2609 - mape: 8.3370 - val_loss: 0.2606 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2606 - val_mape: 8.2937 - lr: 1.0000e-05\n",
      "Epoch 496/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2595 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2595 - mape: 8.3016\n",
      "Epoch 496: val_loss did not improve from 0.26054\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2611 - mape: 8.3506 - val_loss: 0.2611 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2611 - val_mape: 8.3686 - lr: 1.0000e-05\n",
      "Epoch 497/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2612 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2612 - mape: 8.3159\n",
      "Epoch 497: val_loss improved from 0.26054 to 0.26051, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2612 - mape: 8.3275 - val_loss: 0.2605 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2605 - val_mape: 8.3095 - lr: 1.0000e-05\n",
      "Epoch 498/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2591 - mse: 0.2563 - rmse: 0.5063 - mae: 0.2591 - mape: 8.2453\n",
      "Epoch 498: val_loss did not improve from 0.26051\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2612 - mape: 8.3389 - val_loss: 0.2606 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2606 - val_mape: 8.3394 - lr: 1.0000e-05\n",
      "Epoch 499/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2601 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2601 - mape: 8.2747\n",
      "Epoch 499: val_loss improved from 0.26051 to 0.26048, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2614 - mape: 8.3404 - val_loss: 0.2605 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2605 - val_mape: 8.3200 - lr: 1.0000e-05\n",
      "Epoch 500/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2609 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2609 - mape: 8.3313\n",
      "Epoch 500: val_loss did not improve from 0.26048\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2612 - mape: 8.3397 - val_loss: 0.2606 - val_mse: 0.2596 - val_rmse: 0.5096 - val_mae: 0.2606 - val_mape: 8.3608 - lr: 1.0000e-05\n",
      "Epoch 501/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2608 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2608 - mape: 8.3316\n",
      "Epoch 501: val_loss did not improve from 0.26048\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2612 - mape: 8.3462 - val_loss: 0.2608 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2608 - val_mape: 8.3633 - lr: 1.0000e-05\n",
      "Epoch 502/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2611 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2611 - mape: 8.3385\n",
      "Epoch 502: val_loss did not improve from 0.26048\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2610 - mape: 8.3389 - val_loss: 0.2640 - val_mse: 0.2684 - val_rmse: 0.5181 - val_mae: 0.2640 - val_mape: 8.5395 - lr: 1.0000e-05\n",
      "Epoch 503/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2604 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2604 - mape: 8.3271\n",
      "Epoch 503: val_loss improved from 0.26048 to 0.26045, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2609 - mape: 8.3487 - val_loss: 0.2604 - val_mse: 0.2591 - val_rmse: 0.5091 - val_mae: 0.2604 - val_mape: 8.3310 - lr: 1.0000e-05\n",
      "Epoch 504/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2613 - mape: 8.3492\n",
      "Epoch 504: val_loss did not improve from 0.26045\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2613 - mape: 8.3408 - val_loss: 0.2618 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2618 - val_mape: 8.2963 - lr: 1.0000e-05\n",
      "Epoch 505/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2612 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2612 - mape: 8.3613\n",
      "Epoch 505: val_loss did not improve from 0.26045\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2610 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2610 - mape: 8.3410 - val_loss: 0.2612 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2612 - val_mape: 8.2793 - lr: 1.0000e-05\n",
      "Epoch 506/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2613 - mape: 8.3417\n",
      "Epoch 506: val_loss did not improve from 0.26045\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2614 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2614 - mape: 8.3515 - val_loss: 0.2612 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2612 - val_mape: 8.2901 - lr: 1.0000e-05\n",
      "Epoch 507/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2615 - mse: 0.2614 - rmse: 0.5112 - mae: 0.2615 - mape: 8.3594\n",
      "Epoch 507: val_loss improved from 0.26045 to 0.26043, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2612 - mape: 8.3489 - val_loss: 0.2604 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2604 - val_mape: 8.3299 - lr: 1.0000e-05\n",
      "Epoch 508/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2614 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2614 - mape: 8.3349\n",
      "Epoch 508: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2611 - mape: 8.3350 - val_loss: 0.2606 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2606 - val_mape: 8.3570 - lr: 1.0000e-05\n",
      "Epoch 509/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2619 - mse: 0.2626 - rmse: 0.5124 - mae: 0.2619 - mape: 8.3668\n",
      "Epoch 509: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2612 - mape: 8.3439 - val_loss: 0.2619 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2619 - val_mape: 8.2868 - lr: 1.0000e-05\n",
      "Epoch 510/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2601 - mse: 0.2585 - rmse: 0.5085 - mae: 0.2601 - mape: 8.3118\n",
      "Epoch 510: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2614 - rmse: 0.5112 - mae: 0.2612 - mape: 8.3550 - val_loss: 0.2604 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2604 - val_mape: 8.3189 - lr: 1.0000e-05\n",
      "Epoch 511/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2616 - mape: 8.3476\n",
      "Epoch 511: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2612 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2612 - mape: 8.3368 - val_loss: 0.2605 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2605 - val_mape: 8.3203 - lr: 1.0000e-05\n",
      "Epoch 512/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2615 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2615 - mape: 8.3516\n",
      "Epoch 512: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2611 - mape: 8.3348 - val_loss: 0.2605 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2605 - val_mape: 8.3590 - lr: 1.0000e-05\n",
      "Epoch 513/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2598 - mape: 8.3122\n",
      "Epoch 513: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2604 - rmse: 0.5102 - mae: 0.2613 - mape: 8.3553 - val_loss: 0.2607 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2607 - val_mape: 8.2976 - lr: 1.0000e-05\n",
      "Epoch 514/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2605 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2605 - mape: 8.3249\n",
      "Epoch 514: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2614 - mape: 8.3553 - val_loss: 0.2606 - val_mse: 0.2579 - val_rmse: 0.5079 - val_mae: 0.2606 - val_mape: 8.2852 - lr: 1.0000e-05\n",
      "Epoch 515/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2622 - mse: 0.2628 - rmse: 0.5127 - mae: 0.2622 - mape: 8.3642\n",
      "Epoch 515: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2613 - mape: 8.3461 - val_loss: 0.2607 - val_mse: 0.2577 - val_rmse: 0.5076 - val_mae: 0.2607 - val_mape: 8.2957 - lr: 1.0000e-05\n",
      "Epoch 516/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2604 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2604 - mape: 8.2986\n",
      "Epoch 516: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2608 - mape: 8.3294 - val_loss: 0.2607 - val_mse: 0.2602 - val_rmse: 0.5101 - val_mae: 0.2607 - val_mape: 8.3604 - lr: 1.0000e-05\n",
      "Epoch 517/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2616 - mape: 8.3513\n",
      "Epoch 517: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2589 - rmse: 0.5089 - mae: 0.2610 - mape: 8.3315 - val_loss: 0.2605 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2605 - val_mape: 8.3208 - lr: 1.0000e-05\n",
      "Epoch 518/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2590 - rmse: 0.5090 - mae: 0.2607 - mape: 8.3339\n",
      "Epoch 518: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2611 - mape: 8.3373 - val_loss: 0.2624 - val_mse: 0.2642 - val_rmse: 0.5140 - val_mae: 0.2624 - val_mape: 8.4620 - lr: 1.0000e-05\n",
      "Epoch 519/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2611 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2611 - mape: 8.3384\n",
      "Epoch 519: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2611 - mape: 8.3362 - val_loss: 0.2605 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2605 - val_mape: 8.3277 - lr: 1.0000e-05\n",
      "Epoch 520/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2609 - mape: 8.3401\n",
      "Epoch 520: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2610 - mape: 8.3357 - val_loss: 0.2613 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2613 - val_mape: 8.2809 - lr: 1.0000e-05\n",
      "Epoch 521/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2615 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2615 - mape: 8.3470\n",
      "Epoch 521: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2612 - mape: 8.3409 - val_loss: 0.2605 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2605 - val_mape: 8.3202 - lr: 1.0000e-05\n",
      "Epoch 522/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2595 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2595 - mape: 8.2926\n",
      "Epoch 522: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2610 - mape: 8.3372 - val_loss: 0.2605 - val_mse: 0.2580 - val_rmse: 0.5080 - val_mae: 0.2605 - val_mape: 8.3093 - lr: 1.0000e-05\n",
      "Epoch 523/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2614 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2614 - mape: 8.3345\n",
      "Epoch 523: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2613 - mape: 8.3443 - val_loss: 0.2611 - val_mse: 0.2622 - val_rmse: 0.5121 - val_mae: 0.2611 - val_mape: 8.4138 - lr: 1.0000e-05\n",
      "Epoch 524/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2604 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2604 - mape: 8.3042\n",
      "Epoch 524: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2614 - mape: 8.3477 - val_loss: 0.2606 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2606 - val_mape: 8.3570 - lr: 1.0000e-05\n",
      "Epoch 525/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2607 - mape: 8.3299\n",
      "Epoch 525: val_loss did not improve from 0.26043\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2613 - mape: 8.3517 - val_loss: 0.2608 - val_mse: 0.2574 - val_rmse: 0.5073 - val_mae: 0.2608 - val_mape: 8.2810 - lr: 1.0000e-05\n",
      "Epoch 526/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2609 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2609 - mape: 8.3289\n",
      "Epoch 526: val_loss improved from 0.26043 to 0.26033, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2610 - mape: 8.3307 - val_loss: 0.2603 - val_mse: 0.2580 - val_rmse: 0.5080 - val_mae: 0.2603 - val_mape: 8.3011 - lr: 1.0000e-05\n",
      "Epoch 527/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2610 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2610 - mape: 8.3323\n",
      "Epoch 527: val_loss did not improve from 0.26033\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2611 - mape: 8.3375 - val_loss: 0.2605 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2605 - val_mape: 8.2835 - lr: 1.0000e-05\n",
      "Epoch 528/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2613 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2613 - mape: 8.3390\n",
      "Epoch 528: val_loss did not improve from 0.26033\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2613 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2613 - mape: 8.3450 - val_loss: 0.2605 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2605 - val_mape: 8.3386 - lr: 1.0000e-05\n",
      "Epoch 529/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2616 - mape: 8.3185\n",
      "Epoch 529: val_loss did not improve from 0.26033\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2609 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2609 - mape: 8.3198 - val_loss: 0.2610 - val_mse: 0.2619 - val_rmse: 0.5117 - val_mae: 0.2610 - val_mape: 8.4087 - lr: 1.0000e-05\n",
      "Epoch 530/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2623 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2623 - mape: 8.3787\n",
      "Epoch 530: val_loss did not improve from 0.26033\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2609 - mape: 8.3347 - val_loss: 0.2604 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2604 - val_mape: 8.3332 - lr: 1.0000e-05\n",
      "Epoch 531/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2617 - mse: 0.2623 - rmse: 0.5122 - mae: 0.2617 - mape: 8.3630\n",
      "Epoch 531: val_loss did not improve from 0.26033\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2610 - mape: 8.3402 - val_loss: 0.2604 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2604 - val_mape: 8.3213 - lr: 1.0000e-05\n",
      "Epoch 532/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2613 - mape: 8.3504\n",
      "Epoch 532: val_loss did not improve from 0.26033\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2614 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2614 - mape: 8.3462 - val_loss: 0.2625 - val_mse: 0.2661 - val_rmse: 0.5159 - val_mae: 0.2625 - val_mape: 8.4908 - lr: 1.0000e-05\n",
      "Epoch 533/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2606 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2606 - mape: 8.3466\n",
      "Epoch 533: val_loss did not improve from 0.26033\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2609 - mape: 8.3536 - val_loss: 0.2604 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2604 - val_mape: 8.2850 - lr: 1.0000e-05\n",
      "Epoch 534/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2613 - mape: 8.3395\n",
      "Epoch 534: val_loss did not improve from 0.26033\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2610 - mse: 0.2592 - rmse: 0.5092 - mae: 0.2610 - mape: 8.3266 - val_loss: 0.2604 - val_mse: 0.2581 - val_rmse: 0.5081 - val_mae: 0.2604 - val_mape: 8.3006 - lr: 1.0000e-05\n",
      "Epoch 535/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2616 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2616 - mape: 8.3551\n",
      "Epoch 535: val_loss improved from 0.26033 to 0.26029, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2610 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2610 - mape: 8.3369 - val_loss: 0.2603 - val_mse: 0.2585 - val_rmse: 0.5085 - val_mae: 0.2603 - val_mape: 8.3103 - lr: 1.0000e-05\n",
      "Epoch 536/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2604 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2604 - mape: 8.3251\n",
      "Epoch 536: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2608 - mape: 8.3302 - val_loss: 0.2608 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2608 - val_mape: 8.3787 - lr: 1.0000e-05\n",
      "Epoch 537/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2613 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2613 - mape: 8.3316\n",
      "Epoch 537: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2612 - mape: 8.3353 - val_loss: 0.2604 - val_mse: 0.2585 - val_rmse: 0.5085 - val_mae: 0.2604 - val_mape: 8.3212 - lr: 1.0000e-05\n",
      "Epoch 538/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2621 - mse: 0.2616 - rmse: 0.5114 - mae: 0.2621 - mape: 8.3638\n",
      "Epoch 538: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2610 - mape: 8.3357 - val_loss: 0.2605 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2605 - val_mape: 8.3097 - lr: 1.0000e-05\n",
      "Epoch 539/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2608 - mse: 0.2591 - rmse: 0.5091 - mae: 0.2608 - mape: 8.3245\n",
      "Epoch 539: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2607 - mape: 8.3224 - val_loss: 0.2609 - val_mse: 0.2618 - val_rmse: 0.5116 - val_mae: 0.2609 - val_mape: 8.3995 - lr: 1.0000e-05\n",
      "Epoch 540/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2613 - mse: 0.2629 - rmse: 0.5127 - mae: 0.2613 - mape: 8.3666\n",
      "Epoch 540: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2609 - mape: 8.3461 - val_loss: 0.2606 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2606 - val_mape: 8.3352 - lr: 1.0000e-05\n",
      "Epoch 541/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2615 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2615 - mape: 8.3384\n",
      "Epoch 541: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2611 - mape: 8.3327 - val_loss: 0.2606 - val_mse: 0.2573 - val_rmse: 0.5072 - val_mae: 0.2606 - val_mape: 8.2777 - lr: 1.0000e-05\n",
      "Epoch 542/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2611 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2611 - mape: 8.3442\n",
      "Epoch 542: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2610 - mape: 8.3381 - val_loss: 0.2613 - val_mse: 0.2571 - val_rmse: 0.5070 - val_mae: 0.2613 - val_mape: 8.2500 - lr: 1.0000e-05\n",
      "Epoch 543/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2611 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2611 - mape: 8.3334\n",
      "Epoch 543: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2610 - mape: 8.3327 - val_loss: 0.2628 - val_mse: 0.2640 - val_rmse: 0.5138 - val_mae: 0.2628 - val_mape: 8.4647 - lr: 1.0000e-05\n",
      "Epoch 544/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2612 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2612 - mape: 8.3463\n",
      "Epoch 544: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2610 - mape: 8.3352 - val_loss: 0.2605 - val_mse: 0.2579 - val_rmse: 0.5079 - val_mae: 0.2605 - val_mape: 8.2859 - lr: 1.0000e-05\n",
      "Epoch 545/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2613 - mape: 8.3402\n",
      "Epoch 545: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2610 - mape: 8.3271 - val_loss: 0.2606 - val_mse: 0.2599 - val_rmse: 0.5098 - val_mae: 0.2606 - val_mape: 8.3700 - lr: 1.0000e-05\n",
      "Epoch 546/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2597 - mse: 0.2559 - rmse: 0.5059 - mae: 0.2597 - mape: 8.2955\n",
      "Epoch 546: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2607 - mape: 8.3279 - val_loss: 0.2618 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2618 - val_mape: 8.2893 - lr: 1.0000e-05\n",
      "Epoch 547/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2613 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2613 - mape: 8.3410\n",
      "Epoch 547: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2609 - mape: 8.3325 - val_loss: 0.2623 - val_mse: 0.2643 - val_rmse: 0.5141 - val_mae: 0.2623 - val_mape: 8.4678 - lr: 1.0000e-05\n",
      "Epoch 548/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2610 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2610 - mape: 8.3409\n",
      "Epoch 548: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2606 - mape: 8.3325 - val_loss: 0.2604 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2604 - val_mape: 8.2752 - lr: 1.0000e-05\n",
      "Epoch 549/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2609 - mape: 8.3212\n",
      "Epoch 549: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2609 - mape: 8.3169 - val_loss: 0.2605 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2605 - val_mape: 8.3544 - lr: 1.0000e-05\n",
      "Epoch 550/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2611 - mse: 0.2595 - rmse: 0.5095 - mae: 0.2611 - mape: 8.3332\n",
      "Epoch 550: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2608 - mape: 8.3263 - val_loss: 0.2612 - val_mse: 0.2619 - val_rmse: 0.5118 - val_mae: 0.2612 - val_mape: 8.4054 - lr: 1.0000e-05\n",
      "Epoch 551/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2616 - mape: 8.3340\n",
      "Epoch 551: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2608 - mape: 8.3111 - val_loss: 0.2604 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2604 - val_mape: 8.3465 - lr: 1.0000e-05\n",
      "Epoch 552/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2606 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2606 - mape: 8.3179\n",
      "Epoch 552: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2610 - mape: 8.3298 - val_loss: 0.2619 - val_mse: 0.2636 - val_rmse: 0.5134 - val_mae: 0.2619 - val_mape: 8.4447 - lr: 1.0000e-05\n",
      "Epoch 553/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2609 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2609 - mape: 8.3334\n",
      "Epoch 553: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2608 - mape: 8.3269 - val_loss: 0.2618 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2618 - val_mape: 8.2855 - lr: 1.0000e-05\n",
      "Epoch 554/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2622 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2622 - mape: 8.3646\n",
      "Epoch 554: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2613 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2613 - mape: 8.3304 - val_loss: 0.2605 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2605 - val_mape: 8.3785 - lr: 1.0000e-05\n",
      "Epoch 555/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2611 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2611 - mape: 8.3599\n",
      "Epoch 555: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2610 - mape: 8.3342 - val_loss: 0.2610 - val_mse: 0.2614 - val_rmse: 0.5113 - val_mae: 0.2610 - val_mape: 8.3906 - lr: 1.0000e-05\n",
      "Epoch 556/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2609 - mape: 8.3359\n",
      "Epoch 556: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2607 - mape: 8.3328 - val_loss: 0.2605 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2605 - val_mape: 8.3519 - lr: 1.0000e-05\n",
      "Epoch 557/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2611 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2611 - mape: 8.3355\n",
      "Epoch 557: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2609 - mape: 8.3192 - val_loss: 0.2610 - val_mse: 0.2615 - val_rmse: 0.5114 - val_mae: 0.2610 - val_mape: 8.3991 - lr: 1.0000e-05\n",
      "Epoch 558/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2618 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2618 - mape: 8.3699\n",
      "Epoch 558: val_loss did not improve from 0.26029\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2612 - mape: 8.3525 - val_loss: 0.2603 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2603 - val_mape: 8.3150 - lr: 1.0000e-05\n",
      "Epoch 559/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2626 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2626 - mape: 8.3807\n",
      "Epoch 559: val_loss improved from 0.26029 to 0.26016, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2608 - mape: 8.3281 - val_loss: 0.2602 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2602 - val_mape: 8.3231 - lr: 1.0000e-05\n",
      "Epoch 560/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2611 - mse: 0.2598 - rmse: 0.5098 - mae: 0.2611 - mape: 8.3418\n",
      "Epoch 560: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2591 - rmse: 0.5091 - mae: 0.2607 - mape: 8.3211 - val_loss: 0.2602 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2602 - val_mape: 8.3357 - lr: 1.0000e-05\n",
      "Epoch 561/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2614 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2614 - mape: 8.3358\n",
      "Epoch 561: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2606 - mape: 8.3185 - val_loss: 0.2612 - val_mse: 0.2621 - val_rmse: 0.5119 - val_mae: 0.2612 - val_mape: 8.3976 - lr: 1.0000e-05\n",
      "Epoch 562/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2615 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2615 - mape: 8.3596\n",
      "Epoch 562: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2611 - mape: 8.3441 - val_loss: 0.2604 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2604 - val_mape: 8.2682 - lr: 1.0000e-05\n",
      "Epoch 563/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2607 - mape: 8.3297\n",
      "Epoch 563: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2607 - mape: 8.3246 - val_loss: 0.2602 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2602 - val_mape: 8.3000 - lr: 1.0000e-05\n",
      "Epoch 564/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2592 - rmse: 0.5092 - mae: 0.2609 - mape: 8.3283\n",
      "Epoch 564: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2591 - rmse: 0.5091 - mae: 0.2606 - mape: 8.3270 - val_loss: 0.2606 - val_mse: 0.2599 - val_rmse: 0.5098 - val_mae: 0.2606 - val_mape: 8.3460 - lr: 1.0000e-05\n",
      "Epoch 565/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2611 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2611 - mape: 8.3642\n",
      "Epoch 565: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2610 - mape: 8.3322 - val_loss: 0.2614 - val_mse: 0.2629 - val_rmse: 0.5127 - val_mae: 0.2614 - val_mape: 8.4180 - lr: 1.0000e-05\n",
      "Epoch 566/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2619 - mse: 0.2629 - rmse: 0.5127 - mae: 0.2619 - mape: 8.3699\n",
      "Epoch 566: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2610 - mape: 8.3396 - val_loss: 0.2602 - val_mse: 0.2586 - val_rmse: 0.5086 - val_mae: 0.2602 - val_mape: 8.3005 - lr: 1.0000e-05\n",
      "Epoch 567/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2606 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2606 - mape: 8.3214\n",
      "Epoch 567: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2609 - mape: 8.3288 - val_loss: 0.2608 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2608 - val_mape: 8.2815 - lr: 1.0000e-05\n",
      "Epoch 568/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2608 - mape: 8.3448\n",
      "Epoch 568: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2612 - mape: 8.3505 - val_loss: 0.2605 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2605 - val_mape: 8.3243 - lr: 1.0000e-05\n",
      "Epoch 569/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2603 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2603 - mape: 8.2934\n",
      "Epoch 569: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2607 - mape: 8.3240 - val_loss: 0.2603 - val_mse: 0.2577 - val_rmse: 0.5077 - val_mae: 0.2603 - val_mape: 8.2845 - lr: 1.0000e-05\n",
      "Epoch 570/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2611 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2611 - mape: 8.3350\n",
      "Epoch 570: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2608 - mape: 8.3290 - val_loss: 0.2602 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2602 - val_mape: 8.3067 - lr: 1.0000e-05\n",
      "Epoch 571/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2612 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2612 - mape: 8.3389\n",
      "Epoch 571: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2609 - mape: 8.3392 - val_loss: 0.2607 - val_mse: 0.2575 - val_rmse: 0.5075 - val_mae: 0.2607 - val_mape: 8.2681 - lr: 1.0000e-05\n",
      "Epoch 572/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2609 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2609 - mape: 8.3218\n",
      "Epoch 572: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2610 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2610 - mape: 8.3238 - val_loss: 0.2603 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2603 - val_mape: 8.3288 - lr: 1.0000e-05\n",
      "Epoch 573/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2594 - mse: 0.2568 - rmse: 0.5067 - mae: 0.2594 - mape: 8.2840\n",
      "Epoch 573: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2611 - mape: 8.3397 - val_loss: 0.2602 - val_mse: 0.2587 - val_rmse: 0.5087 - val_mae: 0.2602 - val_mape: 8.3373 - lr: 1.0000e-05\n",
      "Epoch 574/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2610 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2610 - mape: 8.3335\n",
      "Epoch 574: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2610 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2610 - mape: 8.3335 - val_loss: 0.2607 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2607 - val_mape: 8.2933 - lr: 1.0000e-05\n",
      "Epoch 575/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2607 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2607 - mape: 8.3507\n",
      "Epoch 575: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2607 - mape: 8.3369 - val_loss: 0.2619 - val_mse: 0.2591 - val_rmse: 0.5091 - val_mae: 0.2619 - val_mape: 8.2764 - lr: 1.0000e-05\n",
      "Epoch 576/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2608 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2608 - mape: 8.3427\n",
      "Epoch 576: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2612 - mape: 8.3427 - val_loss: 0.2605 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2605 - val_mape: 8.2754 - lr: 1.0000e-05\n",
      "Epoch 577/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2609 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2609 - mape: 8.3345\n",
      "Epoch 577: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2604 - mape: 8.3187 - val_loss: 0.2609 - val_mse: 0.2612 - val_rmse: 0.5110 - val_mae: 0.2609 - val_mape: 8.3829 - lr: 1.0000e-05\n",
      "Epoch 578/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2606 - mse: 0.2585 - rmse: 0.5085 - mae: 0.2606 - mape: 8.3211\n",
      "Epoch 578: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2606 - mape: 8.3233 - val_loss: 0.2606 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2606 - val_mape: 8.3236 - lr: 1.0000e-05\n",
      "Epoch 579/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2610 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2610 - mape: 8.3370\n",
      "Epoch 579: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2610 - mape: 8.3370 - val_loss: 0.2603 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2603 - val_mape: 8.2790 - lr: 1.0000e-05\n",
      "Epoch 580/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2608 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2608 - mape: 8.3318\n",
      "Epoch 580: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2608 - mape: 8.3318 - val_loss: 0.2603 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2603 - val_mape: 8.3387 - lr: 1.0000e-05\n",
      "Epoch 581/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2605 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2605 - mape: 8.3269\n",
      "Epoch 581: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2609 - mape: 8.3409 - val_loss: 0.2615 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2615 - val_mape: 8.2581 - lr: 1.0000e-05\n",
      "Epoch 582/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2604 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2604 - mape: 8.3087\n",
      "Epoch 582: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2608 - mape: 8.3221 - val_loss: 0.2602 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2602 - val_mape: 8.3321 - lr: 1.0000e-05\n",
      "Epoch 583/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2602 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2602 - mape: 8.3066\n",
      "Epoch 583: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2608 - mape: 8.3327 - val_loss: 0.2603 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2603 - val_mape: 8.3451 - lr: 1.0000e-05\n",
      "Epoch 584/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2615 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2615 - mape: 8.3281\n",
      "Epoch 584: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2591 - rmse: 0.5091 - mae: 0.2612 - mape: 8.3284 - val_loss: 0.2604 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2604 - val_mape: 8.2985 - lr: 1.0000e-05\n",
      "Epoch 585/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2606 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2606 - mape: 8.3191\n",
      "Epoch 585: val_loss did not improve from 0.26016\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2607 - mape: 8.3269 - val_loss: 0.2605 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2605 - val_mape: 8.2829 - lr: 1.0000e-05\n",
      "Epoch 586/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2600 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2600 - mape: 8.3135\n",
      "Epoch 586: val_loss improved from 0.26016 to 0.26007, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2607 - mape: 8.3355 - val_loss: 0.2601 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2601 - val_mape: 8.3151 - lr: 1.0000e-05\n",
      "Epoch 587/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2608 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2608 - mape: 8.3187\n",
      "Epoch 587: val_loss did not improve from 0.26007\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2607 - mape: 8.3219 - val_loss: 0.2602 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2602 - val_mape: 8.3311 - lr: 1.0000e-05\n",
      "Epoch 588/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2605 - mse: 0.2592 - rmse: 0.5092 - mae: 0.2605 - mape: 8.3277\n",
      "Epoch 588: val_loss improved from 0.26007 to 0.26003, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2592 - rmse: 0.5092 - mae: 0.2605 - mape: 8.3277 - val_loss: 0.2600 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2600 - val_mape: 8.3011 - lr: 1.0000e-05\n",
      "Epoch 589/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2602 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2602 - mape: 8.3137\n",
      "Epoch 589: val_loss did not improve from 0.26003\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2597 - rmse: 0.5097 - mae: 0.2608 - mape: 8.3347 - val_loss: 0.2613 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2613 - val_mape: 8.3717 - lr: 1.0000e-05\n",
      "Epoch 590/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2607 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2607 - mape: 8.2978\n",
      "Epoch 590: val_loss did not improve from 0.26003\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2605 - mape: 8.3031 - val_loss: 0.2600 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2600 - val_mape: 8.3192 - lr: 1.0000e-05\n",
      "Epoch 591/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2604 - mape: 8.3246\n",
      "Epoch 591: val_loss did not improve from 0.26003\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2608 - mape: 8.3367 - val_loss: 0.2600 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2600 - val_mape: 8.3117 - lr: 1.0000e-05\n",
      "Epoch 592/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2609 - mape: 8.3339\n",
      "Epoch 592: val_loss did not improve from 0.26003\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2592 - rmse: 0.5092 - mae: 0.2607 - mape: 8.3340 - val_loss: 0.2610 - val_mse: 0.2578 - val_rmse: 0.5077 - val_mae: 0.2610 - val_mape: 8.2719 - lr: 1.0000e-05\n",
      "Epoch 593/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2604 - mape: 8.3216\n",
      "Epoch 593: val_loss did not improve from 0.26003\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2610 - mape: 8.3441 - val_loss: 0.2605 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2605 - val_mape: 8.2702 - lr: 1.0000e-05\n",
      "Epoch 594/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2602 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2602 - mape: 8.3015\n",
      "Epoch 594: val_loss did not improve from 0.26003\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2609 - mape: 8.3209 - val_loss: 0.2613 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2613 - val_mape: 8.2736 - lr: 1.0000e-05\n",
      "Epoch 595/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2607 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2607 - mape: 8.3171\n",
      "Epoch 595: val_loss did not improve from 0.26003\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2606 - mape: 8.3173 - val_loss: 0.2605 - val_mse: 0.2612 - val_rmse: 0.5110 - val_mae: 0.2605 - val_mape: 8.3819 - lr: 1.0000e-05\n",
      "Epoch 596/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2612 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2612 - mape: 8.3520\n",
      "Epoch 596: val_loss did not improve from 0.26003\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2607 - mape: 8.3508 - val_loss: 0.2608 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2608 - val_mape: 8.2688 - lr: 1.0000e-05\n",
      "Epoch 597/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2606 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2606 - mape: 8.2982\n",
      "Epoch 597: val_loss improved from 0.26003 to 0.26001, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2610 - mape: 8.3246 - val_loss: 0.2600 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2600 - val_mape: 8.3302 - lr: 1.0000e-05\n",
      "Epoch 598/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2612 - mse: 0.2610 - rmse: 0.5108 - mae: 0.2612 - mape: 8.3413\n",
      "Epoch 598: val_loss did not improve from 0.26001\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2610 - mape: 8.3370 - val_loss: 0.2603 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2603 - val_mape: 8.2745 - lr: 1.0000e-05\n",
      "Epoch 599/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2594 - mse: 0.2562 - rmse: 0.5062 - mae: 0.2594 - mape: 8.2787\n",
      "Epoch 599: val_loss did not improve from 0.26001\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2604 - mape: 8.3176 - val_loss: 0.2603 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2603 - val_mape: 8.3703 - lr: 1.0000e-05\n",
      "Epoch 600/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2607 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2607 - mape: 8.3249\n",
      "Epoch 600: val_loss did not improve from 0.26001\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2606 - mape: 8.3299 - val_loss: 0.2601 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2601 - val_mape: 8.2839 - lr: 1.0000e-05\n",
      "Epoch 601/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2593 - rmse: 0.5093 - mae: 0.2607 - mape: 8.3312\n",
      "Epoch 601: val_loss improved from 0.26001 to 0.26000, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2606 - mape: 8.3241 - val_loss: 0.2600 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2600 - val_mape: 8.3224 - lr: 1.0000e-05\n",
      "Epoch 602/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2615 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2615 - mape: 8.3340\n",
      "Epoch 602: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2608 - mape: 8.3156 - val_loss: 0.2602 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2602 - val_mape: 8.3474 - lr: 1.0000e-05\n",
      "Epoch 603/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2609 - mape: 8.3611\n",
      "Epoch 603: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2606 - mape: 8.3250 - val_loss: 0.2607 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2607 - val_mape: 8.2709 - lr: 1.0000e-05\n",
      "Epoch 604/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2600 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2600 - mape: 8.3080\n",
      "Epoch 604: val_loss improved from 0.26000 to 0.26000, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2602 - mape: 8.3230 - val_loss: 0.2600 - val_mse: 0.2577 - val_rmse: 0.5076 - val_mae: 0.2600 - val_mape: 8.2905 - lr: 1.0000e-05\n",
      "Epoch 605/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2588 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2588 - mape: 8.2743\n",
      "Epoch 605: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2604 - mape: 8.3237 - val_loss: 0.2601 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2601 - val_mape: 8.2902 - lr: 1.0000e-05\n",
      "Epoch 606/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2610 - mape: 8.3474\n",
      "Epoch 606: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2607 - mape: 8.3154 - val_loss: 0.2604 - val_mse: 0.2573 - val_rmse: 0.5072 - val_mae: 0.2604 - val_mape: 8.2667 - lr: 1.0000e-05\n",
      "Epoch 607/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2611 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2611 - mape: 8.3389\n",
      "Epoch 607: val_loss improved from 0.26000 to 0.25993, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2608 - mape: 8.3276 - val_loss: 0.2599 - val_mse: 0.2581 - val_rmse: 0.5081 - val_mae: 0.2599 - val_mape: 8.3031 - lr: 1.0000e-05\n",
      "Epoch 608/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2604 - mape: 8.3293\n",
      "Epoch 608: val_loss did not improve from 0.25993\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2602 - mape: 8.3253 - val_loss: 0.2610 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2610 - val_mape: 8.2712 - lr: 1.0000e-05\n",
      "Epoch 609/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2597 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2597 - mape: 8.3139\n",
      "Epoch 609: val_loss did not improve from 0.25993\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2608 - mape: 8.3268 - val_loss: 0.2600 - val_mse: 0.2573 - val_rmse: 0.5073 - val_mae: 0.2600 - val_mape: 8.2889 - lr: 1.0000e-05\n",
      "Epoch 610/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2610 - mape: 8.3349\n",
      "Epoch 610: val_loss did not improve from 0.25993\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2606 - mape: 8.3282 - val_loss: 0.2617 - val_mse: 0.2635 - val_rmse: 0.5133 - val_mae: 0.2617 - val_mape: 8.4442 - lr: 1.0000e-05\n",
      "Epoch 611/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2606 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2606 - mape: 8.3156\n",
      "Epoch 611: val_loss did not improve from 0.25993\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2609 - mape: 8.3319 - val_loss: 0.2603 - val_mse: 0.2577 - val_rmse: 0.5077 - val_mae: 0.2603 - val_mape: 8.2901 - lr: 1.0000e-05\n",
      "Epoch 612/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2599 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2599 - mape: 8.3016\n",
      "Epoch 612: val_loss did not improve from 0.25993\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2604 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2604 - mape: 8.3133 - val_loss: 0.2600 - val_mse: 0.2584 - val_rmse: 0.5084 - val_mae: 0.2600 - val_mape: 8.3309 - lr: 1.0000e-05\n",
      "Epoch 613/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2611 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2611 - mape: 8.3476\n",
      "Epoch 613: val_loss did not improve from 0.25993\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2607 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2607 - mape: 8.3285 - val_loss: 0.2599 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2599 - val_mape: 8.3135 - lr: 1.0000e-05\n",
      "Epoch 614/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2605 - mse: 0.2567 - rmse: 0.5066 - mae: 0.2605 - mape: 8.3264\n",
      "Epoch 614: val_loss did not improve from 0.25993\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2604 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2604 - mape: 8.3217 - val_loss: 0.2600 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2600 - val_mape: 8.3027 - lr: 1.0000e-05\n",
      "Epoch 615/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2600 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2600 - mape: 8.2951\n",
      "Epoch 615: val_loss did not improve from 0.25993\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2603 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2603 - mape: 8.3077 - val_loss: 0.2602 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2602 - val_mape: 8.3573 - lr: 1.0000e-05\n",
      "Epoch 616/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2596 - mape: 8.2948\n",
      "Epoch 616: val_loss did not improve from 0.25993\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2608 - mape: 8.3369 - val_loss: 0.2600 - val_mse: 0.2586 - val_rmse: 0.5086 - val_mae: 0.2600 - val_mape: 8.3296 - lr: 1.0000e-05\n",
      "Epoch 617/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2602 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2602 - mape: 8.3293\n",
      "Epoch 617: val_loss did not improve from 0.25993\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2606 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2606 - mape: 8.3300 - val_loss: 0.2605 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2605 - val_mape: 8.3735 - lr: 1.0000e-05\n",
      "Epoch 618/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2607 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2607 - mape: 8.3303\n",
      "Epoch 618: val_loss did not improve from 0.25993\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2607 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2607 - mape: 8.3303 - val_loss: 0.2601 - val_mse: 0.2585 - val_rmse: 0.5084 - val_mae: 0.2601 - val_mape: 8.3333 - lr: 1.0000e-05\n",
      "Epoch 619/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2603 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2603 - mape: 8.3285\n",
      "Epoch 619: val_loss improved from 0.25993 to 0.25990, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2604 - mse: 0.2590 - rmse: 0.5090 - mae: 0.2604 - mape: 8.3307 - val_loss: 0.2599 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2599 - val_mape: 8.3183 - lr: 1.0000e-05\n",
      "Epoch 620/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2604 - mape: 8.3278\n",
      "Epoch 620: val_loss did not improve from 0.25990\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2604 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2604 - mape: 8.3233 - val_loss: 0.2602 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2602 - val_mape: 8.3637 - lr: 1.0000e-05\n",
      "Epoch 621/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2601 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2601 - mape: 8.3225\n",
      "Epoch 621: val_loss did not improve from 0.25990\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2606 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2606 - mape: 8.3242 - val_loss: 0.2602 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2602 - val_mape: 8.3007 - lr: 1.0000e-05\n",
      "Epoch 622/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2598 - mape: 8.3070\n",
      "Epoch 622: val_loss did not improve from 0.25990\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2605 - mape: 8.3195 - val_loss: 0.2600 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2600 - val_mape: 8.3234 - lr: 1.0000e-05\n",
      "Epoch 623/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2603 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2603 - mape: 8.3264\n",
      "Epoch 623: val_loss did not improve from 0.25990\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2610 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2610 - mape: 8.3540 - val_loss: 0.2599 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2599 - val_mape: 8.3069 - lr: 1.0000e-05\n",
      "Epoch 624/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2608 - mape: 8.3175\n",
      "Epoch 624: val_loss did not improve from 0.25990\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2605 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2605 - mape: 8.3164 - val_loss: 0.2611 - val_mse: 0.2574 - val_rmse: 0.5074 - val_mae: 0.2611 - val_mape: 8.2641 - lr: 1.0000e-05\n",
      "Epoch 625/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2607 - mape: 8.3157\n",
      "Epoch 625: val_loss did not improve from 0.25990\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2608 - mape: 8.3275 - val_loss: 0.2600 - val_mse: 0.2573 - val_rmse: 0.5072 - val_mae: 0.2600 - val_mape: 8.2833 - lr: 1.0000e-05\n",
      "Epoch 626/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2606 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2606 - mape: 8.3173\n",
      "Epoch 626: val_loss did not improve from 0.25990\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2606 - mape: 8.3173 - val_loss: 0.2603 - val_mse: 0.2573 - val_rmse: 0.5073 - val_mae: 0.2603 - val_mape: 8.2671 - lr: 1.0000e-05\n",
      "Epoch 627/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2601 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2601 - mape: 8.3053\n",
      "Epoch 627: val_loss did not improve from 0.25990\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2604 - mape: 8.3189 - val_loss: 0.2605 - val_mse: 0.2604 - val_rmse: 0.5103 - val_mae: 0.2605 - val_mape: 8.3681 - lr: 1.0000e-05\n",
      "Epoch 628/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2608 - mape: 8.3198\n",
      "Epoch 628: val_loss improved from 0.25990 to 0.25985, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2607 - mape: 8.3249 - val_loss: 0.2599 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2599 - val_mape: 8.3151 - lr: 1.0000e-05\n",
      "Epoch 629/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2611 - mse: 0.2606 - rmse: 0.5104 - mae: 0.2611 - mape: 8.3377\n",
      "Epoch 629: val_loss did not improve from 0.25985\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2604 - mape: 8.3144 - val_loss: 0.2600 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2600 - val_mape: 8.2913 - lr: 1.0000e-05\n",
      "Epoch 630/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2591 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2591 - mape: 8.2814\n",
      "Epoch 630: val_loss did not improve from 0.25985\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2603 - mape: 8.3059 - val_loss: 0.2601 - val_mse: 0.2581 - val_rmse: 0.5081 - val_mae: 0.2601 - val_mape: 8.3113 - lr: 1.0000e-05\n",
      "Epoch 631/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2612 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2612 - mape: 8.3407\n",
      "Epoch 631: val_loss did not improve from 0.25985\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2608 - mape: 8.3368 - val_loss: 0.2607 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2607 - val_mape: 8.2803 - lr: 1.0000e-05\n",
      "Epoch 632/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2604 - mape: 8.3170\n",
      "Epoch 632: val_loss did not improve from 0.25985\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2603 - mape: 8.3124 - val_loss: 0.2599 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2599 - val_mape: 8.3308 - lr: 1.0000e-05\n",
      "Epoch 633/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2605 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2605 - mape: 8.3468\n",
      "Epoch 633: val_loss did not improve from 0.25985\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2606 - mape: 8.3306 - val_loss: 0.2599 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2599 - val_mape: 8.2980 - lr: 1.0000e-05\n",
      "Epoch 634/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2604 - mse: 0.2596 - rmse: 0.5096 - mae: 0.2604 - mape: 8.3203\n",
      "Epoch 634: val_loss improved from 0.25985 to 0.25983, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2605 - mape: 8.3332 - val_loss: 0.2598 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2598 - val_mape: 8.2891 - lr: 1.0000e-05\n",
      "Epoch 635/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2591 - mse: 0.2542 - rmse: 0.5042 - mae: 0.2591 - mape: 8.2899\n",
      "Epoch 635: val_loss did not improve from 0.25983\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2604 - mape: 8.3162 - val_loss: 0.2626 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2626 - val_mape: 8.2718 - lr: 1.0000e-05\n",
      "Epoch 636/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2602 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2602 - mape: 8.2903\n",
      "Epoch 636: val_loss did not improve from 0.25983\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2609 - mape: 8.3205 - val_loss: 0.2599 - val_mse: 0.2578 - val_rmse: 0.5077 - val_mae: 0.2599 - val_mape: 8.2880 - lr: 1.0000e-05\n",
      "Epoch 637/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2606 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2606 - mape: 8.3480\n",
      "Epoch 637: val_loss did not improve from 0.25983\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2605 - mape: 8.3399 - val_loss: 0.2604 - val_mse: 0.2565 - val_rmse: 0.5065 - val_mae: 0.2604 - val_mape: 8.2452 - lr: 1.0000e-05\n",
      "Epoch 638/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2604 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2604 - mape: 8.3008\n",
      "Epoch 638: val_loss did not improve from 0.25983\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2607 - mape: 8.3049 - val_loss: 0.2601 - val_mse: 0.2577 - val_rmse: 0.5076 - val_mae: 0.2601 - val_mape: 8.3234 - lr: 1.0000e-05\n",
      "Epoch 639/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2604 - mse: 0.2562 - rmse: 0.5061 - mae: 0.2604 - mape: 8.3395\n",
      "Epoch 639: val_loss did not improve from 0.25983\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2606 - mape: 8.3216 - val_loss: 0.2600 - val_mse: 0.2587 - val_rmse: 0.5087 - val_mae: 0.2600 - val_mape: 8.3246 - lr: 1.0000e-05\n",
      "Epoch 640/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2603 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2603 - mape: 8.3413\n",
      "Epoch 640: val_loss did not improve from 0.25983\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2606 - mape: 8.3322 - val_loss: 0.2613 - val_mse: 0.2631 - val_rmse: 0.5129 - val_mae: 0.2613 - val_mape: 8.4298 - lr: 1.0000e-05\n",
      "Epoch 641/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2616 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2616 - mape: 8.3551\n",
      "Epoch 641: val_loss did not improve from 0.25983\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2605 - mape: 8.3288 - val_loss: 0.2609 - val_mse: 0.2619 - val_rmse: 0.5117 - val_mae: 0.2609 - val_mape: 8.3967 - lr: 1.0000e-05\n",
      "Epoch 642/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2599 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2599 - mape: 8.2913\n",
      "Epoch 642: val_loss did not improve from 0.25983\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2598 - rmse: 0.5098 - mae: 0.2604 - mape: 8.3241 - val_loss: 0.2602 - val_mse: 0.2568 - val_rmse: 0.5068 - val_mae: 0.2602 - val_mape: 8.2644 - lr: 1.0000e-05\n",
      "Epoch 643/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2605 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2605 - mape: 8.3144\n",
      "Epoch 643: val_loss did not improve from 0.25983\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2605 - mape: 8.3156 - val_loss: 0.2599 - val_mse: 0.2580 - val_rmse: 0.5080 - val_mae: 0.2599 - val_mape: 8.3070 - lr: 1.0000e-05\n",
      "Epoch 644/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2596 - mape: 8.2980\n",
      "Epoch 644: val_loss did not improve from 0.25983\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2595 - rmse: 0.5095 - mae: 0.2604 - mape: 8.3230 - val_loss: 0.2601 - val_mse: 0.2577 - val_rmse: 0.5076 - val_mae: 0.2601 - val_mape: 8.2853 - lr: 1.0000e-05\n",
      "Epoch 645/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2614 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2614 - mape: 8.3548\n",
      "Epoch 645: val_loss did not improve from 0.25983\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2607 - mape: 8.3209 - val_loss: 0.2599 - val_mse: 0.2576 - val_rmse: 0.5076 - val_mae: 0.2599 - val_mape: 8.2972 - lr: 1.0000e-05\n",
      "Epoch 646/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2605 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2605 - mape: 8.3363\n",
      "Epoch 646: val_loss did not improve from 0.25983\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2603 - mape: 8.3209 - val_loss: 0.2599 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2599 - val_mape: 8.2695 - lr: 1.0000e-05\n",
      "Epoch 647/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2605 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2605 - mape: 8.3371\n",
      "Epoch 647: val_loss did not improve from 0.25983\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2603 - mape: 8.3296 - val_loss: 0.2617 - val_mse: 0.2576 - val_rmse: 0.5076 - val_mae: 0.2617 - val_mape: 8.2395 - lr: 1.0000e-05\n",
      "Epoch 648/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2615 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2615 - mape: 8.3554\n",
      "Epoch 648: val_loss improved from 0.25983 to 0.25976, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2609 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2609 - mape: 8.3266 - val_loss: 0.2598 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2598 - val_mape: 8.3061 - lr: 1.0000e-05\n",
      "Epoch 649/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2592 - rmse: 0.5092 - mae: 0.2604 - mape: 8.3201\n",
      "Epoch 649: val_loss did not improve from 0.25976\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2605 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2605 - mape: 8.3190 - val_loss: 0.2615 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2615 - val_mape: 8.2597 - lr: 1.0000e-05\n",
      "Epoch 650/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2596 - mape: 8.2836\n",
      "Epoch 650: val_loss did not improve from 0.25976\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2607 - mape: 8.3218 - val_loss: 0.2598 - val_mse: 0.2575 - val_rmse: 0.5075 - val_mae: 0.2598 - val_mape: 8.2799 - lr: 1.0000e-05\n",
      "Epoch 651/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2610 - mape: 8.3339\n",
      "Epoch 651: val_loss did not improve from 0.25976\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2604 - mape: 8.3076 - val_loss: 0.2599 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2599 - val_mape: 8.3260 - lr: 1.0000e-05\n",
      "Epoch 652/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2604 - mape: 8.3275\n",
      "Epoch 652: val_loss did not improve from 0.25976\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2600 - mape: 8.3122 - val_loss: 0.2605 - val_mse: 0.2577 - val_rmse: 0.5077 - val_mae: 0.2605 - val_mape: 8.2613 - lr: 1.0000e-05\n",
      "Epoch 653/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2600 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2600 - mape: 8.3088\n",
      "Epoch 653: val_loss did not improve from 0.25976\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2602 - mape: 8.3098 - val_loss: 0.2598 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2598 - val_mape: 8.3090 - lr: 1.0000e-05\n",
      "Epoch 654/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2594 - mse: 0.2556 - rmse: 0.5056 - mae: 0.2594 - mape: 8.2877\n",
      "Epoch 654: val_loss did not improve from 0.25976\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2603 - mape: 8.3222 - val_loss: 0.2599 - val_mse: 0.2578 - val_rmse: 0.5078 - val_mae: 0.2599 - val_mape: 8.3111 - lr: 1.0000e-05\n",
      "Epoch 655/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2597 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2597 - mape: 8.2990\n",
      "Epoch 655: val_loss did not improve from 0.25976\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2603 - mape: 8.3119 - val_loss: 0.2619 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2619 - val_mape: 8.2478 - lr: 1.0000e-05\n",
      "Epoch 656/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2593 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2593 - mape: 8.2787\n",
      "Epoch 656: val_loss did not improve from 0.25976\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2604 - mape: 8.3068 - val_loss: 0.2600 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2600 - val_mape: 8.3308 - lr: 1.0000e-05\n",
      "Epoch 657/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2610 - mape: 8.3223\n",
      "Epoch 657: val_loss did not improve from 0.25976\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2590 - rmse: 0.5090 - mae: 0.2604 - mape: 8.3128 - val_loss: 0.2636 - val_mse: 0.2696 - val_rmse: 0.5192 - val_mae: 0.2636 - val_mape: 8.5572 - lr: 1.0000e-05\n",
      "Epoch 658/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2603 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2603 - mape: 8.3225\n",
      "Epoch 658: val_loss improved from 0.25976 to 0.25974, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2606 - mape: 8.3344 - val_loss: 0.2597 - val_mse: 0.2579 - val_rmse: 0.5079 - val_mae: 0.2597 - val_mape: 8.3013 - lr: 1.0000e-05\n",
      "Epoch 659/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2596 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2596 - mape: 8.2770\n",
      "Epoch 659: val_loss did not improve from 0.25974\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2604 - mape: 8.3222 - val_loss: 0.2598 - val_mse: 0.2581 - val_rmse: 0.5081 - val_mae: 0.2598 - val_mape: 8.3012 - lr: 1.0000e-05\n",
      "Epoch 660/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2596 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2596 - mape: 8.3068\n",
      "Epoch 660: val_loss did not improve from 0.25974\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2605 - mape: 8.3249 - val_loss: 0.2598 - val_mse: 0.2570 - val_rmse: 0.5070 - val_mae: 0.2598 - val_mape: 8.2689 - lr: 1.0000e-05\n",
      "Epoch 661/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2608 - mse: 0.2592 - rmse: 0.5092 - mae: 0.2608 - mape: 8.3177\n",
      "Epoch 661: val_loss did not improve from 0.25974\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2604 - mape: 8.3108 - val_loss: 0.2599 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2599 - val_mape: 8.2718 - lr: 1.0000e-05\n",
      "Epoch 662/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2605 - mse: 0.2566 - rmse: 0.5065 - mae: 0.2605 - mape: 8.3089\n",
      "Epoch 662: val_loss did not improve from 0.25974\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2605 - mape: 8.3287 - val_loss: 0.2606 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2606 - val_mape: 8.2655 - lr: 1.0000e-05\n",
      "Epoch 663/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2605 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2605 - mape: 8.3368\n",
      "Epoch 663: val_loss did not improve from 0.25974\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2607 - mape: 8.3274 - val_loss: 0.2599 - val_mse: 0.2592 - val_rmse: 0.5092 - val_mae: 0.2599 - val_mape: 8.3287 - lr: 1.0000e-05\n",
      "Epoch 664/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2570 - rmse: 0.5070 - mae: 0.2598 - mape: 8.2980\n",
      "Epoch 664: val_loss did not improve from 0.25974\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2604 - mape: 8.3100 - val_loss: 0.2609 - val_mse: 0.2617 - val_rmse: 0.5116 - val_mae: 0.2609 - val_mape: 8.4042 - lr: 1.0000e-05\n",
      "Epoch 665/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2608 - mape: 8.3292\n",
      "Epoch 665: val_loss did not improve from 0.25974\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2603 - mape: 8.3235 - val_loss: 0.2600 - val_mse: 0.2579 - val_rmse: 0.5079 - val_mae: 0.2600 - val_mape: 8.2859 - lr: 1.0000e-05\n",
      "Epoch 666/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2607 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2607 - mape: 8.3168\n",
      "Epoch 666: val_loss did not improve from 0.25974\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2605 - mape: 8.3204 - val_loss: 0.2619 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2619 - val_mape: 8.2800 - lr: 1.0000e-05\n",
      "Epoch 667/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2618 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2618 - mape: 8.3640\n",
      "Epoch 667: val_loss did not improve from 0.25974\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2610 - mape: 8.3396 - val_loss: 0.2597 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2597 - val_mape: 8.2958 - lr: 1.0000e-05\n",
      "Epoch 668/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2607 - mape: 8.3214\n",
      "Epoch 668: val_loss improved from 0.25974 to 0.25973, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2603 - mape: 8.3178 - val_loss: 0.2597 - val_mse: 0.2585 - val_rmse: 0.5084 - val_mae: 0.2597 - val_mape: 8.3157 - lr: 1.0000e-05\n",
      "Epoch 669/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2599 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2599 - mape: 8.2814\n",
      "Epoch 669: val_loss did not improve from 0.25973\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2604 - mape: 8.3108 - val_loss: 0.2599 - val_mse: 0.2579 - val_rmse: 0.5079 - val_mae: 0.2599 - val_mape: 8.2875 - lr: 1.0000e-05\n",
      "Epoch 670/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2601 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2601 - mape: 8.3108\n",
      "Epoch 670: val_loss did not improve from 0.25973\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2605 - mape: 8.3220 - val_loss: 0.2598 - val_mse: 0.2577 - val_rmse: 0.5076 - val_mae: 0.2598 - val_mape: 8.2915 - lr: 1.0000e-05\n",
      "Epoch 671/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2598 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2598 - mape: 8.3052\n",
      "Epoch 671: val_loss did not improve from 0.25973\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2602 - mape: 8.3147 - val_loss: 0.2598 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2598 - val_mape: 8.3120 - lr: 1.0000e-05\n",
      "Epoch 672/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2603 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2603 - mape: 8.3207\n",
      "Epoch 672: val_loss improved from 0.25973 to 0.25965, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2606 - mape: 8.3316 - val_loss: 0.2596 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2596 - val_mape: 8.3095 - lr: 1.0000e-05\n",
      "Epoch 673/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2606 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2606 - mape: 8.3395\n",
      "Epoch 673: val_loss improved from 0.25965 to 0.25963, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2605 - mape: 8.3229 - val_loss: 0.2596 - val_mse: 0.2578 - val_rmse: 0.5077 - val_mae: 0.2596 - val_mape: 8.2971 - lr: 1.0000e-05\n",
      "Epoch 674/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2601 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2601 - mape: 8.3092\n",
      "Epoch 674: val_loss improved from 0.25963 to 0.25961, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2606 - mape: 8.3223 - val_loss: 0.2596 - val_mse: 0.2581 - val_rmse: 0.5081 - val_mae: 0.2596 - val_mape: 8.2999 - lr: 1.0000e-05\n",
      "Epoch 675/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2613 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2613 - mape: 8.3418\n",
      "Epoch 675: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2607 - mape: 8.3331 - val_loss: 0.2596 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2596 - val_mape: 8.2982 - lr: 1.0000e-05\n",
      "Epoch 676/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2607 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2607 - mape: 8.3438\n",
      "Epoch 676: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2604 - mape: 8.3267 - val_loss: 0.2603 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2603 - val_mape: 8.3688 - lr: 1.0000e-05\n",
      "Epoch 677/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2603 - mse: 0.2594 - rmse: 0.5094 - mae: 0.2603 - mape: 8.3414\n",
      "Epoch 677: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2607 - mape: 8.3507 - val_loss: 0.2598 - val_mse: 0.2577 - val_rmse: 0.5077 - val_mae: 0.2598 - val_mape: 8.2961 - lr: 1.0000e-05\n",
      "Epoch 678/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2610 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2610 - mape: 8.3308\n",
      "Epoch 678: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2602 - mape: 8.3093 - val_loss: 0.2596 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2596 - val_mape: 8.3036 - lr: 1.0000e-05\n",
      "Epoch 679/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2607 - mape: 8.3484\n",
      "Epoch 679: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2602 - mape: 8.3308 - val_loss: 0.2603 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2603 - val_mape: 8.2562 - lr: 1.0000e-05\n",
      "Epoch 680/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2605 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2605 - mape: 8.3146\n",
      "Epoch 680: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2601 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2601 - mape: 8.3063 - val_loss: 0.2604 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2604 - val_mape: 8.2552 - lr: 1.0000e-05\n",
      "Epoch 681/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2604 - mape: 8.3073\n",
      "Epoch 681: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2603 - mape: 8.3039 - val_loss: 0.2599 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2599 - val_mape: 8.3320 - lr: 1.0000e-05\n",
      "Epoch 682/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2594 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2594 - mape: 8.3103\n",
      "Epoch 682: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2605 - mape: 8.3357 - val_loss: 0.2596 - val_mse: 0.2583 - val_rmse: 0.5083 - val_mae: 0.2596 - val_mape: 8.3146 - lr: 1.0000e-05\n",
      "Epoch 683/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2611 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2611 - mape: 8.3537\n",
      "Epoch 683: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2606 - mape: 8.3258 - val_loss: 0.2596 - val_mse: 0.2574 - val_rmse: 0.5073 - val_mae: 0.2596 - val_mape: 8.2842 - lr: 1.0000e-05\n",
      "Epoch 684/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2606 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2606 - mape: 8.3401\n",
      "Epoch 684: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2605 - mape: 8.3334 - val_loss: 0.2602 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2602 - val_mape: 8.2483 - lr: 1.0000e-05\n",
      "Epoch 685/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2607 - mape: 8.3289\n",
      "Epoch 685: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2605 - mape: 8.3213 - val_loss: 0.2628 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2628 - val_mape: 8.2515 - lr: 1.0000e-05\n",
      "Epoch 686/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2606 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2606 - mape: 8.3206\n",
      "Epoch 686: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2605 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2605 - mape: 8.3227 - val_loss: 0.2596 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2596 - val_mape: 8.2744 - lr: 1.0000e-05\n",
      "Epoch 687/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2608 - mape: 8.3249\n",
      "Epoch 687: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2603 - mape: 8.3011 - val_loss: 0.2605 - val_mse: 0.2612 - val_rmse: 0.5111 - val_mae: 0.2605 - val_mape: 8.3952 - lr: 1.0000e-05\n",
      "Epoch 688/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2618 - mse: 0.2620 - rmse: 0.5118 - mae: 0.2618 - mape: 8.3379\n",
      "Epoch 688: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2608 - mape: 8.3196 - val_loss: 0.2601 - val_mse: 0.2575 - val_rmse: 0.5075 - val_mae: 0.2601 - val_mape: 8.2761 - lr: 1.0000e-05\n",
      "Epoch 689/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2597 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2597 - mape: 8.2874\n",
      "Epoch 689: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2606 - mape: 8.3261 - val_loss: 0.2633 - val_mse: 0.2612 - val_rmse: 0.5111 - val_mae: 0.2633 - val_mape: 8.2834 - lr: 1.0000e-05\n",
      "Epoch 690/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2608 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2608 - mape: 8.3323\n",
      "Epoch 690: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2608 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2608 - mape: 8.3266 - val_loss: 0.2596 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2596 - val_mape: 8.3059 - lr: 1.0000e-05\n",
      "Epoch 691/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2614 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2614 - mape: 8.3360\n",
      "Epoch 691: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2611 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2611 - mape: 8.3318 - val_loss: 0.2599 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2599 - val_mape: 8.3626 - lr: 1.0000e-05\n",
      "Epoch 692/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2608 - mape: 8.3441\n",
      "Epoch 692: val_loss did not improve from 0.25961\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2600 - mape: 8.3113 - val_loss: 0.2599 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2599 - val_mape: 8.2851 - lr: 1.0000e-05\n",
      "Epoch 693/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2606 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2606 - mape: 8.3279\n",
      "Epoch 693: val_loss improved from 0.25961 to 0.25951, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2601 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2601 - mape: 8.3140 - val_loss: 0.2595 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2595 - val_mape: 8.2935 - lr: 1.0000e-05\n",
      "Epoch 694/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2604 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2604 - mape: 8.3195\n",
      "Epoch 694: val_loss did not improve from 0.25951\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2603 - mape: 8.3093 - val_loss: 0.2596 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2596 - val_mape: 8.2927 - lr: 1.0000e-05\n",
      "Epoch 695/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2603 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2603 - mape: 8.3454\n",
      "Epoch 695: val_loss did not improve from 0.25951\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2603 - mape: 8.3227 - val_loss: 0.2597 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2597 - val_mape: 8.2901 - lr: 1.0000e-05\n",
      "Epoch 696/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2606 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2606 - mape: 8.3283\n",
      "Epoch 696: val_loss did not improve from 0.25951\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2606 - mape: 8.3283 - val_loss: 0.2597 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2597 - val_mape: 8.2870 - lr: 1.0000e-05\n",
      "Epoch 697/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2604 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2604 - mape: 8.3513\n",
      "Epoch 697: val_loss did not improve from 0.25951\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2602 - mape: 8.3182 - val_loss: 0.2607 - val_mse: 0.2575 - val_rmse: 0.5075 - val_mae: 0.2607 - val_mape: 8.2602 - lr: 1.0000e-05\n",
      "Epoch 698/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2603 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2603 - mape: 8.3007\n",
      "Epoch 698: val_loss did not improve from 0.25951\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2603 - mape: 8.3182 - val_loss: 0.2599 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2599 - val_mape: 8.3276 - lr: 1.0000e-05\n",
      "Epoch 699/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2610 - mape: 8.3475\n",
      "Epoch 699: val_loss did not improve from 0.25951\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2601 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2601 - mape: 8.2990 - val_loss: 0.2617 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2617 - val_mape: 8.2429 - lr: 1.0000e-05\n",
      "Epoch 700/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2607 - mape: 8.3188\n",
      "Epoch 700: val_loss did not improve from 0.25951\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2600 - mape: 8.2997 - val_loss: 0.2600 - val_mse: 0.2570 - val_rmse: 0.5069 - val_mae: 0.2600 - val_mape: 8.2584 - lr: 1.0000e-05\n",
      "Epoch 701/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2603 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2603 - mape: 8.3153\n",
      "Epoch 701: val_loss did not improve from 0.25951\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2602 - mape: 8.3191 - val_loss: 0.2597 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2597 - val_mape: 8.3281 - lr: 1.0000e-05\n",
      "Epoch 702/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2606 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2606 - mape: 8.3329\n",
      "Epoch 702: val_loss did not improve from 0.25951\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2607 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2607 - mape: 8.3315 - val_loss: 0.2595 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2595 - val_mape: 8.3034 - lr: 1.0000e-05\n",
      "Epoch 703/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2604 - mape: 8.3183\n",
      "Epoch 703: val_loss did not improve from 0.25951\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2603 - mape: 8.3246 - val_loss: 0.2596 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2596 - val_mape: 8.3088 - lr: 1.0000e-05\n",
      "Epoch 704/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2603 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2603 - mape: 8.3125\n",
      "Epoch 704: val_loss did not improve from 0.25951\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2602 - mape: 8.3124 - val_loss: 0.2597 - val_mse: 0.2574 - val_rmse: 0.5074 - val_mae: 0.2597 - val_mape: 8.3040 - lr: 1.0000e-05\n",
      "Epoch 705/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2611 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2611 - mape: 8.3147\n",
      "Epoch 705: val_loss did not improve from 0.25951\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2604 - mape: 8.3147 - val_loss: 0.2596 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2596 - val_mape: 8.2800 - lr: 1.0000e-05\n",
      "Epoch 706/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2591 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2591 - mape: 8.2885\n",
      "Epoch 706: val_loss improved from 0.25951 to 0.25948, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2599 - mape: 8.3102 - val_loss: 0.2595 - val_mse: 0.2573 - val_rmse: 0.5073 - val_mae: 0.2595 - val_mape: 8.2862 - lr: 1.0000e-05\n",
      "Epoch 707/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2605 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2605 - mape: 8.3301\n",
      "Epoch 707: val_loss did not improve from 0.25948\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2603 - mape: 8.3172 - val_loss: 0.2602 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2602 - val_mape: 8.3578 - lr: 1.0000e-05\n",
      "Epoch 708/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2619 - mse: 0.2618 - rmse: 0.5116 - mae: 0.2619 - mape: 8.3409\n",
      "Epoch 708: val_loss did not improve from 0.25948\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2612 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2612 - mape: 8.3308 - val_loss: 0.2607 - val_mse: 0.2621 - val_rmse: 0.5120 - val_mae: 0.2607 - val_mape: 8.4113 - lr: 1.0000e-05\n",
      "Epoch 709/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2602 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2602 - mape: 8.3296\n",
      "Epoch 709: val_loss did not improve from 0.25948\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2601 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2601 - mape: 8.3150 - val_loss: 0.2595 - val_mse: 0.2575 - val_rmse: 0.5075 - val_mae: 0.2595 - val_mape: 8.3052 - lr: 1.0000e-05\n",
      "Epoch 710/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2601 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2601 - mape: 8.3231\n",
      "Epoch 710: val_loss did not improve from 0.25948\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2602 - mape: 8.3268 - val_loss: 0.2596 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2596 - val_mape: 8.2763 - lr: 1.0000e-05\n",
      "Epoch 711/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2604 - mape: 8.3169\n",
      "Epoch 711: val_loss did not improve from 0.25948\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2589 - rmse: 0.5089 - mae: 0.2602 - mape: 8.3164 - val_loss: 0.2596 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2596 - val_mape: 8.2877 - lr: 1.0000e-05\n",
      "Epoch 712/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2611 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2611 - mape: 8.3417\n",
      "Epoch 712: val_loss did not improve from 0.25948\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2602 - mape: 8.3206 - val_loss: 0.2596 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2596 - val_mape: 8.2953 - lr: 1.0000e-05\n",
      "Epoch 713/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2610 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2610 - mape: 8.3441\n",
      "Epoch 713: val_loss did not improve from 0.25948\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2602 - mape: 8.3238 - val_loss: 0.2605 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2605 - val_mape: 8.2619 - lr: 1.0000e-05\n",
      "Epoch 714/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2604 - mse: 0.2589 - rmse: 0.5089 - mae: 0.2604 - mape: 8.3089\n",
      "Epoch 714: val_loss did not improve from 0.25948\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2602 - mape: 8.3110 - val_loss: 0.2595 - val_mse: 0.2577 - val_rmse: 0.5076 - val_mae: 0.2595 - val_mape: 8.3007 - lr: 1.0000e-05\n",
      "Epoch 715/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2595 - mse: 0.2558 - rmse: 0.5058 - mae: 0.2595 - mape: 8.2805\n",
      "Epoch 715: val_loss did not improve from 0.25948\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2603 - mape: 8.3051 - val_loss: 0.2622 - val_mse: 0.2667 - val_rmse: 0.5164 - val_mae: 0.2622 - val_mape: 8.5094 - lr: 1.0000e-05\n",
      "Epoch 716/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2595 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2595 - mape: 8.3191\n",
      "Epoch 716: val_loss improved from 0.25948 to 0.25945, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2599 - mape: 8.3094 - val_loss: 0.2595 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2595 - val_mape: 8.2792 - lr: 1.0000e-05\n",
      "Epoch 717/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2612 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2612 - mape: 8.3482\n",
      "Epoch 717: val_loss did not improve from 0.25945\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2602 - mape: 8.3149 - val_loss: 0.2599 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2599 - val_mape: 8.3423 - lr: 1.0000e-05\n",
      "Epoch 718/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2593 - mse: 0.2565 - rmse: 0.5065 - mae: 0.2593 - mape: 8.2916\n",
      "Epoch 718: val_loss did not improve from 0.25945\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2599 - mape: 8.3131 - val_loss: 0.2599 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2599 - val_mape: 8.3424 - lr: 1.0000e-05\n",
      "Epoch 719/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2601 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2601 - mape: 8.3311\n",
      "Epoch 719: val_loss did not improve from 0.25945\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2603 - mape: 8.3194 - val_loss: 0.2595 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2595 - val_mape: 8.2839 - lr: 1.0000e-05\n",
      "Epoch 720/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2600 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2600 - mape: 8.3197\n",
      "Epoch 720: val_loss improved from 0.25945 to 0.25945, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2601 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2601 - mape: 8.3185 - val_loss: 0.2595 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2595 - val_mape: 8.2865 - lr: 1.0000e-05\n",
      "Epoch 721/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2612 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2612 - mape: 8.3156\n",
      "Epoch 721: val_loss did not improve from 0.25945\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2604 - mape: 8.3000 - val_loss: 0.2599 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2599 - val_mape: 8.3573 - lr: 1.0000e-05\n",
      "Epoch 722/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2602 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2602 - mape: 8.3181\n",
      "Epoch 722: val_loss did not improve from 0.25945\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2591 - rmse: 0.5091 - mae: 0.2603 - mape: 8.3260 - val_loss: 0.2596 - val_mse: 0.2568 - val_rmse: 0.5068 - val_mae: 0.2596 - val_mape: 8.2591 - lr: 1.0000e-05\n",
      "Epoch 723/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2605 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2605 - mape: 8.2955\n",
      "Epoch 723: val_loss did not improve from 0.25945\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2606 - mape: 8.3125 - val_loss: 0.2607 - val_mse: 0.2626 - val_rmse: 0.5124 - val_mae: 0.2607 - val_mape: 8.4227 - lr: 1.0000e-05\n",
      "Epoch 724/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2602 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2602 - mape: 8.3466\n",
      "Epoch 724: val_loss did not improve from 0.25945\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2602 - mape: 8.3441 - val_loss: 0.2599 - val_mse: 0.2571 - val_rmse: 0.5070 - val_mae: 0.2599 - val_mape: 8.2639 - lr: 1.0000e-05\n",
      "Epoch 725/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2601 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2601 - mape: 8.3160\n",
      "Epoch 725: val_loss did not improve from 0.25945\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2602 - mape: 8.3175 - val_loss: 0.2599 - val_mse: 0.2586 - val_rmse: 0.5086 - val_mae: 0.2599 - val_mape: 8.3563 - lr: 1.0000e-05\n",
      "Epoch 726/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2588 - mse: 0.2549 - rmse: 0.5049 - mae: 0.2588 - mape: 8.2704\n",
      "Epoch 726: val_loss did not improve from 0.25945\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2600 - mape: 8.3086 - val_loss: 0.2600 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2600 - val_mape: 8.3556 - lr: 1.0000e-05\n",
      "Epoch 727/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2601 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2601 - mape: 8.3086\n",
      "Epoch 727: val_loss did not improve from 0.25945\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2597 - mape: 8.2984 - val_loss: 0.2603 - val_mse: 0.2615 - val_rmse: 0.5114 - val_mae: 0.2603 - val_mape: 8.4011 - lr: 1.0000e-05\n",
      "Epoch 728/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2600 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2600 - mape: 8.3234\n",
      "Epoch 728: val_loss improved from 0.25945 to 0.25938, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2600 - mape: 8.3211 - val_loss: 0.2594 - val_mse: 0.2577 - val_rmse: 0.5077 - val_mae: 0.2594 - val_mape: 8.2843 - lr: 1.0000e-05\n",
      "Epoch 729/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2592 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2592 - mape: 8.2819\n",
      "Epoch 729: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2599 - mape: 8.3088 - val_loss: 0.2596 - val_mse: 0.2573 - val_rmse: 0.5072 - val_mae: 0.2596 - val_mape: 8.2690 - lr: 1.0000e-05\n",
      "Epoch 730/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2605 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2605 - mape: 8.3148\n",
      "Epoch 730: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2599 - mape: 8.2963 - val_loss: 0.2607 - val_mse: 0.2630 - val_rmse: 0.5128 - val_mae: 0.2607 - val_mape: 8.4266 - lr: 1.0000e-05\n",
      "Epoch 731/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2606 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2606 - mape: 8.3438\n",
      "Epoch 731: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2604 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2604 - mape: 8.3338 - val_loss: 0.2606 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2606 - val_mape: 8.2423 - lr: 1.0000e-05\n",
      "Epoch 732/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2602 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2602 - mape: 8.3146\n",
      "Epoch 732: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2600 - mape: 8.3066 - val_loss: 0.2594 - val_mse: 0.2573 - val_rmse: 0.5073 - val_mae: 0.2594 - val_mape: 8.2747 - lr: 1.0000e-05\n",
      "Epoch 733/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2600 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2600 - mape: 8.3303\n",
      "Epoch 733: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2602 - mape: 8.3248 - val_loss: 0.2596 - val_mse: 0.2567 - val_rmse: 0.5067 - val_mae: 0.2596 - val_mape: 8.2547 - lr: 1.0000e-05\n",
      "Epoch 734/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2607 - rmse: 0.5105 - mae: 0.2609 - mape: 8.3441\n",
      "Epoch 734: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2598 - mape: 8.3025 - val_loss: 0.2603 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2603 - val_mape: 8.3570 - lr: 1.0000e-05\n",
      "Epoch 735/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2607 - mse: 0.2590 - rmse: 0.5090 - mae: 0.2607 - mape: 8.3283\n",
      "Epoch 735: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2599 - mape: 8.3146 - val_loss: 0.2606 - val_mse: 0.2573 - val_rmse: 0.5072 - val_mae: 0.2606 - val_mape: 8.2538 - lr: 1.0000e-05\n",
      "Epoch 736/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2606 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2606 - mape: 8.3320\n",
      "Epoch 736: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2606 - mape: 8.3306 - val_loss: 0.2594 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2594 - val_mape: 8.2910 - lr: 1.0000e-05\n",
      "Epoch 737/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2594 - rmse: 0.5094 - mae: 0.2604 - mape: 8.3166\n",
      "Epoch 737: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2601 - mse: 0.2589 - rmse: 0.5089 - mae: 0.2601 - mape: 8.3165 - val_loss: 0.2601 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2601 - val_mape: 8.2805 - lr: 1.0000e-05\n",
      "Epoch 738/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2607 - mse: 0.2611 - rmse: 0.5109 - mae: 0.2607 - mape: 8.3500\n",
      "Epoch 738: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2596 - rmse: 0.5096 - mae: 0.2603 - mape: 8.3221 - val_loss: 0.2594 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2594 - val_mape: 8.3062 - lr: 1.0000e-05\n",
      "Epoch 739/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2610 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2610 - mape: 8.3492\n",
      "Epoch 739: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2598 - mape: 8.3102 - val_loss: 0.2596 - val_mse: 0.2578 - val_rmse: 0.5077 - val_mae: 0.2596 - val_mape: 8.3060 - lr: 1.0000e-05\n",
      "Epoch 740/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2605 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2605 - mape: 8.3204\n",
      "Epoch 740: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2603 - mape: 8.3093 - val_loss: 0.2598 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2598 - val_mape: 8.3567 - lr: 1.0000e-05\n",
      "Epoch 741/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2593 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2593 - mape: 8.2799\n",
      "Epoch 741: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2600 - mape: 8.3065 - val_loss: 0.2597 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2597 - val_mape: 8.2702 - lr: 1.0000e-05\n",
      "Epoch 742/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2593 - mse: 0.2562 - rmse: 0.5061 - mae: 0.2593 - mape: 8.2759\n",
      "Epoch 742: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2602 - mape: 8.3140 - val_loss: 0.2595 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2595 - val_mape: 8.2691 - lr: 1.0000e-05\n",
      "Epoch 743/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2608 - mse: 0.2593 - rmse: 0.5093 - mae: 0.2608 - mape: 8.3218\n",
      "Epoch 743: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2602 - mape: 8.3042 - val_loss: 0.2594 - val_mse: 0.2577 - val_rmse: 0.5077 - val_mae: 0.2594 - val_mape: 8.2861 - lr: 1.0000e-05\n",
      "Epoch 744/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2598 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2598 - mape: 8.3195\n",
      "Epoch 744: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2598 - mape: 8.3253 - val_loss: 0.2603 - val_mse: 0.2568 - val_rmse: 0.5068 - val_mae: 0.2603 - val_mape: 8.2442 - lr: 1.0000e-05\n",
      "Epoch 745/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2601 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2601 - mape: 8.3114\n",
      "Epoch 745: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2601 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2601 - mape: 8.3118 - val_loss: 0.2632 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2632 - val_mape: 8.2621 - lr: 1.0000e-05\n",
      "Epoch 746/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2593 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2593 - mape: 8.2863\n",
      "Epoch 746: val_loss did not improve from 0.25938\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2599 - mape: 8.3022 - val_loss: 0.2596 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2596 - val_mape: 8.2717 - lr: 1.0000e-05\n",
      "Epoch 747/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2600 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2600 - mape: 8.2983\n",
      "Epoch 747: val_loss improved from 0.25938 to 0.25927, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2598 - mape: 8.2996 - val_loss: 0.2593 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2593 - val_mape: 8.2979 - lr: 1.0000e-05\n",
      "Epoch 748/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2602 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2602 - mape: 8.3209\n",
      "Epoch 748: val_loss did not improve from 0.25927\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2597 - mape: 8.2935 - val_loss: 0.2604 - val_mse: 0.2612 - val_rmse: 0.5111 - val_mae: 0.2604 - val_mape: 8.4015 - lr: 1.0000e-05\n",
      "Epoch 749/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2605 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2605 - mape: 8.3173\n",
      "Epoch 749: val_loss did not improve from 0.25927\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2600 - mape: 8.3120 - val_loss: 0.2593 - val_mse: 0.2583 - val_rmse: 0.5083 - val_mae: 0.2593 - val_mape: 8.3155 - lr: 1.0000e-05\n",
      "Epoch 750/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2598 - mape: 8.3000\n",
      "Epoch 750: val_loss did not improve from 0.25927\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2598 - mape: 8.3092 - val_loss: 0.2594 - val_mse: 0.2566 - val_rmse: 0.5066 - val_mae: 0.2594 - val_mape: 8.2737 - lr: 1.0000e-05\n",
      "Epoch 751/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2602 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2602 - mape: 8.3001\n",
      "Epoch 751: val_loss did not improve from 0.25927\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2601 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2601 - mape: 8.3242 - val_loss: 0.2603 - val_mse: 0.2571 - val_rmse: 0.5070 - val_mae: 0.2603 - val_mape: 8.2490 - lr: 1.0000e-05\n",
      "Epoch 752/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2600 - mse: 0.2568 - rmse: 0.5067 - mae: 0.2600 - mape: 8.2850\n",
      "Epoch 752: val_loss did not improve from 0.25927\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2599 - mape: 8.3012 - val_loss: 0.2618 - val_mse: 0.2643 - val_rmse: 0.5141 - val_mae: 0.2618 - val_mape: 8.4664 - lr: 1.0000e-05\n",
      "Epoch 753/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2596 - mape: 8.3280\n",
      "Epoch 753: val_loss did not improve from 0.25927\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2598 - mape: 8.3271 - val_loss: 0.2594 - val_mse: 0.2567 - val_rmse: 0.5066 - val_mae: 0.2594 - val_mape: 8.2644 - lr: 1.0000e-05\n",
      "Epoch 754/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2596 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2596 - mape: 8.3096\n",
      "Epoch 754: val_loss improved from 0.25927 to 0.25926, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2598 - mape: 8.3105 - val_loss: 0.2593 - val_mse: 0.2574 - val_rmse: 0.5074 - val_mae: 0.2593 - val_mape: 8.2767 - lr: 1.0000e-05\n",
      "Epoch 755/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2597 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2597 - mape: 8.3127\n",
      "Epoch 755: val_loss did not improve from 0.25926\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2595 - rmse: 0.5095 - mae: 0.2603 - mape: 8.3216 - val_loss: 0.2604 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2604 - val_mape: 8.2355 - lr: 1.0000e-05\n",
      "Epoch 756/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2606 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2606 - mape: 8.3217\n",
      "Epoch 756: val_loss did not improve from 0.25926\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2606 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2606 - mape: 8.3217 - val_loss: 0.2606 - val_mse: 0.2616 - val_rmse: 0.5115 - val_mae: 0.2606 - val_mape: 8.4011 - lr: 1.0000e-05\n",
      "Epoch 757/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2597 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2597 - mape: 8.3124\n",
      "Epoch 757: val_loss did not improve from 0.25926\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2597 - mape: 8.3146 - val_loss: 0.2605 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2605 - val_mape: 8.2449 - lr: 1.0000e-05\n",
      "Epoch 758/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2601 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2601 - mape: 8.3065\n",
      "Epoch 758: val_loss improved from 0.25926 to 0.25924, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2599 - mape: 8.3001 - val_loss: 0.2592 - val_mse: 0.2573 - val_rmse: 0.5072 - val_mae: 0.2592 - val_mape: 8.2740 - lr: 1.0000e-05\n",
      "Epoch 759/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2598 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2598 - mape: 8.3209\n",
      "Epoch 759: val_loss did not improve from 0.25924\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2598 - mape: 8.3202 - val_loss: 0.2612 - val_mse: 0.2621 - val_rmse: 0.5120 - val_mae: 0.2612 - val_mape: 8.4094 - lr: 1.0000e-05\n",
      "Epoch 760/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2596 - mape: 8.2826\n",
      "Epoch 760: val_loss did not improve from 0.25924\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2598 - mape: 8.2965 - val_loss: 0.2594 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2594 - val_mape: 8.3356 - lr: 1.0000e-05\n",
      "Epoch 761/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2584 - mse: 0.2535 - rmse: 0.5035 - mae: 0.2584 - mape: 8.2510\n",
      "Epoch 761: val_loss improved from 0.25924 to 0.25923, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2599 - mape: 8.3020 - val_loss: 0.2592 - val_mse: 0.2585 - val_rmse: 0.5085 - val_mae: 0.2592 - val_mape: 8.3053 - lr: 1.0000e-05\n",
      "Epoch 762/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2602 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2602 - mape: 8.3071\n",
      "Epoch 762: val_loss did not improve from 0.25923\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2598 - mape: 8.2980 - val_loss: 0.2598 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2598 - val_mape: 8.2716 - lr: 1.0000e-05\n",
      "Epoch 763/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2603 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2603 - mape: 8.3309\n",
      "Epoch 763: val_loss did not improve from 0.25923\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2598 - mape: 8.3152 - val_loss: 0.2593 - val_mse: 0.2568 - val_rmse: 0.5068 - val_mae: 0.2593 - val_mape: 8.2614 - lr: 1.0000e-05\n",
      "Epoch 764/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2603 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2603 - mape: 8.3116\n",
      "Epoch 764: val_loss improved from 0.25923 to 0.25918, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2598 - mape: 8.3056 - val_loss: 0.2592 - val_mse: 0.2574 - val_rmse: 0.5073 - val_mae: 0.2592 - val_mape: 8.2956 - lr: 1.0000e-05\n",
      "Epoch 765/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2597 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2597 - mape: 8.3022\n",
      "Epoch 765: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2595 - mape: 8.3040 - val_loss: 0.2599 - val_mse: 0.2570 - val_rmse: 0.5069 - val_mae: 0.2599 - val_mape: 8.2699 - lr: 1.0000e-05\n",
      "Epoch 766/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2589 - mse: 0.2559 - rmse: 0.5059 - mae: 0.2589 - mape: 8.2918\n",
      "Epoch 766: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2599 - mape: 8.3075 - val_loss: 0.2601 - val_mse: 0.2568 - val_rmse: 0.5067 - val_mae: 0.2601 - val_mape: 8.2596 - lr: 1.0000e-05\n",
      "Epoch 767/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2597 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2597 - mape: 8.3124\n",
      "Epoch 767: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2602 - mape: 8.3035 - val_loss: 0.2602 - val_mse: 0.2578 - val_rmse: 0.5077 - val_mae: 0.2602 - val_mape: 8.2874 - lr: 1.0000e-05\n",
      "Epoch 768/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2600 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2600 - mape: 8.3168\n",
      "Epoch 768: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2597 - mape: 8.3148 - val_loss: 0.2606 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2606 - val_mape: 8.2376 - lr: 1.0000e-05\n",
      "Epoch 769/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2592 - mse: 0.2561 - rmse: 0.5060 - mae: 0.2592 - mape: 8.2970\n",
      "Epoch 769: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2597 - mape: 8.3033 - val_loss: 0.2595 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2595 - val_mape: 8.3471 - lr: 1.0000e-05\n",
      "Epoch 770/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2610 - mape: 8.3312\n",
      "Epoch 770: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2597 - mape: 8.3033 - val_loss: 0.2594 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2594 - val_mape: 8.2643 - lr: 1.0000e-05\n",
      "Epoch 771/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2592 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2592 - mape: 8.2706\n",
      "Epoch 771: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2602 - mape: 8.3077 - val_loss: 0.2593 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2593 - val_mape: 8.3316 - lr: 1.0000e-05\n",
      "Epoch 772/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2598 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2598 - mape: 8.2998\n",
      "Epoch 772: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2600 - mape: 8.3096 - val_loss: 0.2604 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2604 - val_mape: 8.2531 - lr: 1.0000e-05\n",
      "Epoch 773/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2605 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2605 - mape: 8.3113\n",
      "Epoch 773: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2603 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2603 - mape: 8.3071 - val_loss: 0.2592 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2592 - val_mape: 8.3046 - lr: 1.0000e-05\n",
      "Epoch 774/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2610 - mse: 0.2611 - rmse: 0.5109 - mae: 0.2610 - mape: 8.3426\n",
      "Epoch 774: val_loss improved from 0.25918 to 0.25914, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2601 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2601 - mape: 8.3214 - val_loss: 0.2591 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2591 - val_mape: 8.2875 - lr: 1.0000e-05\n",
      "Epoch 775/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2585 - mse: 0.2554 - rmse: 0.5054 - mae: 0.2585 - mape: 8.2737\n",
      "Epoch 775: val_loss did not improve from 0.25914\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2598 - mape: 8.3068 - val_loss: 0.2595 - val_mse: 0.2561 - val_rmse: 0.5061 - val_mae: 0.2595 - val_mape: 8.2755 - lr: 1.0000e-05\n",
      "Epoch 776/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2605 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2605 - mape: 8.3360\n",
      "Epoch 776: val_loss did not improve from 0.25914\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2600 - mape: 8.3066 - val_loss: 0.2611 - val_mse: 0.2625 - val_rmse: 0.5123 - val_mae: 0.2611 - val_mape: 8.4244 - lr: 1.0000e-05\n",
      "Epoch 777/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2601 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2601 - mape: 8.3133\n",
      "Epoch 777: val_loss improved from 0.25914 to 0.25910, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2600 - mape: 8.3087 - val_loss: 0.2591 - val_mse: 0.2574 - val_rmse: 0.5073 - val_mae: 0.2591 - val_mape: 8.2882 - lr: 1.0000e-05\n",
      "Epoch 778/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2601 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2601 - mape: 8.3127\n",
      "Epoch 778: val_loss improved from 0.25910 to 0.25910, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2598 - mape: 8.3075 - val_loss: 0.2591 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2591 - val_mape: 8.2750 - lr: 1.0000e-05\n",
      "Epoch 779/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2594 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2594 - mape: 8.2806\n",
      "Epoch 779: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2598 - mape: 8.2969 - val_loss: 0.2596 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2596 - val_mape: 8.3342 - lr: 1.0000e-05\n",
      "Epoch 780/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2590 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2590 - mape: 8.3023\n",
      "Epoch 780: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2585 - rmse: 0.5085 - mae: 0.2598 - mape: 8.3112 - val_loss: 0.2598 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2598 - val_mape: 8.3650 - lr: 1.0000e-05\n",
      "Epoch 781/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2598 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2598 - mape: 8.2923\n",
      "Epoch 781: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2600 - mape: 8.2941 - val_loss: 0.2597 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2597 - val_mape: 8.3555 - lr: 1.0000e-05\n",
      "Epoch 782/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2589 - mse: 0.2553 - rmse: 0.5053 - mae: 0.2589 - mape: 8.2566\n",
      "Epoch 782: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2597 - mape: 8.3002 - val_loss: 0.2592 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2592 - val_mape: 8.3091 - lr: 1.0000e-05\n",
      "Epoch 783/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2593 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2593 - mape: 8.2925\n",
      "Epoch 783: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2597 - mape: 8.3023 - val_loss: 0.2597 - val_mse: 0.2570 - val_rmse: 0.5069 - val_mae: 0.2597 - val_mape: 8.2774 - lr: 1.0000e-05\n",
      "Epoch 784/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2584 - mse: 0.2562 - rmse: 0.5062 - mae: 0.2584 - mape: 8.2486\n",
      "Epoch 784: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2597 - mape: 8.3059 - val_loss: 0.2596 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2596 - val_mape: 8.2888 - lr: 1.0000e-05\n",
      "Epoch 785/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2609 - mape: 8.3286\n",
      "Epoch 785: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2599 - mape: 8.3140 - val_loss: 0.2593 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2593 - val_mape: 8.2856 - lr: 1.0000e-05\n",
      "Epoch 786/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2597 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2597 - mape: 8.3202\n",
      "Epoch 786: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2597 - mape: 8.3213 - val_loss: 0.2600 - val_mse: 0.2574 - val_rmse: 0.5074 - val_mae: 0.2600 - val_mape: 8.2626 - lr: 1.0000e-05\n",
      "Epoch 787/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2602 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2602 - mape: 8.3268\n",
      "Epoch 787: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2599 - mape: 8.3210 - val_loss: 0.2593 - val_mse: 0.2565 - val_rmse: 0.5065 - val_mae: 0.2593 - val_mape: 8.2528 - lr: 1.0000e-05\n",
      "Epoch 788/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2604 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2604 - mape: 8.3144\n",
      "Epoch 788: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2598 - mape: 8.2995 - val_loss: 0.2595 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2595 - val_mape: 8.3479 - lr: 1.0000e-05\n",
      "Epoch 789/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2594 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2594 - mape: 8.2988\n",
      "Epoch 789: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2597 - mape: 8.3202 - val_loss: 0.2597 - val_mse: 0.2563 - val_rmse: 0.5063 - val_mae: 0.2597 - val_mape: 8.2643 - lr: 1.0000e-05\n",
      "Epoch 790/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2605 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2605 - mape: 8.3264\n",
      "Epoch 790: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2598 - mape: 8.2985 - val_loss: 0.2602 - val_mse: 0.2573 - val_rmse: 0.5073 - val_mae: 0.2602 - val_mape: 8.2517 - lr: 1.0000e-05\n",
      "Epoch 791/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2596 - mape: 8.3199\n",
      "Epoch 791: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2598 - mape: 8.3162 - val_loss: 0.2598 - val_mse: 0.2558 - val_rmse: 0.5058 - val_mae: 0.2598 - val_mape: 8.2514 - lr: 1.0000e-05\n",
      "Epoch 792/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2595 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2595 - mape: 8.2755\n",
      "Epoch 792: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2598 - mape: 8.2967 - val_loss: 0.2594 - val_mse: 0.2559 - val_rmse: 0.5058 - val_mae: 0.2594 - val_mape: 8.2374 - lr: 1.0000e-05\n",
      "Epoch 793/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2602 - mse: 0.2591 - rmse: 0.5091 - mae: 0.2602 - mape: 8.3147\n",
      "Epoch 793: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2598 - mape: 8.2998 - val_loss: 0.2611 - val_mse: 0.2577 - val_rmse: 0.5076 - val_mae: 0.2611 - val_mape: 8.2573 - lr: 1.0000e-05\n",
      "Epoch 794/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2563 - rmse: 0.5063 - mae: 0.2598 - mape: 8.2730\n",
      "Epoch 794: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2602 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2602 - mape: 8.2972 - val_loss: 0.2591 - val_mse: 0.2580 - val_rmse: 0.5080 - val_mae: 0.2591 - val_mape: 8.2913 - lr: 1.0000e-05\n",
      "Epoch 795/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2601 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2601 - mape: 8.3244\n",
      "Epoch 795: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2597 - mape: 8.3130 - val_loss: 0.2596 - val_mse: 0.2583 - val_rmse: 0.5083 - val_mae: 0.2596 - val_mape: 8.3338 - lr: 1.0000e-05\n",
      "Epoch 796/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2593 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2593 - mape: 8.3161\n",
      "Epoch 796: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2598 - mape: 8.3274 - val_loss: 0.2594 - val_mse: 0.2566 - val_rmse: 0.5065 - val_mae: 0.2594 - val_mape: 8.2481 - lr: 1.0000e-05\n",
      "Epoch 797/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2592 - mse: 0.2561 - rmse: 0.5061 - mae: 0.2592 - mape: 8.2701\n",
      "Epoch 797: val_loss did not improve from 0.25910\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2597 - mape: 8.2813 - val_loss: 0.2614 - val_mse: 0.2646 - val_rmse: 0.5144 - val_mae: 0.2614 - val_mape: 8.4639 - lr: 1.0000e-05\n",
      "Epoch 798/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2600 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2600 - mape: 8.3133\n",
      "Epoch 798: val_loss improved from 0.25910 to 0.25900, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2600 - mape: 8.3169 - val_loss: 0.2590 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2590 - val_mape: 8.2831 - lr: 1.0000e-05\n",
      "Epoch 799/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2588 - mse: 0.2564 - rmse: 0.5063 - mae: 0.2588 - mape: 8.2915\n",
      "Epoch 799: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2589 - rmse: 0.5089 - mae: 0.2596 - mape: 8.3043 - val_loss: 0.2590 - val_mse: 0.2565 - val_rmse: 0.5065 - val_mae: 0.2590 - val_mape: 8.2670 - lr: 1.0000e-05\n",
      "Epoch 800/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2596 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2596 - mape: 8.2874\n",
      "Epoch 800: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2596 - mape: 8.2874 - val_loss: 0.2593 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2593 - val_mape: 8.3068 - lr: 1.0000e-05\n",
      "Epoch 801/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2592 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2592 - mape: 8.3037\n",
      "Epoch 801: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2598 - mape: 8.3093 - val_loss: 0.2591 - val_mse: 0.2562 - val_rmse: 0.5062 - val_mae: 0.2591 - val_mape: 8.2578 - lr: 1.0000e-05\n",
      "Epoch 802/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2590 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2590 - mape: 8.2773\n",
      "Epoch 802: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2598 - mape: 8.3047 - val_loss: 0.2590 - val_mse: 0.2567 - val_rmse: 0.5067 - val_mae: 0.2590 - val_mape: 8.2699 - lr: 1.0000e-05\n",
      "Epoch 803/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2608 - mape: 8.3175\n",
      "Epoch 803: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2597 - mape: 8.2848 - val_loss: 0.2606 - val_mse: 0.2631 - val_rmse: 0.5130 - val_mae: 0.2606 - val_mape: 8.4273 - lr: 1.0000e-05\n",
      "Epoch 804/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2602 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2602 - mape: 8.3551\n",
      "Epoch 804: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2599 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2599 - mape: 8.3229 - val_loss: 0.2594 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2594 - val_mape: 8.3297 - lr: 1.0000e-05\n",
      "Epoch 805/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2590 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2590 - mape: 8.2813\n",
      "Epoch 805: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2595 - mape: 8.2963 - val_loss: 0.2591 - val_mse: 0.2577 - val_rmse: 0.5076 - val_mae: 0.2591 - val_mape: 8.3049 - lr: 1.0000e-05\n",
      "Epoch 806/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2608 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2608 - mape: 8.3155\n",
      "Epoch 806: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2571 - rmse: 0.5071 - mae: 0.2596 - mape: 8.2840 - val_loss: 0.2592 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2592 - val_mape: 8.3363 - lr: 1.0000e-05\n",
      "Epoch 807/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2594 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2594 - mape: 8.3068\n",
      "Epoch 807: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2598 - mape: 8.3079 - val_loss: 0.2591 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2591 - val_mape: 8.2850 - lr: 1.0000e-05\n",
      "Epoch 808/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2588 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2588 - mape: 8.2829\n",
      "Epoch 808: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2596 - mape: 8.3091 - val_loss: 0.2620 - val_mse: 0.2665 - val_rmse: 0.5163 - val_mae: 0.2620 - val_mape: 8.4882 - lr: 1.0000e-05\n",
      "Epoch 809/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2597 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2597 - mape: 8.2992\n",
      "Epoch 809: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2597 - mape: 8.2992 - val_loss: 0.2599 - val_mse: 0.2609 - val_rmse: 0.5108 - val_mae: 0.2599 - val_mape: 8.3872 - lr: 1.0000e-05\n",
      "Epoch 810/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2606 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2606 - mape: 8.3288\n",
      "Epoch 810: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2597 - mape: 8.3134 - val_loss: 0.2602 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2602 - val_mape: 8.2295 - lr: 1.0000e-05\n",
      "Epoch 811/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2591 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2591 - mape: 8.2684\n",
      "Epoch 811: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2597 - mape: 8.2932 - val_loss: 0.2591 - val_mse: 0.2566 - val_rmse: 0.5066 - val_mae: 0.2591 - val_mape: 8.2746 - lr: 1.0000e-05\n",
      "Epoch 812/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2585 - mse: 0.2556 - rmse: 0.5056 - mae: 0.2585 - mape: 8.2391\n",
      "Epoch 812: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2568 - rmse: 0.5067 - mae: 0.2593 - mape: 8.2786 - val_loss: 0.2596 - val_mse: 0.2604 - val_rmse: 0.5103 - val_mae: 0.2596 - val_mape: 8.3527 - lr: 1.0000e-05\n",
      "Epoch 813/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2598 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2598 - mape: 8.3150\n",
      "Epoch 813: val_loss did not improve from 0.25900\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2596 - mape: 8.3096 - val_loss: 0.2595 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2595 - val_mape: 8.3357 - lr: 1.0000e-05\n",
      "Epoch 814/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2603 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2603 - mape: 8.3400\n",
      "Epoch 814: val_loss improved from 0.25900 to 0.25891, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2596 - mape: 8.2949 - val_loss: 0.2589 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2589 - val_mape: 8.2831 - lr: 1.0000e-05\n",
      "Epoch 815/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2609 - mse: 0.2624 - rmse: 0.5122 - mae: 0.2609 - mape: 8.3450\n",
      "Epoch 815: val_loss did not improve from 0.25891\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2595 - mape: 8.2986 - val_loss: 0.2589 - val_mse: 0.2565 - val_rmse: 0.5064 - val_mae: 0.2589 - val_mape: 8.2800 - lr: 1.0000e-05\n",
      "Epoch 816/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2595 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2595 - mape: 8.3024\n",
      "Epoch 816: val_loss did not improve from 0.25891\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2598 - mape: 8.3170 - val_loss: 0.2592 - val_mse: 0.2562 - val_rmse: 0.5061 - val_mae: 0.2592 - val_mape: 8.2413 - lr: 1.0000e-05\n",
      "Epoch 817/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2597 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2597 - mape: 8.3101\n",
      "Epoch 817: val_loss did not improve from 0.25891\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2598 - mape: 8.2911 - val_loss: 0.2589 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2589 - val_mape: 8.2926 - lr: 1.0000e-05\n",
      "Epoch 818/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2603 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2603 - mape: 8.3361\n",
      "Epoch 818: val_loss did not improve from 0.25891\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2596 - mape: 8.3084 - val_loss: 0.2590 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2590 - val_mape: 8.2601 - lr: 1.0000e-05\n",
      "Epoch 819/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2596 - mape: 8.3083\n",
      "Epoch 819: val_loss did not improve from 0.25891\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2596 - mape: 8.3037 - val_loss: 0.2592 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2592 - val_mape: 8.3410 - lr: 1.0000e-05\n",
      "Epoch 820/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2585 - mse: 0.2550 - rmse: 0.5049 - mae: 0.2585 - mape: 8.2798\n",
      "Epoch 820: val_loss did not improve from 0.25891\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2596 - mape: 8.3179 - val_loss: 0.2609 - val_mse: 0.2573 - val_rmse: 0.5072 - val_mae: 0.2609 - val_mape: 8.2460 - lr: 1.0000e-05\n",
      "Epoch 821/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2597 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2597 - mape: 8.3073\n",
      "Epoch 821: val_loss did not improve from 0.25891\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2593 - mape: 8.2967 - val_loss: 0.2601 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2601 - val_mape: 8.3782 - lr: 1.0000e-05\n",
      "Epoch 822/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2591 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2591 - mape: 8.2967\n",
      "Epoch 822: val_loss did not improve from 0.25891\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2594 - mape: 8.3031 - val_loss: 0.2596 - val_mse: 0.2567 - val_rmse: 0.5066 - val_mae: 0.2596 - val_mape: 8.2512 - lr: 1.0000e-05\n",
      "Epoch 823/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2593 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2593 - mape: 8.2966\n",
      "Epoch 823: val_loss improved from 0.25891 to 0.25884, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2570 - rmse: 0.5070 - mae: 0.2595 - mape: 8.2947 - val_loss: 0.2588 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2588 - val_mape: 8.2860 - lr: 1.0000e-05\n",
      "Epoch 824/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2593 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2593 - mape: 8.3005\n",
      "Epoch 824: val_loss did not improve from 0.25884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2597 - mape: 8.3084 - val_loss: 0.2591 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2591 - val_mape: 8.3290 - lr: 1.0000e-05\n",
      "Epoch 825/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2591 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2591 - mape: 8.3089\n",
      "Epoch 825: val_loss did not improve from 0.25884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2591 - rmse: 0.5091 - mae: 0.2598 - mape: 8.3184 - val_loss: 0.2590 - val_mse: 0.2560 - val_rmse: 0.5060 - val_mae: 0.2590 - val_mape: 8.2477 - lr: 1.0000e-05\n",
      "Epoch 826/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2593 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2593 - mape: 8.2889\n",
      "Epoch 826: val_loss did not improve from 0.25884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2597 - mape: 8.3066 - val_loss: 0.2594 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2594 - val_mape: 8.3177 - lr: 1.0000e-05\n",
      "Epoch 827/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2589 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2589 - mape: 8.2784\n",
      "Epoch 827: val_loss did not improve from 0.25884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2597 - mape: 8.2975 - val_loss: 0.2604 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2604 - val_mape: 8.2528 - lr: 1.0000e-05\n",
      "Epoch 828/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2600 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2600 - mape: 8.3192\n",
      "Epoch 828: val_loss did not improve from 0.25884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2595 - mape: 8.2985 - val_loss: 0.2595 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2595 - val_mape: 8.2579 - lr: 1.0000e-05\n",
      "Epoch 829/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2596 - mape: 8.2880\n",
      "Epoch 829: val_loss did not improve from 0.25884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2596 - mape: 8.2907 - val_loss: 0.2592 - val_mse: 0.2566 - val_rmse: 0.5065 - val_mae: 0.2592 - val_mape: 8.2509 - lr: 1.0000e-05\n",
      "Epoch 830/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2599 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2599 - mape: 8.3215\n",
      "Epoch 830: val_loss did not improve from 0.25884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2597 - mape: 8.3091 - val_loss: 0.2589 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2589 - val_mape: 8.2708 - lr: 1.0000e-05\n",
      "Epoch 831/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2609 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2609 - mape: 8.3411\n",
      "Epoch 831: val_loss did not improve from 0.25884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2594 - mape: 8.2956 - val_loss: 0.2616 - val_mse: 0.2653 - val_rmse: 0.5151 - val_mae: 0.2616 - val_mape: 8.4707 - lr: 1.0000e-05\n",
      "Epoch 832/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2586 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2586 - mape: 8.2716\n",
      "Epoch 832: val_loss did not improve from 0.25884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2592 - mape: 8.2978 - val_loss: 0.2590 - val_mse: 0.2560 - val_rmse: 0.5060 - val_mae: 0.2590 - val_mape: 8.2536 - lr: 1.0000e-05\n",
      "Epoch 833/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2596 - mape: 8.3148\n",
      "Epoch 833: val_loss did not improve from 0.25884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2597 - mape: 8.3106 - val_loss: 0.2599 - val_mse: 0.2562 - val_rmse: 0.5062 - val_mae: 0.2599 - val_mape: 8.2304 - lr: 1.0000e-05\n",
      "Epoch 834/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2601 - mse: 0.2566 - rmse: 0.5065 - mae: 0.2601 - mape: 8.3193\n",
      "Epoch 834: val_loss did not improve from 0.25884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2596 - mape: 8.2978 - val_loss: 0.2591 - val_mse: 0.2567 - val_rmse: 0.5067 - val_mae: 0.2591 - val_mape: 8.2622 - lr: 1.0000e-05\n",
      "Epoch 835/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2593 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2593 - mape: 8.3009\n",
      "Epoch 835: val_loss did not improve from 0.25884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2593 - mape: 8.2990 - val_loss: 0.2589 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2589 - val_mape: 8.2738 - lr: 1.0000e-05\n",
      "Epoch 836/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2599 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2599 - mape: 8.3315\n",
      "Epoch 836: val_loss improved from 0.25884 to 0.25878, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2597 - mape: 8.3159 - val_loss: 0.2588 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2588 - val_mape: 8.2764 - lr: 1.0000e-05\n",
      "Epoch 837/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2585 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2585 - mape: 8.2594\n",
      "Epoch 837: val_loss did not improve from 0.25878\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2591 - mape: 8.2855 - val_loss: 0.2599 - val_mse: 0.2611 - val_rmse: 0.5110 - val_mae: 0.2599 - val_mape: 8.3817 - lr: 1.0000e-05\n",
      "Epoch 838/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2597 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2597 - mape: 8.3062\n",
      "Epoch 838: val_loss did not improve from 0.25878\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2596 - mape: 8.2956 - val_loss: 0.2588 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2588 - val_mape: 8.2695 - lr: 1.0000e-05\n",
      "Epoch 839/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2582 - mse: 0.2551 - rmse: 0.5051 - mae: 0.2582 - mape: 8.2404\n",
      "Epoch 839: val_loss did not improve from 0.25878\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2593 - mape: 8.2876 - val_loss: 0.2591 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2591 - val_mape: 8.3029 - lr: 1.0000e-05\n",
      "Epoch 840/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2610 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2610 - mape: 8.3542\n",
      "Epoch 840: val_loss did not improve from 0.25878\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2597 - mape: 8.3089 - val_loss: 0.2589 - val_mse: 0.2573 - val_rmse: 0.5072 - val_mae: 0.2589 - val_mape: 8.2964 - lr: 1.0000e-05\n",
      "Epoch 841/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2600 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2600 - mape: 8.3061\n",
      "Epoch 841: val_loss did not improve from 0.25878\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2595 - mape: 8.2940 - val_loss: 0.2593 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2593 - val_mape: 8.3438 - lr: 1.0000e-05\n",
      "Epoch 842/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2598 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2598 - mape: 8.2820\n",
      "Epoch 842: val_loss did not improve from 0.25878\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2596 - mape: 8.2955 - val_loss: 0.2590 - val_mse: 0.2565 - val_rmse: 0.5065 - val_mae: 0.2590 - val_mape: 8.2572 - lr: 1.0000e-05\n",
      "Epoch 843/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2595 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2595 - mape: 8.3045\n",
      "Epoch 843: val_loss did not improve from 0.25878\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2594 - mape: 8.3029 - val_loss: 0.2596 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2596 - val_mape: 8.2328 - lr: 1.0000e-05\n",
      "Epoch 844/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2594 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2594 - mape: 8.2983\n",
      "Epoch 844: val_loss did not improve from 0.25878\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2598 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2598 - mape: 8.3038 - val_loss: 0.2593 - val_mse: 0.2563 - val_rmse: 0.5063 - val_mae: 0.2593 - val_mape: 8.2425 - lr: 1.0000e-05\n",
      "Epoch 845/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2590 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2590 - mape: 8.2785\n",
      "Epoch 845: val_loss did not improve from 0.25878\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2594 - mape: 8.2892 - val_loss: 0.2592 - val_mse: 0.2566 - val_rmse: 0.5066 - val_mae: 0.2592 - val_mape: 8.2525 - lr: 1.0000e-05\n",
      "Epoch 846/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2589 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2589 - mape: 8.2928\n",
      "Epoch 846: val_loss did not improve from 0.25878\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2597 - mape: 8.3067 - val_loss: 0.2589 - val_mse: 0.2565 - val_rmse: 0.5065 - val_mae: 0.2589 - val_mape: 8.2722 - lr: 1.0000e-05\n",
      "Epoch 847/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2591 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2591 - mape: 8.2935\n",
      "Epoch 847: val_loss did not improve from 0.25878\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2591 - mape: 8.2935 - val_loss: 0.2590 - val_mse: 0.2578 - val_rmse: 0.5077 - val_mae: 0.2590 - val_mape: 8.3051 - lr: 1.0000e-05\n",
      "Epoch 848/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2593 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2593 - mape: 8.2784\n",
      "Epoch 848: val_loss did not improve from 0.25878\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2595 - mape: 8.2833 - val_loss: 0.2589 - val_mse: 0.2584 - val_rmse: 0.5084 - val_mae: 0.2589 - val_mape: 8.3119 - lr: 1.0000e-05\n",
      "Epoch 849/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2598 - mse: 0.2593 - rmse: 0.5093 - mae: 0.2598 - mape: 8.3325\n",
      "Epoch 849: val_loss improved from 0.25878 to 0.25876, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2597 - mape: 8.3064 - val_loss: 0.2588 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2588 - val_mape: 8.2734 - lr: 1.0000e-05\n",
      "Epoch 850/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2594 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2594 - mape: 8.2914\n",
      "Epoch 850: val_loss did not improve from 0.25876\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2593 - mape: 8.2907 - val_loss: 0.2599 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2599 - val_mape: 8.3826 - lr: 1.0000e-05\n",
      "Epoch 851/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2594 - mse: 0.2570 - rmse: 0.5070 - mae: 0.2594 - mape: 8.2871\n",
      "Epoch 851: val_loss did not improve from 0.25876\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2594 - mape: 8.2993 - val_loss: 0.2588 - val_mse: 0.2579 - val_rmse: 0.5079 - val_mae: 0.2588 - val_mape: 8.2802 - lr: 1.0000e-05\n",
      "Epoch 852/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2597 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2597 - mape: 8.3133\n",
      "Epoch 852: val_loss did not improve from 0.25876\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2596 - mape: 8.3089 - val_loss: 0.2590 - val_mse: 0.2579 - val_rmse: 0.5079 - val_mae: 0.2590 - val_mape: 8.3171 - lr: 1.0000e-05\n",
      "Epoch 853/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2598 - mape: 8.3191\n",
      "Epoch 853: val_loss did not improve from 0.25876\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2595 - mape: 8.3000 - val_loss: 0.2642 - val_mse: 0.2718 - val_rmse: 0.5213 - val_mae: 0.2642 - val_mape: 8.5940 - lr: 1.0000e-05\n",
      "Epoch 854/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2595 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2595 - mape: 8.3113\n",
      "Epoch 854: val_loss did not improve from 0.25876\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2594 - mape: 8.3003 - val_loss: 0.2591 - val_mse: 0.2574 - val_rmse: 0.5073 - val_mae: 0.2591 - val_mape: 8.2708 - lr: 1.0000e-05\n",
      "Epoch 855/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2586 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2586 - mape: 8.2780\n",
      "Epoch 855: val_loss did not improve from 0.25876\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2593 - mape: 8.3082 - val_loss: 0.2589 - val_mse: 0.2577 - val_rmse: 0.5076 - val_mae: 0.2589 - val_mape: 8.2929 - lr: 1.0000e-05\n",
      "Epoch 856/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2590 - mse: 0.2546 - rmse: 0.5046 - mae: 0.2590 - mape: 8.2824\n",
      "Epoch 856: val_loss did not improve from 0.25876\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2594 - mape: 8.2889 - val_loss: 0.2589 - val_mse: 0.2562 - val_rmse: 0.5062 - val_mae: 0.2589 - val_mape: 8.2526 - lr: 1.0000e-05\n",
      "Epoch 857/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2595 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2595 - mape: 8.3094\n",
      "Epoch 857: val_loss did not improve from 0.25876\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2593 - mape: 8.2952 - val_loss: 0.2594 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2594 - val_mape: 8.2718 - lr: 1.0000e-05\n",
      "Epoch 858/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2571 - mse: 0.2534 - rmse: 0.5034 - mae: 0.2571 - mape: 8.2534\n",
      "Epoch 858: val_loss did not improve from 0.25876\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2591 - mape: 8.3092 - val_loss: 0.2588 - val_mse: 0.2578 - val_rmse: 0.5077 - val_mae: 0.2588 - val_mape: 8.2902 - lr: 1.0000e-05\n",
      "Epoch 859/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2597 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2597 - mape: 8.3131\n",
      "Epoch 859: val_loss improved from 0.25876 to 0.25873, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2596 - mape: 8.3024 - val_loss: 0.2587 - val_mse: 0.2565 - val_rmse: 0.5065 - val_mae: 0.2587 - val_mape: 8.2656 - lr: 1.0000e-05\n",
      "Epoch 860/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2590 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2590 - mape: 8.2959\n",
      "Epoch 860: val_loss did not improve from 0.25873\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2590 - mape: 8.2959 - val_loss: 0.2601 - val_mse: 0.2610 - val_rmse: 0.5108 - val_mae: 0.2601 - val_mape: 8.3795 - lr: 1.0000e-05\n",
      "Epoch 861/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2598 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2598 - mape: 8.2998\n",
      "Epoch 861: val_loss did not improve from 0.25873\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2596 - mape: 8.2942 - val_loss: 0.2587 - val_mse: 0.2564 - val_rmse: 0.5063 - val_mae: 0.2587 - val_mape: 8.2625 - lr: 1.0000e-05\n",
      "Epoch 862/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2598 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2598 - mape: 8.3182\n",
      "Epoch 862: val_loss did not improve from 0.25873\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2593 - mape: 8.2977 - val_loss: 0.2587 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2587 - val_mape: 8.2639 - lr: 1.0000e-05\n",
      "Epoch 863/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2591 - mse: 0.2570 - rmse: 0.5070 - mae: 0.2591 - mape: 8.2850\n",
      "Epoch 863: val_loss did not improve from 0.25873\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2592 - mape: 8.2886 - val_loss: 0.2609 - val_mse: 0.2633 - val_rmse: 0.5131 - val_mae: 0.2609 - val_mape: 8.4265 - lr: 1.0000e-05\n",
      "Epoch 864/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2596 - mape: 8.3305\n",
      "Epoch 864: val_loss did not improve from 0.25873\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2596 - mape: 8.3156 - val_loss: 0.2588 - val_mse: 0.2567 - val_rmse: 0.5067 - val_mae: 0.2588 - val_mape: 8.2524 - lr: 1.0000e-05\n",
      "Epoch 865/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2602 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2602 - mape: 8.3247\n",
      "Epoch 865: val_loss did not improve from 0.25873\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2591 - mape: 8.2977 - val_loss: 0.2590 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2590 - val_mape: 8.2475 - lr: 1.0000e-05\n",
      "Epoch 866/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2600 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2600 - mape: 8.3150\n",
      "Epoch 866: val_loss improved from 0.25873 to 0.25868, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2600 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2600 - mape: 8.3157 - val_loss: 0.2587 - val_mse: 0.2574 - val_rmse: 0.5073 - val_mae: 0.2587 - val_mape: 8.2898 - lr: 1.0000e-05\n",
      "Epoch 867/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2591 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2591 - mape: 8.2938\n",
      "Epoch 867: val_loss did not improve from 0.25868\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2589 - mape: 8.2847 - val_loss: 0.2591 - val_mse: 0.2581 - val_rmse: 0.5081 - val_mae: 0.2591 - val_mape: 8.3216 - lr: 1.0000e-05\n",
      "Epoch 868/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2603 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2603 - mape: 8.3224\n",
      "Epoch 868: val_loss did not improve from 0.25868\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2594 - mape: 8.2919 - val_loss: 0.2590 - val_mse: 0.2562 - val_rmse: 0.5062 - val_mae: 0.2590 - val_mape: 8.2495 - lr: 1.0000e-05\n",
      "Epoch 869/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2600 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2600 - mape: 8.3346\n",
      "Epoch 869: val_loss did not improve from 0.25868\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2593 - mape: 8.2904 - val_loss: 0.2592 - val_mse: 0.2563 - val_rmse: 0.5062 - val_mae: 0.2592 - val_mape: 8.2460 - lr: 1.0000e-05\n",
      "Epoch 870/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2600 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2600 - mape: 8.3261\n",
      "Epoch 870: val_loss did not improve from 0.25868\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2591 - rmse: 0.5091 - mae: 0.2595 - mape: 8.3110 - val_loss: 0.2590 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2590 - val_mape: 8.3141 - lr: 1.0000e-05\n",
      "Epoch 871/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2592 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2592 - mape: 8.3001\n",
      "Epoch 871: val_loss did not improve from 0.25868\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2571 - rmse: 0.5071 - mae: 0.2593 - mape: 8.2932 - val_loss: 0.2590 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2590 - val_mape: 8.2730 - lr: 1.0000e-05\n",
      "Epoch 872/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2590 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2590 - mape: 8.2895\n",
      "Epoch 872: val_loss did not improve from 0.25868\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2591 - mape: 8.2876 - val_loss: 0.2587 - val_mse: 0.2566 - val_rmse: 0.5065 - val_mae: 0.2587 - val_mape: 8.2645 - lr: 1.0000e-05\n",
      "Epoch 873/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2590 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2590 - mape: 8.3035\n",
      "Epoch 873: val_loss did not improve from 0.25868\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2593 - mape: 8.3031 - val_loss: 0.2596 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2596 - val_mape: 8.3686 - lr: 1.0000e-05\n",
      "Epoch 874/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2597 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2597 - mape: 8.3138\n",
      "Epoch 874: val_loss did not improve from 0.25868\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2595 - mape: 8.2993 - val_loss: 0.2596 - val_mse: 0.2570 - val_rmse: 0.5069 - val_mae: 0.2596 - val_mape: 8.2470 - lr: 1.0000e-05\n",
      "Epoch 875/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2584 - mse: 0.2571 - rmse: 0.5071 - mae: 0.2584 - mape: 8.2906\n",
      "Epoch 875: val_loss did not improve from 0.25868\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2592 - mape: 8.2971 - val_loss: 0.2589 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2589 - val_mape: 8.3271 - lr: 1.0000e-05\n",
      "Epoch 876/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2590 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2590 - mape: 8.3143\n",
      "Epoch 876: val_loss improved from 0.25868 to 0.25861, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2590 - mape: 8.2991 - val_loss: 0.2586 - val_mse: 0.2567 - val_rmse: 0.5067 - val_mae: 0.2586 - val_mape: 8.2593 - lr: 1.0000e-05\n",
      "Epoch 877/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2597 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2597 - mape: 8.3233\n",
      "Epoch 877: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2594 - mape: 8.3128 - val_loss: 0.2600 - val_mse: 0.2562 - val_rmse: 0.5061 - val_mae: 0.2600 - val_mape: 8.2312 - lr: 1.0000e-05\n",
      "Epoch 878/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2595 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2595 - mape: 8.3049\n",
      "Epoch 878: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2594 - mape: 8.2963 - val_loss: 0.2591 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2591 - val_mape: 8.2399 - lr: 1.0000e-05\n",
      "Epoch 879/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2595 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2595 - mape: 8.3058\n",
      "Epoch 879: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2595 - mape: 8.3058 - val_loss: 0.2588 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2588 - val_mape: 8.2975 - lr: 1.0000e-05\n",
      "Epoch 880/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2598 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2598 - mape: 8.3303\n",
      "Epoch 880: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2594 - mape: 8.3034 - val_loss: 0.2586 - val_mse: 0.2562 - val_rmse: 0.5062 - val_mae: 0.2586 - val_mape: 8.2552 - lr: 1.0000e-05\n",
      "Epoch 881/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2588 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2588 - mape: 8.2864\n",
      "Epoch 881: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2595 - mape: 8.3043 - val_loss: 0.2588 - val_mse: 0.2577 - val_rmse: 0.5076 - val_mae: 0.2588 - val_mape: 8.2784 - lr: 1.0000e-05\n",
      "Epoch 882/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2592 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2592 - mape: 8.3042\n",
      "Epoch 882: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2592 - mape: 8.3059 - val_loss: 0.2592 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2592 - val_mape: 8.3609 - lr: 1.0000e-05\n",
      "Epoch 883/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2590 - mse: 0.2562 - rmse: 0.5061 - mae: 0.2590 - mape: 8.2841\n",
      "Epoch 883: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2591 - mape: 8.2968 - val_loss: 0.2588 - val_mse: 0.2563 - val_rmse: 0.5062 - val_mae: 0.2588 - val_mape: 8.2485 - lr: 1.0000e-05\n",
      "Epoch 884/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2583 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2583 - mape: 8.2812\n",
      "Epoch 884: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2590 - mape: 8.2915 - val_loss: 0.2590 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2590 - val_mape: 8.3438 - lr: 1.0000e-05\n",
      "Epoch 885/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2592 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2592 - mape: 8.3031\n",
      "Epoch 885: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2592 - mape: 8.2970 - val_loss: 0.2616 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2616 - val_mape: 8.2437 - lr: 1.0000e-05\n",
      "Epoch 886/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2595 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2595 - mape: 8.3024\n",
      "Epoch 886: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2591 - mape: 8.3001 - val_loss: 0.2591 - val_mse: 0.2561 - val_rmse: 0.5060 - val_mae: 0.2591 - val_mape: 8.2348 - lr: 1.0000e-05\n",
      "Epoch 887/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2592 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2592 - mape: 8.2916\n",
      "Epoch 887: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2564 - rmse: 0.5063 - mae: 0.2592 - mape: 8.2786 - val_loss: 0.2602 - val_mse: 0.2574 - val_rmse: 0.5073 - val_mae: 0.2602 - val_mape: 8.2528 - lr: 1.0000e-05\n",
      "Epoch 888/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2594 - mse: 0.2589 - rmse: 0.5089 - mae: 0.2594 - mape: 8.3043\n",
      "Epoch 888: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2593 - mape: 8.3033 - val_loss: 0.2592 - val_mse: 0.2559 - val_rmse: 0.5058 - val_mae: 0.2592 - val_mape: 8.2355 - lr: 1.0000e-05\n",
      "Epoch 889/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2583 - mse: 0.2554 - rmse: 0.5054 - mae: 0.2583 - mape: 8.2610\n",
      "Epoch 889: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2593 - mape: 8.2884 - val_loss: 0.2587 - val_mse: 0.2580 - val_rmse: 0.5080 - val_mae: 0.2587 - val_mape: 8.3157 - lr: 1.0000e-05\n",
      "Epoch 890/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2596 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2596 - mape: 8.2954\n",
      "Epoch 890: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2595 - mape: 8.2931 - val_loss: 0.2605 - val_mse: 0.2640 - val_rmse: 0.5138 - val_mae: 0.2605 - val_mape: 8.4367 - lr: 1.0000e-05\n",
      "Epoch 891/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2593 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2593 - mape: 8.3242\n",
      "Epoch 891: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2591 - mape: 8.3067 - val_loss: 0.2589 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2589 - val_mape: 8.3158 - lr: 1.0000e-05\n",
      "Epoch 892/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2594 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2594 - mape: 8.3047\n",
      "Epoch 892: val_loss did not improve from 0.25861\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2594 - mape: 8.3039 - val_loss: 0.2606 - val_mse: 0.2632 - val_rmse: 0.5130 - val_mae: 0.2606 - val_mape: 8.4277 - lr: 1.0000e-05\n",
      "Epoch 893/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2592 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2592 - mape: 8.3075\n",
      "Epoch 893: val_loss improved from 0.25861 to 0.25857, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2591 - mape: 8.3060 - val_loss: 0.2586 - val_mse: 0.2566 - val_rmse: 0.5066 - val_mae: 0.2586 - val_mape: 8.2606 - lr: 1.0000e-05\n",
      "Epoch 894/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2591 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2591 - mape: 8.2946\n",
      "Epoch 894: val_loss did not improve from 0.25857\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2592 - mape: 8.2974 - val_loss: 0.2586 - val_mse: 0.2562 - val_rmse: 0.5061 - val_mae: 0.2586 - val_mape: 8.2460 - lr: 1.0000e-05\n",
      "Epoch 895/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2618 - rmse: 0.5116 - mae: 0.2609 - mape: 8.3395\n",
      "Epoch 895: val_loss did not improve from 0.25857\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2594 - mape: 8.2953 - val_loss: 0.2589 - val_mse: 0.2559 - val_rmse: 0.5059 - val_mae: 0.2589 - val_mape: 8.2322 - lr: 1.0000e-05\n",
      "Epoch 896/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2582 - mse: 0.2562 - rmse: 0.5061 - mae: 0.2582 - mape: 8.2611\n",
      "Epoch 896: val_loss did not improve from 0.25857\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2587 - mse: 0.2570 - rmse: 0.5070 - mae: 0.2587 - mape: 8.2814 - val_loss: 0.2591 - val_mse: 0.2570 - val_rmse: 0.5070 - val_mae: 0.2591 - val_mape: 8.2471 - lr: 1.0000e-05\n",
      "Epoch 897/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2595 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2595 - mape: 8.2932\n",
      "Epoch 897: val_loss did not improve from 0.25857\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2590 - mape: 8.2787 - val_loss: 0.2586 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2586 - val_mape: 8.2581 - lr: 1.0000e-05\n",
      "Epoch 898/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2604 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2604 - mape: 8.3045\n",
      "Epoch 898: val_loss did not improve from 0.25857\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2593 - mape: 8.2880 - val_loss: 0.2586 - val_mse: 0.2563 - val_rmse: 0.5062 - val_mae: 0.2586 - val_mape: 8.2424 - lr: 1.0000e-05\n",
      "Epoch 899/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2597 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2597 - mape: 8.2998\n",
      "Epoch 899: val_loss did not improve from 0.25857\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2596 - mape: 8.2964 - val_loss: 0.2586 - val_mse: 0.2575 - val_rmse: 0.5075 - val_mae: 0.2586 - val_mape: 8.2929 - lr: 1.0000e-05\n",
      "Epoch 900/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2589 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2589 - mape: 8.2737\n",
      "Epoch 900: val_loss did not improve from 0.25857\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2590 - mape: 8.2693 - val_loss: 0.2587 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2587 - val_mape: 8.3210 - lr: 1.0000e-05\n",
      "Epoch 901/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2595 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2595 - mape: 8.3021\n",
      "Epoch 901: val_loss did not improve from 0.25857\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2591 - mape: 8.2943 - val_loss: 0.2592 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2592 - val_mape: 8.3461 - lr: 1.0000e-05\n",
      "Epoch 902/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2592 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2592 - mape: 8.2907\n",
      "Epoch 902: val_loss did not improve from 0.25857\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2592 - mape: 8.2907 - val_loss: 0.2586 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2586 - val_mape: 8.3022 - lr: 1.0000e-05\n",
      "Epoch 903/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2595 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2595 - mape: 8.2923\n",
      "Epoch 903: val_loss did not improve from 0.25857\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2592 - mape: 8.3015 - val_loss: 0.2586 - val_mse: 0.2567 - val_rmse: 0.5067 - val_mae: 0.2586 - val_mape: 8.2495 - lr: 1.0000e-05\n",
      "Epoch 904/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2587 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2587 - mape: 8.2774\n",
      "Epoch 904: val_loss improved from 0.25857 to 0.25853, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2590 - mape: 8.2889 - val_loss: 0.2585 - val_mse: 0.2561 - val_rmse: 0.5061 - val_mae: 0.2585 - val_mape: 8.2416 - lr: 1.0000e-05\n",
      "Epoch 905/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2595 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2595 - mape: 8.3002\n",
      "Epoch 905: val_loss did not improve from 0.25853\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2595 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2595 - mape: 8.2956 - val_loss: 0.2591 - val_mse: 0.2567 - val_rmse: 0.5066 - val_mae: 0.2591 - val_mape: 8.2437 - lr: 1.0000e-05\n",
      "Epoch 906/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2588 - mse: 0.2563 - rmse: 0.5063 - mae: 0.2588 - mape: 8.2671\n",
      "Epoch 906: val_loss did not improve from 0.25853\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2563 - rmse: 0.5063 - mae: 0.2588 - mape: 8.2690 - val_loss: 0.2587 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2587 - val_mape: 8.3256 - lr: 1.0000e-05\n",
      "Epoch 907/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2586 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2586 - mape: 8.2872\n",
      "Epoch 907: val_loss did not improve from 0.25853\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2593 - mape: 8.2948 - val_loss: 0.2590 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2590 - val_mape: 8.3435 - lr: 1.0000e-05\n",
      "Epoch 908/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2583 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2583 - mape: 8.2691\n",
      "Epoch 908: val_loss improved from 0.25853 to 0.25845, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2591 - mape: 8.2925 - val_loss: 0.2584 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2584 - val_mape: 8.2882 - lr: 1.0000e-05\n",
      "Epoch 909/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2588 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2588 - mape: 8.2862\n",
      "Epoch 909: val_loss did not improve from 0.25845\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2590 - mape: 8.2904 - val_loss: 0.2587 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2587 - val_mape: 8.3101 - lr: 1.0000e-05\n",
      "Epoch 910/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2590 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2590 - mape: 8.2911\n",
      "Epoch 910: val_loss did not improve from 0.25845\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2591 - mape: 8.3035 - val_loss: 0.2585 - val_mse: 0.2568 - val_rmse: 0.5067 - val_mae: 0.2585 - val_mape: 8.2577 - lr: 1.0000e-05\n",
      "Epoch 911/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2596 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2596 - mape: 8.3040\n",
      "Epoch 911: val_loss did not improve from 0.25845\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2596 - mape: 8.3053 - val_loss: 0.2589 - val_mse: 0.2561 - val_rmse: 0.5061 - val_mae: 0.2589 - val_mape: 8.2344 - lr: 1.0000e-05\n",
      "Epoch 912/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2589 - mse: 0.2571 - rmse: 0.5071 - mae: 0.2589 - mape: 8.2651\n",
      "Epoch 912: val_loss did not improve from 0.25845\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2591 - mape: 8.2828 - val_loss: 0.2657 - val_mse: 0.2658 - val_rmse: 0.5156 - val_mae: 0.2657 - val_mape: 8.2939 - lr: 1.0000e-05\n",
      "Epoch 913/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2581 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2581 - mape: 8.2369\n",
      "Epoch 913: val_loss did not improve from 0.25845\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2596 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2596 - mape: 8.3156 - val_loss: 0.2591 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2591 - val_mape: 8.3522 - lr: 1.0000e-05\n",
      "Epoch 914/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2596 - mape: 8.3053\n",
      "Epoch 914: val_loss improved from 0.25845 to 0.25835, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2589 - mape: 8.2817 - val_loss: 0.2583 - val_mse: 0.2575 - val_rmse: 0.5075 - val_mae: 0.2583 - val_mape: 8.2794 - lr: 1.0000e-05\n",
      "Epoch 915/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2593 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2593 - mape: 8.3040\n",
      "Epoch 915: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2592 - mape: 8.3107 - val_loss: 0.2596 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2596 - val_mape: 8.2381 - lr: 1.0000e-05\n",
      "Epoch 916/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2591 - mse: 0.2570 - rmse: 0.5070 - mae: 0.2591 - mape: 8.2877\n",
      "Epoch 916: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2590 - mape: 8.2844 - val_loss: 0.2591 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2591 - val_mape: 8.2357 - lr: 1.0000e-05\n",
      "Epoch 917/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2588 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2588 - mape: 8.2793\n",
      "Epoch 917: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2591 - mape: 8.2838 - val_loss: 0.2595 - val_mse: 0.2609 - val_rmse: 0.5108 - val_mae: 0.2595 - val_mape: 8.3686 - lr: 1.0000e-05\n",
      "Epoch 918/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2588 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2588 - mape: 8.2870\n",
      "Epoch 918: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2590 - mape: 8.2855 - val_loss: 0.2589 - val_mse: 0.2580 - val_rmse: 0.5080 - val_mae: 0.2589 - val_mape: 8.3031 - lr: 1.0000e-05\n",
      "Epoch 919/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2595 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2595 - mape: 8.2917\n",
      "Epoch 919: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2592 - mape: 8.2921 - val_loss: 0.2594 - val_mse: 0.2606 - val_rmse: 0.5104 - val_mae: 0.2594 - val_mape: 8.3585 - lr: 1.0000e-05\n",
      "Epoch 920/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2598 - mape: 8.3054\n",
      "Epoch 920: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2591 - mape: 8.2893 - val_loss: 0.2585 - val_mse: 0.2563 - val_rmse: 0.5063 - val_mae: 0.2585 - val_mape: 8.2637 - lr: 1.0000e-05\n",
      "Epoch 921/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2587 - mse: 0.2555 - rmse: 0.5054 - mae: 0.2587 - mape: 8.2505\n",
      "Epoch 921: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2566 - rmse: 0.5066 - mae: 0.2590 - mape: 8.2652 - val_loss: 0.2584 - val_mse: 0.2570 - val_rmse: 0.5070 - val_mae: 0.2584 - val_mape: 8.2636 - lr: 1.0000e-05\n",
      "Epoch 922/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2585 - mse: 0.2566 - rmse: 0.5066 - mae: 0.2585 - mape: 8.2999\n",
      "Epoch 922: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2592 - mape: 8.2973 - val_loss: 0.2588 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2588 - val_mape: 8.3326 - lr: 1.0000e-05\n",
      "Epoch 923/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2606 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2606 - mape: 8.3395\n",
      "Epoch 923: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2591 - mape: 8.2980 - val_loss: 0.2585 - val_mse: 0.2561 - val_rmse: 0.5061 - val_mae: 0.2585 - val_mape: 8.2496 - lr: 1.0000e-05\n",
      "Epoch 924/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2602 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2602 - mape: 8.3337\n",
      "Epoch 924: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2597 - mape: 8.3084 - val_loss: 0.2584 - val_mse: 0.2577 - val_rmse: 0.5076 - val_mae: 0.2584 - val_mape: 8.2931 - lr: 1.0000e-05\n",
      "Epoch 925/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2589 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2589 - mape: 8.2888\n",
      "Epoch 925: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2590 - mape: 8.2953 - val_loss: 0.2590 - val_mse: 0.2602 - val_rmse: 0.5101 - val_mae: 0.2590 - val_mape: 8.3522 - lr: 1.0000e-05\n",
      "Epoch 926/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2592 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2592 - mape: 8.3056\n",
      "Epoch 926: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2589 - mape: 8.3030 - val_loss: 0.2588 - val_mse: 0.2560 - val_rmse: 0.5059 - val_mae: 0.2588 - val_mape: 8.2317 - lr: 1.0000e-05\n",
      "Epoch 927/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2589 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2589 - mape: 8.2927\n",
      "Epoch 927: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2589 - mape: 8.2927 - val_loss: 0.2584 - val_mse: 0.2575 - val_rmse: 0.5075 - val_mae: 0.2584 - val_mape: 8.2917 - lr: 1.0000e-05\n",
      "Epoch 928/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2589 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2589 - mape: 8.2904\n",
      "Epoch 928: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2591 - mape: 8.2954 - val_loss: 0.2585 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2585 - val_mape: 8.2962 - lr: 1.0000e-05\n",
      "Epoch 929/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2578 - mse: 0.2551 - rmse: 0.5050 - mae: 0.2578 - mape: 8.2543\n",
      "Epoch 929: val_loss did not improve from 0.25835\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2588 - mape: 8.2941 - val_loss: 0.2601 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2601 - val_mape: 8.2325 - lr: 1.0000e-05\n",
      "Epoch 930/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2589 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2589 - mape: 8.2967\n",
      "Epoch 930: val_loss improved from 0.25835 to 0.25831, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2591 - mape: 8.2867 - val_loss: 0.2583 - val_mse: 0.2567 - val_rmse: 0.5066 - val_mae: 0.2583 - val_mape: 8.2522 - lr: 1.0000e-05\n",
      "Epoch 931/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2583 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2583 - mape: 8.2883\n",
      "Epoch 931: val_loss did not improve from 0.25831\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2590 - mape: 8.3045 - val_loss: 0.2585 - val_mse: 0.2567 - val_rmse: 0.5066 - val_mae: 0.2585 - val_mape: 8.2448 - lr: 1.0000e-05\n",
      "Epoch 932/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2598 - mape: 8.3070\n",
      "Epoch 932: val_loss did not improve from 0.25831\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2593 - mape: 8.2930 - val_loss: 0.2588 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2588 - val_mape: 8.3199 - lr: 1.0000e-05\n",
      "Epoch 933/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2589 - mse: 0.2561 - rmse: 0.5061 - mae: 0.2589 - mape: 8.2975\n",
      "Epoch 933: val_loss improved from 0.25831 to 0.25829, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2588 - mape: 8.2881 - val_loss: 0.2583 - val_mse: 0.2568 - val_rmse: 0.5067 - val_mae: 0.2583 - val_mape: 8.2615 - lr: 1.0000e-05\n",
      "Epoch 934/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2589 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2589 - mape: 8.2836\n",
      "Epoch 934: val_loss improved from 0.25829 to 0.25827, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2589 - mape: 8.2842 - val_loss: 0.2583 - val_mse: 0.2567 - val_rmse: 0.5067 - val_mae: 0.2583 - val_mape: 8.2626 - lr: 1.0000e-05\n",
      "Epoch 935/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2591 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2591 - mape: 8.3005\n",
      "Epoch 935: val_loss did not improve from 0.25827\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2591 - mape: 8.2918 - val_loss: 0.2596 - val_mse: 0.2614 - val_rmse: 0.5113 - val_mae: 0.2596 - val_mape: 8.3878 - lr: 1.0000e-05\n",
      "Epoch 936/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2595 - mse: 0.2596 - rmse: 0.5096 - mae: 0.2595 - mape: 8.3070\n",
      "Epoch 936: val_loss did not improve from 0.25827\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2593 - mape: 8.3039 - val_loss: 0.2590 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2590 - val_mape: 8.3511 - lr: 1.0000e-05\n",
      "Epoch 937/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2586 - mse: 0.2589 - rmse: 0.5089 - mae: 0.2586 - mape: 8.2800\n",
      "Epoch 937: val_loss did not improve from 0.25827\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2588 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2588 - mape: 8.2907 - val_loss: 0.2593 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2593 - val_mape: 8.3417 - lr: 1.0000e-05\n",
      "Epoch 938/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2591 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2591 - mape: 8.2774\n",
      "Epoch 938: val_loss did not improve from 0.25827\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2591 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2591 - mape: 8.2774 - val_loss: 0.2587 - val_mse: 0.2568 - val_rmse: 0.5068 - val_mae: 0.2587 - val_mape: 8.2455 - lr: 1.0000e-05\n",
      "Epoch 939/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2588 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2588 - mape: 8.2911\n",
      "Epoch 939: val_loss did not improve from 0.25827\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2592 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2592 - mape: 8.2891 - val_loss: 0.2599 - val_mse: 0.2632 - val_rmse: 0.5130 - val_mae: 0.2599 - val_mape: 8.4063 - lr: 1.0000e-05\n",
      "Epoch 940/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2589 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2589 - mape: 8.2922\n",
      "Epoch 940: val_loss did not improve from 0.25827\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2589 - mape: 8.2911 - val_loss: 0.2609 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2609 - val_mape: 8.2434 - lr: 1.0000e-05\n",
      "Epoch 941/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2590 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2590 - mape: 8.2892\n",
      "Epoch 941: val_loss did not improve from 0.25827\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2597 - mape: 8.3051 - val_loss: 0.2600 - val_mse: 0.2628 - val_rmse: 0.5126 - val_mae: 0.2600 - val_mape: 8.4183 - lr: 1.0000e-05\n",
      "Epoch 942/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2588 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2588 - mape: 8.2980\n",
      "Epoch 942: val_loss did not improve from 0.25827\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2588 - mape: 8.2980 - val_loss: 0.2584 - val_mse: 0.2568 - val_rmse: 0.5067 - val_mae: 0.2584 - val_mape: 8.2428 - lr: 1.0000e-05\n",
      "Epoch 943/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2593 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2593 - mape: 8.2929\n",
      "Epoch 943: val_loss did not improve from 0.25827\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2589 - mape: 8.2734 - val_loss: 0.2583 - val_mse: 0.2567 - val_rmse: 0.5066 - val_mae: 0.2583 - val_mape: 8.2424 - lr: 1.0000e-05\n",
      "Epoch 944/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2585 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2585 - mape: 8.2614\n",
      "Epoch 944: val_loss did not improve from 0.25827\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2590 - mape: 8.2850 - val_loss: 0.2586 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2586 - val_mape: 8.3075 - lr: 1.0000e-05\n",
      "Epoch 945/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2597 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2597 - mape: 8.2816\n",
      "Epoch 945: val_loss did not improve from 0.25827\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2592 - mape: 8.2707 - val_loss: 0.2592 - val_mse: 0.2608 - val_rmse: 0.5107 - val_mae: 0.2592 - val_mape: 8.3723 - lr: 1.0000e-05\n",
      "Epoch 946/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2581 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2581 - mape: 8.2688\n",
      "Epoch 946: val_loss did not improve from 0.25827\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2589 - mape: 8.2887 - val_loss: 0.2587 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2587 - val_mape: 8.3086 - lr: 1.0000e-05\n",
      "Epoch 947/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2584 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2584 - mape: 8.2641\n",
      "Epoch 947: val_loss did not improve from 0.25827\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2592 - mape: 8.2963 - val_loss: 0.2584 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2584 - val_mape: 8.3049 - lr: 1.0000e-05\n",
      "Epoch 948/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2589 - mse: 0.2571 - rmse: 0.5071 - mae: 0.2589 - mape: 8.2775\n",
      "Epoch 948: val_loss improved from 0.25827 to 0.25824, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2589 - mape: 8.2866 - val_loss: 0.2582 - val_mse: 0.2568 - val_rmse: 0.5067 - val_mae: 0.2582 - val_mape: 8.2526 - lr: 1.0000e-05\n",
      "Epoch 949/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2586 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2586 - mape: 8.2643\n",
      "Epoch 949: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2587 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2587 - mape: 8.2746 - val_loss: 0.2583 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2583 - val_mape: 8.2705 - lr: 1.0000e-05\n",
      "Epoch 950/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2584 - mse: 0.2571 - rmse: 0.5071 - mae: 0.2584 - mape: 8.2702\n",
      "Epoch 950: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2587 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2587 - mape: 8.2726 - val_loss: 0.2587 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2587 - val_mape: 8.3305 - lr: 1.0000e-05\n",
      "Epoch 951/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2587 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2587 - mape: 8.2845\n",
      "Epoch 951: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2590 - mape: 8.2850 - val_loss: 0.2585 - val_mse: 0.2585 - val_rmse: 0.5084 - val_mae: 0.2585 - val_mape: 8.2847 - lr: 1.0000e-05\n",
      "Epoch 952/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2592 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2592 - mape: 8.3054\n",
      "Epoch 952: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2590 - mape: 8.2983 - val_loss: 0.2583 - val_mse: 0.2573 - val_rmse: 0.5073 - val_mae: 0.2583 - val_mape: 8.2657 - lr: 1.0000e-05\n",
      "Epoch 953/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2597 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2597 - mape: 8.3106\n",
      "Epoch 953: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2591 - mape: 8.2962 - val_loss: 0.2583 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2583 - val_mape: 8.2875 - lr: 1.0000e-05\n",
      "Epoch 954/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2587 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2587 - mape: 8.2916\n",
      "Epoch 954: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2588 - mape: 8.2933 - val_loss: 0.2583 - val_mse: 0.2570 - val_rmse: 0.5070 - val_mae: 0.2583 - val_mape: 8.2606 - lr: 1.0000e-05\n",
      "Epoch 955/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2602 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2602 - mape: 8.3019\n",
      "Epoch 955: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2594 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2594 - mape: 8.2884 - val_loss: 0.2586 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2586 - val_mape: 8.2969 - lr: 1.0000e-05\n",
      "Epoch 956/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2586 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2586 - mape: 8.2741\n",
      "Epoch 956: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2591 - mape: 8.2833 - val_loss: 0.2586 - val_mse: 0.2579 - val_rmse: 0.5079 - val_mae: 0.2586 - val_mape: 8.3067 - lr: 1.0000e-05\n",
      "Epoch 957/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2588 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2588 - mape: 8.2662\n",
      "Epoch 957: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2588 - mape: 8.2757 - val_loss: 0.2599 - val_mse: 0.2621 - val_rmse: 0.5119 - val_mae: 0.2599 - val_mape: 8.3919 - lr: 1.0000e-05\n",
      "Epoch 958/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2588 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2588 - mape: 8.2599\n",
      "Epoch 958: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2592 - mape: 8.2937 - val_loss: 0.2583 - val_mse: 0.2568 - val_rmse: 0.5068 - val_mae: 0.2583 - val_mape: 8.2681 - lr: 1.0000e-05\n",
      "Epoch 959/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2587 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2587 - mape: 8.2730\n",
      "Epoch 959: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2587 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2587 - mape: 8.2730 - val_loss: 0.2585 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2585 - val_mape: 8.3211 - lr: 1.0000e-05\n",
      "Epoch 960/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2594 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2594 - mape: 8.2937\n",
      "Epoch 960: val_loss improved from 0.25824 to 0.25823, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2588 - mape: 8.2729 - val_loss: 0.2582 - val_mse: 0.2584 - val_rmse: 0.5084 - val_mae: 0.2582 - val_mape: 8.2886 - lr: 1.0000e-05\n",
      "Epoch 961/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2587 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2587 - mape: 8.2693\n",
      "Epoch 961: val_loss did not improve from 0.25823\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2588 - mape: 8.2870 - val_loss: 0.2588 - val_mse: 0.2561 - val_rmse: 0.5060 - val_mae: 0.2588 - val_mape: 8.2183 - lr: 1.0000e-05\n",
      "Epoch 962/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2603 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2603 - mape: 8.3146\n",
      "Epoch 962: val_loss did not improve from 0.25823\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2590 - mape: 8.2886 - val_loss: 0.2591 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2591 - val_mape: 8.3399 - lr: 1.0000e-05\n",
      "Epoch 963/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2593 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2593 - mape: 8.2818\n",
      "Epoch 963: val_loss improved from 0.25823 to 0.25814, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2589 - mape: 8.2780 - val_loss: 0.2581 - val_mse: 0.2578 - val_rmse: 0.5077 - val_mae: 0.2581 - val_mape: 8.2737 - lr: 1.0000e-05\n",
      "Epoch 964/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2595 - mse: 0.2595 - rmse: 0.5095 - mae: 0.2595 - mape: 8.3280\n",
      "Epoch 964: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2591 - mape: 8.3024 - val_loss: 0.2587 - val_mse: 0.2576 - val_rmse: 0.5076 - val_mae: 0.2587 - val_mape: 8.3046 - lr: 1.0000e-05\n",
      "Epoch 965/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2594 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2594 - mape: 8.2991\n",
      "Epoch 965: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2593 - mape: 8.2933 - val_loss: 0.2582 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2582 - val_mape: 8.2664 - lr: 1.0000e-05\n",
      "Epoch 966/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2591 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2591 - mape: 8.3205\n",
      "Epoch 966: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2590 - mape: 8.2956 - val_loss: 0.2582 - val_mse: 0.2576 - val_rmse: 0.5076 - val_mae: 0.2582 - val_mape: 8.2810 - lr: 1.0000e-05\n",
      "Epoch 967/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2577 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2577 - mape: 8.2635\n",
      "Epoch 967: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2587 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2587 - mape: 8.2782 - val_loss: 0.2588 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2588 - val_mape: 8.2653 - lr: 1.0000e-05\n",
      "Epoch 968/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2590 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2590 - mape: 8.2976\n",
      "Epoch 968: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2589 - mape: 8.2899 - val_loss: 0.2590 - val_mse: 0.2565 - val_rmse: 0.5064 - val_mae: 0.2590 - val_mape: 8.2234 - lr: 1.0000e-05\n",
      "Epoch 969/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2586 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2586 - mape: 8.2708\n",
      "Epoch 969: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2586 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2586 - mape: 8.2750 - val_loss: 0.2583 - val_mse: 0.2560 - val_rmse: 0.5060 - val_mae: 0.2583 - val_mape: 8.2381 - lr: 1.0000e-05\n",
      "Epoch 970/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2582 - mse: 0.2552 - rmse: 0.5051 - mae: 0.2582 - mape: 8.2706\n",
      "Epoch 970: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2590 - mape: 8.2917 - val_loss: 0.2583 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2583 - val_mape: 8.2740 - lr: 1.0000e-05\n",
      "Epoch 971/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2594 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2594 - mape: 8.2990\n",
      "Epoch 971: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2591 - mape: 8.2909 - val_loss: 0.2587 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2587 - val_mape: 8.3363 - lr: 1.0000e-05\n",
      "Epoch 972/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2589 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2589 - mape: 8.2862\n",
      "Epoch 972: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2589 - mape: 8.2873 - val_loss: 0.2589 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2589 - val_mape: 8.3341 - lr: 1.0000e-05\n",
      "Epoch 973/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2591 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2591 - mape: 8.2673\n",
      "Epoch 973: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2591 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2591 - mape: 8.2752 - val_loss: 0.2584 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2584 - val_mape: 8.2825 - lr: 1.0000e-05\n",
      "Epoch 974/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2599 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2599 - mape: 8.3022\n",
      "Epoch 974: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2587 - mse: 0.2566 - rmse: 0.5066 - mae: 0.2587 - mape: 8.2666 - val_loss: 0.2583 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2583 - val_mape: 8.2996 - lr: 1.0000e-05\n",
      "Epoch 975/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2587 - mse: 0.2571 - rmse: 0.5071 - mae: 0.2587 - mape: 8.2845\n",
      "Epoch 975: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2584 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2584 - mape: 8.2811 - val_loss: 0.2583 - val_mse: 0.2563 - val_rmse: 0.5063 - val_mae: 0.2583 - val_mape: 8.2425 - lr: 1.0000e-05\n",
      "Epoch 976/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2591 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2591 - mape: 8.2901\n",
      "Epoch 976: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2587 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2587 - mape: 8.2783 - val_loss: 0.2595 - val_mse: 0.2620 - val_rmse: 0.5118 - val_mae: 0.2595 - val_mape: 8.3995 - lr: 1.0000e-05\n",
      "Epoch 977/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2587 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2587 - mape: 8.2891\n",
      "Epoch 977: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2587 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2587 - mape: 8.2863 - val_loss: 0.2582 - val_mse: 0.2555 - val_rmse: 0.5055 - val_mae: 0.2582 - val_mape: 8.2386 - lr: 1.0000e-05\n",
      "Epoch 978/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2588 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2588 - mape: 8.2801\n",
      "Epoch 978: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2590 - mape: 8.2882 - val_loss: 0.2582 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2582 - val_mape: 8.2888 - lr: 1.0000e-05\n",
      "Epoch 979/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2574 - mse: 0.2546 - rmse: 0.5046 - mae: 0.2574 - mape: 8.2376\n",
      "Epoch 979: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2587 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2587 - mape: 8.2827 - val_loss: 0.2583 - val_mse: 0.2575 - val_rmse: 0.5075 - val_mae: 0.2583 - val_mape: 8.2552 - lr: 1.0000e-05\n",
      "Epoch 980/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2589 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2589 - mape: 8.2904\n",
      "Epoch 980: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2588 - mape: 8.2920 - val_loss: 0.2583 - val_mse: 0.2566 - val_rmse: 0.5066 - val_mae: 0.2583 - val_mape: 8.2590 - lr: 1.0000e-05\n",
      "Epoch 981/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2587 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2587 - mape: 8.2692\n",
      "Epoch 981: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2588 - mape: 8.2710 - val_loss: 0.2603 - val_mse: 0.2646 - val_rmse: 0.5144 - val_mae: 0.2603 - val_mape: 8.4356 - lr: 1.0000e-05\n",
      "Epoch 982/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2593 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2593 - mape: 8.3097\n",
      "Epoch 982: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2593 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2593 - mape: 8.3097 - val_loss: 0.2588 - val_mse: 0.2562 - val_rmse: 0.5062 - val_mae: 0.2588 - val_mape: 8.2198 - lr: 1.0000e-05\n",
      "Epoch 983/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2591 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2591 - mape: 8.2967\n",
      "Epoch 983: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2589 - mape: 8.2840 - val_loss: 0.2587 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2587 - val_mape: 8.3361 - lr: 1.0000e-05\n",
      "Epoch 984/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2590 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2590 - mape: 8.2948\n",
      "Epoch 984: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2589 - mape: 8.2932 - val_loss: 0.2604 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2604 - val_mape: 8.2345 - lr: 1.0000e-05\n",
      "Epoch 985/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2590 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2590 - mape: 8.2811\n",
      "Epoch 985: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2590 - mape: 8.2811 - val_loss: 0.2589 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2589 - val_mape: 8.3455 - lr: 1.0000e-05\n",
      "Epoch 986/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2587 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2587 - mape: 8.2816\n",
      "Epoch 986: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2588 - mape: 8.2869 - val_loss: 0.2582 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2582 - val_mape: 8.2782 - lr: 1.0000e-05\n",
      "Epoch 987/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2603 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2603 - mape: 8.3100\n",
      "Epoch 987: val_loss did not improve from 0.25814\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2588 - mape: 8.2645 - val_loss: 0.2582 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2582 - val_mape: 8.3044 - lr: 1.0000e-05\n",
      "Epoch 988/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2580 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2580 - mape: 8.2840\n",
      "Epoch 988: val_loss improved from 0.25814 to 0.25805, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2585 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2585 - mape: 8.2957 - val_loss: 0.2580 - val_mse: 0.2566 - val_rmse: 0.5066 - val_mae: 0.2580 - val_mape: 8.2444 - lr: 1.0000e-05\n",
      "Epoch 989/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2588 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2588 - mape: 8.2721\n",
      "Epoch 989: val_loss did not improve from 0.25805\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2588 - mape: 8.2764 - val_loss: 0.2585 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2585 - val_mape: 8.3058 - lr: 1.0000e-05\n",
      "Epoch 990/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2586 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2586 - mape: 8.2569\n",
      "Epoch 990: val_loss did not improve from 0.25805\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2589 - mape: 8.2790 - val_loss: 0.2581 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2581 - val_mape: 8.2863 - lr: 1.0000e-05\n",
      "Epoch 991/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2594 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2594 - mape: 8.3129\n",
      "Epoch 991: val_loss did not improve from 0.25805\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2590 - mape: 8.2988 - val_loss: 0.2581 - val_mse: 0.2568 - val_rmse: 0.5067 - val_mae: 0.2581 - val_mape: 8.2471 - lr: 1.0000e-05\n",
      "Epoch 992/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2591 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2591 - mape: 8.2881\n",
      "Epoch 992: val_loss improved from 0.25805 to 0.25804, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2590 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2590 - mape: 8.2849 - val_loss: 0.2580 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2580 - val_mape: 8.2686 - lr: 1.0000e-05\n",
      "Epoch 993/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2583 - mse: 0.2558 - rmse: 0.5058 - mae: 0.2583 - mape: 8.2703\n",
      "Epoch 993: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2588 - mape: 8.2711 - val_loss: 0.2582 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2582 - val_mape: 8.2692 - lr: 1.0000e-05\n",
      "Epoch 994/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2583 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2583 - mape: 8.2649\n",
      "Epoch 994: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2587 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2587 - mape: 8.2824 - val_loss: 0.2594 - val_mse: 0.2562 - val_rmse: 0.5062 - val_mae: 0.2594 - val_mape: 8.2044 - lr: 1.0000e-05\n",
      "Epoch 995/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2602 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2602 - mape: 8.2902\n",
      "Epoch 995: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2592 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2592 - mape: 8.2715 - val_loss: 0.2582 - val_mse: 0.2581 - val_rmse: 0.5081 - val_mae: 0.2582 - val_mape: 8.2898 - lr: 1.0000e-05\n",
      "Epoch 996/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2596 - mape: 8.3029\n",
      "Epoch 996: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2589 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2589 - mape: 8.2884 - val_loss: 0.2599 - val_mse: 0.2635 - val_rmse: 0.5133 - val_mae: 0.2599 - val_mape: 8.4149 - lr: 1.0000e-05\n",
      "Epoch 997/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2585 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2585 - mape: 8.2756\n",
      "Epoch 997: val_loss improved from 0.25804 to 0.25800, saving model to model_weights/20221123-142828_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2587 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2587 - mape: 8.2848 - val_loss: 0.2580 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2580 - val_mape: 8.2630 - lr: 1.0000e-05\n",
      "Epoch 998/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2576 - mse: 0.2565 - rmse: 0.5065 - mae: 0.2576 - mape: 8.2619\n",
      "Epoch 998: val_loss did not improve from 0.25800\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2587 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2587 - mape: 8.2756 - val_loss: 0.2592 - val_mse: 0.2621 - val_rmse: 0.5119 - val_mae: 0.2592 - val_mape: 8.3818 - lr: 1.0000e-05\n",
      "Epoch 999/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2599 - mse: 0.2615 - rmse: 0.5113 - mae: 0.2599 - mape: 8.3286\n",
      "Epoch 999: val_loss did not improve from 0.25800\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.2591 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2591 - mape: 8.2978 - val_loss: 0.2581 - val_mse: 0.2565 - val_rmse: 0.5065 - val_mae: 0.2581 - val_mape: 8.2436 - lr: 1.0000e-05\n",
      "Epoch 1000/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2588 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2588 - mape: 8.2789\n",
      "Epoch 1000: val_loss did not improve from 0.25800\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2588 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2588 - mape: 8.2789 - val_loss: 0.2581 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2581 - val_mape: 8.2716 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f93646dbc70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 1000, batch_size = 64, validation_data = (X_val, y_val), callbacks=[checkpoint_callback, es_callback, tf.keras.callbacks.ReduceLROnPlateau(patience=40)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20221123-142828\n"
     ]
    }
   ],
   "source": [
    "print(date_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models.load_model(f'model_weights/{date_actual}_mlp_best_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 1s 1ms/step - loss: 0.2844 - mse: 0.2358 - rmse: 0.4856 - mae: 0.2844 - mape: 9.8956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28441405296325684,\n",
       " 0.23579490184783936,\n",
       " 0.4855871796607971,\n",
       " 0.28441405296325684,\n",
       " 9.895563125610352]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 711us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.3965154354097874\n",
      "mse:  0.23579493874735413\n",
      "rmse:  0.48558721023864926\n",
      "mae:  0.28441401380495906\n",
      "mape:  0.0989556581775407\n",
      "Error estandar:  0.4826090738703818\n"
     ]
    }
   ],
   "source": [
    "print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "print(\"mse: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"rmse: \", mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"mae: \", mean_absolute_error(y_test, y_pred))\n",
    "print(\"mape: \", mean_absolute_percentage_error(y_test, y_pred))\n",
    "print(\"Error estandar: \", stde(y_test.squeeze(),\n",
    "      y_pred.squeeze(), ddof=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x640 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residuals = y_test - y_pred\n",
    "residuals_std = residuals/residuals.std()\n",
    "\n",
    "y_real_stage = np.array([i[0] for i in y_test])\n",
    "residual_stage = np.array([i[0] for i in residuals])\n",
    "\n",
    "y_real_discharge = np.array([i[-1] for i in y_test])\n",
    "residual_discharge = np.array([i[-1] for i in residuals])\n",
    "\n",
    "\n",
    "figure, ax = plt.subplots(ncols=2, figsize=(20, 8), dpi=80)\n",
    "\n",
    "ax[0].scatter(y_real_stage, residual_stage / residual_stage.std(), label=\"stage residuals\")\n",
    "ax[1].scatter(y_real_discharge, residual_discharge / residual_discharge.std(), label=\"discharge residuals\")\n",
    "ax[0].axhline(y=0.0, color='r', linestyle='-')\n",
    "ax[1].axhline(y=0.0, color='r', linestyle='-')\n",
    "\n",
    "ax[0].set_title(\"Stage residuals\")\n",
    "ax[1].set_title(\"Discharge residuals\")\n",
    "\n",
    "ax[1].set_xlabel(\"Fitted values\")\n",
    "ax[0].set_xlabel(\"Fitted values\")\n",
    "ax[1].set_ylabel(\"Standarized residuals\")\n",
    "ax[0].set_ylabel(\"Standarized residuals\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import normal_ad\n",
    "\n",
    "#figure = sm.qqplot(residual_stage / residual_stage.std(), line ='45', label='stage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = sm.qqplot(residual_discharge / residual_discharge.std(), line='45', label='discharge')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(residual_stage / residual_stage.std(), density=True, bins = 60)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Residual stage');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(residual_discharge / residual_discharge.std(), density=True, bins = 60)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Residual discharge');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.0\n",
      "Hay evidencia de que los residuos no provienen de una distribución normal.\n"
     ]
    }
   ],
   "source": [
    "stat, pval = normal_ad(residual_discharge / residual_discharge.std())\n",
    "print(\"p-value:\", pval)\n",
    "\n",
    "if pval < 0.05:\n",
    "    print(\"Hay evidencia de que los residuos no provienen de una distribución normal.\")\n",
    "else:\n",
    "    print(\"No hay evidencia para rechazar la hipótesis de que los residuos vienen de una distribución normal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x640 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure, ax = plt.subplots(ncols=2, figsize=(20, 8), dpi=80)\n",
    "\n",
    "ax[0].plot(np.arange(len(y_test)), y_test, label=\"Stage real\")\n",
    "ax[0].plot(np.arange(len(y_test)), y_pred, label=\"Stage pred\")\n",
    "\n",
    "ax[0].set_title(\"Stage residuals\")\n",
    "ax[1].set_title(\"Discharge residuals\")\n",
    "\n",
    "ax[1].set_ylabel(\"Values\")\n",
    "ax[0].set_ylabel(\"Values\")\n",
    "ax[1].set_xlabel(\"Time\")\n",
    "ax[0].set_xlabel(\"Time\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79f576286c1276b480d6696ed40f6607e18214e4a2875a618cb5be817ff26007"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
