{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 21:12:37.089694: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 21:12:37.542169: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-23 21:12:38.472383: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nkspartan/miniconda3/envs/tf-gpu/lib/\n",
      "2022-11-23 21:12:38.472466: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nkspartan/miniconda3/envs/tf-gpu/lib/\n",
      "2022-11-23 21:12:38.472472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/tmp/ipykernel_88124/891238804.py:15: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tools.eval_measures import stde\n",
    "\n",
    "import kerastuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 21:12:39.564782: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 21:12:39.578067: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-11-23 21:12:39.578094: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pop-os\n",
      "2022-11-23 21:12:39.578102: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pop-os\n",
      "2022-11-23 21:12:39.578185: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 515.65.1\n",
      "2022-11-23 21:12:39.578200: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.65.1\n",
      "2022-11-23 21:12:39.578203: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 515.65.1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the etl info results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>remove_time_features</th>\n",
       "      <th>generic_features</th>\n",
       "      <th>remove_atypical_values</th>\n",
       "      <th>feature_combination</th>\n",
       "      <th>remove_feature_selection</th>\n",
       "      <th>remove_invalid_correlated_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   remove_time_features  generic_features  remove_atypical_values  \\\n",
       "0                 False             False                   False   \n",
       "\n",
       "   feature_combination  remove_feature_selection  \\\n",
       "0                False                     False   \n",
       "\n",
       "   remove_invalid_correlated_features  \n",
       "0                               False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info = pd.read_csv('../dataset_clean/options_csv_v1_etl.csv')\n",
    "df_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SensorTime</th>\n",
       "      <th>CaptureTime</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Agency</th>\n",
       "      <th>SiteNumber</th>\n",
       "      <th>TimeZone</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Discharge</th>\n",
       "      <th>...</th>\n",
       "      <th>WwRawLineMin</th>\n",
       "      <th>WwRawLineMax</th>\n",
       "      <th>WwRawLineMean</th>\n",
       "      <th>WwRawLineSigma</th>\n",
       "      <th>WwCurveLineMin</th>\n",
       "      <th>WwCurveLineMax</th>\n",
       "      <th>WwCurveLineMean</th>\n",
       "      <th>WwCurveLineSigma</th>\n",
       "      <th>RiverArea</th>\n",
       "      <th>RiverWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2012-06-09T13:09:07</td>\n",
       "      <td>statelineweir_20120609_farrell_001.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49975.0</td>\n",
       "      <td>207.508733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2012-06-09T13:10:29</td>\n",
       "      <td>statelineweir_20120609_farrell_002.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50184.0</td>\n",
       "      <td>208.663145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-06-09 13:45:00</td>\n",
       "      <td>2012-06-09T13:44:01</td>\n",
       "      <td>statelineweir_20120609_farrell_003.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.96</td>\n",
       "      <td>873.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50543.0</td>\n",
       "      <td>209.445067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2012-06-09 14:45:00</td>\n",
       "      <td>2012-06-09T14:44:30</td>\n",
       "      <td>statelineweir_20120609_farrell_004.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50856.0</td>\n",
       "      <td>211.265690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-06-09 15:45:00</td>\n",
       "      <td>2012-06-09T15:44:59</td>\n",
       "      <td>statelineweir_20120609_farrell_005.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51004.0</td>\n",
       "      <td>211.250274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42054</th>\n",
       "      <td>42054</td>\n",
       "      <td>42054</td>\n",
       "      <td>2019-10-11 09:00:00</td>\n",
       "      <td>2019-10-11T08:59:53</td>\n",
       "      <td>statelineweir_20191011_farrell_409.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9284.0</td>\n",
       "      <td>77521.0</td>\n",
       "      <td>38385.370066</td>\n",
       "      <td>15952.029728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70085.0</td>\n",
       "      <td>37550.894823</td>\n",
       "      <td>16444.401209</td>\n",
       "      <td>45842.0</td>\n",
       "      <td>194.934605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42055</th>\n",
       "      <td>42055</td>\n",
       "      <td>42055</td>\n",
       "      <td>2019-10-11 10:00:00</td>\n",
       "      <td>2019-10-11T09:59:52</td>\n",
       "      <td>statelineweir_20191011_farrell_410.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10092.0</td>\n",
       "      <td>74614.0</td>\n",
       "      <td>40162.989292</td>\n",
       "      <td>15467.708856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70061.0</td>\n",
       "      <td>39397.339095</td>\n",
       "      <td>16009.008049</td>\n",
       "      <td>42300.0</td>\n",
       "      <td>194.762264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42056</th>\n",
       "      <td>42056</td>\n",
       "      <td>42056</td>\n",
       "      <td>2019-10-11 11:00:00</td>\n",
       "      <td>2019-10-11T10:59:52</td>\n",
       "      <td>statelineweir_20191011_farrell_411.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7067.0</td>\n",
       "      <td>83260.0</td>\n",
       "      <td>42095.946590</td>\n",
       "      <td>16770.357949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76335.0</td>\n",
       "      <td>41350.006568</td>\n",
       "      <td>17489.374617</td>\n",
       "      <td>41080.0</td>\n",
       "      <td>196.480105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42057</th>\n",
       "      <td>42057</td>\n",
       "      <td>42057</td>\n",
       "      <td>2019-10-11 12:00:00</td>\n",
       "      <td>2019-10-11T11:59:53</td>\n",
       "      <td>statelineweir_20191011_farrell_412.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6283.0</td>\n",
       "      <td>83045.0</td>\n",
       "      <td>45345.490954</td>\n",
       "      <td>17498.432849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78882.0</td>\n",
       "      <td>44553.920296</td>\n",
       "      <td>18268.294896</td>\n",
       "      <td>40976.0</td>\n",
       "      <td>193.595245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42058</th>\n",
       "      <td>42058</td>\n",
       "      <td>42058</td>\n",
       "      <td>2019-10-11 12:45:00</td>\n",
       "      <td>2019-10-11T12:59:52</td>\n",
       "      <td>statelineweir_20191011_farrell_413.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7375.0</td>\n",
       "      <td>89813.0</td>\n",
       "      <td>47877.870782</td>\n",
       "      <td>19963.166359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82630.0</td>\n",
       "      <td>47280.270559</td>\n",
       "      <td>20559.358767</td>\n",
       "      <td>41435.0</td>\n",
       "      <td>196.801994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42059 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0           SensorTime          CaptureTime  \\\n",
       "0                 0           0  2012-06-09 13:15:00  2012-06-09T13:09:07   \n",
       "1                 1           1  2012-06-09 13:15:00  2012-06-09T13:10:29   \n",
       "2                 2           2  2012-06-09 13:45:00  2012-06-09T13:44:01   \n",
       "3                 3           3  2012-06-09 14:45:00  2012-06-09T14:44:30   \n",
       "4                 4           4  2012-06-09 15:45:00  2012-06-09T15:44:59   \n",
       "...             ...         ...                  ...                  ...   \n",
       "42054         42054       42054  2019-10-11 09:00:00  2019-10-11T08:59:53   \n",
       "42055         42055       42055  2019-10-11 10:00:00  2019-10-11T09:59:52   \n",
       "42056         42056       42056  2019-10-11 11:00:00  2019-10-11T10:59:52   \n",
       "42057         42057       42057  2019-10-11 12:00:00  2019-10-11T11:59:53   \n",
       "42058         42058       42058  2019-10-11 12:45:00  2019-10-11T12:59:52   \n",
       "\n",
       "                                     Filename Agency  SiteNumber TimeZone  \\\n",
       "0      statelineweir_20120609_farrell_001.jpg   USGS     6674500      MDT   \n",
       "1      statelineweir_20120609_farrell_002.jpg   USGS     6674500      MDT   \n",
       "2      statelineweir_20120609_farrell_003.jpg   USGS     6674500      MDT   \n",
       "3      statelineweir_20120609_farrell_004.jpg   USGS     6674500      MDT   \n",
       "4      statelineweir_20120609_farrell_005.jpg   USGS     6674500      MDT   \n",
       "...                                       ...    ...         ...      ...   \n",
       "42054  statelineweir_20191011_farrell_409.jpg   USGS     6674500      MDT   \n",
       "42055  statelineweir_20191011_farrell_410.jpg   USGS     6674500      MDT   \n",
       "42056  statelineweir_20191011_farrell_411.jpg   USGS     6674500      MDT   \n",
       "42057  statelineweir_20191011_farrell_412.jpg   USGS     6674500      MDT   \n",
       "42058  statelineweir_20191011_farrell_413.jpg   USGS     6674500      MDT   \n",
       "\n",
       "       Stage  Discharge  ... WwRawLineMin  WwRawLineMax  WwRawLineMean  \\\n",
       "0       2.99      916.0  ...          0.0           0.0       0.000000   \n",
       "1       2.99      916.0  ...          0.0           0.0       0.000000   \n",
       "2       2.96      873.0  ...          0.0           0.0       0.000000   \n",
       "3       2.94      846.0  ...          0.0           0.0       0.000000   \n",
       "4       2.94      846.0  ...          0.0           0.0       0.000000   \n",
       "...      ...        ...  ...          ...           ...            ...   \n",
       "42054   2.54      434.0  ...       9284.0       77521.0   38385.370066   \n",
       "42055   2.54      434.0  ...      10092.0       74614.0   40162.989292   \n",
       "42056   2.54      434.0  ...       7067.0       83260.0   42095.946590   \n",
       "42057   2.54      434.0  ...       6283.0       83045.0   45345.490954   \n",
       "42058   2.54      434.0  ...       7375.0       89813.0   47877.870782   \n",
       "\n",
       "       WwRawLineSigma  WwCurveLineMin  WwCurveLineMax  WwCurveLineMean  \\\n",
       "0            0.000000             0.0             0.0         0.000000   \n",
       "1            0.000000             0.0             0.0         0.000000   \n",
       "2            0.000000             0.0             0.0         0.000000   \n",
       "3            0.000000             0.0             0.0         0.000000   \n",
       "4            0.000000             0.0             0.0         0.000000   \n",
       "...               ...             ...             ...              ...   \n",
       "42054    15952.029728             0.0         70085.0     37550.894823   \n",
       "42055    15467.708856             0.0         70061.0     39397.339095   \n",
       "42056    16770.357949             0.0         76335.0     41350.006568   \n",
       "42057    17498.432849             0.0         78882.0     44553.920296   \n",
       "42058    19963.166359             0.0         82630.0     47280.270559   \n",
       "\n",
       "       WwCurveLineSigma  RiverArea  RiverWidth  \n",
       "0              0.000000    49975.0  207.508733  \n",
       "1              0.000000    50184.0  208.663145  \n",
       "2              0.000000    50543.0  209.445067  \n",
       "3              0.000000    50856.0  211.265690  \n",
       "4              0.000000    51004.0  211.250274  \n",
       "...                 ...        ...         ...  \n",
       "42054      16444.401209    45842.0  194.934605  \n",
       "42055      16009.008049    42300.0  194.762264  \n",
       "42056      17489.374617    41080.0  196.480105  \n",
       "42057      18268.294896    40976.0  193.595245  \n",
       "42058      20559.358767    41435.0  196.801994  \n",
       "\n",
       "[42059 rows x 63 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset/V2_PlatteRiverWeir_features_merged_all.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SensorTime'] = pd.to_datetime(df['SensorTime'])\n",
    "df['Year'] = df['SensorTime'].dt.year\n",
    "df['Month'] = df['SensorTime'].dt.month\n",
    "df['date_offset'] = (df.SensorTime.dt.month * 100 + df.SensorTime.dt.day - 320)%1300\n",
    "\n",
    "df['Season'] = pd.cut(df['date_offset'], [0, 300, 602, 900, 1300], \n",
    "                      labels=['spring', 'summer', 'autumn', 'winter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CaptureTime</th>\n",
       "      <th>SensorTime</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Discharge</th>\n",
       "      <th>RiverArea</th>\n",
       "      <th>RiverWidth</th>\n",
       "      <th>Month</th>\n",
       "      <th>Season</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-06-09T13:09:07</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>49975.0</td>\n",
       "      <td>207.508733</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-06-09T13:10:29</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>50184.0</td>\n",
       "      <td>208.663145</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-06-09T13:44:01</td>\n",
       "      <td>2012-06-09 13:45:00</td>\n",
       "      <td>2.96</td>\n",
       "      <td>873.0</td>\n",
       "      <td>50543.0</td>\n",
       "      <td>209.445067</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-06-09T14:44:30</td>\n",
       "      <td>2012-06-09 14:45:00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>50856.0</td>\n",
       "      <td>211.265690</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-06-09T15:44:59</td>\n",
       "      <td>2012-06-09 15:45:00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>51004.0</td>\n",
       "      <td>211.250274</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42054</th>\n",
       "      <td>2019-10-11T08:59:53</td>\n",
       "      <td>2019-10-11 09:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>45842.0</td>\n",
       "      <td>194.934605</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42055</th>\n",
       "      <td>2019-10-11T09:59:52</td>\n",
       "      <td>2019-10-11 10:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>42300.0</td>\n",
       "      <td>194.762264</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42056</th>\n",
       "      <td>2019-10-11T10:59:52</td>\n",
       "      <td>2019-10-11 11:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>41080.0</td>\n",
       "      <td>196.480105</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42057</th>\n",
       "      <td>2019-10-11T11:59:53</td>\n",
       "      <td>2019-10-11 12:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>40976.0</td>\n",
       "      <td>193.595245</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42058</th>\n",
       "      <td>2019-10-11T12:59:52</td>\n",
       "      <td>2019-10-11 12:45:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>41435.0</td>\n",
       "      <td>196.801994</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42059 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               CaptureTime          SensorTime  Stage  Discharge  RiverArea  \\\n",
       "0      2012-06-09T13:09:07 2012-06-09 13:15:00   2.99      916.0    49975.0   \n",
       "1      2012-06-09T13:10:29 2012-06-09 13:15:00   2.99      916.0    50184.0   \n",
       "2      2012-06-09T13:44:01 2012-06-09 13:45:00   2.96      873.0    50543.0   \n",
       "3      2012-06-09T14:44:30 2012-06-09 14:45:00   2.94      846.0    50856.0   \n",
       "4      2012-06-09T15:44:59 2012-06-09 15:45:00   2.94      846.0    51004.0   \n",
       "...                    ...                 ...    ...        ...        ...   \n",
       "42054  2019-10-11T08:59:53 2019-10-11 09:00:00   2.54      434.0    45842.0   \n",
       "42055  2019-10-11T09:59:52 2019-10-11 10:00:00   2.54      434.0    42300.0   \n",
       "42056  2019-10-11T10:59:52 2019-10-11 11:00:00   2.54      434.0    41080.0   \n",
       "42057  2019-10-11T11:59:53 2019-10-11 12:00:00   2.54      434.0    40976.0   \n",
       "42058  2019-10-11T12:59:52 2019-10-11 12:45:00   2.54      434.0    41435.0   \n",
       "\n",
       "       RiverWidth  Month  Season  Year  \n",
       "0      207.508733      6  spring  2012  \n",
       "1      208.663145      6  spring  2012  \n",
       "2      209.445067      6  spring  2012  \n",
       "3      211.265690      6  spring  2012  \n",
       "4      211.250274      6  spring  2012  \n",
       "...           ...    ...     ...   ...  \n",
       "42054  194.934605     10  autumn  2019  \n",
       "42055  194.762264     10  autumn  2019  \n",
       "42056  196.480105     10  autumn  2019  \n",
       "42057  193.595245     10  autumn  2019  \n",
       "42058  196.801994     10  autumn  2019  \n",
       "\n",
       "[42059 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"CaptureTime\", \"SensorTime\", \"Stage\", \"Discharge\", \"RiverArea\", \"RiverWidth\", \"Month\", \"Season\", \"Year\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaptureTime            object\n",
       "SensorTime     datetime64[ns]\n",
       "Stage                 float64\n",
       "Discharge             float64\n",
       "RiverArea             float64\n",
       "RiverWidth            float64\n",
       "Month                   int64\n",
       "Season               category\n",
       "Year                    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40148, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.Stage > 0]\n",
    "df = df[df.Discharge > 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40142, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.RiverWidth > 0]\n",
    "#df = df[df.Discharge > 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.Season != \"winter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaptureTime      0\n",
       "SensorTime       0\n",
       "Stage            0\n",
       "Discharge        0\n",
       "RiverArea        0\n",
       "RiverWidth       0\n",
       "Month            0\n",
       "Season         126\n",
       "Year             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide dataset to X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "df_train = df[(df.Year >= 2012) & (df.Year <= 2016)]\n",
    "df_train = df_train.iloc[np.random.permutation(len(df_train))]\n",
    "\n",
    "df_val = df[(df.Year >= 2017) & (df.Year <= 2017)]\n",
    "df_val = df_val.iloc[np.random.permutation(len(df_val))]\n",
    "\n",
    "df_test = df[(df.Year >= 2018) & (df.Year <= 2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])\n",
    "df_val = df_val.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])\n",
    "df_test = df_test.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = df_train[[\"Stage\"]].values\n",
    "X_train = df_train[[\"RiverWidth\", \"Month\"]].values\n",
    "\n",
    "y_val = df_train[[\"Stage\"]].values\n",
    "X_val = df_train[[\"RiverWidth\", \"Month\"]].values\n",
    "\n",
    "y_test = df_test[[\"Stage\"]].values\n",
    "X_test = df_test[[\"RiverWidth\", \"Month\"]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20304, 2)\n",
      "(20304, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "print(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(lr):\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.Input(shape=input_shape))\n",
    "  \n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(32, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(128, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"tanh\"))\n",
    "  \"\"\"model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(512, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(512, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(128, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(32, activation=\"tanh\"))\"\"\"\n",
    "\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(output_shape, activation = 'linear'))\n",
    "\n",
    "  \n",
    "  model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr), loss = 'mae', metrics = ['mse', tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mae', 'mape'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_builder(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date_actual = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/fit/\" + date_actual\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"model_weights/{date_actual}_mlp_best_weights.hdf5\",\n",
    "                               monitor='val_loss',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "258/318 [=======================>......] - ETA: 0s - loss: 0.7267 - mse: 1.1444 - rmse: 1.0698 - mae: 0.7267 - mape: 23.3100\n",
      "Epoch 1: val_loss improved from inf to 0.48146, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.6964 - mse: 1.0733 - rmse: 1.0360 - mae: 0.6964 - mape: 22.3842 - val_loss: 0.4815 - val_mse: 0.6165 - val_rmse: 0.7852 - val_mae: 0.4815 - val_mape: 15.7323 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.4467 - mse: 0.6671 - rmse: 0.8168 - mae: 0.4467 - mape: 13.3465\n",
      "Epoch 2: val_loss improved from 0.48146 to 0.42737, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4456 - mse: 0.6651 - rmse: 0.8156 - mae: 0.4456 - mape: 13.3222 - val_loss: 0.4274 - val_mse: 0.6735 - val_rmse: 0.8207 - val_mae: 0.4274 - val_mape: 12.4175 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.4300 - mse: 0.6470 - rmse: 0.8044 - mae: 0.4300 - mape: 12.7379\n",
      "Epoch 3: val_loss improved from 0.42737 to 0.41177, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4294 - mse: 0.6462 - rmse: 0.8038 - mae: 0.4294 - mape: 12.7221 - val_loss: 0.4118 - val_mse: 0.6154 - val_rmse: 0.7845 - val_mae: 0.4118 - val_mape: 12.2814 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "254/318 [======================>.......] - ETA: 0s - loss: 0.4151 - mse: 0.6153 - rmse: 0.7844 - mae: 0.4151 - mape: 12.4378\n",
      "Epoch 4: val_loss improved from 0.41177 to 0.40179, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4105 - mse: 0.6070 - rmse: 0.7791 - mae: 0.4105 - mape: 12.2899 - val_loss: 0.4018 - val_mse: 0.5631 - val_rmse: 0.7504 - val_mae: 0.4018 - val_mape: 12.3885 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.4042 - mse: 0.5857 - rmse: 0.7653 - mae: 0.4042 - mape: 12.2320\n",
      "Epoch 5: val_loss improved from 0.40179 to 0.38963, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4031 - mse: 0.5825 - rmse: 0.7632 - mae: 0.4031 - mape: 12.2206 - val_loss: 0.3896 - val_mse: 0.5757 - val_rmse: 0.7587 - val_mae: 0.3896 - val_mape: 11.7691 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "255/318 [=======================>......] - ETA: 0s - loss: 0.4068 - mse: 0.5746 - rmse: 0.7581 - mae: 0.4068 - mape: 12.4505\n",
      "Epoch 6: val_loss did not improve from 0.38963\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4012 - mse: 0.5648 - rmse: 0.7515 - mae: 0.4012 - mape: 12.3122 - val_loss: 0.4156 - val_mse: 0.5465 - val_rmse: 0.7392 - val_mae: 0.4156 - val_mape: 13.3871 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "255/318 [=======================>......] - ETA: 0s - loss: 0.3871 - mse: 0.5361 - rmse: 0.7322 - mae: 0.3871 - mape: 11.9281\n",
      "Epoch 7: val_loss did not improve from 0.38963\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3825 - mse: 0.5257 - rmse: 0.7250 - mae: 0.3825 - mape: 11.8069 - val_loss: 0.3969 - val_mse: 0.5295 - val_rmse: 0.7276 - val_mae: 0.3969 - val_mape: 12.1349 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.3802 - mse: 0.5116 - rmse: 0.7153 - mae: 0.3802 - mape: 11.8084\n",
      "Epoch 8: val_loss improved from 0.38963 to 0.37316, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3802 - mse: 0.5117 - rmse: 0.7153 - mae: 0.3802 - mape: 11.7978 - val_loss: 0.3732 - val_mse: 0.5061 - val_rmse: 0.7114 - val_mae: 0.3732 - val_mape: 11.3807 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.4025 - mse: 0.6236 - rmse: 0.7897 - mae: 0.4025 - mape: 11.7876\n",
      "Epoch 9: val_loss did not improve from 0.37316\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4015 - mse: 0.6211 - rmse: 0.7881 - mae: 0.4015 - mape: 11.7624 - val_loss: 0.3963 - val_mse: 0.6022 - val_rmse: 0.7760 - val_mae: 0.3963 - val_mape: 11.5568 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "260/318 [=======================>......] - ETA: 0s - loss: 0.4047 - mse: 0.6267 - rmse: 0.7916 - mae: 0.4047 - mape: 11.9731\n",
      "Epoch 10: val_loss improved from 0.37316 to 0.36834, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4005 - mse: 0.6063 - rmse: 0.7786 - mae: 0.4005 - mape: 11.8892 - val_loss: 0.3683 - val_mse: 0.4904 - val_rmse: 0.7003 - val_mae: 0.3683 - val_mape: 11.3283 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "254/318 [======================>.......] - ETA: 0s - loss: 0.3770 - mse: 0.5094 - rmse: 0.7137 - mae: 0.3770 - mape: 11.5990\n",
      "Epoch 11: val_loss did not improve from 0.36834\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3787 - mse: 0.5128 - rmse: 0.7161 - mae: 0.3787 - mape: 11.6474 - val_loss: 0.5139 - val_mse: 1.0294 - val_rmse: 1.0146 - val_mae: 0.5139 - val_mape: 13.4462 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.3883 - mse: 0.5595 - rmse: 0.7480 - mae: 0.3883 - mape: 11.7846\n",
      "Epoch 12: val_loss did not improve from 0.36834\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3851 - mse: 0.5479 - rmse: 0.7402 - mae: 0.3851 - mape: 11.6877 - val_loss: 0.3743 - val_mse: 0.4685 - val_rmse: 0.6844 - val_mae: 0.3743 - val_mape: 11.7551 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3706 - mse: 0.4825 - rmse: 0.6947 - mae: 0.3706 - mape: 11.4296\n",
      "Epoch 13: val_loss did not improve from 0.36834\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3705 - mse: 0.4831 - rmse: 0.6950 - mae: 0.3705 - mape: 11.4261 - val_loss: 0.3988 - val_mse: 0.5621 - val_rmse: 0.7497 - val_mae: 0.3988 - val_mape: 11.8593 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.3646 - mse: 0.4606 - rmse: 0.6787 - mae: 0.3646 - mape: 11.2857\n",
      "Epoch 14: val_loss improved from 0.36834 to 0.35826, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3635 - mse: 0.4590 - rmse: 0.6775 - mae: 0.3635 - mape: 11.2474 - val_loss: 0.3583 - val_mse: 0.4424 - val_rmse: 0.6652 - val_mae: 0.3583 - val_mape: 11.1410 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.3670 - mse: 0.4706 - rmse: 0.6860 - mae: 0.3670 - mape: 11.3555\n",
      "Epoch 15: val_loss did not improve from 0.35826\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3683 - mse: 0.4714 - rmse: 0.6866 - mae: 0.3683 - mape: 11.3989 - val_loss: 0.3865 - val_mse: 0.4856 - val_rmse: 0.6968 - val_mae: 0.3865 - val_mape: 12.3471 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3905 - mse: 0.5785 - rmse: 0.7606 - mae: 0.3905 - mape: 11.6623\n",
      "Epoch 16: val_loss did not improve from 0.35826\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3904 - mse: 0.5788 - rmse: 0.7608 - mae: 0.3904 - mape: 11.6546 - val_loss: 0.4373 - val_mse: 0.7769 - val_rmse: 0.8814 - val_mae: 0.4373 - val_mape: 13.2605 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.4122 - mse: 0.6741 - rmse: 0.8210 - mae: 0.4122 - mape: 11.9237\n",
      "Epoch 17: val_loss did not improve from 0.35826\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4115 - mse: 0.6715 - rmse: 0.8195 - mae: 0.4115 - mape: 11.9179 - val_loss: 0.3833 - val_mse: 0.5688 - val_rmse: 0.7542 - val_mae: 0.3833 - val_mape: 11.4588 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.3837 - mse: 0.5313 - rmse: 0.7289 - mae: 0.3837 - mape: 11.7107\n",
      "Epoch 18: val_loss did not improve from 0.35826\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3856 - mse: 0.5338 - rmse: 0.7306 - mae: 0.3856 - mape: 11.7710 - val_loss: 0.3716 - val_mse: 0.4428 - val_rmse: 0.6654 - val_mae: 0.3716 - val_mape: 11.9636 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.3552 - mse: 0.4390 - rmse: 0.6626 - mae: 0.3552 - mape: 10.8026\n",
      "Epoch 19: val_loss improved from 0.35826 to 0.32577, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3549 - mse: 0.4376 - rmse: 0.6615 - mae: 0.3549 - mape: 10.7693 - val_loss: 0.3258 - val_mse: 0.3390 - val_rmse: 0.5822 - val_mae: 0.3258 - val_mape: 10.0061 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.3563 - mse: 0.4423 - rmse: 0.6651 - mae: 0.3563 - mape: 10.8090\n",
      "Epoch 20: val_loss improved from 0.32577 to 0.31840, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3543 - mse: 0.4368 - rmse: 0.6609 - mae: 0.3543 - mape: 10.7485 - val_loss: 0.3184 - val_mse: 0.3435 - val_rmse: 0.5861 - val_mae: 0.3184 - val_mape: 9.2361 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.4151 - mse: 0.6554 - rmse: 0.8095 - mae: 0.4151 - mape: 12.6644\n",
      "Epoch 21: val_loss did not improve from 0.31840\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4100 - mse: 0.6337 - rmse: 0.7961 - mae: 0.4100 - mape: 12.5188 - val_loss: 0.3755 - val_mse: 0.4965 - val_rmse: 0.7046 - val_mae: 0.3755 - val_mape: 11.9443 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3715 - mse: 0.5063 - rmse: 0.7115 - mae: 0.3715 - mape: 11.0492\n",
      "Epoch 22: val_loss did not improve from 0.31840\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3714 - mse: 0.5060 - rmse: 0.7114 - mae: 0.3714 - mape: 11.0477 - val_loss: 0.3289 - val_mse: 0.3826 - val_rmse: 0.6185 - val_mae: 0.3289 - val_mape: 9.6959 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.3390 - mse: 0.4039 - rmse: 0.6355 - mae: 0.3390 - mape: 10.1590\n",
      "Epoch 23: val_loss did not improve from 0.31840\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3395 - mse: 0.4047 - rmse: 0.6362 - mae: 0.3395 - mape: 10.1478 - val_loss: 0.3220 - val_mse: 0.3600 - val_rmse: 0.6000 - val_mae: 0.3220 - val_mape: 9.5689 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.3358 - mse: 0.3807 - rmse: 0.6170 - mae: 0.3358 - mape: 10.1480\n",
      "Epoch 24: val_loss improved from 0.31840 to 0.30474, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3373 - mse: 0.3843 - rmse: 0.6199 - mae: 0.3373 - mape: 10.1922 - val_loss: 0.3047 - val_mse: 0.3255 - val_rmse: 0.5705 - val_mae: 0.3047 - val_mape: 9.0457 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.3275 - mse: 0.3538 - rmse: 0.5948 - mae: 0.3275 - mape: 9.9820 \n",
      "Epoch 25: val_loss did not improve from 0.30474\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3283 - mse: 0.3558 - rmse: 0.5965 - mae: 0.3283 - mape: 9.9973 - val_loss: 0.3051 - val_mse: 0.2908 - val_rmse: 0.5392 - val_mae: 0.3051 - val_mape: 9.7061 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.3250 - mse: 0.3495 - rmse: 0.5912 - mae: 0.3250 - mape: 10.1061\n",
      "Epoch 26: val_loss did not improve from 0.30474\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3247 - mse: 0.3492 - rmse: 0.5909 - mae: 0.3247 - mape: 10.0927 - val_loss: 0.3257 - val_mse: 0.3225 - val_rmse: 0.5679 - val_mae: 0.3257 - val_mape: 10.3661 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3116 - mse: 0.3171 - rmse: 0.5632 - mae: 0.3116 - mape: 9.6869\n",
      "Epoch 27: val_loss improved from 0.30474 to 0.29579, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3116 - mse: 0.3172 - rmse: 0.5632 - mae: 0.3116 - mape: 9.6881 - val_loss: 0.2958 - val_mse: 0.2882 - val_rmse: 0.5368 - val_mae: 0.2958 - val_mape: 9.7022 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.3277 - mse: 0.3695 - rmse: 0.6079 - mae: 0.3277 - mape: 10.2207\n",
      "Epoch 28: val_loss improved from 0.29579 to 0.28232, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3266 - mse: 0.3654 - rmse: 0.6044 - mae: 0.3266 - mape: 10.1863 - val_loss: 0.2823 - val_mse: 0.2656 - val_rmse: 0.5154 - val_mae: 0.2823 - val_mape: 8.9473 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.3046 - mse: 0.3112 - rmse: 0.5578 - mae: 0.3046 - mape: 9.5704\n",
      "Epoch 29: val_loss did not improve from 0.28232\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3063 - mse: 0.3153 - rmse: 0.5615 - mae: 0.3063 - mape: 9.6300 - val_loss: 0.4517 - val_mse: 0.9616 - val_rmse: 0.9806 - val_mae: 0.4517 - val_mape: 11.7673 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.3157 - mse: 0.3419 - rmse: 0.5847 - mae: 0.3157 - mape: 9.8735\n",
      "Epoch 30: val_loss did not improve from 0.28232\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3158 - mse: 0.3424 - rmse: 0.5851 - mae: 0.3158 - mape: 9.8677 - val_loss: 0.3757 - val_mse: 0.4885 - val_rmse: 0.6990 - val_mae: 0.3757 - val_mape: 12.2207 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.3284 - mse: 0.3731 - rmse: 0.6108 - mae: 0.3284 - mape: 10.2374\n",
      "Epoch 31: val_loss did not improve from 0.28232\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3283 - mse: 0.3725 - rmse: 0.6104 - mae: 0.3283 - mape: 10.2316 - val_loss: 0.2947 - val_mse: 0.3011 - val_rmse: 0.5487 - val_mae: 0.2947 - val_mape: 9.5338 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.3109 - mse: 0.3355 - rmse: 0.5792 - mae: 0.3109 - mape: 9.7974\n",
      "Epoch 32: val_loss did not improve from 0.28232\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3100 - mse: 0.3339 - rmse: 0.5779 - mae: 0.3100 - mape: 9.7700 - val_loss: 0.3394 - val_mse: 0.3974 - val_rmse: 0.6304 - val_mae: 0.3394 - val_mape: 10.3042 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.3360 - mse: 0.4329 - rmse: 0.6579 - mae: 0.3360 - mape: 10.2219\n",
      "Epoch 33: val_loss did not improve from 0.28232\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3382 - mse: 0.4418 - rmse: 0.6646 - mae: 0.3382 - mape: 10.2496 - val_loss: 0.3977 - val_mse: 0.7022 - val_rmse: 0.8380 - val_mae: 0.3977 - val_mape: 10.9080 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "255/318 [=======================>......] - ETA: 0s - loss: 0.4067 - mse: 0.7034 - rmse: 0.8387 - mae: 0.4067 - mape: 11.4975\n",
      "Epoch 34: val_loss did not improve from 0.28232\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4057 - mse: 0.7049 - rmse: 0.8396 - mae: 0.4057 - mape: 11.4689 - val_loss: 0.4468 - val_mse: 0.7327 - val_rmse: 0.8560 - val_mae: 0.4468 - val_mape: 12.6982 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.4107 - mse: 0.7152 - rmse: 0.8457 - mae: 0.4107 - mape: 11.4551\n",
      "Epoch 35: val_loss did not improve from 0.28232\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4107 - mse: 0.7143 - rmse: 0.8452 - mae: 0.4107 - mape: 11.4503 - val_loss: 0.3898 - val_mse: 0.6757 - val_rmse: 0.8220 - val_mae: 0.3898 - val_mape: 10.9606 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3783 - mse: 0.5232 - rmse: 0.7233 - mae: 0.3783 - mape: 11.6255\n",
      "Epoch 36: val_loss did not improve from 0.28232\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3783 - mse: 0.5231 - rmse: 0.7233 - mae: 0.3783 - mape: 11.6266 - val_loss: 0.3520 - val_mse: 0.3512 - val_rmse: 0.5926 - val_mae: 0.3520 - val_mape: 11.8169 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "258/318 [=======================>......] - ETA: 0s - loss: 0.3299 - mse: 0.3512 - rmse: 0.5926 - mae: 0.3299 - mape: 10.3938\n",
      "Epoch 37: val_loss did not improve from 0.28232\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3251 - mse: 0.3462 - rmse: 0.5884 - mae: 0.3251 - mape: 10.2279 - val_loss: 0.3272 - val_mse: 0.3574 - val_rmse: 0.5978 - val_mae: 0.3272 - val_mape: 10.3719 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3345 - mse: 0.3927 - rmse: 0.6267 - mae: 0.3345 - mape: 10.3465\n",
      "Epoch 38: val_loss did not improve from 0.28232\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3346 - mse: 0.3925 - rmse: 0.6265 - mae: 0.3346 - mape: 10.3443 - val_loss: 0.3834 - val_mse: 0.4490 - val_rmse: 0.6701 - val_mae: 0.3834 - val_mape: 11.0122 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.3177 - mse: 0.3429 - rmse: 0.5855 - mae: 0.3177 - mape: 9.9467 \n",
      "Epoch 39: val_loss did not improve from 0.28232\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3179 - mse: 0.3440 - rmse: 0.5865 - mae: 0.3179 - mape: 9.9597 - val_loss: 0.3325 - val_mse: 0.3908 - val_rmse: 0.6251 - val_mae: 0.3325 - val_mape: 11.1727 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.3124 - mse: 0.3382 - rmse: 0.5816 - mae: 0.3124 - mape: 9.8208\n",
      "Epoch 40: val_loss improved from 0.28232 to 0.27923, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3131 - mse: 0.3410 - rmse: 0.5840 - mae: 0.3131 - mape: 9.8298 - val_loss: 0.2792 - val_mse: 0.2865 - val_rmse: 0.5353 - val_mae: 0.2792 - val_mape: 8.9768 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "257/318 [=======================>......] - ETA: 0s - loss: 0.3593 - mse: 0.5167 - rmse: 0.7188 - mae: 0.3593 - mape: 10.7829\n",
      "Epoch 41: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3708 - mse: 0.5580 - rmse: 0.7470 - mae: 0.3708 - mape: 10.9963 - val_loss: 0.4013 - val_mse: 0.6888 - val_rmse: 0.8299 - val_mae: 0.4013 - val_mape: 11.5396 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.4122 - mse: 0.7211 - rmse: 0.8492 - mae: 0.4122 - mape: 11.7809\n",
      "Epoch 42: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4126 - mse: 0.7224 - rmse: 0.8499 - mae: 0.4126 - mape: 11.7933 - val_loss: 0.4253 - val_mse: 0.7905 - val_rmse: 0.8891 - val_mae: 0.4253 - val_mape: 11.6544 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.4091 - mse: 0.7067 - rmse: 0.8407 - mae: 0.4091 - mape: 11.5499\n",
      "Epoch 43: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4087 - mse: 0.7056 - rmse: 0.8400 - mae: 0.4087 - mape: 11.5381 - val_loss: 0.4003 - val_mse: 0.6668 - val_rmse: 0.8166 - val_mae: 0.4003 - val_mape: 11.5745 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.3953 - mse: 0.6524 - rmse: 0.8077 - mae: 0.3953 - mape: 11.2040\n",
      "Epoch 44: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3943 - mse: 0.6477 - rmse: 0.8048 - mae: 0.3943 - mape: 11.1710 - val_loss: 0.3504 - val_mse: 0.4521 - val_rmse: 0.6724 - val_mae: 0.3504 - val_mape: 10.1984 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "258/318 [=======================>......] - ETA: 0s - loss: 0.3413 - mse: 0.4153 - rmse: 0.6444 - mae: 0.3413 - mape: 10.2060\n",
      "Epoch 45: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3416 - mse: 0.4096 - rmse: 0.6400 - mae: 0.3416 - mape: 10.2225 - val_loss: 0.3279 - val_mse: 0.3399 - val_rmse: 0.5830 - val_mae: 0.3279 - val_mape: 9.9218 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "261/318 [=======================>......] - ETA: 0s - loss: 0.3214 - mse: 0.3381 - rmse: 0.5814 - mae: 0.3214 - mape: 9.8437\n",
      "Epoch 46: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3198 - mse: 0.3357 - rmse: 0.5794 - mae: 0.3198 - mape: 9.8062 - val_loss: 0.3343 - val_mse: 0.3234 - val_rmse: 0.5687 - val_mae: 0.3343 - val_mape: 10.3462 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "259/318 [=======================>......] - ETA: 0s - loss: 0.3114 - mse: 0.3082 - rmse: 0.5552 - mae: 0.3114 - mape: 9.5836\n",
      "Epoch 47: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3111 - mse: 0.3115 - rmse: 0.5582 - mae: 0.3111 - mape: 9.6076 - val_loss: 0.3825 - val_mse: 0.4964 - val_rmse: 0.7045 - val_mae: 0.3825 - val_mape: 11.4552 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "258/318 [=======================>......] - ETA: 0s - loss: 0.3687 - mse: 0.5428 - rmse: 0.7367 - mae: 0.3687 - mape: 10.8359\n",
      "Epoch 48: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3737 - mse: 0.5635 - rmse: 0.7507 - mae: 0.3737 - mape: 11.0179 - val_loss: 0.4216 - val_mse: 0.6976 - val_rmse: 0.8352 - val_mae: 0.4216 - val_mape: 12.0969 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "257/318 [=======================>......] - ETA: 0s - loss: 0.3489 - mse: 0.4470 - rmse: 0.6685 - mae: 0.3489 - mape: 10.4539\n",
      "Epoch 49: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3408 - mse: 0.4203 - rmse: 0.6483 - mae: 0.3408 - mape: 10.2478 - val_loss: 0.3198 - val_mse: 0.3401 - val_rmse: 0.5832 - val_mae: 0.3198 - val_mape: 10.2160 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "260/318 [=======================>......] - ETA: 0s - loss: 0.3011 - mse: 0.3061 - rmse: 0.5533 - mae: 0.3011 - mape: 9.3975\n",
      "Epoch 50: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3019 - mse: 0.3086 - rmse: 0.5555 - mae: 0.3019 - mape: 9.3983 - val_loss: 0.2970 - val_mse: 0.2878 - val_rmse: 0.5365 - val_mae: 0.2970 - val_mape: 9.2133 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "259/318 [=======================>......] - ETA: 0s - loss: 0.3124 - mse: 0.3367 - rmse: 0.5802 - mae: 0.3124 - mape: 9.7967\n",
      "Epoch 51: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3077 - mse: 0.3240 - rmse: 0.5692 - mae: 0.3077 - mape: 9.6488 - val_loss: 0.2906 - val_mse: 0.2929 - val_rmse: 0.5412 - val_mae: 0.2906 - val_mape: 9.4142 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "260/318 [=======================>......] - ETA: 0s - loss: 0.3002 - mse: 0.3113 - rmse: 0.5579 - mae: 0.3002 - mape: 9.5875\n",
      "Epoch 52: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3029 - mse: 0.3194 - rmse: 0.5652 - mae: 0.3029 - mape: 9.5843 - val_loss: 0.2957 - val_mse: 0.2875 - val_rmse: 0.5362 - val_mae: 0.2957 - val_mape: 9.4243 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.3164 - mse: 0.3424 - rmse: 0.5851 - mae: 0.3164 - mape: 9.9195\n",
      "Epoch 53: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3154 - mse: 0.3403 - rmse: 0.5834 - mae: 0.3154 - mape: 9.8744 - val_loss: 0.2989 - val_mse: 0.3111 - val_rmse: 0.5577 - val_mae: 0.2989 - val_mape: 10.1487 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.3072 - mse: 0.3380 - rmse: 0.5813 - mae: 0.3072 - mape: 9.6915\n",
      "Epoch 54: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3121 - mse: 0.3477 - rmse: 0.5897 - mae: 0.3121 - mape: 9.8676 - val_loss: 0.3110 - val_mse: 0.3267 - val_rmse: 0.5716 - val_mae: 0.3110 - val_mape: 10.5218 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.3196 - mse: 0.3535 - rmse: 0.5946 - mae: 0.3196 - mape: 10.0388\n",
      "Epoch 55: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3173 - mse: 0.3492 - rmse: 0.5909 - mae: 0.3173 - mape: 9.9776 - val_loss: 0.2861 - val_mse: 0.2799 - val_rmse: 0.5291 - val_mae: 0.2861 - val_mape: 9.3641 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.3243 - mse: 0.3838 - rmse: 0.6195 - mae: 0.3243 - mape: 10.1267\n",
      "Epoch 56: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3206 - mse: 0.3715 - rmse: 0.6095 - mae: 0.3206 - mape: 10.0230 - val_loss: 0.2876 - val_mse: 0.2718 - val_rmse: 0.5214 - val_mae: 0.2876 - val_mape: 9.1586 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "257/318 [=======================>......] - ETA: 0s - loss: 0.2988 - mse: 0.3130 - rmse: 0.5595 - mae: 0.2988 - mape: 9.4668\n",
      "Epoch 57: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3041 - mse: 0.3236 - rmse: 0.5689 - mae: 0.3041 - mape: 9.6171 - val_loss: 0.3313 - val_mse: 0.4028 - val_rmse: 0.6346 - val_mae: 0.3313 - val_mape: 9.3160 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "258/318 [=======================>......] - ETA: 0s - loss: 0.3163 - mse: 0.3536 - rmse: 0.5947 - mae: 0.3163 - mape: 9.9240\n",
      "Epoch 58: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3126 - mse: 0.3446 - rmse: 0.5870 - mae: 0.3126 - mape: 9.8416 - val_loss: 0.2844 - val_mse: 0.2877 - val_rmse: 0.5364 - val_mae: 0.2844 - val_mape: 8.6776 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.3182 - mse: 0.3467 - rmse: 0.5888 - mae: 0.3182 - mape: 9.9527\n",
      "Epoch 59: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3182 - mse: 0.3467 - rmse: 0.5888 - mae: 0.3182 - mape: 9.9527 - val_loss: 0.3252 - val_mse: 0.3206 - val_rmse: 0.5662 - val_mae: 0.3252 - val_mape: 10.3003 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3145 - mse: 0.3519 - rmse: 0.5932 - mae: 0.3145 - mape: 9.8865 \n",
      "Epoch 60: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3144 - mse: 0.3511 - rmse: 0.5926 - mae: 0.3144 - mape: 9.8826 - val_loss: 0.2893 - val_mse: 0.2703 - val_rmse: 0.5199 - val_mae: 0.2893 - val_mape: 8.9748 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "256/318 [=======================>......] - ETA: 0s - loss: 0.3093 - mse: 0.3339 - rmse: 0.5779 - mae: 0.3093 - mape: 9.7353\n",
      "Epoch 61: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3093 - mse: 0.3369 - rmse: 0.5804 - mae: 0.3093 - mape: 9.7094 - val_loss: 0.2892 - val_mse: 0.2936 - val_rmse: 0.5419 - val_mae: 0.2892 - val_mape: 9.0831 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.3138 - mse: 0.3419 - rmse: 0.5847 - mae: 0.3138 - mape: 9.8289 \n",
      "Epoch 62: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3138 - mse: 0.3419 - rmse: 0.5847 - mae: 0.3138 - mape: 9.8289 - val_loss: 0.2894 - val_mse: 0.2813 - val_rmse: 0.5303 - val_mae: 0.2894 - val_mape: 9.0531 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.3076 - mse: 0.3320 - rmse: 0.5762 - mae: 0.3076 - mape: 9.5712\n",
      "Epoch 63: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3072 - mse: 0.3326 - rmse: 0.5767 - mae: 0.3072 - mape: 9.5750 - val_loss: 0.3346 - val_mse: 0.4060 - val_rmse: 0.6372 - val_mae: 0.3346 - val_mape: 10.8631 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3042 - mse: 0.3252 - rmse: 0.5702 - mae: 0.3042 - mape: 9.5630\n",
      "Epoch 64: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3046 - mse: 0.3263 - rmse: 0.5712 - mae: 0.3046 - mape: 9.5793 - val_loss: 0.3036 - val_mse: 0.3430 - val_rmse: 0.5857 - val_mae: 0.3036 - val_mape: 9.3781 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3071 - mse: 0.3412 - rmse: 0.5841 - mae: 0.3071 - mape: 9.6621\n",
      "Epoch 65: val_loss did not improve from 0.27923\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3071 - mse: 0.3411 - rmse: 0.5840 - mae: 0.3071 - mape: 9.6617 - val_loss: 0.4097 - val_mse: 0.6268 - val_rmse: 0.7917 - val_mae: 0.4097 - val_mape: 13.1151 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.3053 - mse: 0.3372 - rmse: 0.5806 - mae: 0.3053 - mape: 9.6498\n",
      "Epoch 66: val_loss improved from 0.27923 to 0.27914, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3043 - mse: 0.3349 - rmse: 0.5787 - mae: 0.3043 - mape: 9.6161 - val_loss: 0.2791 - val_mse: 0.2739 - val_rmse: 0.5234 - val_mae: 0.2791 - val_mape: 8.6818 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.3153 - mse: 0.3578 - rmse: 0.5982 - mae: 0.3153 - mape: 9.9187\n",
      "Epoch 67: val_loss improved from 0.27914 to 0.27589, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3148 - mse: 0.3563 - rmse: 0.5969 - mae: 0.3148 - mape: 9.9166 - val_loss: 0.2759 - val_mse: 0.2617 - val_rmse: 0.5116 - val_mae: 0.2759 - val_mape: 8.6584 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.3227 - mse: 0.3671 - rmse: 0.6059 - mae: 0.3227 - mape: 10.0840\n",
      "Epoch 68: val_loss did not improve from 0.27589\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3227 - mse: 0.3666 - rmse: 0.6055 - mae: 0.3227 - mape: 10.0715 - val_loss: 0.2945 - val_mse: 0.2950 - val_rmse: 0.5432 - val_mae: 0.2945 - val_mape: 9.0472 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "259/318 [=======================>......] - ETA: 0s - loss: 0.3212 - mse: 0.3656 - rmse: 0.6046 - mae: 0.3212 - mape: 10.0916\n",
      "Epoch 69: val_loss did not improve from 0.27589\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3237 - mse: 0.3665 - rmse: 0.6054 - mae: 0.3237 - mape: 10.1890 - val_loss: 0.2781 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2781 - val_mape: 8.8882 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "261/318 [=======================>......] - ETA: 0s - loss: 0.3097 - mse: 0.3347 - rmse: 0.5786 - mae: 0.3097 - mape: 9.6776\n",
      "Epoch 70: val_loss improved from 0.27589 to 0.27331, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3030 - mse: 0.3217 - rmse: 0.5672 - mae: 0.3030 - mape: 9.4960 - val_loss: 0.2733 - val_mse: 0.2658 - val_rmse: 0.5156 - val_mae: 0.2733 - val_mape: 8.7325 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "258/318 [=======================>......] - ETA: 0s - loss: 0.3090 - mse: 0.3386 - rmse: 0.5819 - mae: 0.3090 - mape: 9.5966\n",
      "Epoch 71: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3069 - mse: 0.3337 - rmse: 0.5777 - mae: 0.3069 - mape: 9.5827 - val_loss: 0.3169 - val_mse: 0.3568 - val_rmse: 0.5974 - val_mae: 0.3169 - val_mape: 10.4599 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "261/318 [=======================>......] - ETA: 0s - loss: 0.3026 - mse: 0.3314 - rmse: 0.5757 - mae: 0.3026 - mape: 9.5497\n",
      "Epoch 72: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3058 - mse: 0.3404 - rmse: 0.5834 - mae: 0.3058 - mape: 9.5750 - val_loss: 0.3108 - val_mse: 0.3294 - val_rmse: 0.5739 - val_mae: 0.3108 - val_mape: 10.2298 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.3241 - mse: 0.3777 - rmse: 0.6146 - mae: 0.3241 - mape: 10.1295\n",
      "Epoch 73: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3175 - mse: 0.3625 - rmse: 0.6021 - mae: 0.3175 - mape: 9.8724 - val_loss: 0.2762 - val_mse: 0.2654 - val_rmse: 0.5152 - val_mae: 0.2762 - val_mape: 8.8589 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.3026 - mse: 0.3250 - rmse: 0.5701 - mae: 0.3026 - mape: 9.5028\n",
      "Epoch 74: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3026 - mse: 0.3247 - rmse: 0.5699 - mae: 0.3026 - mape: 9.5011 - val_loss: 0.3208 - val_mse: 0.3393 - val_rmse: 0.5825 - val_mae: 0.3208 - val_mape: 10.2536 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "262/318 [=======================>......] - ETA: 0s - loss: 0.3919 - mse: 0.6306 - rmse: 0.7941 - mae: 0.3919 - mape: 11.3972\n",
      "Epoch 75: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3961 - mse: 0.6480 - rmse: 0.8050 - mae: 0.3961 - mape: 11.4576 - val_loss: 0.4095 - val_mse: 0.7141 - val_rmse: 0.8450 - val_mae: 0.4095 - val_mape: 12.0700 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.4099 - mse: 0.7169 - rmse: 0.8467 - mae: 0.4099 - mape: 11.8289\n",
      "Epoch 76: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4099 - mse: 0.7169 - rmse: 0.8467 - mae: 0.4099 - mape: 11.8289 - val_loss: 0.4054 - val_mse: 0.6917 - val_rmse: 0.8317 - val_mae: 0.4054 - val_mape: 11.8421 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.4148 - mse: 0.7279 - rmse: 0.8532 - mae: 0.4148 - mape: 12.0493\n",
      "Epoch 77: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4138 - mse: 0.7250 - rmse: 0.8515 - mae: 0.4138 - mape: 12.0327 - val_loss: 0.4088 - val_mse: 0.7074 - val_rmse: 0.8411 - val_mae: 0.4088 - val_mape: 11.2943 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.4011 - mse: 0.6927 - rmse: 0.8323 - mae: 0.4011 - mape: 11.4856\n",
      "Epoch 78: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4036 - mse: 0.7014 - rmse: 0.8375 - mae: 0.4036 - mape: 11.5733 - val_loss: 0.3983 - val_mse: 0.6944 - val_rmse: 0.8333 - val_mae: 0.3983 - val_mape: 11.6674 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "255/318 [=======================>......] - ETA: 0s - loss: 0.4017 - mse: 0.6501 - rmse: 0.8063 - mae: 0.4017 - mape: 11.7868\n",
      "Epoch 79: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3934 - mse: 0.6046 - rmse: 0.7776 - mae: 0.3934 - mape: 11.6717 - val_loss: 0.3421 - val_mse: 0.3545 - val_rmse: 0.5954 - val_mae: 0.3421 - val_mape: 11.0239 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.3478 - mse: 0.4474 - rmse: 0.6689 - mae: 0.3478 - mape: 10.1480\n",
      "Epoch 80: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3477 - mse: 0.4457 - rmse: 0.6676 - mae: 0.3477 - mape: 10.1337 - val_loss: 0.3728 - val_mse: 0.4939 - val_rmse: 0.7028 - val_mae: 0.3728 - val_mape: 10.8363 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.3364 - mse: 0.4057 - rmse: 0.6369 - mae: 0.3364 - mape: 9.7802\n",
      "Epoch 81: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3382 - mse: 0.4102 - rmse: 0.6405 - mae: 0.3382 - mape: 9.8265 - val_loss: 0.3343 - val_mse: 0.4037 - val_rmse: 0.6354 - val_mae: 0.3343 - val_mape: 9.6821 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.3357 - mse: 0.3917 - rmse: 0.6258 - mae: 0.3357 - mape: 9.9042\n",
      "Epoch 82: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3347 - mse: 0.3920 - rmse: 0.6261 - mae: 0.3347 - mape: 9.8693 - val_loss: 0.3270 - val_mse: 0.3561 - val_rmse: 0.5967 - val_mae: 0.3270 - val_mape: 9.8905 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.3260 - mse: 0.3576 - rmse: 0.5980 - mae: 0.3260 - mape: 9.6797\n",
      "Epoch 83: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3260 - mse: 0.3586 - rmse: 0.5988 - mae: 0.3260 - mape: 9.6945 - val_loss: 0.3039 - val_mse: 0.3067 - val_rmse: 0.5538 - val_mae: 0.3039 - val_mape: 9.0970 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.3291 - mse: 0.3685 - rmse: 0.6071 - mae: 0.3291 - mape: 9.9647 \n",
      "Epoch 84: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3293 - mse: 0.3688 - rmse: 0.6073 - mae: 0.3293 - mape: 9.9687 - val_loss: 0.3051 - val_mse: 0.3236 - val_rmse: 0.5689 - val_mae: 0.3051 - val_mape: 8.9976 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.4212 - mse: 0.7354 - rmse: 0.8576 - mae: 0.4212 - mape: 12.0121\n",
      "Epoch 85: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4213 - mse: 0.7357 - rmse: 0.8577 - mae: 0.4213 - mape: 12.0139 - val_loss: 0.4880 - val_mse: 0.9636 - val_rmse: 0.9816 - val_mae: 0.4880 - val_mape: 13.1164 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.4206 - mse: 0.7509 - rmse: 0.8665 - mae: 0.4206 - mape: 11.7082\n",
      "Epoch 86: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4196 - mse: 0.7463 - rmse: 0.8639 - mae: 0.4196 - mape: 11.7417 - val_loss: 0.4472 - val_mse: 0.8584 - val_rmse: 0.9265 - val_mae: 0.4472 - val_mape: 12.2390 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.4218 - mse: 0.7464 - rmse: 0.8640 - mae: 0.4218 - mape: 11.9243\n",
      "Epoch 87: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4235 - mse: 0.7529 - rmse: 0.8677 - mae: 0.4235 - mape: 11.9447 - val_loss: 0.4790 - val_mse: 0.9814 - val_rmse: 0.9907 - val_mae: 0.4790 - val_mape: 12.6304 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.3938 - mse: 0.5951 - rmse: 0.7714 - mae: 0.3938 - mape: 11.6987\n",
      "Epoch 88: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3947 - mse: 0.5974 - rmse: 0.7729 - mae: 0.3947 - mape: 11.7185 - val_loss: 0.3901 - val_mse: 0.5735 - val_rmse: 0.7573 - val_mae: 0.3901 - val_mape: 11.5331 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.4219 - mse: 0.7507 - rmse: 0.8664 - mae: 0.4219 - mape: 11.9354\n",
      "Epoch 89: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4211 - mse: 0.7423 - rmse: 0.8616 - mae: 0.4211 - mape: 11.9564 - val_loss: 0.4305 - val_mse: 0.7499 - val_rmse: 0.8659 - val_mae: 0.4305 - val_mape: 12.8440 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.3875 - mse: 0.5664 - rmse: 0.7526 - mae: 0.3875 - mape: 11.6102\n",
      "Epoch 90: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3871 - mse: 0.5630 - rmse: 0.7503 - mae: 0.3871 - mape: 11.6021 - val_loss: 0.3695 - val_mse: 0.4483 - val_rmse: 0.6695 - val_mae: 0.3695 - val_mape: 11.7054 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.3549 - mse: 0.4407 - rmse: 0.6638 - mae: 0.3549 - mape: 11.2356\n",
      "Epoch 91: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3556 - mse: 0.4443 - rmse: 0.6665 - mae: 0.3556 - mape: 11.2432 - val_loss: 0.3406 - val_mse: 0.4136 - val_rmse: 0.6431 - val_mae: 0.3406 - val_mape: 10.7243 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.3513 - mse: 0.4501 - rmse: 0.6709 - mae: 0.3513 - mape: 10.6096\n",
      "Epoch 92: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3494 - mse: 0.4462 - rmse: 0.6680 - mae: 0.3494 - mape: 10.5562 - val_loss: 0.3840 - val_mse: 0.5519 - val_rmse: 0.7429 - val_mae: 0.3840 - val_mape: 11.0500 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.3839 - mse: 0.5638 - rmse: 0.7508 - mae: 0.3839 - mape: 11.2209\n",
      "Epoch 93: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3845 - mse: 0.5654 - rmse: 0.7519 - mae: 0.3845 - mape: 11.2423 - val_loss: 0.4174 - val_mse: 0.7364 - val_rmse: 0.8581 - val_mae: 0.4174 - val_mape: 11.4604 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.4099 - mse: 0.6901 - rmse: 0.8307 - mae: 0.4099 - mape: 11.6620\n",
      "Epoch 94: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4085 - mse: 0.6852 - rmse: 0.8278 - mae: 0.4085 - mape: 11.6250 - val_loss: 0.3437 - val_mse: 0.4472 - val_rmse: 0.6687 - val_mae: 0.3437 - val_mape: 10.0197 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.3360 - mse: 0.4030 - rmse: 0.6348 - mae: 0.3360 - mape: 10.0943\n",
      "Epoch 95: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3358 - mse: 0.4021 - rmse: 0.6341 - mae: 0.3358 - mape: 10.0829 - val_loss: 0.4566 - val_mse: 0.8012 - val_rmse: 0.8951 - val_mae: 0.4566 - val_mape: 13.6419 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.3953 - mse: 0.6141 - rmse: 0.7837 - mae: 0.3953 - mape: 11.5229\n",
      "Epoch 96: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3953 - mse: 0.6141 - rmse: 0.7837 - mae: 0.3953 - mape: 11.5229 - val_loss: 0.3337 - val_mse: 0.4290 - val_rmse: 0.6550 - val_mae: 0.3337 - val_mape: 9.5253 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.3349 - mse: 0.4157 - rmse: 0.6447 - mae: 0.3349 - mape: 9.9345\n",
      "Epoch 97: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3355 - mse: 0.4164 - rmse: 0.6453 - mae: 0.3355 - mape: 9.9481 - val_loss: 0.3584 - val_mse: 0.4827 - val_rmse: 0.6947 - val_mae: 0.3584 - val_mape: 10.4081 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.3340 - mse: 0.3984 - rmse: 0.6312 - mae: 0.3340 - mape: 10.0550\n",
      "Epoch 98: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3326 - mse: 0.3948 - rmse: 0.6283 - mae: 0.3326 - mape: 10.0424 - val_loss: 0.2990 - val_mse: 0.3095 - val_rmse: 0.5563 - val_mae: 0.2990 - val_mape: 9.0185 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.3329 - mse: 0.3999 - rmse: 0.6324 - mae: 0.3329 - mape: 10.0930\n",
      "Epoch 99: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3321 - mse: 0.3971 - rmse: 0.6302 - mae: 0.3321 - mape: 10.0813 - val_loss: 0.3087 - val_mse: 0.3314 - val_rmse: 0.5757 - val_mae: 0.3087 - val_mape: 9.2322 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3329 - mse: 0.3987 - rmse: 0.6314 - mae: 0.3329 - mape: 10.1555\n",
      "Epoch 100: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3335 - mse: 0.3995 - rmse: 0.6321 - mae: 0.3335 - mape: 10.1700 - val_loss: 0.3545 - val_mse: 0.4385 - val_rmse: 0.6622 - val_mae: 0.3545 - val_mape: 11.2508 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.3169 - mse: 0.3441 - rmse: 0.5866 - mae: 0.3169 - mape: 9.7410\n",
      "Epoch 101: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3169 - mse: 0.3441 - rmse: 0.5866 - mae: 0.3169 - mape: 9.7410 - val_loss: 0.3007 - val_mse: 0.3075 - val_rmse: 0.5545 - val_mae: 0.3007 - val_mape: 9.1886 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.3379 - mse: 0.4238 - rmse: 0.6510 - mae: 0.3379 - mape: 10.3193\n",
      "Epoch 102: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3417 - mse: 0.4383 - rmse: 0.6620 - mae: 0.3417 - mape: 10.4059 - val_loss: 0.4351 - val_mse: 0.8051 - val_rmse: 0.8973 - val_mae: 0.4351 - val_mape: 12.9315 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.3312 - mse: 0.3900 - rmse: 0.6245 - mae: 0.3312 - mape: 10.2005\n",
      "Epoch 103: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3293 - mse: 0.3851 - rmse: 0.6205 - mae: 0.3293 - mape: 10.1429 - val_loss: 0.3095 - val_mse: 0.3350 - val_rmse: 0.5788 - val_mae: 0.3095 - val_mape: 9.3151 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3216 - mse: 0.3649 - rmse: 0.6041 - mae: 0.3216 - mape: 9.9474 \n",
      "Epoch 104: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3212 - mse: 0.3642 - rmse: 0.6035 - mae: 0.3212 - mape: 9.9305 - val_loss: 0.3212 - val_mse: 0.3557 - val_rmse: 0.5964 - val_mae: 0.3212 - val_mape: 9.5750 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.3160 - mse: 0.3436 - rmse: 0.5862 - mae: 0.3160 - mape: 9.8909\n",
      "Epoch 105: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3154 - mse: 0.3412 - rmse: 0.5842 - mae: 0.3154 - mape: 9.8612 - val_loss: 0.3416 - val_mse: 0.3812 - val_rmse: 0.6174 - val_mae: 0.3416 - val_mape: 10.7712 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3179 - mse: 0.3514 - rmse: 0.5928 - mae: 0.3179 - mape: 9.9139\n",
      "Epoch 106: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3180 - mse: 0.3514 - rmse: 0.5928 - mae: 0.3180 - mape: 9.9162 - val_loss: 0.2893 - val_mse: 0.2783 - val_rmse: 0.5276 - val_mae: 0.2893 - val_mape: 9.2505 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.3187 - mse: 0.3484 - rmse: 0.5903 - mae: 0.3187 - mape: 10.0804\n",
      "Epoch 107: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3174 - mse: 0.3459 - rmse: 0.5881 - mae: 0.3174 - mape: 10.0423 - val_loss: 0.3233 - val_mse: 0.3629 - val_rmse: 0.6024 - val_mae: 0.3233 - val_mape: 10.4487 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.3560 - mse: 0.5001 - rmse: 0.7072 - mae: 0.3560 - mape: 10.7093\n",
      "Epoch 108: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3624 - mse: 0.5219 - rmse: 0.7224 - mae: 0.3624 - mape: 10.8383 - val_loss: 0.4292 - val_mse: 0.7900 - val_rmse: 0.8888 - val_mae: 0.4292 - val_mape: 11.8449 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.4150 - mse: 0.7114 - rmse: 0.8435 - mae: 0.4150 - mape: 11.9422\n",
      "Epoch 109: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4180 - mse: 0.7206 - rmse: 0.8489 - mae: 0.4180 - mape: 11.9922 - val_loss: 0.5122 - val_mse: 1.0133 - val_rmse: 1.0066 - val_mae: 0.5122 - val_mape: 13.4696 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.4168 - mse: 0.6963 - rmse: 0.8344 - mae: 0.4168 - mape: 12.0860\n",
      "Epoch 110: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4166 - mse: 0.6958 - rmse: 0.8341 - mae: 0.4166 - mape: 12.0816 - val_loss: 0.4473 - val_mse: 0.8918 - val_rmse: 0.9443 - val_mae: 0.4473 - val_mape: 11.9073 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.4031 - mse: 0.7157 - rmse: 0.8460 - mae: 0.4031 - mape: 11.1595\n",
      "Epoch 111: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4031 - mse: 0.7139 - rmse: 0.8449 - mae: 0.4031 - mape: 11.1850 - val_loss: 0.3951 - val_mse: 0.6844 - val_rmse: 0.8273 - val_mae: 0.3951 - val_mape: 11.0562 - lr: 1.0000e-04\n",
      "Epoch 112/1000\n",
      "261/318 [=======================>......] - ETA: 0s - loss: 0.3940 - mse: 0.6769 - rmse: 0.8227 - mae: 0.3940 - mape: 11.0645\n",
      "Epoch 112: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3953 - mse: 0.6827 - rmse: 0.8262 - mae: 0.3953 - mape: 11.1175 - val_loss: 0.3933 - val_mse: 0.6792 - val_rmse: 0.8241 - val_mae: 0.3933 - val_mape: 11.1130 - lr: 1.0000e-04\n",
      "Epoch 113/1000\n",
      "262/318 [=======================>......] - ETA: 0s - loss: 0.3952 - mse: 0.6807 - rmse: 0.8251 - mae: 0.3952 - mape: 11.1256\n",
      "Epoch 113: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3939 - mse: 0.6806 - rmse: 0.8250 - mae: 0.3939 - mape: 11.1112 - val_loss: 0.3916 - val_mse: 0.6795 - val_rmse: 0.8243 - val_mae: 0.3916 - val_mape: 11.0770 - lr: 1.0000e-04\n",
      "Epoch 114/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.3931 - mse: 0.6837 - rmse: 0.8269 - mae: 0.3931 - mape: 11.1266\n",
      "Epoch 114: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3930 - mse: 0.6820 - rmse: 0.8258 - mae: 0.3930 - mape: 11.1251 - val_loss: 0.3898 - val_mse: 0.6689 - val_rmse: 0.8179 - val_mae: 0.3898 - val_mape: 11.1095 - lr: 1.0000e-04\n",
      "Epoch 115/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.3919 - mse: 0.6808 - rmse: 0.8251 - mae: 0.3919 - mape: 11.0949\n",
      "Epoch 115: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3919 - mse: 0.6808 - rmse: 0.8251 - mae: 0.3919 - mape: 11.0949 - val_loss: 0.3953 - val_mse: 0.6862 - val_rmse: 0.8284 - val_mae: 0.3953 - val_mape: 11.3936 - lr: 1.0000e-04\n",
      "Epoch 116/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.3931 - mse: 0.6859 - rmse: 0.8282 - mae: 0.3931 - mape: 11.1430\n",
      "Epoch 116: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3925 - mse: 0.6825 - rmse: 0.8261 - mae: 0.3925 - mape: 11.1463 - val_loss: 0.3903 - val_mse: 0.6766 - val_rmse: 0.8225 - val_mae: 0.3903 - val_mape: 11.1141 - lr: 1.0000e-04\n",
      "Epoch 117/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3906 - mse: 0.6778 - rmse: 0.8233 - mae: 0.3906 - mape: 11.1053\n",
      "Epoch 117: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3904 - mse: 0.6773 - rmse: 0.8230 - mae: 0.3904 - mape: 11.1004 - val_loss: 0.3986 - val_mse: 0.7028 - val_rmse: 0.8383 - val_mae: 0.3986 - val_mape: 11.5105 - lr: 1.0000e-04\n",
      "Epoch 118/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.3921 - mse: 0.6827 - rmse: 0.8263 - mae: 0.3921 - mape: 11.1638\n",
      "Epoch 118: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3916 - mse: 0.6805 - rmse: 0.8249 - mae: 0.3916 - mape: 11.1467 - val_loss: 0.3889 - val_mse: 0.6768 - val_rmse: 0.8227 - val_mae: 0.3889 - val_mape: 10.9845 - lr: 1.0000e-04\n",
      "Epoch 119/1000\n",
      "254/318 [======================>.......] - ETA: 0s - loss: 0.3896 - mse: 0.6825 - rmse: 0.8261 - mae: 0.3896 - mape: 11.0987\n",
      "Epoch 119: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3885 - mse: 0.6782 - rmse: 0.8235 - mae: 0.3885 - mape: 11.0135 - val_loss: 0.3860 - val_mse: 0.6622 - val_rmse: 0.8138 - val_mae: 0.3860 - val_mape: 10.9741 - lr: 1.0000e-04\n",
      "Epoch 120/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.3843 - mse: 0.6834 - rmse: 0.8267 - mae: 0.3843 - mape: 10.8068\n",
      "Epoch 120: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3844 - mse: 0.6810 - rmse: 0.8252 - mae: 0.3844 - mape: 10.8107 - val_loss: 0.3804 - val_mse: 0.6642 - val_rmse: 0.8150 - val_mae: 0.3804 - val_mape: 10.7210 - lr: 1.0000e-04\n",
      "Epoch 121/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.3837 - mse: 0.6737 - rmse: 0.8208 - mae: 0.3837 - mape: 10.8185\n",
      "Epoch 121: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3824 - mse: 0.6708 - rmse: 0.8190 - mae: 0.3824 - mape: 10.7812 - val_loss: 0.3788 - val_mse: 0.6657 - val_rmse: 0.8159 - val_mae: 0.3788 - val_mape: 10.6467 - lr: 1.0000e-04\n",
      "Epoch 122/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.3780 - mse: 0.6561 - rmse: 0.8100 - mae: 0.3780 - mape: 10.7110\n",
      "Epoch 122: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3799 - mse: 0.6626 - rmse: 0.8140 - mae: 0.3799 - mape: 10.7369 - val_loss: 0.3757 - val_mse: 0.6379 - val_rmse: 0.7987 - val_mae: 0.3757 - val_mape: 10.7009 - lr: 1.0000e-04\n",
      "Epoch 123/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3758 - mse: 0.6369 - rmse: 0.7981 - mae: 0.3758 - mape: 10.7000\n",
      "Epoch 123: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3759 - mse: 0.6371 - rmse: 0.7982 - mae: 0.3759 - mape: 10.6947 - val_loss: 0.3718 - val_mse: 0.6249 - val_rmse: 0.7905 - val_mae: 0.3718 - val_mape: 10.6223 - lr: 1.0000e-04\n",
      "Epoch 124/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.3737 - mse: 0.6251 - rmse: 0.7906 - mae: 0.3737 - mape: 10.7010\n",
      "Epoch 124: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3718 - mse: 0.6197 - rmse: 0.7872 - mae: 0.3718 - mape: 10.6488 - val_loss: 0.3766 - val_mse: 0.6189 - val_rmse: 0.7867 - val_mae: 0.3766 - val_mape: 10.9275 - lr: 1.0000e-04\n",
      "Epoch 125/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.3690 - mse: 0.6000 - rmse: 0.7746 - mae: 0.3690 - mape: 10.6369\n",
      "Epoch 125: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3687 - mse: 0.5987 - rmse: 0.7738 - mae: 0.3687 - mape: 10.6319 - val_loss: 0.3652 - val_mse: 0.5756 - val_rmse: 0.7587 - val_mae: 0.3652 - val_mape: 10.5606 - lr: 1.0000e-04\n",
      "Epoch 126/1000\n",
      "260/318 [=======================>......] - ETA: 0s - loss: 0.3657 - mse: 0.5815 - rmse: 0.7626 - mae: 0.3657 - mape: 10.6075\n",
      "Epoch 126: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3631 - mse: 0.5728 - rmse: 0.7568 - mae: 0.3631 - mape: 10.5477 - val_loss: 0.3617 - val_mse: 0.5597 - val_rmse: 0.7481 - val_mae: 0.3617 - val_mape: 10.5678 - lr: 1.0000e-04\n",
      "Epoch 127/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.3582 - mse: 0.5510 - rmse: 0.7423 - mae: 0.3582 - mape: 10.4864\n",
      "Epoch 127: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3584 - mse: 0.5512 - rmse: 0.7424 - mae: 0.3584 - mape: 10.4845 - val_loss: 0.3535 - val_mse: 0.5292 - val_rmse: 0.7274 - val_mae: 0.3535 - val_mape: 10.3449 - lr: 1.0000e-04\n",
      "Epoch 128/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.3527 - mse: 0.5285 - rmse: 0.7270 - mae: 0.3527 - mape: 10.3571\n",
      "Epoch 128: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3531 - mse: 0.5289 - rmse: 0.7272 - mae: 0.3531 - mape: 10.3693 - val_loss: 0.3491 - val_mse: 0.5141 - val_rmse: 0.7170 - val_mae: 0.3491 - val_mape: 10.1841 - lr: 1.0000e-04\n",
      "Epoch 129/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.3469 - mse: 0.5041 - rmse: 0.7100 - mae: 0.3469 - mape: 10.2255\n",
      "Epoch 129: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3468 - mse: 0.5043 - rmse: 0.7102 - mae: 0.3468 - mape: 10.2164 - val_loss: 0.3434 - val_mse: 0.4772 - val_rmse: 0.6908 - val_mae: 0.3434 - val_mape: 10.2162 - lr: 1.0000e-04\n",
      "Epoch 130/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.3409 - mse: 0.4800 - rmse: 0.6928 - mae: 0.3409 - mape: 10.1200\n",
      "Epoch 130: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3405 - mse: 0.4789 - rmse: 0.6920 - mae: 0.3405 - mape: 10.1058 - val_loss: 0.3405 - val_mse: 0.4774 - val_rmse: 0.6910 - val_mae: 0.3405 - val_mape: 10.1444 - lr: 1.0000e-04\n",
      "Epoch 131/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.3325 - mse: 0.4548 - rmse: 0.6744 - mae: 0.3325 - mape: 9.8683\n",
      "Epoch 131: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3323 - mse: 0.4541 - rmse: 0.6739 - mae: 0.3323 - mape: 9.8648 - val_loss: 0.3285 - val_mse: 0.4484 - val_rmse: 0.6696 - val_mae: 0.3285 - val_mape: 9.7171 - lr: 1.0000e-04\n",
      "Epoch 132/1000\n",
      "259/318 [=======================>......] - ETA: 0s - loss: 0.3233 - mse: 0.4279 - rmse: 0.6541 - mae: 0.3233 - mape: 9.6381\n",
      "Epoch 132: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3250 - mse: 0.4265 - rmse: 0.6531 - mae: 0.3250 - mape: 9.6558 - val_loss: 0.3212 - val_mse: 0.4055 - val_rmse: 0.6368 - val_mae: 0.3212 - val_mape: 9.6088 - lr: 1.0000e-04\n",
      "Epoch 133/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.3210 - mse: 0.4074 - rmse: 0.6383 - mae: 0.3210 - mape: 9.5875\n",
      "Epoch 133: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3205 - mse: 0.4051 - rmse: 0.6365 - mae: 0.3205 - mape: 9.5734 - val_loss: 0.3156 - val_mse: 0.3900 - val_rmse: 0.6245 - val_mae: 0.3156 - val_mape: 9.4418 - lr: 1.0000e-04\n",
      "Epoch 134/1000\n",
      "255/318 [=======================>......] - ETA: 0s - loss: 0.3192 - mse: 0.3970 - rmse: 0.6301 - mae: 0.3192 - mape: 9.6152\n",
      "Epoch 134: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3171 - mse: 0.3941 - rmse: 0.6277 - mae: 0.3171 - mape: 9.5221 - val_loss: 0.3129 - val_mse: 0.3835 - val_rmse: 0.6192 - val_mae: 0.3129 - val_mape: 9.4159 - lr: 1.0000e-04\n",
      "Epoch 135/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.3137 - mse: 0.3799 - rmse: 0.6163 - mae: 0.3137 - mape: 9.4733\n",
      "Epoch 135: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3141 - mse: 0.3809 - rmse: 0.6172 - mae: 0.3141 - mape: 9.4740 - val_loss: 0.3092 - val_mse: 0.3692 - val_rmse: 0.6076 - val_mae: 0.3092 - val_mape: 9.2400 - lr: 1.0000e-04\n",
      "Epoch 136/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3116 - mse: 0.3704 - rmse: 0.6086 - mae: 0.3116 - mape: 9.3843\n",
      "Epoch 136: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3114 - mse: 0.3697 - rmse: 0.6080 - mae: 0.3114 - mape: 9.3887 - val_loss: 0.3126 - val_mse: 0.3635 - val_rmse: 0.6029 - val_mae: 0.3126 - val_mape: 9.4322 - lr: 1.0000e-04\n",
      "Epoch 137/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.3108 - mse: 0.3652 - rmse: 0.6043 - mae: 0.3108 - mape: 9.4130\n",
      "Epoch 137: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3115 - mse: 0.3659 - rmse: 0.6049 - mae: 0.3115 - mape: 9.4344 - val_loss: 0.3222 - val_mse: 0.3849 - val_rmse: 0.6204 - val_mae: 0.3222 - val_mape: 9.9760 - lr: 1.0000e-04\n",
      "Epoch 138/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.3060 - mse: 0.3518 - rmse: 0.5931 - mae: 0.3060 - mape: 9.2960\n",
      "Epoch 138: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3061 - mse: 0.3523 - rmse: 0.5935 - mae: 0.3061 - mape: 9.3129 - val_loss: 0.3079 - val_mse: 0.3556 - val_rmse: 0.5963 - val_mae: 0.3079 - val_mape: 9.3828 - lr: 1.0000e-04\n",
      "Epoch 139/1000\n",
      "258/318 [=======================>......] - ETA: 0s - loss: 0.3025 - mse: 0.3368 - rmse: 0.5803 - mae: 0.3025 - mape: 9.1682\n",
      "Epoch 139: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3049 - mse: 0.3435 - rmse: 0.5861 - mae: 0.3049 - mape: 9.2917 - val_loss: 0.3042 - val_mse: 0.3475 - val_rmse: 0.5895 - val_mae: 0.3042 - val_mape: 9.3939 - lr: 1.0000e-04\n",
      "Epoch 140/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.3027 - mse: 0.3383 - rmse: 0.5816 - mae: 0.3027 - mape: 9.2948\n",
      "Epoch 140: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3020 - mse: 0.3361 - rmse: 0.5798 - mae: 0.3020 - mape: 9.2777 - val_loss: 0.2984 - val_mse: 0.3268 - val_rmse: 0.5717 - val_mae: 0.2984 - val_mape: 9.1940 - lr: 1.0000e-04\n",
      "Epoch 141/1000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.3001 - mse: 0.3285 - rmse: 0.5731 - mae: 0.3001 - mape: 9.2252\n",
      "Epoch 141: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3016 - mse: 0.3308 - rmse: 0.5751 - mae: 0.3016 - mape: 9.2844 - val_loss: 0.2953 - val_mse: 0.3157 - val_rmse: 0.5619 - val_mae: 0.2953 - val_mape: 9.0576 - lr: 1.0000e-04\n",
      "Epoch 142/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2977 - mse: 0.3183 - rmse: 0.5642 - mae: 0.2977 - mape: 9.1702\n",
      "Epoch 142: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2981 - mse: 0.3186 - rmse: 0.5644 - mae: 0.2981 - mape: 9.1837 - val_loss: 0.2946 - val_mse: 0.3127 - val_rmse: 0.5592 - val_mae: 0.2946 - val_mape: 9.0329 - lr: 1.0000e-04\n",
      "Epoch 143/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2959 - mse: 0.3138 - rmse: 0.5602 - mae: 0.2959 - mape: 9.1614\n",
      "Epoch 143: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2959 - mse: 0.3138 - rmse: 0.5602 - mae: 0.2959 - mape: 9.1614 - val_loss: 0.2914 - val_mse: 0.3036 - val_rmse: 0.5510 - val_mae: 0.2914 - val_mape: 9.0359 - lr: 1.0000e-04\n",
      "Epoch 144/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2951 - mse: 0.3106 - rmse: 0.5573 - mae: 0.2951 - mape: 9.1809\n",
      "Epoch 144: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2940 - mse: 0.3082 - rmse: 0.5551 - mae: 0.2940 - mape: 9.1446 - val_loss: 0.2913 - val_mse: 0.3027 - val_rmse: 0.5502 - val_mae: 0.2913 - val_mape: 8.9561 - lr: 1.0000e-04\n",
      "Epoch 145/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2917 - mse: 0.3015 - rmse: 0.5491 - mae: 0.2917 - mape: 9.0665\n",
      "Epoch 145: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2917 - mse: 0.3015 - rmse: 0.5491 - mae: 0.2917 - mape: 9.0665 - val_loss: 0.2969 - val_mse: 0.3104 - val_rmse: 0.5572 - val_mae: 0.2969 - val_mape: 9.1376 - lr: 1.0000e-04\n",
      "Epoch 146/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2887 - mse: 0.2953 - rmse: 0.5435 - mae: 0.2887 - mape: 8.9823\n",
      "Epoch 146: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2892 - mse: 0.2967 - rmse: 0.5447 - mae: 0.2892 - mape: 8.9883 - val_loss: 0.2864 - val_mse: 0.2910 - val_rmse: 0.5395 - val_mae: 0.2864 - val_mape: 8.9168 - lr: 1.0000e-04\n",
      "Epoch 147/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2864 - mse: 0.2893 - rmse: 0.5379 - mae: 0.2864 - mape: 8.9624\n",
      "Epoch 147: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2870 - mse: 0.2904 - rmse: 0.5389 - mae: 0.2870 - mape: 8.9667 - val_loss: 0.2846 - val_mse: 0.2878 - val_rmse: 0.5365 - val_mae: 0.2846 - val_mape: 8.8336 - lr: 1.0000e-04\n",
      "Epoch 148/1000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2880 - mse: 0.2955 - rmse: 0.5436 - mae: 0.2880 - mape: 8.9874\n",
      "Epoch 148: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2862 - mse: 0.2907 - rmse: 0.5391 - mae: 0.2862 - mape: 8.9534 - val_loss: 0.2815 - val_mse: 0.2812 - val_rmse: 0.5303 - val_mae: 0.2815 - val_mape: 8.9575 - lr: 1.0000e-04\n",
      "Epoch 149/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2840 - mse: 0.2861 - rmse: 0.5349 - mae: 0.2840 - mape: 8.9458\n",
      "Epoch 149: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2840 - mse: 0.2861 - rmse: 0.5349 - mae: 0.2840 - mape: 8.9458 - val_loss: 0.2851 - val_mse: 0.2924 - val_rmse: 0.5407 - val_mae: 0.2851 - val_mape: 8.8389 - lr: 1.0000e-04\n",
      "Epoch 150/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2829 - mse: 0.2874 - rmse: 0.5361 - mae: 0.2829 - mape: 8.8720\n",
      "Epoch 150: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2818 - mse: 0.2845 - rmse: 0.5334 - mae: 0.2818 - mape: 8.8468 - val_loss: 0.2802 - val_mse: 0.2757 - val_rmse: 0.5250 - val_mae: 0.2802 - val_mape: 8.7343 - lr: 1.0000e-04\n",
      "Epoch 151/1000\n",
      "259/318 [=======================>......] - ETA: 0s - loss: 0.2790 - mse: 0.2768 - rmse: 0.5261 - mae: 0.2790 - mape: 8.7359\n",
      "Epoch 151: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2772 - mse: 0.2729 - rmse: 0.5224 - mae: 0.2772 - mape: 8.7043 - val_loss: 0.2767 - val_mse: 0.2729 - val_rmse: 0.5224 - val_mae: 0.2767 - val_mape: 8.7041 - lr: 1.0000e-05\n",
      "Epoch 152/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2762 - mse: 0.2709 - rmse: 0.5204 - mae: 0.2762 - mape: 8.6544\n",
      "Epoch 152: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2770 - mse: 0.2734 - rmse: 0.5228 - mae: 0.2770 - mape: 8.7020 - val_loss: 0.2763 - val_mse: 0.2720 - val_rmse: 0.5215 - val_mae: 0.2763 - val_mape: 8.6955 - lr: 1.0000e-05\n",
      "Epoch 153/1000\n",
      "255/318 [=======================>......] - ETA: 0s - loss: 0.2767 - mse: 0.2711 - rmse: 0.5207 - mae: 0.2767 - mape: 8.7031\n",
      "Epoch 153: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2767 - mse: 0.2736 - rmse: 0.5231 - mae: 0.2767 - mape: 8.7010 - val_loss: 0.2761 - val_mse: 0.2729 - val_rmse: 0.5224 - val_mae: 0.2761 - val_mape: 8.6732 - lr: 1.0000e-05\n",
      "Epoch 154/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2758 - mse: 0.2735 - rmse: 0.5230 - mae: 0.2758 - mape: 8.6916\n",
      "Epoch 154: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2762 - mse: 0.2740 - rmse: 0.5234 - mae: 0.2762 - mape: 8.6988 - val_loss: 0.2762 - val_mse: 0.2732 - val_rmse: 0.5227 - val_mae: 0.2762 - val_mape: 8.6924 - lr: 1.0000e-05\n",
      "Epoch 155/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2768 - mse: 0.2746 - rmse: 0.5241 - mae: 0.2768 - mape: 8.7153\n",
      "Epoch 155: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2761 - mse: 0.2744 - rmse: 0.5238 - mae: 0.2761 - mape: 8.6943 - val_loss: 0.2760 - val_mse: 0.2747 - val_rmse: 0.5241 - val_mae: 0.2760 - val_mape: 8.7535 - lr: 1.0000e-05\n",
      "Epoch 156/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2761 - mse: 0.2747 - rmse: 0.5242 - mae: 0.2761 - mape: 8.7122\n",
      "Epoch 156: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2761 - mse: 0.2745 - rmse: 0.5239 - mae: 0.2761 - mape: 8.7174 - val_loss: 0.2761 - val_mse: 0.2745 - val_rmse: 0.5239 - val_mae: 0.2761 - val_mape: 8.6510 - lr: 1.0000e-05\n",
      "Epoch 157/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2758 - mse: 0.2745 - rmse: 0.5239 - mae: 0.2758 - mape: 8.7028\n",
      "Epoch 157: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2758 - mse: 0.2745 - rmse: 0.5239 - mae: 0.2758 - mape: 8.7012 - val_loss: 0.2754 - val_mse: 0.2739 - val_rmse: 0.5234 - val_mae: 0.2754 - val_mape: 8.6514 - lr: 1.0000e-05\n",
      "Epoch 158/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2747 - mse: 0.2720 - rmse: 0.5215 - mae: 0.2747 - mape: 8.6780\n",
      "Epoch 158: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2756 - mse: 0.2745 - rmse: 0.5240 - mae: 0.2756 - mape: 8.7046 - val_loss: 0.2750 - val_mse: 0.2747 - val_rmse: 0.5241 - val_mae: 0.2750 - val_mape: 8.6782 - lr: 1.0000e-05\n",
      "Epoch 159/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2750 - mse: 0.2746 - rmse: 0.5240 - mae: 0.2750 - mape: 8.6886\n",
      "Epoch 159: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2752 - mse: 0.2747 - rmse: 0.5241 - mae: 0.2752 - mape: 8.6891 - val_loss: 0.2760 - val_mse: 0.2754 - val_rmse: 0.5248 - val_mae: 0.2760 - val_mape: 8.6470 - lr: 1.0000e-05\n",
      "Epoch 160/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2762 - mse: 0.2763 - rmse: 0.5256 - mae: 0.2762 - mape: 8.7173\n",
      "Epoch 160: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2753 - mse: 0.2747 - rmse: 0.5241 - mae: 0.2753 - mape: 8.6883 - val_loss: 0.2751 - val_mse: 0.2738 - val_rmse: 0.5232 - val_mae: 0.2751 - val_mape: 8.6534 - lr: 1.0000e-05\n",
      "Epoch 161/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2754 - mse: 0.2756 - rmse: 0.5250 - mae: 0.2754 - mape: 8.7137\n",
      "Epoch 161: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2749 - mse: 0.2741 - rmse: 0.5235 - mae: 0.2749 - mape: 8.6877 - val_loss: 0.2745 - val_mse: 0.2739 - val_rmse: 0.5233 - val_mae: 0.2745 - val_mape: 8.6413 - lr: 1.0000e-05\n",
      "Epoch 162/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2750 - mse: 0.2748 - rmse: 0.5242 - mae: 0.2750 - mape: 8.6839\n",
      "Epoch 162: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2749 - mse: 0.2746 - rmse: 0.5241 - mae: 0.2749 - mape: 8.6799 - val_loss: 0.2741 - val_mse: 0.2733 - val_rmse: 0.5228 - val_mae: 0.2741 - val_mape: 8.6640 - lr: 1.0000e-05\n",
      "Epoch 163/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2742 - mse: 0.2746 - rmse: 0.5240 - mae: 0.2742 - mape: 8.6483\n",
      "Epoch 163: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2745 - mse: 0.2751 - rmse: 0.5245 - mae: 0.2745 - mape: 8.6748 - val_loss: 0.2741 - val_mse: 0.2730 - val_rmse: 0.5225 - val_mae: 0.2741 - val_mape: 8.6556 - lr: 1.0000e-05\n",
      "Epoch 164/1000\n",
      "255/318 [=======================>......] - ETA: 0s - loss: 0.2743 - mse: 0.2721 - rmse: 0.5217 - mae: 0.2743 - mape: 8.6801\n",
      "Epoch 164: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2746 - mse: 0.2743 - rmse: 0.5237 - mae: 0.2746 - mape: 8.6773 - val_loss: 0.2742 - val_mse: 0.2740 - val_rmse: 0.5234 - val_mae: 0.2742 - val_mape: 8.7024 - lr: 1.0000e-05\n",
      "Epoch 165/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2751 - mse: 0.2747 - rmse: 0.5241 - mae: 0.2751 - mape: 8.6821\n",
      "Epoch 165: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2743 - mse: 0.2739 - rmse: 0.5233 - mae: 0.2743 - mape: 8.6838 - val_loss: 0.2738 - val_mse: 0.2737 - val_rmse: 0.5231 - val_mae: 0.2738 - val_mape: 8.6382 - lr: 1.0000e-05\n",
      "Epoch 166/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2741 - mse: 0.2741 - rmse: 0.5236 - mae: 0.2741 - mape: 8.6713\n",
      "Epoch 166: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2741 - mse: 0.2741 - rmse: 0.5236 - mae: 0.2741 - mape: 8.6713 - val_loss: 0.2739 - val_mse: 0.2743 - val_rmse: 0.5238 - val_mae: 0.2739 - val_mape: 8.6788 - lr: 1.0000e-05\n",
      "Epoch 167/1000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2757 - mse: 0.2777 - rmse: 0.5269 - mae: 0.2757 - mape: 8.6718\n",
      "Epoch 167: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2741 - mse: 0.2734 - rmse: 0.5229 - mae: 0.2741 - mape: 8.6697 - val_loss: 0.2734 - val_mse: 0.2730 - val_rmse: 0.5225 - val_mae: 0.2734 - val_mape: 8.6482 - lr: 1.0000e-05\n",
      "Epoch 168/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2741 - mse: 0.2743 - rmse: 0.5237 - mae: 0.2741 - mape: 8.6660\n",
      "Epoch 168: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2739 - mse: 0.2737 - rmse: 0.5231 - mae: 0.2739 - mape: 8.6599 - val_loss: 0.2736 - val_mse: 0.2721 - val_rmse: 0.5216 - val_mae: 0.2736 - val_mape: 8.6747 - lr: 1.0000e-05\n",
      "Epoch 169/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2731 - mse: 0.2715 - rmse: 0.5210 - mae: 0.2731 - mape: 8.6442\n",
      "Epoch 169: val_loss did not improve from 0.27331\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2740 - mse: 0.2735 - rmse: 0.5230 - mae: 0.2740 - mape: 8.6712 - val_loss: 0.2737 - val_mse: 0.2742 - val_rmse: 0.5237 - val_mae: 0.2737 - val_mape: 8.7107 - lr: 1.0000e-05\n",
      "Epoch 170/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2744 - mse: 0.2748 - rmse: 0.5242 - mae: 0.2744 - mape: 8.6956\n",
      "Epoch 170: val_loss improved from 0.27331 to 0.27305, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2738 - mse: 0.2739 - rmse: 0.5233 - mae: 0.2738 - mape: 8.6783 - val_loss: 0.2730 - val_mse: 0.2723 - val_rmse: 0.5219 - val_mae: 0.2730 - val_mape: 8.6405 - lr: 1.0000e-05\n",
      "Epoch 171/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2729 - mse: 0.2725 - rmse: 0.5220 - mae: 0.2729 - mape: 8.6368\n",
      "Epoch 171: val_loss did not improve from 0.27305\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2733 - mse: 0.2730 - rmse: 0.5225 - mae: 0.2733 - mape: 8.6558 - val_loss: 0.2732 - val_mse: 0.2734 - val_rmse: 0.5228 - val_mae: 0.2732 - val_mape: 8.6723 - lr: 1.0000e-05\n",
      "Epoch 172/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2723 - mse: 0.2697 - rmse: 0.5194 - mae: 0.2723 - mape: 8.6222\n",
      "Epoch 172: val_loss improved from 0.27305 to 0.27296, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2731 - mse: 0.2728 - rmse: 0.5223 - mae: 0.2731 - mape: 8.6490 - val_loss: 0.2730 - val_mse: 0.2734 - val_rmse: 0.5228 - val_mae: 0.2730 - val_mape: 8.6751 - lr: 1.0000e-05\n",
      "Epoch 173/1000\n",
      "271/318 [========================>.....] - ETA: 0s - loss: 0.2735 - mse: 0.2742 - rmse: 0.5236 - mae: 0.2735 - mape: 8.6761\n",
      "Epoch 173: val_loss improved from 0.27296 to 0.27271, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2730 - mse: 0.2725 - rmse: 0.5220 - mae: 0.2730 - mape: 8.6590 - val_loss: 0.2727 - val_mse: 0.2717 - val_rmse: 0.5212 - val_mae: 0.2727 - val_mape: 8.6124 - lr: 1.0000e-05\n",
      "Epoch 174/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2747 - mse: 0.2747 - rmse: 0.5242 - mae: 0.2747 - mape: 8.7222\n",
      "Epoch 174: val_loss did not improve from 0.27271\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2732 - mse: 0.2729 - rmse: 0.5224 - mae: 0.2732 - mape: 8.6577 - val_loss: 0.2730 - val_mse: 0.2740 - val_rmse: 0.5234 - val_mae: 0.2730 - val_mape: 8.6751 - lr: 1.0000e-05\n",
      "Epoch 175/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2727 - mse: 0.2710 - rmse: 0.5206 - mae: 0.2727 - mape: 8.6521\n",
      "Epoch 175: val_loss improved from 0.27271 to 0.27221, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2729 - mse: 0.2728 - rmse: 0.5223 - mae: 0.2729 - mape: 8.6557 - val_loss: 0.2722 - val_mse: 0.2708 - val_rmse: 0.5204 - val_mae: 0.2722 - val_mape: 8.6250 - lr: 1.0000e-05\n",
      "Epoch 176/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2727 - mse: 0.2717 - rmse: 0.5213 - mae: 0.2727 - mape: 8.6485\n",
      "Epoch 176: val_loss improved from 0.27221 to 0.27221, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2729 - mse: 0.2722 - rmse: 0.5217 - mae: 0.2729 - mape: 8.6486 - val_loss: 0.2722 - val_mse: 0.2714 - val_rmse: 0.5209 - val_mae: 0.2722 - val_mape: 8.5989 - lr: 1.0000e-05\n",
      "Epoch 177/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2708 - mse: 0.2687 - rmse: 0.5184 - mae: 0.2708 - mape: 8.6025\n",
      "Epoch 177: val_loss improved from 0.27221 to 0.27185, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2727 - mse: 0.2722 - rmse: 0.5218 - mae: 0.2727 - mape: 8.6421 - val_loss: 0.2719 - val_mse: 0.2711 - val_rmse: 0.5207 - val_mae: 0.2719 - val_mape: 8.6284 - lr: 1.0000e-05\n",
      "Epoch 178/1000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2712 - mse: 0.2691 - rmse: 0.5188 - mae: 0.2712 - mape: 8.5976\n",
      "Epoch 178: val_loss did not improve from 0.27185\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2723 - mse: 0.2718 - rmse: 0.5213 - mae: 0.2723 - mape: 8.6322 - val_loss: 0.2720 - val_mse: 0.2709 - val_rmse: 0.5205 - val_mae: 0.2720 - val_mape: 8.6057 - lr: 1.0000e-05\n",
      "Epoch 179/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2715 - mse: 0.2699 - rmse: 0.5195 - mae: 0.2715 - mape: 8.6265\n",
      "Epoch 179: val_loss improved from 0.27185 to 0.27165, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2722 - mse: 0.2720 - rmse: 0.5216 - mae: 0.2722 - mape: 8.6416 - val_loss: 0.2716 - val_mse: 0.2710 - val_rmse: 0.5206 - val_mae: 0.2716 - val_mape: 8.6126 - lr: 1.0000e-05\n",
      "Epoch 180/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2729 - mse: 0.2735 - rmse: 0.5229 - mae: 0.2729 - mape: 8.6431\n",
      "Epoch 180: val_loss did not improve from 0.27165\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2724 - mse: 0.2718 - rmse: 0.5213 - mae: 0.2724 - mape: 8.6414 - val_loss: 0.2719 - val_mse: 0.2704 - val_rmse: 0.5200 - val_mae: 0.2719 - val_mape: 8.6062 - lr: 1.0000e-05\n",
      "Epoch 181/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2728 - mse: 0.2731 - rmse: 0.5226 - mae: 0.2728 - mape: 8.6532\n",
      "Epoch 181: val_loss did not improve from 0.27165\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2726 - mse: 0.2722 - rmse: 0.5217 - mae: 0.2726 - mape: 8.6527 - val_loss: 0.2722 - val_mse: 0.2717 - val_rmse: 0.5212 - val_mae: 0.2722 - val_mape: 8.5901 - lr: 1.0000e-05\n",
      "Epoch 182/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2711 - mse: 0.2695 - rmse: 0.5191 - mae: 0.2711 - mape: 8.6028\n",
      "Epoch 182: val_loss improved from 0.27165 to 0.27145, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2720 - mse: 0.2718 - rmse: 0.5214 - mae: 0.2720 - mape: 8.6278 - val_loss: 0.2714 - val_mse: 0.2710 - val_rmse: 0.5206 - val_mae: 0.2714 - val_mape: 8.6157 - lr: 1.0000e-05\n",
      "Epoch 183/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2727 - mse: 0.2733 - rmse: 0.5228 - mae: 0.2727 - mape: 8.6499\n",
      "Epoch 183: val_loss improved from 0.27145 to 0.27129, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2721 - mse: 0.2720 - rmse: 0.5215 - mae: 0.2721 - mape: 8.6255 - val_loss: 0.2713 - val_mse: 0.2710 - val_rmse: 0.5206 - val_mae: 0.2713 - val_mape: 8.6192 - lr: 1.0000e-05\n",
      "Epoch 184/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2708 - mse: 0.2701 - rmse: 0.5197 - mae: 0.2708 - mape: 8.6119\n",
      "Epoch 184: val_loss did not improve from 0.27129\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2718 - mse: 0.2716 - rmse: 0.5212 - mae: 0.2718 - mape: 8.6325 - val_loss: 0.2715 - val_mse: 0.2710 - val_rmse: 0.5206 - val_mae: 0.2715 - val_mape: 8.5987 - lr: 1.0000e-05\n",
      "Epoch 185/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2720 - mse: 0.2728 - rmse: 0.5223 - mae: 0.2720 - mape: 8.6343\n",
      "Epoch 185: val_loss improved from 0.27129 to 0.27099, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2718 - mse: 0.2722 - rmse: 0.5217 - mae: 0.2718 - mape: 8.6328 - val_loss: 0.2710 - val_mse: 0.2706 - val_rmse: 0.5202 - val_mae: 0.2710 - val_mape: 8.5979 - lr: 1.0000e-05\n",
      "Epoch 186/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2711 - mse: 0.2708 - rmse: 0.5204 - mae: 0.2711 - mape: 8.6030\n",
      "Epoch 186: val_loss did not improve from 0.27099\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2716 - mse: 0.2718 - rmse: 0.5213 - mae: 0.2716 - mape: 8.6156 - val_loss: 0.2711 - val_mse: 0.2702 - val_rmse: 0.5198 - val_mae: 0.2711 - val_mape: 8.5867 - lr: 1.0000e-05\n",
      "Epoch 187/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2701 - mse: 0.2700 - rmse: 0.5196 - mae: 0.2701 - mape: 8.5629\n",
      "Epoch 187: val_loss did not improve from 0.27099\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2714 - mse: 0.2715 - rmse: 0.5211 - mae: 0.2714 - mape: 8.6091 - val_loss: 0.2712 - val_mse: 0.2704 - val_rmse: 0.5200 - val_mae: 0.2712 - val_mape: 8.5952 - lr: 1.0000e-05\n",
      "Epoch 188/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2719 - mse: 0.2717 - rmse: 0.5212 - mae: 0.2719 - mape: 8.6144\n",
      "Epoch 188: val_loss improved from 0.27099 to 0.27074, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2716 - mse: 0.2712 - rmse: 0.5208 - mae: 0.2716 - mape: 8.6156 - val_loss: 0.2707 - val_mse: 0.2700 - val_rmse: 0.5196 - val_mae: 0.2707 - val_mape: 8.5683 - lr: 1.0000e-05\n",
      "Epoch 189/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2698 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2698 - mape: 8.5534\n",
      "Epoch 189: val_loss did not improve from 0.27074\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2712 - mse: 0.2714 - rmse: 0.5210 - mae: 0.2712 - mape: 8.6046 - val_loss: 0.2712 - val_mse: 0.2715 - val_rmse: 0.5210 - val_mae: 0.2712 - val_mape: 8.6453 - lr: 1.0000e-05\n",
      "Epoch 190/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2714 - mse: 0.2715 - rmse: 0.5211 - mae: 0.2714 - mape: 8.6127\n",
      "Epoch 190: val_loss did not improve from 0.27074\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2712 - mse: 0.2714 - rmse: 0.5210 - mae: 0.2712 - mape: 8.6176 - val_loss: 0.2708 - val_mse: 0.2700 - val_rmse: 0.5196 - val_mae: 0.2708 - val_mape: 8.5776 - lr: 1.0000e-05\n",
      "Epoch 191/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2718 - mse: 0.2731 - rmse: 0.5226 - mae: 0.2718 - mape: 8.6446\n",
      "Epoch 191: val_loss did not improve from 0.27074\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2716 - mse: 0.2712 - rmse: 0.5208 - mae: 0.2716 - mape: 8.6264 - val_loss: 0.2711 - val_mse: 0.2700 - val_rmse: 0.5196 - val_mae: 0.2711 - val_mape: 8.5700 - lr: 1.0000e-05\n",
      "Epoch 192/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2714 - mse: 0.2714 - rmse: 0.5210 - mae: 0.2714 - mape: 8.6092\n",
      "Epoch 192: val_loss did not improve from 0.27074\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2714 - mse: 0.2711 - rmse: 0.5206 - mae: 0.2714 - mape: 8.6109 - val_loss: 0.2710 - val_mse: 0.2713 - val_rmse: 0.5209 - val_mae: 0.2710 - val_mape: 8.6237 - lr: 1.0000e-05\n",
      "Epoch 193/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2700 - mse: 0.2690 - rmse: 0.5186 - mae: 0.2700 - mape: 8.5637\n",
      "Epoch 193: val_loss improved from 0.27074 to 0.27048, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2709 - mse: 0.2704 - rmse: 0.5200 - mae: 0.2709 - mape: 8.6002 - val_loss: 0.2705 - val_mse: 0.2699 - val_rmse: 0.5195 - val_mae: 0.2705 - val_mape: 8.5603 - lr: 1.0000e-05\n",
      "Epoch 194/1000\n",
      "255/318 [=======================>......] - ETA: 0s - loss: 0.2691 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2691 - mape: 8.5019\n",
      "Epoch 194: val_loss improved from 0.27048 to 0.27027, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2709 - mse: 0.2709 - rmse: 0.5205 - mae: 0.2709 - mape: 8.6004 - val_loss: 0.2703 - val_mse: 0.2701 - val_rmse: 0.5197 - val_mae: 0.2703 - val_mape: 8.5728 - lr: 1.0000e-05\n",
      "Epoch 195/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2714 - mse: 0.2723 - rmse: 0.5218 - mae: 0.2714 - mape: 8.6166\n",
      "Epoch 195: val_loss did not improve from 0.27027\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2705 - mse: 0.2706 - rmse: 0.5202 - mae: 0.2705 - mape: 8.5986 - val_loss: 0.2733 - val_mse: 0.2747 - val_rmse: 0.5241 - val_mae: 0.2733 - val_mape: 8.5835 - lr: 1.0000e-05\n",
      "Epoch 196/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2707 - mse: 0.2699 - rmse: 0.5196 - mae: 0.2707 - mape: 8.5886\n",
      "Epoch 196: val_loss did not improve from 0.27027\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2710 - mse: 0.2707 - rmse: 0.5203 - mae: 0.2710 - mape: 8.6021 - val_loss: 0.2705 - val_mse: 0.2702 - val_rmse: 0.5198 - val_mae: 0.2705 - val_mape: 8.6250 - lr: 1.0000e-05\n",
      "Epoch 197/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2707 - mse: 0.2701 - rmse: 0.5197 - mae: 0.2707 - mape: 8.6048\n",
      "Epoch 197: val_loss improved from 0.27027 to 0.27013, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2707 - mse: 0.2701 - rmse: 0.5197 - mae: 0.2707 - mape: 8.5990 - val_loss: 0.2701 - val_mse: 0.2695 - val_rmse: 0.5191 - val_mae: 0.2701 - val_mape: 8.6206 - lr: 1.0000e-05\n",
      "Epoch 198/1000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2698 - mse: 0.2682 - rmse: 0.5179 - mae: 0.2698 - mape: 8.5663\n",
      "Epoch 198: val_loss did not improve from 0.27013\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2703 - mse: 0.2699 - rmse: 0.5195 - mae: 0.2703 - mape: 8.5825 - val_loss: 0.2703 - val_mse: 0.2710 - val_rmse: 0.5206 - val_mae: 0.2703 - val_mape: 8.6219 - lr: 1.0000e-05\n",
      "Epoch 199/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2706 - mse: 0.2710 - rmse: 0.5206 - mae: 0.2706 - mape: 8.6164\n",
      "Epoch 199: val_loss improved from 0.27013 to 0.27001, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2707 - mse: 0.2713 - rmse: 0.5209 - mae: 0.2707 - mape: 8.6034 - val_loss: 0.2700 - val_mse: 0.2701 - val_rmse: 0.5197 - val_mae: 0.2700 - val_mape: 8.5913 - lr: 1.0000e-05\n",
      "Epoch 200/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2704 - mse: 0.2694 - rmse: 0.5190 - mae: 0.2704 - mape: 8.5663\n",
      "Epoch 200: val_loss improved from 0.27001 to 0.26981, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2705 - mse: 0.2703 - rmse: 0.5199 - mae: 0.2705 - mape: 8.5938 - val_loss: 0.2698 - val_mse: 0.2692 - val_rmse: 0.5188 - val_mae: 0.2698 - val_mape: 8.5587 - lr: 1.0000e-05\n",
      "Epoch 201/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2711 - mse: 0.2709 - rmse: 0.5204 - mae: 0.2711 - mape: 8.6032\n",
      "Epoch 201: val_loss did not improve from 0.26981\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2704 - mse: 0.2704 - rmse: 0.5200 - mae: 0.2704 - mape: 8.5900 - val_loss: 0.2698 - val_mse: 0.2701 - val_rmse: 0.5197 - val_mae: 0.2698 - val_mape: 8.5916 - lr: 1.0000e-05\n",
      "Epoch 202/1000\n",
      "262/318 [=======================>......] - ETA: 0s - loss: 0.2713 - mse: 0.2723 - rmse: 0.5218 - mae: 0.2713 - mape: 8.5815\n",
      "Epoch 202: val_loss did not improve from 0.26981\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2701 - mse: 0.2701 - rmse: 0.5197 - mae: 0.2701 - mape: 8.5857 - val_loss: 0.2701 - val_mse: 0.2698 - val_rmse: 0.5195 - val_mae: 0.2701 - val_mape: 8.5484 - lr: 1.0000e-05\n",
      "Epoch 203/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2706 - mse: 0.2716 - rmse: 0.5211 - mae: 0.2706 - mape: 8.5958\n",
      "Epoch 203: val_loss did not improve from 0.26981\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2704 - mse: 0.2701 - rmse: 0.5197 - mae: 0.2704 - mape: 8.5798 - val_loss: 0.2698 - val_mse: 0.2703 - val_rmse: 0.5199 - val_mae: 0.2698 - val_mape: 8.5990 - lr: 1.0000e-05\n",
      "Epoch 204/1000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2709 - mse: 0.2728 - rmse: 0.5223 - mae: 0.2709 - mape: 8.6159\n",
      "Epoch 204: val_loss did not improve from 0.26981\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2700 - mse: 0.2706 - rmse: 0.5202 - mae: 0.2700 - mape: 8.5836 - val_loss: 0.2700 - val_mse: 0.2699 - val_rmse: 0.5195 - val_mae: 0.2700 - val_mape: 8.5702 - lr: 1.0000e-05\n",
      "Epoch 205/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2689 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2689 - mape: 8.5435\n",
      "Epoch 205: val_loss improved from 0.26981 to 0.26957, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2702 - mse: 0.2699 - rmse: 0.5195 - mae: 0.2702 - mape: 8.5814 - val_loss: 0.2696 - val_mse: 0.2703 - val_rmse: 0.5199 - val_mae: 0.2696 - val_mape: 8.5681 - lr: 1.0000e-05\n",
      "Epoch 206/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2712 - mse: 0.2712 - rmse: 0.5207 - mae: 0.2712 - mape: 8.6213\n",
      "Epoch 206: val_loss did not improve from 0.26957\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2700 - mse: 0.2698 - rmse: 0.5194 - mae: 0.2700 - mape: 8.5813 - val_loss: 0.2697 - val_mse: 0.2705 - val_rmse: 0.5201 - val_mae: 0.2697 - val_mape: 8.5870 - lr: 1.0000e-05\n",
      "Epoch 207/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2708 - mse: 0.2743 - rmse: 0.5237 - mae: 0.2708 - mape: 8.6073\n",
      "Epoch 207: val_loss improved from 0.26957 to 0.26916, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2699 - mse: 0.2700 - rmse: 0.5196 - mae: 0.2699 - mape: 8.5742 - val_loss: 0.2692 - val_mse: 0.2692 - val_rmse: 0.5188 - val_mae: 0.2692 - val_mape: 8.5481 - lr: 1.0000e-05\n",
      "Epoch 208/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2701 - mse: 0.2706 - rmse: 0.5202 - mae: 0.2701 - mape: 8.5699\n",
      "Epoch 208: val_loss did not improve from 0.26916\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2698 - mse: 0.2704 - rmse: 0.5200 - mae: 0.2698 - mape: 8.5762 - val_loss: 0.2692 - val_mse: 0.2692 - val_rmse: 0.5189 - val_mae: 0.2692 - val_mape: 8.5742 - lr: 1.0000e-05\n",
      "Epoch 209/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2707 - mse: 0.2729 - rmse: 0.5224 - mae: 0.2707 - mape: 8.6117\n",
      "Epoch 209: val_loss did not improve from 0.26916\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2698 - mse: 0.2703 - rmse: 0.5199 - mae: 0.2698 - mape: 8.5773 - val_loss: 0.2703 - val_mse: 0.2709 - val_rmse: 0.5205 - val_mae: 0.2703 - val_mape: 8.5185 - lr: 1.0000e-05\n",
      "Epoch 210/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2692 - mse: 0.2691 - rmse: 0.5188 - mae: 0.2692 - mape: 8.5698\n",
      "Epoch 210: val_loss did not improve from 0.26916\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2697 - mse: 0.2700 - rmse: 0.5196 - mae: 0.2697 - mape: 8.5742 - val_loss: 0.2694 - val_mse: 0.2700 - val_rmse: 0.5196 - val_mae: 0.2694 - val_mape: 8.5997 - lr: 1.0000e-05\n",
      "Epoch 211/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2695 - mse: 0.2695 - rmse: 0.5192 - mae: 0.2695 - mape: 8.5677\n",
      "Epoch 211: val_loss did not improve from 0.26916\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2698 - mse: 0.2705 - rmse: 0.5201 - mae: 0.2698 - mape: 8.5794 - val_loss: 0.2693 - val_mse: 0.2688 - val_rmse: 0.5185 - val_mae: 0.2693 - val_mape: 8.5314 - lr: 1.0000e-05\n",
      "Epoch 212/1000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2697 - mse: 0.2687 - rmse: 0.5183 - mae: 0.2697 - mape: 8.5842\n",
      "Epoch 212: val_loss improved from 0.26916 to 0.26892, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2694 - mse: 0.2692 - rmse: 0.5189 - mae: 0.2694 - mape: 8.5621 - val_loss: 0.2689 - val_mse: 0.2689 - val_rmse: 0.5186 - val_mae: 0.2689 - val_mape: 8.5396 - lr: 1.0000e-05\n",
      "Epoch 213/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2698 - mse: 0.2705 - rmse: 0.5201 - mae: 0.2698 - mape: 8.5718\n",
      "Epoch 213: val_loss improved from 0.26892 to 0.26880, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2696 - mse: 0.2702 - rmse: 0.5198 - mae: 0.2696 - mape: 8.5714 - val_loss: 0.2688 - val_mse: 0.2684 - val_rmse: 0.5180 - val_mae: 0.2688 - val_mape: 8.5413 - lr: 1.0000e-05\n",
      "Epoch 214/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2686 - mse: 0.2695 - rmse: 0.5191 - mae: 0.2686 - mape: 8.5576\n",
      "Epoch 214: val_loss did not improve from 0.26880\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2690 - mse: 0.2695 - rmse: 0.5191 - mae: 0.2690 - mape: 8.5636 - val_loss: 0.2699 - val_mse: 0.2701 - val_rmse: 0.5197 - val_mae: 0.2699 - val_mape: 8.5249 - lr: 1.0000e-05\n",
      "Epoch 215/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2687 - mse: 0.2679 - rmse: 0.5176 - mae: 0.2687 - mape: 8.5153\n",
      "Epoch 215: val_loss improved from 0.26880 to 0.26869, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2693 - mse: 0.2693 - rmse: 0.5190 - mae: 0.2693 - mape: 8.5514 - val_loss: 0.2687 - val_mse: 0.2687 - val_rmse: 0.5183 - val_mae: 0.2687 - val_mape: 8.5526 - lr: 1.0000e-05\n",
      "Epoch 216/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2690 - mse: 0.2694 - rmse: 0.5190 - mae: 0.2690 - mape: 8.5504\n",
      "Epoch 216: val_loss did not improve from 0.26869\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2695 - mse: 0.2697 - rmse: 0.5193 - mae: 0.2695 - mape: 8.5682 - val_loss: 0.2692 - val_mse: 0.2694 - val_rmse: 0.5190 - val_mae: 0.2692 - val_mape: 8.6184 - lr: 1.0000e-05\n",
      "Epoch 217/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2697 - mse: 0.2710 - rmse: 0.5206 - mae: 0.2697 - mape: 8.6039\n",
      "Epoch 217: val_loss did not improve from 0.26869\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2690 - mse: 0.2692 - rmse: 0.5189 - mae: 0.2690 - mape: 8.5679 - val_loss: 0.2688 - val_mse: 0.2693 - val_rmse: 0.5189 - val_mae: 0.2688 - val_mape: 8.5312 - lr: 1.0000e-05\n",
      "Epoch 218/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2690 - mse: 0.2702 - rmse: 0.5198 - mae: 0.2690 - mape: 8.5674\n",
      "Epoch 218: val_loss did not improve from 0.26869\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2692 - mse: 0.2699 - rmse: 0.5196 - mae: 0.2692 - mape: 8.5639 - val_loss: 0.2688 - val_mse: 0.2698 - val_rmse: 0.5194 - val_mae: 0.2688 - val_mape: 8.5719 - lr: 1.0000e-05\n",
      "Epoch 219/1000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2694 - mse: 0.2701 - rmse: 0.5197 - mae: 0.2694 - mape: 8.5824\n",
      "Epoch 219: val_loss improved from 0.26869 to 0.26843, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2691 - mse: 0.2690 - rmse: 0.5187 - mae: 0.2691 - mape: 8.5570 - val_loss: 0.2684 - val_mse: 0.2685 - val_rmse: 0.5181 - val_mae: 0.2684 - val_mape: 8.5382 - lr: 1.0000e-05\n",
      "Epoch 220/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2671 - mse: 0.2661 - rmse: 0.5159 - mae: 0.2671 - mape: 8.4637\n",
      "Epoch 220: val_loss did not improve from 0.26843\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2689 - mse: 0.2689 - rmse: 0.5186 - mae: 0.2689 - mape: 8.5457 - val_loss: 0.2685 - val_mse: 0.2688 - val_rmse: 0.5184 - val_mae: 0.2685 - val_mape: 8.5575 - lr: 1.0000e-05\n",
      "Epoch 221/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2689 - mse: 0.2708 - rmse: 0.5204 - mae: 0.2689 - mape: 8.5521\n",
      "Epoch 221: val_loss did not improve from 0.26843\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2688 - mse: 0.2692 - rmse: 0.5188 - mae: 0.2688 - mape: 8.5608 - val_loss: 0.2693 - val_mse: 0.2690 - val_rmse: 0.5187 - val_mae: 0.2693 - val_mape: 8.5131 - lr: 1.0000e-05\n",
      "Epoch 222/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2697 - mse: 0.2704 - rmse: 0.5200 - mae: 0.2697 - mape: 8.5646\n",
      "Epoch 222: val_loss did not improve from 0.26843\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2687 - mse: 0.2686 - rmse: 0.5182 - mae: 0.2687 - mape: 8.5413 - val_loss: 0.2685 - val_mse: 0.2691 - val_rmse: 0.5188 - val_mae: 0.2685 - val_mape: 8.5767 - lr: 1.0000e-05\n",
      "Epoch 223/1000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2692 - mse: 0.2708 - rmse: 0.5204 - mae: 0.2692 - mape: 8.5761\n",
      "Epoch 223: val_loss improved from 0.26843 to 0.26818, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2688 - mse: 0.2692 - rmse: 0.5188 - mae: 0.2688 - mape: 8.5429 - val_loss: 0.2682 - val_mse: 0.2682 - val_rmse: 0.5179 - val_mae: 0.2682 - val_mape: 8.5439 - lr: 1.0000e-05\n",
      "Epoch 224/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2689 - mse: 0.2704 - rmse: 0.5200 - mae: 0.2689 - mape: 8.5510\n",
      "Epoch 224: val_loss did not improve from 0.26818\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2688 - mse: 0.2699 - rmse: 0.5195 - mae: 0.2688 - mape: 8.5556 - val_loss: 0.2693 - val_mse: 0.2696 - val_rmse: 0.5193 - val_mae: 0.2693 - val_mape: 8.5159 - lr: 1.0000e-05\n",
      "Epoch 225/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2684 - mse: 0.2687 - rmse: 0.5183 - mae: 0.2684 - mape: 8.5498\n",
      "Epoch 225: val_loss did not improve from 0.26818\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2687 - mse: 0.2692 - rmse: 0.5188 - mae: 0.2687 - mape: 8.5528 - val_loss: 0.2686 - val_mse: 0.2688 - val_rmse: 0.5185 - val_mae: 0.2686 - val_mape: 8.5835 - lr: 1.0000e-05\n",
      "Epoch 226/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2697 - mse: 0.2723 - rmse: 0.5218 - mae: 0.2697 - mape: 8.5819\n",
      "Epoch 226: val_loss did not improve from 0.26818\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2687 - mse: 0.2689 - rmse: 0.5185 - mae: 0.2687 - mape: 8.5500 - val_loss: 0.2684 - val_mse: 0.2687 - val_rmse: 0.5184 - val_mae: 0.2684 - val_mape: 8.5851 - lr: 1.0000e-05\n",
      "Epoch 227/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2674 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2674 - mape: 8.5090\n",
      "Epoch 227: val_loss did not improve from 0.26818\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2687 - mse: 0.2685 - rmse: 0.5182 - mae: 0.2687 - mape: 8.5416 - val_loss: 0.2684 - val_mse: 0.2679 - val_rmse: 0.5176 - val_mae: 0.2684 - val_mape: 8.5386 - lr: 1.0000e-05\n",
      "Epoch 228/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2681 - mse: 0.2688 - rmse: 0.5184 - mae: 0.2681 - mape: 8.5294\n",
      "Epoch 228: val_loss improved from 0.26818 to 0.26805, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2684 - mse: 0.2688 - rmse: 0.5184 - mae: 0.2684 - mape: 8.5462 - val_loss: 0.2680 - val_mse: 0.2689 - val_rmse: 0.5185 - val_mae: 0.2680 - val_mape: 8.5439 - lr: 1.0000e-05\n",
      "Epoch 229/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2681 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2681 - mape: 8.5160\n",
      "Epoch 229: val_loss improved from 0.26805 to 0.26790, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2685 - mse: 0.2684 - rmse: 0.5181 - mae: 0.2685 - mape: 8.5489 - val_loss: 0.2679 - val_mse: 0.2679 - val_rmse: 0.5176 - val_mae: 0.2679 - val_mape: 8.5046 - lr: 1.0000e-05\n",
      "Epoch 230/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2685 - mse: 0.2692 - rmse: 0.5188 - mae: 0.2685 - mape: 8.5544\n",
      "Epoch 230: val_loss did not improve from 0.26790\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2682 - mse: 0.2684 - rmse: 0.5180 - mae: 0.2682 - mape: 8.5426 - val_loss: 0.2702 - val_mse: 0.2710 - val_rmse: 0.5206 - val_mae: 0.2702 - val_mape: 8.5249 - lr: 1.0000e-05\n",
      "Epoch 231/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2667 - mse: 0.2659 - rmse: 0.5157 - mae: 0.2667 - mape: 8.4891\n",
      "Epoch 231: val_loss did not improve from 0.26790\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2683 - mse: 0.2683 - rmse: 0.5179 - mae: 0.2683 - mape: 8.5299 - val_loss: 0.2680 - val_mse: 0.2684 - val_rmse: 0.5181 - val_mae: 0.2680 - val_mape: 8.5584 - lr: 1.0000e-05\n",
      "Epoch 232/1000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2685 - mse: 0.2690 - rmse: 0.5187 - mae: 0.2685 - mape: 8.5445\n",
      "Epoch 232: val_loss improved from 0.26790 to 0.26781, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2686 - mse: 0.2700 - rmse: 0.5196 - mae: 0.2686 - mape: 8.5369 - val_loss: 0.2678 - val_mse: 0.2680 - val_rmse: 0.5177 - val_mae: 0.2678 - val_mape: 8.5517 - lr: 1.0000e-05\n",
      "Epoch 233/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2697 - mse: 0.2722 - rmse: 0.5218 - mae: 0.2697 - mape: 8.5665\n",
      "Epoch 233: val_loss improved from 0.26781 to 0.26756, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2683 - mse: 0.2690 - rmse: 0.5186 - mae: 0.2683 - mape: 8.5362 - val_loss: 0.2676 - val_mse: 0.2680 - val_rmse: 0.5177 - val_mae: 0.2676 - val_mape: 8.5324 - lr: 1.0000e-05\n",
      "Epoch 234/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2691 - mse: 0.2708 - rmse: 0.5204 - mae: 0.2691 - mape: 8.5789\n",
      "Epoch 234: val_loss did not improve from 0.26756\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2683 - mse: 0.2693 - rmse: 0.5190 - mae: 0.2683 - mape: 8.5482 - val_loss: 0.2690 - val_mse: 0.2684 - val_rmse: 0.5181 - val_mae: 0.2690 - val_mape: 8.5224 - lr: 1.0000e-05\n",
      "Epoch 235/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2668 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2668 - mape: 8.5070\n",
      "Epoch 235: val_loss did not improve from 0.26756\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2681 - mse: 0.2685 - rmse: 0.5181 - mae: 0.2681 - mape: 8.5419 - val_loss: 0.2676 - val_mse: 0.2680 - val_rmse: 0.5177 - val_mae: 0.2676 - val_mape: 8.5334 - lr: 1.0000e-05\n",
      "Epoch 236/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2684 - mse: 0.2696 - rmse: 0.5193 - mae: 0.2684 - mape: 8.5662\n",
      "Epoch 236: val_loss improved from 0.26756 to 0.26748, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2681 - mse: 0.2684 - rmse: 0.5181 - mae: 0.2681 - mape: 8.5312 - val_loss: 0.2675 - val_mse: 0.2672 - val_rmse: 0.5170 - val_mae: 0.2675 - val_mape: 8.5312 - lr: 1.0000e-05\n",
      "Epoch 237/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2684 - mse: 0.2697 - rmse: 0.5193 - mae: 0.2684 - mape: 8.5401\n",
      "Epoch 237: val_loss improved from 0.26748 to 0.26742, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2682 - mse: 0.2690 - rmse: 0.5186 - mae: 0.2682 - mape: 8.5408 - val_loss: 0.2674 - val_mse: 0.2678 - val_rmse: 0.5175 - val_mae: 0.2674 - val_mape: 8.5081 - lr: 1.0000e-05\n",
      "Epoch 238/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2673 - mse: 0.2661 - rmse: 0.5158 - mae: 0.2673 - mape: 8.5029\n",
      "Epoch 238: val_loss improved from 0.26742 to 0.26724, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2680 - mse: 0.2684 - rmse: 0.5180 - mae: 0.2680 - mape: 8.5252 - val_loss: 0.2672 - val_mse: 0.2670 - val_rmse: 0.5168 - val_mae: 0.2672 - val_mape: 8.4999 - lr: 1.0000e-05\n",
      "Epoch 239/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2690 - mse: 0.2701 - rmse: 0.5197 - mae: 0.2690 - mape: 8.5659\n",
      "Epoch 239: val_loss did not improve from 0.26724\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2678 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2678 - mape: 8.5219 - val_loss: 0.2673 - val_mse: 0.2674 - val_rmse: 0.5171 - val_mae: 0.2673 - val_mape: 8.4996 - lr: 1.0000e-05\n",
      "Epoch 240/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2688 - mse: 0.2706 - rmse: 0.5202 - mae: 0.2688 - mape: 8.5678\n",
      "Epoch 240: val_loss did not improve from 0.26724\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2678 - mse: 0.2686 - rmse: 0.5182 - mae: 0.2678 - mape: 8.5256 - val_loss: 0.2673 - val_mse: 0.2675 - val_rmse: 0.5172 - val_mae: 0.2673 - val_mape: 8.4934 - lr: 1.0000e-05\n",
      "Epoch 241/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2675 - mse: 0.2665 - rmse: 0.5162 - mae: 0.2675 - mape: 8.5123\n",
      "Epoch 241: val_loss did not improve from 0.26724\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2677 - mse: 0.2679 - rmse: 0.5176 - mae: 0.2677 - mape: 8.5212 - val_loss: 0.2682 - val_mse: 0.2684 - val_rmse: 0.5181 - val_mae: 0.2682 - val_mape: 8.4749 - lr: 1.0000e-05\n",
      "Epoch 242/1000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2674 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2674 - mape: 8.5111\n",
      "Epoch 242: val_loss improved from 0.26724 to 0.26702, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2677 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2677 - mape: 8.5101 - val_loss: 0.2670 - val_mse: 0.2669 - val_rmse: 0.5167 - val_mae: 0.2670 - val_mape: 8.4965 - lr: 1.0000e-05\n",
      "Epoch 243/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2677 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2677 - mape: 8.5149\n",
      "Epoch 243: val_loss did not improve from 0.26702\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2678 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2678 - mape: 8.5212 - val_loss: 0.2676 - val_mse: 0.2680 - val_rmse: 0.5177 - val_mae: 0.2676 - val_mape: 8.5478 - lr: 1.0000e-05\n",
      "Epoch 244/1000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2663 - mse: 0.2648 - rmse: 0.5146 - mae: 0.2663 - mape: 8.4785\n",
      "Epoch 244: val_loss did not improve from 0.26702\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2673 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2673 - mape: 8.5072 - val_loss: 0.2671 - val_mse: 0.2676 - val_rmse: 0.5173 - val_mae: 0.2671 - val_mape: 8.5053 - lr: 1.0000e-05\n",
      "Epoch 245/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2693 - mse: 0.2703 - rmse: 0.5199 - mae: 0.2693 - mape: 8.5663\n",
      "Epoch 245: val_loss did not improve from 0.26702\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2675 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2675 - mape: 8.5102 - val_loss: 0.2672 - val_mse: 0.2678 - val_rmse: 0.5175 - val_mae: 0.2672 - val_mape: 8.5348 - lr: 1.0000e-05\n",
      "Epoch 246/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2665 - mse: 0.2654 - rmse: 0.5151 - mae: 0.2665 - mape: 8.4754\n",
      "Epoch 246: val_loss did not improve from 0.26702\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2676 - mse: 0.2687 - rmse: 0.5183 - mae: 0.2676 - mape: 8.5225 - val_loss: 0.2671 - val_mse: 0.2668 - val_rmse: 0.5165 - val_mae: 0.2671 - val_mape: 8.4823 - lr: 1.0000e-05\n",
      "Epoch 247/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2685 - mse: 0.2699 - rmse: 0.5196 - mae: 0.2685 - mape: 8.5431\n",
      "Epoch 247: val_loss improved from 0.26702 to 0.26698, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2677 - mse: 0.2678 - rmse: 0.5174 - mae: 0.2677 - mape: 8.5259 - val_loss: 0.2670 - val_mse: 0.2670 - val_rmse: 0.5168 - val_mae: 0.2670 - val_mape: 8.5362 - lr: 1.0000e-05\n",
      "Epoch 248/1000\n",
      "271/318 [========================>.....] - ETA: 0s - loss: 0.2679 - mse: 0.2691 - rmse: 0.5188 - mae: 0.2679 - mape: 8.5316\n",
      "Epoch 248: val_loss improved from 0.26698 to 0.26688, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2674 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2674 - mape: 8.5139 - val_loss: 0.2669 - val_mse: 0.2673 - val_rmse: 0.5170 - val_mae: 0.2669 - val_mape: 8.5296 - lr: 1.0000e-05\n",
      "Epoch 249/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2684 - mse: 0.2703 - rmse: 0.5199 - mae: 0.2684 - mape: 8.5171\n",
      "Epoch 249: val_loss did not improve from 0.26688\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2673 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2673 - mape: 8.5026 - val_loss: 0.2671 - val_mse: 0.2675 - val_rmse: 0.5172 - val_mae: 0.2671 - val_mape: 8.4675 - lr: 1.0000e-05\n",
      "Epoch 250/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2684 - mse: 0.2713 - rmse: 0.5209 - mae: 0.2684 - mape: 8.5493\n",
      "Epoch 250: val_loss did not improve from 0.26688\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2674 - mse: 0.2685 - rmse: 0.5181 - mae: 0.2674 - mape: 8.5143 - val_loss: 0.2673 - val_mse: 0.2672 - val_rmse: 0.5169 - val_mae: 0.2673 - val_mape: 8.5491 - lr: 1.0000e-05\n",
      "Epoch 251/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2677 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2677 - mape: 8.5054\n",
      "Epoch 251: val_loss improved from 0.26688 to 0.26655, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2672 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2672 - mape: 8.5068 - val_loss: 0.2665 - val_mse: 0.2662 - val_rmse: 0.5160 - val_mae: 0.2665 - val_mape: 8.4751 - lr: 1.0000e-05\n",
      "Epoch 252/1000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2688 - mse: 0.2702 - rmse: 0.5198 - mae: 0.2688 - mape: 8.5608\n",
      "Epoch 252: val_loss did not improve from 0.26655\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2675 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2675 - mape: 8.5121 - val_loss: 0.2671 - val_mse: 0.2678 - val_rmse: 0.5175 - val_mae: 0.2671 - val_mape: 8.5224 - lr: 1.0000e-05\n",
      "Epoch 253/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2655 - mse: 0.2649 - rmse: 0.5146 - mae: 0.2655 - mape: 8.4492\n",
      "Epoch 253: val_loss improved from 0.26655 to 0.26654, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2672 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2672 - mape: 8.4987 - val_loss: 0.2665 - val_mse: 0.2668 - val_rmse: 0.5166 - val_mae: 0.2665 - val_mape: 8.5137 - lr: 1.0000e-05\n",
      "Epoch 254/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2684 - mse: 0.2699 - rmse: 0.5195 - mae: 0.2684 - mape: 8.5454\n",
      "Epoch 254: val_loss did not improve from 0.26654\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2671 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2671 - mape: 8.5090 - val_loss: 0.2666 - val_mse: 0.2661 - val_rmse: 0.5159 - val_mae: 0.2666 - val_mape: 8.5189 - lr: 1.0000e-05\n",
      "Epoch 255/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2681 - mse: 0.2695 - rmse: 0.5192 - mae: 0.2681 - mape: 8.5274\n",
      "Epoch 255: val_loss did not improve from 0.26654\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2674 - mse: 0.2683 - rmse: 0.5180 - mae: 0.2674 - mape: 8.5133 - val_loss: 0.2666 - val_mse: 0.2656 - val_rmse: 0.5154 - val_mae: 0.2666 - val_mape: 8.4854 - lr: 1.0000e-05\n",
      "Epoch 256/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2665 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2665 - mape: 8.4947\n",
      "Epoch 256: val_loss improved from 0.26654 to 0.26649, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2669 - mse: 0.2673 - rmse: 0.5171 - mae: 0.2669 - mape: 8.5057 - val_loss: 0.2665 - val_mse: 0.2667 - val_rmse: 0.5164 - val_mae: 0.2665 - val_mape: 8.4595 - lr: 1.0000e-05\n",
      "Epoch 257/1000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2656 - mse: 0.2642 - rmse: 0.5140 - mae: 0.2656 - mape: 8.4817\n",
      "Epoch 257: val_loss improved from 0.26649 to 0.26646, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2669 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2669 - mape: 8.5086 - val_loss: 0.2665 - val_mse: 0.2666 - val_rmse: 0.5164 - val_mae: 0.2665 - val_mape: 8.4984 - lr: 1.0000e-05\n",
      "Epoch 258/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2663 - mse: 0.2655 - rmse: 0.5152 - mae: 0.2663 - mape: 8.4885\n",
      "Epoch 258: val_loss improved from 0.26646 to 0.26609, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2668 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2668 - mape: 8.4951 - val_loss: 0.2661 - val_mse: 0.2664 - val_rmse: 0.5161 - val_mae: 0.2661 - val_mape: 8.4790 - lr: 1.0000e-05\n",
      "Epoch 259/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2650 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2650 - mape: 8.4395\n",
      "Epoch 259: val_loss did not improve from 0.26609\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2668 - mse: 0.2671 - rmse: 0.5169 - mae: 0.2668 - mape: 8.5037 - val_loss: 0.2662 - val_mse: 0.2664 - val_rmse: 0.5161 - val_mae: 0.2662 - val_mape: 8.4837 - lr: 1.0000e-05\n",
      "Epoch 260/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2664 - mse: 0.2666 - rmse: 0.5164 - mae: 0.2664 - mape: 8.4805\n",
      "Epoch 260: val_loss did not improve from 0.26609\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2665 - mape: 8.4983 - val_loss: 0.2663 - val_mse: 0.2668 - val_rmse: 0.5166 - val_mae: 0.2663 - val_mape: 8.4912 - lr: 1.0000e-05\n",
      "Epoch 261/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2670 - mse: 0.2688 - rmse: 0.5185 - mae: 0.2670 - mape: 8.5146\n",
      "Epoch 261: val_loss improved from 0.26609 to 0.26593, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2666 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2666 - mape: 8.4917 - val_loss: 0.2659 - val_mse: 0.2658 - val_rmse: 0.5156 - val_mae: 0.2659 - val_mape: 8.4649 - lr: 1.0000e-05\n",
      "Epoch 262/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2666 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2666 - mape: 8.4906\n",
      "Epoch 262: val_loss did not improve from 0.26593\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2665 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2665 - mape: 8.4859 - val_loss: 0.2660 - val_mse: 0.2653 - val_rmse: 0.5151 - val_mae: 0.2660 - val_mape: 8.4657 - lr: 1.0000e-05\n",
      "Epoch 263/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2673 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2673 - mape: 8.5167\n",
      "Epoch 263: val_loss did not improve from 0.26593\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2666 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2666 - mape: 8.4942 - val_loss: 0.2663 - val_mse: 0.2660 - val_rmse: 0.5158 - val_mae: 0.2663 - val_mape: 8.4625 - lr: 1.0000e-05\n",
      "Epoch 264/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2637 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2637 - mape: 8.3912\n",
      "Epoch 264: val_loss did not improve from 0.26593\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2666 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2666 - mape: 8.4810 - val_loss: 0.2660 - val_mse: 0.2664 - val_rmse: 0.5162 - val_mae: 0.2660 - val_mape: 8.5044 - lr: 1.0000e-05\n",
      "Epoch 265/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2667 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2667 - mape: 8.4766\n",
      "Epoch 265: val_loss improved from 0.26593 to 0.26588, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2666 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2666 - mape: 8.4945 - val_loss: 0.2659 - val_mse: 0.2662 - val_rmse: 0.5160 - val_mae: 0.2659 - val_mape: 8.4861 - lr: 1.0000e-05\n",
      "Epoch 266/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2662 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2662 - mape: 8.4742\n",
      "Epoch 266: val_loss did not improve from 0.26588\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2663 - mse: 0.2670 - rmse: 0.5168 - mae: 0.2663 - mape: 8.4802 - val_loss: 0.2660 - val_mse: 0.2667 - val_rmse: 0.5164 - val_mae: 0.2660 - val_mape: 8.5045 - lr: 1.0000e-05\n",
      "Epoch 267/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2661 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2661 - mape: 8.4801\n",
      "Epoch 267: val_loss improved from 0.26588 to 0.26576, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2662 - mse: 0.2670 - rmse: 0.5168 - mae: 0.2662 - mape: 8.4801 - val_loss: 0.2658 - val_mse: 0.2658 - val_rmse: 0.5156 - val_mae: 0.2658 - val_mape: 8.4706 - lr: 1.0000e-05\n",
      "Epoch 268/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2676 - mse: 0.2698 - rmse: 0.5194 - mae: 0.2676 - mape: 8.4894\n",
      "Epoch 268: val_loss improved from 0.26576 to 0.26572, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2663 - mse: 0.2668 - rmse: 0.5166 - mae: 0.2663 - mape: 8.4786 - val_loss: 0.2657 - val_mse: 0.2668 - val_rmse: 0.5165 - val_mae: 0.2657 - val_mape: 8.4594 - lr: 1.0000e-05\n",
      "Epoch 269/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2662 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2662 - mape: 8.4651\n",
      "Epoch 269: val_loss improved from 0.26572 to 0.26564, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2663 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2663 - mape: 8.4841 - val_loss: 0.2656 - val_mse: 0.2659 - val_rmse: 0.5157 - val_mae: 0.2656 - val_mape: 8.4991 - lr: 1.0000e-05\n",
      "Epoch 270/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2667 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2667 - mape: 8.4792\n",
      "Epoch 270: val_loss improved from 0.26564 to 0.26559, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2663 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2663 - mape: 8.4798 - val_loss: 0.2656 - val_mse: 0.2661 - val_rmse: 0.5158 - val_mae: 0.2656 - val_mape: 8.4401 - lr: 1.0000e-05\n",
      "Epoch 271/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2660 - mse: 0.2669 - rmse: 0.5166 - mae: 0.2660 - mape: 8.4716\n",
      "Epoch 271: val_loss improved from 0.26559 to 0.26551, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2662 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2662 - mape: 8.4826 - val_loss: 0.2655 - val_mse: 0.2664 - val_rmse: 0.5161 - val_mae: 0.2655 - val_mape: 8.4387 - lr: 1.0000e-05\n",
      "Epoch 272/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2664 - mse: 0.2663 - rmse: 0.5160 - mae: 0.2664 - mape: 8.4861\n",
      "Epoch 272: val_loss improved from 0.26551 to 0.26538, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2660 - mse: 0.2665 - rmse: 0.5162 - mae: 0.2660 - mape: 8.4687 - val_loss: 0.2654 - val_mse: 0.2662 - val_rmse: 0.5159 - val_mae: 0.2654 - val_mape: 8.4353 - lr: 1.0000e-05\n",
      "Epoch 273/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2659 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2659 - mape: 8.4744\n",
      "Epoch 273: val_loss did not improve from 0.26538\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2662 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2662 - mape: 8.4782 - val_loss: 0.2661 - val_mse: 0.2679 - val_rmse: 0.5176 - val_mae: 0.2661 - val_mape: 8.5314 - lr: 1.0000e-05\n",
      "Epoch 274/1000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2665 - mse: 0.2687 - rmse: 0.5184 - mae: 0.2665 - mape: 8.4467\n",
      "Epoch 274: val_loss improved from 0.26538 to 0.26536, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2659 - mse: 0.2665 - rmse: 0.5163 - mae: 0.2659 - mape: 8.4662 - val_loss: 0.2654 - val_mse: 0.2666 - val_rmse: 0.5163 - val_mae: 0.2654 - val_mape: 8.4694 - lr: 1.0000e-05\n",
      "Epoch 275/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2659 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2659 - mape: 8.4531\n",
      "Epoch 275: val_loss did not improve from 0.26536\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2659 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2659 - mape: 8.4682 - val_loss: 0.2658 - val_mse: 0.2668 - val_rmse: 0.5165 - val_mae: 0.2658 - val_mape: 8.4378 - lr: 1.0000e-05\n",
      "Epoch 276/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2660 - mse: 0.2689 - rmse: 0.5186 - mae: 0.2660 - mape: 8.4570\n",
      "Epoch 276: val_loss did not improve from 0.26536\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2656 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2656 - mape: 8.4511 - val_loss: 0.2663 - val_mse: 0.2667 - val_rmse: 0.5164 - val_mae: 0.2663 - val_mape: 8.4378 - lr: 1.0000e-05\n",
      "Epoch 277/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2664 - mse: 0.2695 - rmse: 0.5191 - mae: 0.2664 - mape: 8.4724\n",
      "Epoch 277: val_loss did not improve from 0.26536\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2659 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2659 - mape: 8.4613 - val_loss: 0.2654 - val_mse: 0.2669 - val_rmse: 0.5166 - val_mae: 0.2654 - val_mape: 8.4782 - lr: 1.0000e-05\n",
      "Epoch 278/1000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2648 - mse: 0.2633 - rmse: 0.5131 - mae: 0.2648 - mape: 8.4079\n",
      "Epoch 278: val_loss improved from 0.26536 to 0.26498, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2654 - mse: 0.2663 - rmse: 0.5161 - mae: 0.2654 - mape: 8.4458 - val_loss: 0.2650 - val_mse: 0.2659 - val_rmse: 0.5157 - val_mae: 0.2650 - val_mape: 8.4322 - lr: 1.0000e-05\n",
      "Epoch 279/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2664 - mse: 0.2689 - rmse: 0.5185 - mae: 0.2664 - mape: 8.4953\n",
      "Epoch 279: val_loss did not improve from 0.26498\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2654 - mse: 0.2670 - rmse: 0.5168 - mae: 0.2654 - mape: 8.4451 - val_loss: 0.2658 - val_mse: 0.2670 - val_rmse: 0.5167 - val_mae: 0.2658 - val_mape: 8.4691 - lr: 1.0000e-05\n",
      "Epoch 280/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2666 - mse: 0.2688 - rmse: 0.5184 - mae: 0.2666 - mape: 8.4689\n",
      "Epoch 280: val_loss did not improve from 0.26498\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2653 - mse: 0.2667 - rmse: 0.5165 - mae: 0.2653 - mape: 8.4431 - val_loss: 0.2671 - val_mse: 0.2709 - val_rmse: 0.5205 - val_mae: 0.2671 - val_mape: 8.5639 - lr: 1.0000e-05\n",
      "Epoch 281/1000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2645 - mse: 0.2658 - rmse: 0.5156 - mae: 0.2645 - mape: 8.4220\n",
      "Epoch 281: val_loss did not improve from 0.26498\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2657 - mse: 0.2686 - rmse: 0.5182 - mae: 0.2657 - mape: 8.4589 - val_loss: 0.2658 - val_mse: 0.2695 - val_rmse: 0.5191 - val_mae: 0.2658 - val_mape: 8.5211 - lr: 1.0000e-05\n",
      "Epoch 282/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2662 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2662 - mape: 8.4835\n",
      "Epoch 282: val_loss improved from 0.26498 to 0.26495, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2660 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2660 - mape: 8.4738 - val_loss: 0.2650 - val_mse: 0.2669 - val_rmse: 0.5166 - val_mae: 0.2650 - val_mape: 8.4718 - lr: 1.0000e-05\n",
      "Epoch 283/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2661 - mse: 0.2691 - rmse: 0.5188 - mae: 0.2661 - mape: 8.4758\n",
      "Epoch 283: val_loss improved from 0.26495 to 0.26460, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2653 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2653 - mape: 8.4562 - val_loss: 0.2646 - val_mse: 0.2660 - val_rmse: 0.5158 - val_mae: 0.2646 - val_mape: 8.4169 - lr: 1.0000e-05\n",
      "Epoch 284/1000\n",
      "261/318 [=======================>......] - ETA: 0s - loss: 0.2644 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2644 - mape: 8.4156\n",
      "Epoch 284: val_loss did not improve from 0.26460\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2653 - mse: 0.2677 - rmse: 0.5174 - mae: 0.2653 - mape: 8.4521 - val_loss: 0.2656 - val_mse: 0.2678 - val_rmse: 0.5175 - val_mae: 0.2656 - val_mape: 8.4054 - lr: 1.0000e-05\n",
      "Epoch 285/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2652 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2652 - mape: 8.4181\n",
      "Epoch 285: val_loss did not improve from 0.26460\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2650 - mse: 0.2668 - rmse: 0.5166 - mae: 0.2650 - mape: 8.4309 - val_loss: 0.2649 - val_mse: 0.2668 - val_rmse: 0.5165 - val_mae: 0.2649 - val_mape: 8.4082 - lr: 1.0000e-05\n",
      "Epoch 286/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2646 - mse: 0.2674 - rmse: 0.5172 - mae: 0.2646 - mape: 8.4111\n",
      "Epoch 286: val_loss improved from 0.26460 to 0.26424, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2649 - mse: 0.2671 - rmse: 0.5169 - mae: 0.2649 - mape: 8.4290 - val_loss: 0.2642 - val_mse: 0.2659 - val_rmse: 0.5156 - val_mae: 0.2642 - val_mape: 8.4030 - lr: 1.0000e-05\n",
      "Epoch 287/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2652 - mse: 0.2681 - rmse: 0.5178 - mae: 0.2652 - mape: 8.4407\n",
      "Epoch 287: val_loss improved from 0.26424 to 0.26417, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2650 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2650 - mape: 8.4299 - val_loss: 0.2642 - val_mse: 0.2661 - val_rmse: 0.5159 - val_mae: 0.2642 - val_mape: 8.3996 - lr: 1.0000e-05\n",
      "Epoch 288/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2630 - mse: 0.2628 - rmse: 0.5127 - mae: 0.2630 - mape: 8.3719\n",
      "Epoch 288: val_loss did not improve from 0.26417\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2646 - mse: 0.2665 - rmse: 0.5162 - mae: 0.2646 - mape: 8.4105 - val_loss: 0.2648 - val_mse: 0.2672 - val_rmse: 0.5169 - val_mae: 0.2648 - val_mape: 8.3846 - lr: 1.0000e-05\n",
      "Epoch 289/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2653 - mse: 0.2683 - rmse: 0.5180 - mae: 0.2653 - mape: 8.4502\n",
      "Epoch 289: val_loss improved from 0.26417 to 0.26416, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2649 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2649 - mape: 8.4321 - val_loss: 0.2642 - val_mse: 0.2667 - val_rmse: 0.5164 - val_mae: 0.2642 - val_mape: 8.3791 - lr: 1.0000e-05\n",
      "Epoch 290/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2621 - mse: 0.2620 - rmse: 0.5118 - mae: 0.2621 - mape: 8.3374\n",
      "Epoch 290: val_loss improved from 0.26416 to 0.26379, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2648 - mse: 0.2679 - rmse: 0.5176 - mae: 0.2648 - mape: 8.4223 - val_loss: 0.2638 - val_mse: 0.2660 - val_rmse: 0.5157 - val_mae: 0.2638 - val_mape: 8.4082 - lr: 1.0000e-05\n",
      "Epoch 291/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2646 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2646 - mape: 8.4118\n",
      "Epoch 291: val_loss did not improve from 0.26379\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2643 - mse: 0.2665 - rmse: 0.5162 - mae: 0.2643 - mape: 8.4020 - val_loss: 0.2647 - val_mse: 0.2675 - val_rmse: 0.5172 - val_mae: 0.2647 - val_mape: 8.4467 - lr: 1.0000e-05\n",
      "Epoch 292/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2645 - mse: 0.2694 - rmse: 0.5191 - mae: 0.2645 - mape: 8.3939\n",
      "Epoch 292: val_loss improved from 0.26379 to 0.26376, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2643 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2643 - mape: 8.4010 - val_loss: 0.2638 - val_mse: 0.2667 - val_rmse: 0.5164 - val_mae: 0.2638 - val_mape: 8.3906 - lr: 1.0000e-05\n",
      "Epoch 293/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2633 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2633 - mape: 8.3619\n",
      "Epoch 293: val_loss improved from 0.26376 to 0.26363, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2643 - mse: 0.2669 - rmse: 0.5166 - mae: 0.2643 - mape: 8.4022 - val_loss: 0.2636 - val_mse: 0.2662 - val_rmse: 0.5160 - val_mae: 0.2636 - val_mape: 8.3796 - lr: 1.0000e-05\n",
      "Epoch 294/1000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2658 - mse: 0.2704 - rmse: 0.5200 - mae: 0.2658 - mape: 8.4641\n",
      "Epoch 294: val_loss did not improve from 0.26363\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2642 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2642 - mape: 8.4067 - val_loss: 0.2639 - val_mse: 0.2668 - val_rmse: 0.5165 - val_mae: 0.2639 - val_mape: 8.4235 - lr: 1.0000e-05\n",
      "Epoch 295/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2642 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2642 - mape: 8.4080\n",
      "Epoch 295: val_loss did not improve from 0.26363\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2646 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2646 - mape: 8.4116 - val_loss: 0.2643 - val_mse: 0.2676 - val_rmse: 0.5173 - val_mae: 0.2643 - val_mape: 8.4536 - lr: 1.0000e-05\n",
      "Epoch 296/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2650 - mse: 0.2693 - rmse: 0.5189 - mae: 0.2650 - mape: 8.4335\n",
      "Epoch 296: val_loss improved from 0.26363 to 0.26354, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2643 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2643 - mape: 8.4105 - val_loss: 0.2635 - val_mse: 0.2665 - val_rmse: 0.5162 - val_mae: 0.2635 - val_mape: 8.3539 - lr: 1.0000e-05\n",
      "Epoch 297/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2638 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2638 - mape: 8.3900\n",
      "Epoch 297: val_loss did not improve from 0.26354\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2640 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2640 - mape: 8.3908 - val_loss: 0.2636 - val_mse: 0.2668 - val_rmse: 0.5165 - val_mae: 0.2636 - val_mape: 8.4038 - lr: 1.0000e-05\n",
      "Epoch 298/1000\n",
      "258/318 [=======================>......] - ETA: 0s - loss: 0.2640 - mse: 0.2694 - rmse: 0.5190 - mae: 0.2640 - mape: 8.4120\n",
      "Epoch 298: val_loss did not improve from 0.26354\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2639 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2639 - mape: 8.3957 - val_loss: 0.2637 - val_mse: 0.2666 - val_rmse: 0.5163 - val_mae: 0.2637 - val_mape: 8.3858 - lr: 1.0000e-05\n",
      "Epoch 299/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2630 - mse: 0.2659 - rmse: 0.5157 - mae: 0.2630 - mape: 8.3358\n",
      "Epoch 299: val_loss improved from 0.26354 to 0.26345, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2639 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2639 - mape: 8.3926 - val_loss: 0.2635 - val_mse: 0.2663 - val_rmse: 0.5161 - val_mae: 0.2635 - val_mape: 8.3828 - lr: 1.0000e-05\n",
      "Epoch 300/1000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2635 - mse: 0.2665 - rmse: 0.5163 - mae: 0.2635 - mape: 8.3683\n",
      "Epoch 300: val_loss did not improve from 0.26345\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2639 - mse: 0.2670 - rmse: 0.5168 - mae: 0.2639 - mape: 8.3964 - val_loss: 0.2639 - val_mse: 0.2669 - val_rmse: 0.5166 - val_mae: 0.2639 - val_mape: 8.3557 - lr: 1.0000e-05\n",
      "Epoch 301/1000\n",
      "260/318 [=======================>......] - ETA: 0s - loss: 0.2671 - mse: 0.2745 - rmse: 0.5239 - mae: 0.2671 - mape: 8.4798\n",
      "Epoch 301: val_loss improved from 0.26345 to 0.26337, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2639 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2639 - mape: 8.3828 - val_loss: 0.2634 - val_mse: 0.2663 - val_rmse: 0.5160 - val_mae: 0.2634 - val_mape: 8.3664 - lr: 1.0000e-05\n",
      "Epoch 302/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2631 - mse: 0.2654 - rmse: 0.5151 - mae: 0.2631 - mape: 8.3590\n",
      "Epoch 302: val_loss did not improve from 0.26337\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2639 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2639 - mape: 8.3881 - val_loss: 0.2634 - val_mse: 0.2667 - val_rmse: 0.5164 - val_mae: 0.2634 - val_mape: 8.3762 - lr: 1.0000e-05\n",
      "Epoch 303/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2645 - mse: 0.2686 - rmse: 0.5182 - mae: 0.2645 - mape: 8.4005\n",
      "Epoch 303: val_loss improved from 0.26337 to 0.26337, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2641 - mse: 0.2671 - rmse: 0.5169 - mae: 0.2641 - mape: 8.3883 - val_loss: 0.2634 - val_mse: 0.2664 - val_rmse: 0.5161 - val_mae: 0.2634 - val_mape: 8.3946 - lr: 1.0000e-05\n",
      "Epoch 304/1000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2657 - mse: 0.2698 - rmse: 0.5194 - mae: 0.2657 - mape: 8.4337\n",
      "Epoch 304: val_loss improved from 0.26337 to 0.26315, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2638 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2638 - mape: 8.3828 - val_loss: 0.2632 - val_mse: 0.2662 - val_rmse: 0.5160 - val_mae: 0.2632 - val_mape: 8.3741 - lr: 1.0000e-05\n",
      "Epoch 305/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2644 - mse: 0.2692 - rmse: 0.5188 - mae: 0.2644 - mape: 8.4046\n",
      "Epoch 305: val_loss did not improve from 0.26315\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2640 - mse: 0.2681 - rmse: 0.5178 - mae: 0.2640 - mape: 8.3890 - val_loss: 0.2637 - val_mse: 0.2670 - val_rmse: 0.5167 - val_mae: 0.2637 - val_mape: 8.4088 - lr: 1.0000e-05\n",
      "Epoch 306/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2638 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2638 - mape: 8.3952\n",
      "Epoch 306: val_loss did not improve from 0.26315\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2640 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2640 - mape: 8.3959 - val_loss: 0.2632 - val_mse: 0.2660 - val_rmse: 0.5158 - val_mae: 0.2632 - val_mape: 8.3602 - lr: 1.0000e-05\n",
      "Epoch 307/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2640 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2640 - mape: 8.3790\n",
      "Epoch 307: val_loss did not improve from 0.26315\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2639 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2639 - mape: 8.3805 - val_loss: 0.2632 - val_mse: 0.2664 - val_rmse: 0.5161 - val_mae: 0.2632 - val_mape: 8.3878 - lr: 1.0000e-05\n",
      "Epoch 308/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2641 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2641 - mape: 8.4245\n",
      "Epoch 308: val_loss did not improve from 0.26315\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2641 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2641 - mape: 8.3939 - val_loss: 0.2637 - val_mse: 0.2673 - val_rmse: 0.5170 - val_mae: 0.2637 - val_mape: 8.4417 - lr: 1.0000e-05\n",
      "Epoch 309/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2639 - mse: 0.2677 - rmse: 0.5174 - mae: 0.2639 - mape: 8.4046\n",
      "Epoch 309: val_loss did not improve from 0.26315\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2637 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2637 - mape: 8.3924 - val_loss: 0.2633 - val_mse: 0.2661 - val_rmse: 0.5159 - val_mae: 0.2633 - val_mape: 8.3839 - lr: 1.0000e-05\n",
      "Epoch 310/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2624 - mse: 0.2655 - rmse: 0.5153 - mae: 0.2624 - mape: 8.3532\n",
      "Epoch 310: val_loss did not improve from 0.26315\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2636 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2636 - mape: 8.3773 - val_loss: 0.2644 - val_mse: 0.2675 - val_rmse: 0.5172 - val_mae: 0.2644 - val_mape: 8.3386 - lr: 1.0000e-05\n",
      "Epoch 311/1000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2661 - mse: 0.2712 - rmse: 0.5207 - mae: 0.2661 - mape: 8.4485\n",
      "Epoch 311: val_loss improved from 0.26315 to 0.26304, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2639 - mse: 0.2667 - rmse: 0.5165 - mae: 0.2639 - mape: 8.3776 - val_loss: 0.2630 - val_mse: 0.2657 - val_rmse: 0.5154 - val_mae: 0.2630 - val_mape: 8.3463 - lr: 1.0000e-05\n",
      "Epoch 312/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2659 - mse: 0.2701 - rmse: 0.5197 - mae: 0.2659 - mape: 8.4494\n",
      "Epoch 312: val_loss did not improve from 0.26304\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2637 - mse: 0.2665 - rmse: 0.5162 - mae: 0.2637 - mape: 8.3781 - val_loss: 0.2634 - val_mse: 0.2658 - val_rmse: 0.5156 - val_mae: 0.2634 - val_mape: 8.3521 - lr: 1.0000e-05\n",
      "Epoch 313/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2635 - mse: 0.2679 - rmse: 0.5176 - mae: 0.2635 - mape: 8.3841\n",
      "Epoch 313: val_loss did not improve from 0.26304\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2630 - mse: 0.2664 - rmse: 0.5161 - mae: 0.2630 - mape: 8.3664 - val_loss: 0.2642 - val_mse: 0.2684 - val_rmse: 0.5181 - val_mae: 0.2642 - val_mape: 8.4209 - lr: 1.0000e-05\n",
      "Epoch 314/1000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2627 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2627 - mape: 8.3262\n",
      "Epoch 314: val_loss improved from 0.26304 to 0.26300, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2637 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2637 - mape: 8.3787 - val_loss: 0.2630 - val_mse: 0.2656 - val_rmse: 0.5154 - val_mae: 0.2630 - val_mape: 8.3270 - lr: 1.0000e-05\n",
      "Epoch 315/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2629 - mse: 0.2659 - rmse: 0.5157 - mae: 0.2629 - mape: 8.3330\n",
      "Epoch 315: val_loss did not improve from 0.26300\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2639 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2639 - mape: 8.3695 - val_loss: 0.2631 - val_mse: 0.2658 - val_rmse: 0.5155 - val_mae: 0.2631 - val_mape: 8.3627 - lr: 1.0000e-05\n",
      "Epoch 316/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2642 - mse: 0.2690 - rmse: 0.5186 - mae: 0.2642 - mape: 8.3844\n",
      "Epoch 316: val_loss did not improve from 0.26300\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2635 - mse: 0.2671 - rmse: 0.5169 - mae: 0.2635 - mape: 8.3675 - val_loss: 0.2646 - val_mse: 0.2685 - val_rmse: 0.5181 - val_mae: 0.2646 - val_mape: 8.3282 - lr: 1.0000e-05\n",
      "Epoch 317/1000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.2632 - mse: 0.2658 - rmse: 0.5155 - mae: 0.2632 - mape: 8.3697\n",
      "Epoch 317: val_loss improved from 0.26300 to 0.26291, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2637 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2637 - mape: 8.3742 - val_loss: 0.2629 - val_mse: 0.2655 - val_rmse: 0.5153 - val_mae: 0.2629 - val_mape: 8.3426 - lr: 1.0000e-05\n",
      "Epoch 318/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2621 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2621 - mape: 8.3269\n",
      "Epoch 318: val_loss did not improve from 0.26291\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2638 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2638 - mape: 8.3726 - val_loss: 0.2634 - val_mse: 0.2667 - val_rmse: 0.5164 - val_mae: 0.2634 - val_mape: 8.3941 - lr: 1.0000e-05\n",
      "Epoch 319/1000\n",
      "257/318 [=======================>......] - ETA: 0s - loss: 0.2635 - mse: 0.2650 - rmse: 0.5148 - mae: 0.2635 - mape: 8.3193\n",
      "Epoch 319: val_loss did not improve from 0.26291\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2633 - mse: 0.2658 - rmse: 0.5156 - mae: 0.2633 - mape: 8.3578 - val_loss: 0.2649 - val_mse: 0.2702 - val_rmse: 0.5198 - val_mae: 0.2649 - val_mape: 8.4744 - lr: 1.0000e-05\n",
      "Epoch 320/1000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2647 - mse: 0.2711 - rmse: 0.5207 - mae: 0.2647 - mape: 8.3840\n",
      "Epoch 320: val_loss did not improve from 0.26291\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2639 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2639 - mape: 8.3793 - val_loss: 0.2652 - val_mse: 0.2696 - val_rmse: 0.5192 - val_mae: 0.2652 - val_mape: 8.4779 - lr: 1.0000e-05\n",
      "Epoch 321/1000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2637 - mse: 0.2685 - rmse: 0.5181 - mae: 0.2637 - mape: 8.3877\n",
      "Epoch 321: val_loss did not improve from 0.26291\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2638 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2638 - mape: 8.3776 - val_loss: 0.2631 - val_mse: 0.2653 - val_rmse: 0.5151 - val_mae: 0.2631 - val_mape: 8.3594 - lr: 1.0000e-05\n",
      "Epoch 322/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2631 - mse: 0.2662 - rmse: 0.5159 - mae: 0.2631 - mape: 8.3422\n",
      "Epoch 322: val_loss improved from 0.26291 to 0.26290, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2633 - mse: 0.2665 - rmse: 0.5163 - mae: 0.2633 - mape: 8.3606 - val_loss: 0.2629 - val_mse: 0.2657 - val_rmse: 0.5155 - val_mae: 0.2629 - val_mape: 8.3449 - lr: 1.0000e-05\n",
      "Epoch 323/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2645 - mse: 0.2691 - rmse: 0.5188 - mae: 0.2645 - mape: 8.3895\n",
      "Epoch 323: val_loss did not improve from 0.26290\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2635 - mse: 0.2666 - rmse: 0.5164 - mae: 0.2635 - mape: 8.3637 - val_loss: 0.2631 - val_mse: 0.2660 - val_rmse: 0.5157 - val_mae: 0.2631 - val_mape: 8.3564 - lr: 1.0000e-05\n",
      "Epoch 324/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2635 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2635 - mape: 8.3696\n",
      "Epoch 324: val_loss improved from 0.26290 to 0.26288, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2634 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2634 - mape: 8.3657 - val_loss: 0.2629 - val_mse: 0.2664 - val_rmse: 0.5162 - val_mae: 0.2629 - val_mape: 8.3546 - lr: 1.0000e-05\n",
      "Epoch 325/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2631 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2631 - mape: 8.3452\n",
      "Epoch 325: val_loss did not improve from 0.26288\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2636 - mse: 0.2677 - rmse: 0.5174 - mae: 0.2636 - mape: 8.3647 - val_loss: 0.2632 - val_mse: 0.2654 - val_rmse: 0.5152 - val_mae: 0.2632 - val_mape: 8.3795 - lr: 1.0000e-05\n",
      "Epoch 326/1000\n",
      "259/318 [=======================>......] - ETA: 0s - loss: 0.2624 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2624 - mape: 8.3545\n",
      "Epoch 326: val_loss did not improve from 0.26288\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2637 - mse: 0.2662 - rmse: 0.5160 - mae: 0.2637 - mape: 8.3740 - val_loss: 0.2633 - val_mse: 0.2668 - val_rmse: 0.5165 - val_mae: 0.2633 - val_mape: 8.3580 - lr: 1.0000e-05\n",
      "Epoch 327/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2632 - mse: 0.2663 - rmse: 0.5161 - mae: 0.2632 - mape: 8.3737\n",
      "Epoch 327: val_loss did not improve from 0.26288\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2637 - mse: 0.2677 - rmse: 0.5174 - mae: 0.2637 - mape: 8.3696 - val_loss: 0.2642 - val_mse: 0.2689 - val_rmse: 0.5186 - val_mae: 0.2642 - val_mape: 8.4476 - lr: 1.0000e-05\n",
      "Epoch 328/1000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2613 - mse: 0.2632 - rmse: 0.5130 - mae: 0.2613 - mape: 8.3178\n",
      "Epoch 328: val_loss improved from 0.26288 to 0.26277, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2637 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2637 - mape: 8.3708 - val_loss: 0.2628 - val_mse: 0.2653 - val_rmse: 0.5151 - val_mae: 0.2628 - val_mape: 8.3484 - lr: 1.0000e-05\n",
      "Epoch 329/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2631 - mse: 0.2653 - rmse: 0.5151 - mae: 0.2631 - mape: 8.3485\n",
      "Epoch 329: val_loss did not improve from 0.26277\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2630 - mse: 0.2652 - rmse: 0.5150 - mae: 0.2630 - mape: 8.3473 - val_loss: 0.2655 - val_mse: 0.2703 - val_rmse: 0.5199 - val_mae: 0.2655 - val_mape: 8.3308 - lr: 1.0000e-05\n",
      "Epoch 330/1000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2633 - mse: 0.2659 - rmse: 0.5157 - mae: 0.2633 - mape: 8.3374\n",
      "Epoch 330: val_loss improved from 0.26277 to 0.26276, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2636 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2636 - mape: 8.3596 - val_loss: 0.2628 - val_mse: 0.2658 - val_rmse: 0.5156 - val_mae: 0.2628 - val_mape: 8.3471 - lr: 1.0000e-05\n",
      "Epoch 331/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2617 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2617 - mape: 8.3134\n",
      "Epoch 331: val_loss did not improve from 0.26276\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2634 - mse: 0.2658 - rmse: 0.5155 - mae: 0.2634 - mape: 8.3533 - val_loss: 0.2634 - val_mse: 0.2673 - val_rmse: 0.5170 - val_mae: 0.2634 - val_mape: 8.3941 - lr: 1.0000e-05\n",
      "Epoch 332/1000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2637 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2637 - mape: 8.3878\n",
      "Epoch 332: val_loss improved from 0.26276 to 0.26272, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2632 - mse: 0.2669 - rmse: 0.5166 - mae: 0.2632 - mape: 8.3602 - val_loss: 0.2627 - val_mse: 0.2656 - val_rmse: 0.5154 - val_mae: 0.2627 - val_mape: 8.3227 - lr: 1.0000e-05\n",
      "Epoch 333/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2651 - mse: 0.2692 - rmse: 0.5189 - mae: 0.2651 - mape: 8.3963\n",
      "Epoch 333: val_loss did not improve from 0.26272\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2634 - mse: 0.2665 - rmse: 0.5163 - mae: 0.2634 - mape: 8.3538 - val_loss: 0.2635 - val_mse: 0.2655 - val_rmse: 0.5153 - val_mae: 0.2635 - val_mape: 8.3900 - lr: 1.0000e-05\n",
      "Epoch 334/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2633 - mse: 0.2656 - rmse: 0.5153 - mae: 0.2633 - mape: 8.3565\n",
      "Epoch 334: val_loss did not improve from 0.26272\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2631 - mse: 0.2658 - rmse: 0.5156 - mae: 0.2631 - mape: 8.3543 - val_loss: 0.2637 - val_mse: 0.2677 - val_rmse: 0.5174 - val_mae: 0.2637 - val_mape: 8.4225 - lr: 1.0000e-05\n",
      "Epoch 335/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2618 - mse: 0.2633 - rmse: 0.5131 - mae: 0.2618 - mape: 8.2972\n",
      "Epoch 335: val_loss did not improve from 0.26272\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2633 - mse: 0.2662 - rmse: 0.5160 - mae: 0.2633 - mape: 8.3606 - val_loss: 0.2628 - val_mse: 0.2658 - val_rmse: 0.5156 - val_mae: 0.2628 - val_mape: 8.3308 - lr: 1.0000e-05\n",
      "Epoch 336/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2630 - mse: 0.2652 - rmse: 0.5150 - mae: 0.2630 - mape: 8.3330\n",
      "Epoch 336: val_loss did not improve from 0.26272\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2632 - mse: 0.2664 - rmse: 0.5162 - mae: 0.2632 - mape: 8.3470 - val_loss: 0.2631 - val_mse: 0.2660 - val_rmse: 0.5157 - val_mae: 0.2631 - val_mape: 8.3132 - lr: 1.0000e-05\n",
      "Epoch 337/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2619 - mse: 0.2627 - rmse: 0.5126 - mae: 0.2619 - mape: 8.3129\n",
      "Epoch 337: val_loss did not improve from 0.26272\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2633 - mse: 0.2666 - rmse: 0.5164 - mae: 0.2633 - mape: 8.3565 - val_loss: 0.2628 - val_mse: 0.2661 - val_rmse: 0.5159 - val_mae: 0.2628 - val_mape: 8.3661 - lr: 1.0000e-05\n",
      "Epoch 338/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2636 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2636 - mape: 8.3626\n",
      "Epoch 338: val_loss did not improve from 0.26272\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2634 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2634 - mape: 8.3656 - val_loss: 0.2631 - val_mse: 0.2664 - val_rmse: 0.5162 - val_mae: 0.2631 - val_mape: 8.3704 - lr: 1.0000e-05\n",
      "Epoch 339/1000\n",
      "255/318 [=======================>......] - ETA: 0s - loss: 0.2611 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2611 - mape: 8.2937\n",
      "Epoch 339: val_loss did not improve from 0.26272\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2635 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2635 - mape: 8.3583 - val_loss: 0.2628 - val_mse: 0.2658 - val_rmse: 0.5156 - val_mae: 0.2628 - val_mape: 8.3728 - lr: 1.0000e-05\n",
      "Epoch 340/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2645 - mse: 0.2684 - rmse: 0.5181 - mae: 0.2645 - mape: 8.4224\n",
      "Epoch 340: val_loss did not improve from 0.26272\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2636 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2636 - mape: 8.3843 - val_loss: 0.2630 - val_mse: 0.2663 - val_rmse: 0.5161 - val_mae: 0.2630 - val_mape: 8.3224 - lr: 1.0000e-05\n",
      "Epoch 341/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2642 - mse: 0.2695 - rmse: 0.5192 - mae: 0.2642 - mape: 8.3708\n",
      "Epoch 341: val_loss did not improve from 0.26272\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2633 - mse: 0.2666 - rmse: 0.5163 - mae: 0.2633 - mape: 8.3558 - val_loss: 0.2634 - val_mse: 0.2665 - val_rmse: 0.5162 - val_mae: 0.2634 - val_mape: 8.4045 - lr: 1.0000e-05\n",
      "Epoch 342/1000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2634 - mse: 0.2660 - rmse: 0.5158 - mae: 0.2634 - mape: 8.3650\n",
      "Epoch 342: val_loss improved from 0.26272 to 0.26265, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2634 - mse: 0.2663 - rmse: 0.5160 - mae: 0.2634 - mape: 8.3641 - val_loss: 0.2627 - val_mse: 0.2654 - val_rmse: 0.5152 - val_mae: 0.2627 - val_mape: 8.3205 - lr: 1.0000e-05\n",
      "Epoch 343/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2637 - mse: 0.2663 - rmse: 0.5160 - mae: 0.2637 - mape: 8.3655\n",
      "Epoch 343: val_loss did not improve from 0.26265\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2633 - mse: 0.2668 - rmse: 0.5166 - mae: 0.2633 - mape: 8.3531 - val_loss: 0.2633 - val_mse: 0.2669 - val_rmse: 0.5166 - val_mae: 0.2633 - val_mape: 8.3439 - lr: 1.0000e-05\n",
      "Epoch 344/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2630 - mse: 0.2681 - rmse: 0.5177 - mae: 0.2630 - mape: 8.3747\n",
      "Epoch 344: val_loss did not improve from 0.26265\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2632 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2632 - mape: 8.3661 - val_loss: 0.2627 - val_mse: 0.2658 - val_rmse: 0.5156 - val_mae: 0.2627 - val_mape: 8.3283 - lr: 1.0000e-05\n",
      "Epoch 345/1000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2647 - mse: 0.2699 - rmse: 0.5195 - mae: 0.2647 - mape: 8.3760\n",
      "Epoch 345: val_loss did not improve from 0.26265\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2633 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2633 - mape: 8.3501 - val_loss: 0.2641 - val_mse: 0.2684 - val_rmse: 0.5181 - val_mae: 0.2641 - val_mape: 8.4522 - lr: 1.0000e-05\n",
      "Epoch 346/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2625 - mse: 0.2653 - rmse: 0.5150 - mae: 0.2625 - mape: 8.3475\n",
      "Epoch 346: val_loss improved from 0.26265 to 0.26257, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2634 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2634 - mape: 8.3719 - val_loss: 0.2626 - val_mse: 0.2665 - val_rmse: 0.5162 - val_mae: 0.2626 - val_mape: 8.3314 - lr: 1.0000e-05\n",
      "Epoch 347/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2636 - mse: 0.2681 - rmse: 0.5178 - mae: 0.2636 - mape: 8.3865\n",
      "Epoch 347: val_loss improved from 0.26257 to 0.26256, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2629 - mse: 0.2664 - rmse: 0.5162 - mae: 0.2629 - mape: 8.3480 - val_loss: 0.2626 - val_mse: 0.2663 - val_rmse: 0.5161 - val_mae: 0.2626 - val_mape: 8.3559 - lr: 1.0000e-05\n",
      "Epoch 348/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2631 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2631 - mape: 8.3482\n",
      "Epoch 348: val_loss did not improve from 0.26256\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2631 - mse: 0.2671 - rmse: 0.5169 - mae: 0.2631 - mape: 8.3675 - val_loss: 0.2647 - val_mse: 0.2689 - val_rmse: 0.5185 - val_mae: 0.2647 - val_mape: 8.3347 - lr: 1.0000e-05\n",
      "Epoch 349/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2624 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2624 - mape: 8.3381\n",
      "Epoch 349: val_loss did not improve from 0.26256\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2634 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2634 - mape: 8.3572 - val_loss: 0.2632 - val_mse: 0.2674 - val_rmse: 0.5171 - val_mae: 0.2632 - val_mape: 8.4048 - lr: 1.0000e-05\n",
      "Epoch 350/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2637 - mse: 0.2691 - rmse: 0.5188 - mae: 0.2637 - mape: 8.3807\n",
      "Epoch 350: val_loss improved from 0.26256 to 0.26243, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2630 - mse: 0.2664 - rmse: 0.5162 - mae: 0.2630 - mape: 8.3559 - val_loss: 0.2624 - val_mse: 0.2655 - val_rmse: 0.5153 - val_mae: 0.2624 - val_mape: 8.3385 - lr: 1.0000e-05\n",
      "Epoch 351/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2623 - mse: 0.2640 - rmse: 0.5138 - mae: 0.2623 - mape: 8.3086\n",
      "Epoch 351: val_loss did not improve from 0.26243\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2631 - mse: 0.2666 - rmse: 0.5164 - mae: 0.2631 - mape: 8.3521 - val_loss: 0.2625 - val_mse: 0.2657 - val_rmse: 0.5154 - val_mae: 0.2625 - val_mape: 8.3320 - lr: 1.0000e-05\n",
      "Epoch 352/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2639 - mse: 0.2681 - rmse: 0.5178 - mae: 0.2639 - mape: 8.3758\n",
      "Epoch 352: val_loss did not improve from 0.26243\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2630 - mse: 0.2665 - rmse: 0.5162 - mae: 0.2630 - mape: 8.3648 - val_loss: 0.2628 - val_mse: 0.2660 - val_rmse: 0.5157 - val_mae: 0.2628 - val_mape: 8.3804 - lr: 1.0000e-05\n",
      "Epoch 353/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2623 - mse: 0.2664 - rmse: 0.5161 - mae: 0.2623 - mape: 8.3273\n",
      "Epoch 353: val_loss did not improve from 0.26243\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2631 - mse: 0.2666 - rmse: 0.5163 - mae: 0.2631 - mape: 8.3524 - val_loss: 0.2629 - val_mse: 0.2663 - val_rmse: 0.5160 - val_mae: 0.2629 - val_mape: 8.3147 - lr: 1.0000e-05\n",
      "Epoch 354/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2638 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2638 - mape: 8.3566\n",
      "Epoch 354: val_loss did not improve from 0.26243\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2634 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2634 - mape: 8.3516 - val_loss: 0.2627 - val_mse: 0.2659 - val_rmse: 0.5156 - val_mae: 0.2627 - val_mape: 8.3640 - lr: 1.0000e-05\n",
      "Epoch 355/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2637 - mse: 0.2682 - rmse: 0.5179 - mae: 0.2637 - mape: 8.3627\n",
      "Epoch 355: val_loss did not improve from 0.26243\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2631 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2631 - mape: 8.3596 - val_loss: 0.2625 - val_mse: 0.2652 - val_rmse: 0.5150 - val_mae: 0.2625 - val_mape: 8.3301 - lr: 1.0000e-05\n",
      "Epoch 356/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2635 - mse: 0.2681 - rmse: 0.5178 - mae: 0.2635 - mape: 8.3629\n",
      "Epoch 356: val_loss did not improve from 0.26243\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2630 - mse: 0.2667 - rmse: 0.5165 - mae: 0.2630 - mape: 8.3538 - val_loss: 0.2626 - val_mse: 0.2653 - val_rmse: 0.5151 - val_mae: 0.2626 - val_mape: 8.3295 - lr: 1.0000e-05\n",
      "Epoch 357/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2629 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2629 - mape: 8.3457\n",
      "Epoch 357: val_loss did not improve from 0.26243\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2631 - mse: 0.2661 - rmse: 0.5159 - mae: 0.2631 - mape: 8.3542 - val_loss: 0.2625 - val_mse: 0.2655 - val_rmse: 0.5153 - val_mae: 0.2625 - val_mape: 8.3362 - lr: 1.0000e-05\n",
      "Epoch 358/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2632 - mse: 0.2654 - rmse: 0.5152 - mae: 0.2632 - mape: 8.3387\n",
      "Epoch 358: val_loss did not improve from 0.26243\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2630 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2630 - mape: 8.3562 - val_loss: 0.2626 - val_mse: 0.2660 - val_rmse: 0.5158 - val_mae: 0.2626 - val_mape: 8.3047 - lr: 1.0000e-05\n",
      "Epoch 359/1000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2634 - mse: 0.2690 - rmse: 0.5187 - mae: 0.2634 - mape: 8.3560\n",
      "Epoch 359: val_loss did not improve from 0.26243\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2627 - mse: 0.2662 - rmse: 0.5159 - mae: 0.2627 - mape: 8.3421 - val_loss: 0.2626 - val_mse: 0.2662 - val_rmse: 0.5160 - val_mae: 0.2626 - val_mape: 8.3613 - lr: 1.0000e-05\n",
      "Epoch 360/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2632 - mse: 0.2677 - rmse: 0.5174 - mae: 0.2632 - mape: 8.3655\n",
      "Epoch 360: val_loss improved from 0.26243 to 0.26231, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2629 - mse: 0.2669 - rmse: 0.5166 - mae: 0.2629 - mape: 8.3519 - val_loss: 0.2623 - val_mse: 0.2655 - val_rmse: 0.5153 - val_mae: 0.2623 - val_mape: 8.3172 - lr: 1.0000e-05\n",
      "Epoch 361/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2625 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2625 - mape: 8.3594\n",
      "Epoch 361: val_loss did not improve from 0.26231\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2632 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2632 - mape: 8.3622 - val_loss: 0.2632 - val_mse: 0.2662 - val_rmse: 0.5160 - val_mae: 0.2632 - val_mape: 8.3284 - lr: 1.0000e-05\n",
      "Epoch 362/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2627 - mse: 0.2658 - rmse: 0.5156 - mae: 0.2627 - mape: 8.3381\n",
      "Epoch 362: val_loss did not improve from 0.26231\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2630 - mse: 0.2663 - rmse: 0.5160 - mae: 0.2630 - mape: 8.3506 - val_loss: 0.2644 - val_mse: 0.2688 - val_rmse: 0.5185 - val_mae: 0.2644 - val_mape: 8.3277 - lr: 1.0000e-05\n",
      "Epoch 363/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2617 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2617 - mape: 8.2945\n",
      "Epoch 363: val_loss did not improve from 0.26231\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2628 - mse: 0.2662 - rmse: 0.5160 - mae: 0.2628 - mape: 8.3329 - val_loss: 0.2630 - val_mse: 0.2672 - val_rmse: 0.5169 - val_mae: 0.2630 - val_mape: 8.3501 - lr: 1.0000e-05\n",
      "Epoch 364/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2629 - mse: 0.2667 - rmse: 0.5165 - mae: 0.2629 - mape: 8.3653\n",
      "Epoch 364: val_loss did not improve from 0.26231\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2629 - mse: 0.2666 - rmse: 0.5164 - mae: 0.2629 - mape: 8.3541 - val_loss: 0.2625 - val_mse: 0.2649 - val_rmse: 0.5147 - val_mae: 0.2625 - val_mape: 8.3329 - lr: 1.0000e-05\n",
      "Epoch 365/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2624 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2624 - mape: 8.3115\n",
      "Epoch 365: val_loss did not improve from 0.26231\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2629 - mse: 0.2671 - rmse: 0.5169 - mae: 0.2629 - mape: 8.3544 - val_loss: 0.2626 - val_mse: 0.2649 - val_rmse: 0.5147 - val_mae: 0.2626 - val_mape: 8.3579 - lr: 1.0000e-05\n",
      "Epoch 366/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2628 - mse: 0.2650 - rmse: 0.5148 - mae: 0.2628 - mape: 8.3308\n",
      "Epoch 366: val_loss did not improve from 0.26231\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2632 - mse: 0.2664 - rmse: 0.5162 - mae: 0.2632 - mape: 8.3467 - val_loss: 0.2628 - val_mse: 0.2659 - val_rmse: 0.5157 - val_mae: 0.2628 - val_mape: 8.3620 - lr: 1.0000e-05\n",
      "Epoch 367/1000\n",
      "262/318 [=======================>......] - ETA: 0s - loss: 0.2627 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2627 - mape: 8.3378\n",
      "Epoch 367: val_loss did not improve from 0.26231\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2630 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2630 - mape: 8.3572 - val_loss: 0.2623 - val_mse: 0.2661 - val_rmse: 0.5158 - val_mae: 0.2623 - val_mape: 8.3386 - lr: 1.0000e-05\n",
      "Epoch 368/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2626 - mse: 0.2647 - rmse: 0.5144 - mae: 0.2626 - mape: 8.3606\n",
      "Epoch 368: val_loss did not improve from 0.26231\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2627 - mse: 0.2658 - rmse: 0.5155 - mae: 0.2627 - mape: 8.3499 - val_loss: 0.2628 - val_mse: 0.2663 - val_rmse: 0.5160 - val_mae: 0.2628 - val_mape: 8.3711 - lr: 1.0000e-05\n",
      "Epoch 369/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2644 - mse: 0.2687 - rmse: 0.5183 - mae: 0.2644 - mape: 8.3778\n",
      "Epoch 369: val_loss did not improve from 0.26231\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2629 - mse: 0.2658 - rmse: 0.5156 - mae: 0.2629 - mape: 8.3456 - val_loss: 0.2625 - val_mse: 0.2658 - val_rmse: 0.5156 - val_mae: 0.2625 - val_mape: 8.3178 - lr: 1.0000e-05\n",
      "Epoch 370/1000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2627 - mse: 0.2649 - rmse: 0.5146 - mae: 0.2627 - mape: 8.3197\n",
      "Epoch 370: val_loss improved from 0.26231 to 0.26221, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2632 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2632 - mape: 8.3530 - val_loss: 0.2622 - val_mse: 0.2651 - val_rmse: 0.5149 - val_mae: 0.2622 - val_mape: 8.3213 - lr: 1.0000e-05\n",
      "Epoch 371/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2618 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2618 - mape: 8.3102\n",
      "Epoch 371: val_loss did not improve from 0.26221\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2627 - mse: 0.2661 - rmse: 0.5158 - mae: 0.2627 - mape: 8.3464 - val_loss: 0.2633 - val_mse: 0.2666 - val_rmse: 0.5163 - val_mae: 0.2633 - val_mape: 8.3823 - lr: 1.0000e-05\n",
      "Epoch 372/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2626 - mse: 0.2652 - rmse: 0.5150 - mae: 0.2626 - mape: 8.3291\n",
      "Epoch 372: val_loss did not improve from 0.26221\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2630 - mse: 0.2660 - rmse: 0.5158 - mae: 0.2630 - mape: 8.3445 - val_loss: 0.2634 - val_mse: 0.2654 - val_rmse: 0.5151 - val_mae: 0.2634 - val_mape: 8.3991 - lr: 1.0000e-05\n",
      "Epoch 373/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2638 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2638 - mape: 8.3728\n",
      "Epoch 373: val_loss did not improve from 0.26221\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2628 - mse: 0.2658 - rmse: 0.5155 - mae: 0.2628 - mape: 8.3434 - val_loss: 0.2628 - val_mse: 0.2665 - val_rmse: 0.5162 - val_mae: 0.2628 - val_mape: 8.2929 - lr: 1.0000e-05\n",
      "Epoch 374/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2642 - mse: 0.2704 - rmse: 0.5200 - mae: 0.2642 - mape: 8.3888\n",
      "Epoch 374: val_loss did not improve from 0.26221\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2626 - mse: 0.2667 - rmse: 0.5165 - mae: 0.2626 - mape: 8.3353 - val_loss: 0.2641 - val_mse: 0.2683 - val_rmse: 0.5180 - val_mae: 0.2641 - val_mape: 8.3159 - lr: 1.0000e-05\n",
      "Epoch 375/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2635 - mse: 0.2665 - rmse: 0.5163 - mae: 0.2635 - mape: 8.3400\n",
      "Epoch 375: val_loss did not improve from 0.26221\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2628 - mse: 0.2667 - rmse: 0.5165 - mae: 0.2628 - mape: 8.3336 - val_loss: 0.2625 - val_mse: 0.2668 - val_rmse: 0.5165 - val_mae: 0.2625 - val_mape: 8.3543 - lr: 1.0000e-05\n",
      "Epoch 376/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2630 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2630 - mape: 8.3444\n",
      "Epoch 376: val_loss improved from 0.26221 to 0.26216, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2628 - mse: 0.2669 - rmse: 0.5166 - mae: 0.2628 - mape: 8.3389 - val_loss: 0.2622 - val_mse: 0.2654 - val_rmse: 0.5152 - val_mae: 0.2622 - val_mape: 8.3399 - lr: 1.0000e-05\n",
      "Epoch 377/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2621 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2621 - mape: 8.3093\n",
      "Epoch 377: val_loss did not improve from 0.26216\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2626 - mse: 0.2664 - rmse: 0.5161 - mae: 0.2626 - mape: 8.3454 - val_loss: 0.2624 - val_mse: 0.2644 - val_rmse: 0.5142 - val_mae: 0.2624 - val_mape: 8.3202 - lr: 1.0000e-05\n",
      "Epoch 378/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2627 - mse: 0.2653 - rmse: 0.5151 - mae: 0.2627 - mape: 8.3124\n",
      "Epoch 378: val_loss did not improve from 0.26216\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2628 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2628 - mape: 8.3385 - val_loss: 0.2623 - val_mse: 0.2652 - val_rmse: 0.5150 - val_mae: 0.2623 - val_mape: 8.3084 - lr: 1.0000e-05\n",
      "Epoch 379/1000\n",
      "258/318 [=======================>......] - ETA: 0s - loss: 0.2599 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2599 - mape: 8.2665\n",
      "Epoch 379: val_loss did not improve from 0.26216\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2629 - mse: 0.2662 - rmse: 0.5160 - mae: 0.2629 - mape: 8.3330 - val_loss: 0.2624 - val_mse: 0.2651 - val_rmse: 0.5149 - val_mae: 0.2624 - val_mape: 8.3258 - lr: 1.0000e-05\n",
      "Epoch 380/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2636 - mse: 0.2691 - rmse: 0.5187 - mae: 0.2636 - mape: 8.3575\n",
      "Epoch 380: val_loss did not improve from 0.26216\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2630 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2630 - mape: 8.3459 - val_loss: 0.2623 - val_mse: 0.2659 - val_rmse: 0.5156 - val_mae: 0.2623 - val_mape: 8.3333 - lr: 1.0000e-05\n",
      "Epoch 381/1000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2605 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2605 - mape: 8.2952\n",
      "Epoch 381: val_loss did not improve from 0.26216\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2622 - mse: 0.2654 - rmse: 0.5152 - mae: 0.2622 - mape: 8.3257 - val_loss: 0.2623 - val_mse: 0.2652 - val_rmse: 0.5150 - val_mae: 0.2623 - val_mape: 8.3193 - lr: 1.0000e-05\n",
      "Epoch 382/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2638 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2638 - mape: 8.3801\n",
      "Epoch 382: val_loss did not improve from 0.26216\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2626 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2626 - mape: 8.3335 - val_loss: 0.2624 - val_mse: 0.2653 - val_rmse: 0.5150 - val_mae: 0.2624 - val_mape: 8.3057 - lr: 1.0000e-05\n",
      "Epoch 383/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2629 - mse: 0.2666 - rmse: 0.5164 - mae: 0.2629 - mape: 8.3432\n",
      "Epoch 383: val_loss did not improve from 0.26216\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2626 - mse: 0.2663 - rmse: 0.5160 - mae: 0.2626 - mape: 8.3358 - val_loss: 0.2623 - val_mse: 0.2660 - val_rmse: 0.5158 - val_mae: 0.2623 - val_mape: 8.3335 - lr: 1.0000e-05\n",
      "Epoch 384/1000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2639 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2639 - mape: 8.3338\n",
      "Epoch 384: val_loss did not improve from 0.26216\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2629 - mse: 0.2662 - rmse: 0.5159 - mae: 0.2629 - mape: 8.3448 - val_loss: 0.2624 - val_mse: 0.2659 - val_rmse: 0.5156 - val_mae: 0.2624 - val_mape: 8.3069 - lr: 1.0000e-05\n",
      "Epoch 385/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2629 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2629 - mape: 8.3623\n",
      "Epoch 385: val_loss improved from 0.26216 to 0.26216, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2628 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2628 - mape: 8.3411 - val_loss: 0.2622 - val_mse: 0.2648 - val_rmse: 0.5146 - val_mae: 0.2622 - val_mape: 8.3284 - lr: 1.0000e-05\n",
      "Epoch 386/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2643 - mse: 0.2689 - rmse: 0.5185 - mae: 0.2643 - mape: 8.3840\n",
      "Epoch 386: val_loss did not improve from 0.26216\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2628 - mse: 0.2661 - rmse: 0.5158 - mae: 0.2628 - mape: 8.3397 - val_loss: 0.2622 - val_mse: 0.2651 - val_rmse: 0.5148 - val_mae: 0.2622 - val_mape: 8.3094 - lr: 1.0000e-05\n",
      "Epoch 387/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2628 - mse: 0.2683 - rmse: 0.5180 - mae: 0.2628 - mape: 8.3445\n",
      "Epoch 387: val_loss did not improve from 0.26216\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2626 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2626 - mape: 8.3409 - val_loss: 0.2624 - val_mse: 0.2651 - val_rmse: 0.5149 - val_mae: 0.2624 - val_mape: 8.3484 - lr: 1.0000e-05\n",
      "Epoch 388/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2632 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2632 - mape: 8.3701\n",
      "Epoch 388: val_loss did not improve from 0.26216\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2629 - mse: 0.2668 - rmse: 0.5166 - mae: 0.2629 - mape: 8.3489 - val_loss: 0.2622 - val_mse: 0.2655 - val_rmse: 0.5153 - val_mae: 0.2622 - val_mape: 8.3164 - lr: 1.0000e-05\n",
      "Epoch 389/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2628 - mse: 0.2658 - rmse: 0.5156 - mae: 0.2628 - mape: 8.3343\n",
      "Epoch 389: val_loss did not improve from 0.26216\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2625 - mse: 0.2654 - rmse: 0.5152 - mae: 0.2625 - mape: 8.3275 - val_loss: 0.2622 - val_mse: 0.2650 - val_rmse: 0.5148 - val_mae: 0.2622 - val_mape: 8.3256 - lr: 1.0000e-05\n",
      "Epoch 390/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2633 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2633 - mape: 8.3702\n",
      "Epoch 390: val_loss improved from 0.26216 to 0.26202, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2626 - mse: 0.2659 - rmse: 0.5156 - mae: 0.2626 - mape: 8.3421 - val_loss: 0.2620 - val_mse: 0.2648 - val_rmse: 0.5146 - val_mae: 0.2620 - val_mape: 8.3195 - lr: 1.0000e-05\n",
      "Epoch 391/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2613 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2613 - mape: 8.2956\n",
      "Epoch 391: val_loss did not improve from 0.26202\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2623 - mse: 0.2661 - rmse: 0.5159 - mae: 0.2623 - mape: 8.3331 - val_loss: 0.2622 - val_mse: 0.2646 - val_rmse: 0.5144 - val_mae: 0.2622 - val_mape: 8.3232 - lr: 1.0000e-05\n",
      "Epoch 392/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2627 - mse: 0.2657 - rmse: 0.5154 - mae: 0.2627 - mape: 8.3339\n",
      "Epoch 392: val_loss did not improve from 0.26202\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2627 - mse: 0.2657 - rmse: 0.5154 - mae: 0.2627 - mape: 8.3339 - val_loss: 0.2626 - val_mse: 0.2671 - val_rmse: 0.5168 - val_mae: 0.2626 - val_mape: 8.3048 - lr: 1.0000e-05\n",
      "Epoch 393/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2628 - mse: 0.2664 - rmse: 0.5161 - mae: 0.2628 - mape: 8.3669\n",
      "Epoch 393: val_loss did not improve from 0.26202\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2628 - mse: 0.2669 - rmse: 0.5167 - mae: 0.2628 - mape: 8.3489 - val_loss: 0.2625 - val_mse: 0.2662 - val_rmse: 0.5160 - val_mae: 0.2625 - val_mape: 8.3284 - lr: 1.0000e-05\n",
      "Epoch 394/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2645 - mse: 0.2700 - rmse: 0.5196 - mae: 0.2645 - mape: 8.4015\n",
      "Epoch 394: val_loss did not improve from 0.26202\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2627 - mse: 0.2663 - rmse: 0.5160 - mae: 0.2627 - mape: 8.3399 - val_loss: 0.2623 - val_mse: 0.2669 - val_rmse: 0.5166 - val_mae: 0.2623 - val_mape: 8.3664 - lr: 1.0000e-05\n",
      "Epoch 395/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2613 - mape: 8.2946\n",
      "Epoch 395: val_loss did not improve from 0.26202\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2627 - mse: 0.2665 - rmse: 0.5162 - mae: 0.2627 - mape: 8.3370 - val_loss: 0.2633 - val_mse: 0.2671 - val_rmse: 0.5168 - val_mae: 0.2633 - val_mape: 8.4016 - lr: 1.0000e-05\n",
      "Epoch 396/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2622 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2622 - mape: 8.3215\n",
      "Epoch 396: val_loss did not improve from 0.26202\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2626 - mse: 0.2665 - rmse: 0.5163 - mae: 0.2626 - mape: 8.3419 - val_loss: 0.2621 - val_mse: 0.2640 - val_rmse: 0.5138 - val_mae: 0.2621 - val_mape: 8.3079 - lr: 1.0000e-05\n",
      "Epoch 397/1000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2643 - mse: 0.2708 - rmse: 0.5204 - mae: 0.2643 - mape: 8.3583\n",
      "Epoch 397: val_loss did not improve from 0.26202\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2628 - mse: 0.2665 - rmse: 0.5162 - mae: 0.2628 - mape: 8.3386 - val_loss: 0.2638 - val_mse: 0.2707 - val_rmse: 0.5203 - val_mae: 0.2638 - val_mape: 8.4344 - lr: 1.0000e-05\n",
      "Epoch 398/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2620 - mse: 0.2667 - rmse: 0.5165 - mae: 0.2620 - mape: 8.3252\n",
      "Epoch 398: val_loss did not improve from 0.26202\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2623 - mse: 0.2665 - rmse: 0.5163 - mae: 0.2623 - mape: 8.3382 - val_loss: 0.2621 - val_mse: 0.2657 - val_rmse: 0.5155 - val_mae: 0.2621 - val_mape: 8.3279 - lr: 1.0000e-05\n",
      "Epoch 399/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2625 - mse: 0.2660 - rmse: 0.5158 - mae: 0.2625 - mape: 8.3607\n",
      "Epoch 399: val_loss did not improve from 0.26202\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2630 - mse: 0.2669 - rmse: 0.5166 - mae: 0.2630 - mape: 8.3505 - val_loss: 0.2626 - val_mse: 0.2671 - val_rmse: 0.5168 - val_mae: 0.2626 - val_mape: 8.3904 - lr: 1.0000e-05\n",
      "Epoch 400/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2618 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2618 - mape: 8.3244\n",
      "Epoch 400: val_loss did not improve from 0.26202\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2624 - mse: 0.2661 - rmse: 0.5158 - mae: 0.2624 - mape: 8.3374 - val_loss: 0.2632 - val_mse: 0.2677 - val_rmse: 0.5174 - val_mae: 0.2632 - val_mape: 8.3132 - lr: 1.0000e-05\n",
      "Epoch 401/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2617 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2617 - mape: 8.3252\n",
      "Epoch 401: val_loss improved from 0.26202 to 0.26198, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2628 - mse: 0.2669 - rmse: 0.5166 - mae: 0.2628 - mape: 8.3381 - val_loss: 0.2620 - val_mse: 0.2662 - val_rmse: 0.5159 - val_mae: 0.2620 - val_mape: 8.3155 - lr: 1.0000e-05\n",
      "Epoch 402/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2622 - mse: 0.2648 - rmse: 0.5146 - mae: 0.2622 - mape: 8.3189\n",
      "Epoch 402: val_loss did not improve from 0.26198\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2626 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2626 - mape: 8.3393 - val_loss: 0.2623 - val_mse: 0.2663 - val_rmse: 0.5161 - val_mae: 0.2623 - val_mape: 8.2953 - lr: 1.0000e-05\n",
      "Epoch 403/1000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.2607 - mse: 0.2658 - rmse: 0.5156 - mae: 0.2607 - mape: 8.3111\n",
      "Epoch 403: val_loss did not improve from 0.26198\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2621 - mse: 0.2660 - rmse: 0.5158 - mae: 0.2621 - mape: 8.3360 - val_loss: 0.2655 - val_mse: 0.2712 - val_rmse: 0.5208 - val_mae: 0.2655 - val_mape: 8.3281 - lr: 1.0000e-05\n",
      "Epoch 404/1000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2613 - mse: 0.2617 - rmse: 0.5115 - mae: 0.2613 - mape: 8.2699\n",
      "Epoch 404: val_loss did not improve from 0.26198\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2630 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2630 - mape: 8.3433 - val_loss: 0.2623 - val_mse: 0.2662 - val_rmse: 0.5160 - val_mae: 0.2623 - val_mape: 8.3063 - lr: 1.0000e-05\n",
      "Epoch 405/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2639 - mse: 0.2690 - rmse: 0.5186 - mae: 0.2639 - mape: 8.3724\n",
      "Epoch 405: val_loss did not improve from 0.26198\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2624 - mse: 0.2664 - rmse: 0.5161 - mae: 0.2624 - mape: 8.3334 - val_loss: 0.2623 - val_mse: 0.2663 - val_rmse: 0.5160 - val_mae: 0.2623 - val_mape: 8.2956 - lr: 1.0000e-05\n",
      "Epoch 406/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2625 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2625 - mape: 8.3326\n",
      "Epoch 406: val_loss did not improve from 0.26198\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2625 - mse: 0.2658 - rmse: 0.5155 - mae: 0.2625 - mape: 8.3239 - val_loss: 0.2629 - val_mse: 0.2667 - val_rmse: 0.5164 - val_mae: 0.2629 - val_mape: 8.2816 - lr: 1.0000e-05\n",
      "Epoch 407/1000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2626 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2626 - mape: 8.3095\n",
      "Epoch 407: val_loss improved from 0.26198 to 0.26183, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2627 - mse: 0.2665 - rmse: 0.5162 - mae: 0.2627 - mape: 8.3285 - val_loss: 0.2618 - val_mse: 0.2662 - val_rmse: 0.5160 - val_mae: 0.2618 - val_mape: 8.3192 - lr: 1.0000e-05\n",
      "Epoch 408/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2614 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2614 - mape: 8.3048\n",
      "Epoch 408: val_loss improved from 0.26183 to 0.26170, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2626 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2626 - mape: 8.3363 - val_loss: 0.2617 - val_mse: 0.2658 - val_rmse: 0.5155 - val_mae: 0.2617 - val_mape: 8.3189 - lr: 1.0000e-05\n",
      "Epoch 409/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2631 - mse: 0.2665 - rmse: 0.5163 - mae: 0.2631 - mape: 8.3590\n",
      "Epoch 409: val_loss did not improve from 0.26170\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2625 - mse: 0.2658 - rmse: 0.5156 - mae: 0.2625 - mape: 8.3344 - val_loss: 0.2627 - val_mse: 0.2676 - val_rmse: 0.5173 - val_mae: 0.2627 - val_mape: 8.3386 - lr: 1.0000e-05\n",
      "Epoch 410/1000\n",
      "258/318 [=======================>......] - ETA: 0s - loss: 0.2615 - mse: 0.2636 - rmse: 0.5135 - mae: 0.2615 - mape: 8.2811\n",
      "Epoch 410: val_loss did not improve from 0.26170\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2625 - mse: 0.2665 - rmse: 0.5162 - mae: 0.2625 - mape: 8.3300 - val_loss: 0.2630 - val_mse: 0.2658 - val_rmse: 0.5156 - val_mae: 0.2630 - val_mape: 8.3827 - lr: 1.0000e-05\n",
      "Epoch 411/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2640 - mse: 0.2682 - rmse: 0.5178 - mae: 0.2640 - mape: 8.3664\n",
      "Epoch 411: val_loss did not improve from 0.26170\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2627 - mse: 0.2667 - rmse: 0.5165 - mae: 0.2627 - mape: 8.3394 - val_loss: 0.2619 - val_mse: 0.2663 - val_rmse: 0.5160 - val_mae: 0.2619 - val_mape: 8.3065 - lr: 1.0000e-05\n",
      "Epoch 412/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2641 - mse: 0.2688 - rmse: 0.5185 - mae: 0.2641 - mape: 8.3513\n",
      "Epoch 412: val_loss did not improve from 0.26170\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2622 - mse: 0.2652 - rmse: 0.5150 - mae: 0.2622 - mape: 8.3155 - val_loss: 0.2632 - val_mse: 0.2691 - val_rmse: 0.5187 - val_mae: 0.2632 - val_mape: 8.3423 - lr: 1.0000e-05\n",
      "Epoch 413/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2636 - mse: 0.2677 - rmse: 0.5174 - mae: 0.2636 - mape: 8.3629\n",
      "Epoch 413: val_loss did not improve from 0.26170\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2626 - mse: 0.2666 - rmse: 0.5163 - mae: 0.2626 - mape: 8.3367 - val_loss: 0.2640 - val_mse: 0.2688 - val_rmse: 0.5185 - val_mae: 0.2640 - val_mape: 8.4313 - lr: 1.0000e-05\n",
      "Epoch 414/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2623 - mse: 0.2663 - rmse: 0.5161 - mae: 0.2623 - mape: 8.3419\n",
      "Epoch 414: val_loss did not improve from 0.26170\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2630 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2630 - mape: 8.3453 - val_loss: 0.2617 - val_mse: 0.2655 - val_rmse: 0.5152 - val_mae: 0.2617 - val_mape: 8.3251 - lr: 1.0000e-05\n",
      "Epoch 415/1000\n",
      "258/318 [=======================>......] - ETA: 0s - loss: 0.2617 - mse: 0.2643 - rmse: 0.5141 - mae: 0.2617 - mape: 8.3092\n",
      "Epoch 415: val_loss did not improve from 0.26170\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2624 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2624 - mape: 8.3355 - val_loss: 0.2620 - val_mse: 0.2662 - val_rmse: 0.5160 - val_mae: 0.2620 - val_mape: 8.3258 - lr: 1.0000e-05\n",
      "Epoch 416/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2624 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2624 - mape: 8.3275\n",
      "Epoch 416: val_loss did not improve from 0.26170\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2623 - mse: 0.2666 - rmse: 0.5164 - mae: 0.2623 - mape: 8.3275 - val_loss: 0.2619 - val_mse: 0.2659 - val_rmse: 0.5157 - val_mae: 0.2619 - val_mape: 8.3143 - lr: 1.0000e-05\n",
      "Epoch 417/1000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2615 - mse: 0.2635 - rmse: 0.5134 - mae: 0.2615 - mape: 8.3324\n",
      "Epoch 417: val_loss did not improve from 0.26170\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2624 - mse: 0.2652 - rmse: 0.5149 - mae: 0.2624 - mape: 8.3172 - val_loss: 0.2626 - val_mse: 0.2678 - val_rmse: 0.5175 - val_mae: 0.2626 - val_mape: 8.3832 - lr: 1.0000e-05\n",
      "Epoch 418/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2622 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2622 - mape: 8.2977\n",
      "Epoch 418: val_loss did not improve from 0.26170\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2623 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2623 - mape: 8.3358 - val_loss: 0.2621 - val_mse: 0.2650 - val_rmse: 0.5147 - val_mae: 0.2621 - val_mape: 8.2803 - lr: 1.0000e-05\n",
      "Epoch 419/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2637 - mse: 0.2678 - rmse: 0.5174 - mae: 0.2637 - mape: 8.3638\n",
      "Epoch 419: val_loss did not improve from 0.26170\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2622 - mse: 0.2655 - rmse: 0.5152 - mae: 0.2622 - mape: 8.3216 - val_loss: 0.2618 - val_mse: 0.2651 - val_rmse: 0.5149 - val_mae: 0.2618 - val_mape: 8.2948 - lr: 1.0000e-05\n",
      "Epoch 420/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2632 - mse: 0.2685 - rmse: 0.5182 - mae: 0.2632 - mape: 8.3346\n",
      "Epoch 420: val_loss improved from 0.26170 to 0.26169, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2624 - mse: 0.2661 - rmse: 0.5158 - mae: 0.2624 - mape: 8.3220 - val_loss: 0.2617 - val_mse: 0.2656 - val_rmse: 0.5153 - val_mae: 0.2617 - val_mape: 8.3227 - lr: 1.0000e-05\n",
      "Epoch 421/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2630 - mse: 0.2685 - rmse: 0.5181 - mae: 0.2630 - mape: 8.3507\n",
      "Epoch 421: val_loss did not improve from 0.26169\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2626 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2626 - mape: 8.3299 - val_loss: 0.2619 - val_mse: 0.2664 - val_rmse: 0.5161 - val_mae: 0.2619 - val_mape: 8.3265 - lr: 1.0000e-05\n",
      "Epoch 422/1000\n",
      "259/318 [=======================>......] - ETA: 0s - loss: 0.2613 - mse: 0.2635 - rmse: 0.5134 - mae: 0.2613 - mape: 8.3352\n",
      "Epoch 422: val_loss did not improve from 0.26169\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2624 - mse: 0.2666 - rmse: 0.5163 - mae: 0.2624 - mape: 8.3269 - val_loss: 0.2624 - val_mse: 0.2670 - val_rmse: 0.5167 - val_mae: 0.2624 - val_mape: 8.3810 - lr: 1.0000e-05\n",
      "Epoch 423/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2633 - mse: 0.2686 - rmse: 0.5183 - mae: 0.2633 - mape: 8.3765\n",
      "Epoch 423: val_loss did not improve from 0.26169\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2622 - mse: 0.2659 - rmse: 0.5156 - mae: 0.2622 - mape: 8.3383 - val_loss: 0.2626 - val_mse: 0.2671 - val_rmse: 0.5168 - val_mae: 0.2626 - val_mape: 8.2925 - lr: 1.0000e-05\n",
      "Epoch 424/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2610 - mse: 0.2633 - rmse: 0.5132 - mae: 0.2610 - mape: 8.2871\n",
      "Epoch 424: val_loss did not improve from 0.26169\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2622 - mse: 0.2664 - rmse: 0.5161 - mae: 0.2622 - mape: 8.3355 - val_loss: 0.2623 - val_mse: 0.2681 - val_rmse: 0.5177 - val_mae: 0.2623 - val_mape: 8.3527 - lr: 1.0000e-05\n",
      "Epoch 425/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2622 - mse: 0.2656 - rmse: 0.5153 - mae: 0.2622 - mape: 8.3299\n",
      "Epoch 425: val_loss did not improve from 0.26169\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2622 - mse: 0.2655 - rmse: 0.5153 - mae: 0.2622 - mape: 8.3299 - val_loss: 0.2622 - val_mse: 0.2665 - val_rmse: 0.5163 - val_mae: 0.2622 - val_mape: 8.2846 - lr: 1.0000e-05\n",
      "Epoch 426/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2611 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2611 - mape: 8.2783\n",
      "Epoch 426: val_loss did not improve from 0.26169\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2622 - mse: 0.2658 - rmse: 0.5155 - mae: 0.2622 - mape: 8.3232 - val_loss: 0.2617 - val_mse: 0.2653 - val_rmse: 0.5151 - val_mae: 0.2617 - val_mape: 8.3167 - lr: 1.0000e-05\n",
      "Epoch 427/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2638 - mse: 0.2691 - rmse: 0.5188 - mae: 0.2638 - mape: 8.3843\n",
      "Epoch 427: val_loss did not improve from 0.26169\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2624 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2624 - mape: 8.3313 - val_loss: 0.2618 - val_mse: 0.2659 - val_rmse: 0.5157 - val_mae: 0.2618 - val_mape: 8.3439 - lr: 1.0000e-05\n",
      "Epoch 428/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2623 - mse: 0.2666 - rmse: 0.5163 - mae: 0.2623 - mape: 8.3173\n",
      "Epoch 428: val_loss did not improve from 0.26169\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2622 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2622 - mape: 8.3150 - val_loss: 0.2622 - val_mse: 0.2642 - val_rmse: 0.5140 - val_mae: 0.2622 - val_mape: 8.3247 - lr: 1.0000e-05\n",
      "Epoch 429/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2639 - mse: 0.2695 - rmse: 0.5191 - mae: 0.2639 - mape: 8.3806\n",
      "Epoch 429: val_loss improved from 0.26169 to 0.26161, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2627 - mse: 0.2669 - rmse: 0.5166 - mae: 0.2627 - mape: 8.3402 - val_loss: 0.2616 - val_mse: 0.2649 - val_rmse: 0.5147 - val_mae: 0.2616 - val_mape: 8.2915 - lr: 1.0000e-05\n",
      "Epoch 430/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2621 - mse: 0.2662 - rmse: 0.5159 - mae: 0.2621 - mape: 8.3112\n",
      "Epoch 430: val_loss did not improve from 0.26161\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2621 - mse: 0.2663 - rmse: 0.5160 - mae: 0.2621 - mape: 8.3147 - val_loss: 0.2627 - val_mse: 0.2655 - val_rmse: 0.5152 - val_mae: 0.2627 - val_mape: 8.3907 - lr: 1.0000e-05\n",
      "Epoch 431/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2621 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2621 - mape: 8.3234\n",
      "Epoch 431: val_loss improved from 0.26161 to 0.26158, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2625 - mse: 0.2658 - rmse: 0.5155 - mae: 0.2625 - mape: 8.3330 - val_loss: 0.2616 - val_mse: 0.2656 - val_rmse: 0.5154 - val_mae: 0.2616 - val_mape: 8.3112 - lr: 1.0000e-05\n",
      "Epoch 432/1000\n",
      "257/318 [=======================>......] - ETA: 0s - loss: 0.2610 - mse: 0.2634 - rmse: 0.5133 - mae: 0.2610 - mape: 8.2911\n",
      "Epoch 432: val_loss did not improve from 0.26158\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2621 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2621 - mape: 8.3321 - val_loss: 0.2636 - val_mse: 0.2683 - val_rmse: 0.5180 - val_mae: 0.2636 - val_mape: 8.2921 - lr: 1.0000e-05\n",
      "Epoch 433/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2629 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2629 - mape: 8.3454\n",
      "Epoch 433: val_loss did not improve from 0.26158\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2622 - mse: 0.2663 - rmse: 0.5161 - mae: 0.2622 - mape: 8.3282 - val_loss: 0.2620 - val_mse: 0.2657 - val_rmse: 0.5154 - val_mae: 0.2620 - val_mape: 8.2750 - lr: 1.0000e-05\n",
      "Epoch 434/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2628 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2628 - mape: 8.3498\n",
      "Epoch 434: val_loss did not improve from 0.26158\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2620 - mse: 0.2658 - rmse: 0.5156 - mae: 0.2620 - mape: 8.3198 - val_loss: 0.2617 - val_mse: 0.2657 - val_rmse: 0.5155 - val_mae: 0.2617 - val_mape: 8.2936 - lr: 1.0000e-05\n",
      "Epoch 435/1000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2614 - mse: 0.2638 - rmse: 0.5136 - mae: 0.2614 - mape: 8.3109\n",
      "Epoch 435: val_loss did not improve from 0.26158\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2618 - mse: 0.2652 - rmse: 0.5150 - mae: 0.2618 - mape: 8.2988 - val_loss: 0.2631 - val_mse: 0.2688 - val_rmse: 0.5185 - val_mae: 0.2631 - val_mape: 8.3978 - lr: 1.0000e-05\n",
      "Epoch 436/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2626 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2626 - mape: 8.3313\n",
      "Epoch 436: val_loss improved from 0.26158 to 0.26149, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2623 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2623 - mape: 8.3206 - val_loss: 0.2615 - val_mse: 0.2652 - val_rmse: 0.5150 - val_mae: 0.2615 - val_mape: 8.3096 - lr: 1.0000e-05\n",
      "Epoch 437/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2634 - mse: 0.2695 - rmse: 0.5191 - mae: 0.2634 - mape: 8.3826\n",
      "Epoch 437: val_loss did not improve from 0.26149\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2621 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2621 - mape: 8.3331 - val_loss: 0.2619 - val_mse: 0.2631 - val_rmse: 0.5130 - val_mae: 0.2619 - val_mape: 8.2781 - lr: 1.0000e-05\n",
      "Epoch 438/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2630 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2630 - mape: 8.3512\n",
      "Epoch 438: val_loss did not improve from 0.26149\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2626 - mse: 0.2664 - rmse: 0.5161 - mae: 0.2626 - mape: 8.3267 - val_loss: 0.2622 - val_mse: 0.2641 - val_rmse: 0.5139 - val_mae: 0.2622 - val_mape: 8.3459 - lr: 1.0000e-05\n",
      "Epoch 439/1000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2617 - mse: 0.2659 - rmse: 0.5156 - mae: 0.2617 - mape: 8.3076\n",
      "Epoch 439: val_loss did not improve from 0.26149\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2620 - mse: 0.2661 - rmse: 0.5159 - mae: 0.2620 - mape: 8.3166 - val_loss: 0.2618 - val_mse: 0.2641 - val_rmse: 0.5139 - val_mae: 0.2618 - val_mape: 8.3290 - lr: 1.0000e-05\n",
      "Epoch 440/1000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.2623 - mse: 0.2664 - rmse: 0.5162 - mae: 0.2623 - mape: 8.3308\n",
      "Epoch 440: val_loss did not improve from 0.26149\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2623 - mse: 0.2660 - rmse: 0.5157 - mae: 0.2623 - mape: 8.3280 - val_loss: 0.2618 - val_mse: 0.2643 - val_rmse: 0.5141 - val_mae: 0.2618 - val_mape: 8.2731 - lr: 1.0000e-05\n",
      "Epoch 441/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2620 - mse: 0.2664 - rmse: 0.5161 - mae: 0.2620 - mape: 8.3205\n",
      "Epoch 441: val_loss did not improve from 0.26149\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2621 - mse: 0.2656 - rmse: 0.5153 - mae: 0.2621 - mape: 8.3236 - val_loss: 0.2618 - val_mse: 0.2662 - val_rmse: 0.5160 - val_mae: 0.2618 - val_mape: 8.2910 - lr: 1.0000e-05\n",
      "Epoch 442/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2613 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2613 - mape: 8.2819\n",
      "Epoch 442: val_loss did not improve from 0.26149\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2619 - mse: 0.2639 - rmse: 0.5138 - mae: 0.2619 - mape: 8.3068 - val_loss: 0.2620 - val_mse: 0.2672 - val_rmse: 0.5169 - val_mae: 0.2620 - val_mape: 8.3121 - lr: 1.0000e-05\n",
      "Epoch 443/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2625 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2625 - mape: 8.3249\n",
      "Epoch 443: val_loss did not improve from 0.26149\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2623 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2623 - mape: 8.3293 - val_loss: 0.2619 - val_mse: 0.2633 - val_rmse: 0.5131 - val_mae: 0.2619 - val_mape: 8.3181 - lr: 1.0000e-05\n",
      "Epoch 444/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2628 - mse: 0.2662 - rmse: 0.5159 - mae: 0.2628 - mape: 8.3452\n",
      "Epoch 444: val_loss did not improve from 0.26149\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2625 - mse: 0.2652 - rmse: 0.5149 - mae: 0.2625 - mape: 8.3269 - val_loss: 0.2617 - val_mse: 0.2632 - val_rmse: 0.5131 - val_mae: 0.2617 - val_mape: 8.3039 - lr: 1.0000e-05\n",
      "Epoch 445/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2620 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2620 - mape: 8.2909\n",
      "Epoch 445: val_loss did not improve from 0.26149\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2620 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2620 - mape: 8.3217 - val_loss: 0.2620 - val_mse: 0.2671 - val_rmse: 0.5169 - val_mae: 0.2620 - val_mape: 8.2911 - lr: 1.0000e-05\n",
      "Epoch 446/1000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2632 - mse: 0.2692 - rmse: 0.5189 - mae: 0.2632 - mape: 8.3634\n",
      "Epoch 446: val_loss improved from 0.26149 to 0.26137, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2621 - mse: 0.2658 - rmse: 0.5156 - mae: 0.2621 - mape: 8.3209 - val_loss: 0.2614 - val_mse: 0.2641 - val_rmse: 0.5139 - val_mae: 0.2614 - val_mape: 8.2856 - lr: 1.0000e-05\n",
      "Epoch 447/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2622 - mse: 0.2653 - rmse: 0.5151 - mae: 0.2622 - mape: 8.3342\n",
      "Epoch 447: val_loss did not improve from 0.26137\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2620 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2620 - mape: 8.3176 - val_loss: 0.2618 - val_mse: 0.2638 - val_rmse: 0.5136 - val_mae: 0.2618 - val_mape: 8.2693 - lr: 1.0000e-05\n",
      "Epoch 448/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2627 - rmse: 0.5126 - mae: 0.2609 - mape: 8.2922\n",
      "Epoch 448: val_loss did not improve from 0.26137\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2620 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2620 - mape: 8.3110 - val_loss: 0.2617 - val_mse: 0.2648 - val_rmse: 0.5145 - val_mae: 0.2617 - val_mape: 8.2896 - lr: 1.0000e-05\n",
      "Epoch 449/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2627 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2627 - mape: 8.3534\n",
      "Epoch 449: val_loss did not improve from 0.26137\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2621 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2621 - mape: 8.3146 - val_loss: 0.2625 - val_mse: 0.2660 - val_rmse: 0.5158 - val_mae: 0.2625 - val_mape: 8.3618 - lr: 1.0000e-05\n",
      "Epoch 450/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2610 - mse: 0.2631 - rmse: 0.5130 - mae: 0.2610 - mape: 8.3149\n",
      "Epoch 450: val_loss improved from 0.26137 to 0.26137, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2621 - mse: 0.2660 - rmse: 0.5157 - mae: 0.2621 - mape: 8.3238 - val_loss: 0.2614 - val_mse: 0.2636 - val_rmse: 0.5134 - val_mae: 0.2614 - val_mape: 8.2980 - lr: 1.0000e-05\n",
      "Epoch 451/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2632 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2632 - mape: 8.3591\n",
      "Epoch 451: val_loss did not improve from 0.26137\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2621 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2621 - mape: 8.3223 - val_loss: 0.2617 - val_mse: 0.2652 - val_rmse: 0.5149 - val_mae: 0.2617 - val_mape: 8.3290 - lr: 1.0000e-05\n",
      "Epoch 452/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2618 - mse: 0.2661 - rmse: 0.5159 - mae: 0.2618 - mape: 8.3202\n",
      "Epoch 452: val_loss did not improve from 0.26137\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2619 - mse: 0.2662 - rmse: 0.5159 - mae: 0.2619 - mape: 8.3172 - val_loss: 0.2614 - val_mse: 0.2651 - val_rmse: 0.5149 - val_mae: 0.2614 - val_mape: 8.3160 - lr: 1.0000e-05\n",
      "Epoch 453/1000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2624 - mse: 0.2654 - rmse: 0.5151 - mae: 0.2624 - mape: 8.3248\n",
      "Epoch 453: val_loss improved from 0.26137 to 0.26134, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2619 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2619 - mape: 8.3138 - val_loss: 0.2613 - val_mse: 0.2643 - val_rmse: 0.5141 - val_mae: 0.2613 - val_mape: 8.2928 - lr: 1.0000e-05\n",
      "Epoch 454/1000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2612 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2612 - mape: 8.3006\n",
      "Epoch 454: val_loss did not improve from 0.26134\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2618 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2618 - mape: 8.3117 - val_loss: 0.2615 - val_mse: 0.2651 - val_rmse: 0.5149 - val_mae: 0.2615 - val_mape: 8.2864 - lr: 1.0000e-05\n",
      "Epoch 455/1000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2617 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2617 - mape: 8.3186\n",
      "Epoch 455: val_loss did not improve from 0.26134\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2621 - mse: 0.2660 - rmse: 0.5157 - mae: 0.2621 - mape: 8.3134 - val_loss: 0.2618 - val_mse: 0.2631 - val_rmse: 0.5129 - val_mae: 0.2618 - val_mape: 8.3184 - lr: 1.0000e-05\n",
      "Epoch 456/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2612 - mse: 0.2639 - rmse: 0.5137 - mae: 0.2612 - mape: 8.3008\n",
      "Epoch 456: val_loss did not improve from 0.26134\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2619 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2619 - mape: 8.3172 - val_loss: 0.2617 - val_mse: 0.2633 - val_rmse: 0.5132 - val_mae: 0.2617 - val_mape: 8.3125 - lr: 1.0000e-05\n",
      "Epoch 457/1000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.2619 - mse: 0.2662 - rmse: 0.5159 - mae: 0.2619 - mape: 8.3118\n",
      "Epoch 457: val_loss did not improve from 0.26134\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2618 - mse: 0.2654 - rmse: 0.5152 - mae: 0.2618 - mape: 8.3103 - val_loss: 0.2629 - val_mse: 0.2649 - val_rmse: 0.5147 - val_mae: 0.2629 - val_mape: 8.3813 - lr: 1.0000e-05\n",
      "Epoch 458/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2623 - mse: 0.2658 - rmse: 0.5155 - mae: 0.2623 - mape: 8.3448\n",
      "Epoch 458: val_loss did not improve from 0.26134\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2621 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2621 - mape: 8.3175 - val_loss: 0.2625 - val_mse: 0.2686 - val_rmse: 0.5182 - val_mae: 0.2625 - val_mape: 8.3933 - lr: 1.0000e-05\n",
      "Epoch 459/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2618 - mse: 0.2650 - rmse: 0.5148 - mae: 0.2618 - mape: 8.2990\n",
      "Epoch 459: val_loss did not improve from 0.26134\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2616 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2616 - mape: 8.3123 - val_loss: 0.2621 - val_mse: 0.2669 - val_rmse: 0.5166 - val_mae: 0.2621 - val_mape: 8.2949 - lr: 1.0000e-05\n",
      "Epoch 460/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2615 - mse: 0.2649 - rmse: 0.5146 - mae: 0.2615 - mape: 8.2953\n",
      "Epoch 460: val_loss improved from 0.26134 to 0.26126, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2618 - mse: 0.2652 - rmse: 0.5150 - mae: 0.2618 - mape: 8.3081 - val_loss: 0.2613 - val_mse: 0.2630 - val_rmse: 0.5129 - val_mae: 0.2613 - val_mape: 8.2876 - lr: 1.0000e-05\n",
      "Epoch 461/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2616 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2616 - mape: 8.3205\n",
      "Epoch 461: val_loss improved from 0.26126 to 0.26121, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2619 - mse: 0.2648 - rmse: 0.5145 - mae: 0.2619 - mape: 8.3140 - val_loss: 0.2612 - val_mse: 0.2640 - val_rmse: 0.5138 - val_mae: 0.2612 - val_mape: 8.2762 - lr: 1.0000e-05\n",
      "Epoch 462/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2613 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2613 - mape: 8.2937\n",
      "Epoch 462: val_loss did not improve from 0.26121\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2617 - mse: 0.2643 - rmse: 0.5141 - mae: 0.2617 - mape: 8.3124 - val_loss: 0.2612 - val_mse: 0.2653 - val_rmse: 0.5151 - val_mae: 0.2612 - val_mape: 8.3076 - lr: 1.0000e-05\n",
      "Epoch 463/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2633 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2633 - mape: 8.3606\n",
      "Epoch 463: val_loss did not improve from 0.26121\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2618 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2618 - mape: 8.3086 - val_loss: 0.2616 - val_mse: 0.2647 - val_rmse: 0.5145 - val_mae: 0.2616 - val_mape: 8.3316 - lr: 1.0000e-05\n",
      "Epoch 464/1000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2600 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2600 - mape: 8.2564\n",
      "Epoch 464: val_loss did not improve from 0.26121\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2618 - mse: 0.2654 - rmse: 0.5151 - mae: 0.2618 - mape: 8.3132 - val_loss: 0.2613 - val_mse: 0.2648 - val_rmse: 0.5146 - val_mae: 0.2613 - val_mape: 8.3084 - lr: 1.0000e-05\n",
      "Epoch 465/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2628 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2628 - mape: 8.3610\n",
      "Epoch 465: val_loss did not improve from 0.26121\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2615 - mse: 0.2648 - rmse: 0.5146 - mae: 0.2615 - mape: 8.3129 - val_loss: 0.2624 - val_mse: 0.2636 - val_rmse: 0.5134 - val_mae: 0.2624 - val_mape: 8.3651 - lr: 1.0000e-05\n",
      "Epoch 466/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2617 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2617 - mape: 8.3065\n",
      "Epoch 466: val_loss improved from 0.26121 to 0.26113, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2617 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2617 - mape: 8.3065 - val_loss: 0.2611 - val_mse: 0.2634 - val_rmse: 0.5132 - val_mae: 0.2611 - val_mape: 8.2955 - lr: 1.0000e-05\n",
      "Epoch 467/1000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2611 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2611 - mape: 8.3017\n",
      "Epoch 467: val_loss did not improve from 0.26113\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2618 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2618 - mape: 8.3099 - val_loss: 0.2615 - val_mse: 0.2638 - val_rmse: 0.5136 - val_mae: 0.2615 - val_mape: 8.2896 - lr: 1.0000e-05\n",
      "Epoch 468/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2618 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2618 - mape: 8.3225\n",
      "Epoch 468: val_loss did not improve from 0.26113\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2617 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2617 - mape: 8.3020 - val_loss: 0.2614 - val_mse: 0.2662 - val_rmse: 0.5159 - val_mae: 0.2614 - val_mape: 8.3357 - lr: 1.0000e-05\n",
      "Epoch 469/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2620 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2620 - mape: 8.3216\n",
      "Epoch 469: val_loss did not improve from 0.26113\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2618 - mse: 0.2650 - rmse: 0.5148 - mae: 0.2618 - mape: 8.3182 - val_loss: 0.2615 - val_mse: 0.2652 - val_rmse: 0.5149 - val_mae: 0.2615 - val_mape: 8.2656 - lr: 1.0000e-05\n",
      "Epoch 470/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2632 - mse: 0.2681 - rmse: 0.5178 - mae: 0.2632 - mape: 8.3539\n",
      "Epoch 470: val_loss did not improve from 0.26113\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2618 - mse: 0.2657 - rmse: 0.5154 - mae: 0.2618 - mape: 8.3152 - val_loss: 0.2619 - val_mse: 0.2664 - val_rmse: 0.5162 - val_mae: 0.2619 - val_mape: 8.2818 - lr: 1.0000e-05\n",
      "Epoch 471/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2623 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2623 - mape: 8.3253\n",
      "Epoch 471: val_loss did not improve from 0.26113\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2617 - mse: 0.2648 - rmse: 0.5146 - mae: 0.2617 - mape: 8.3088 - val_loss: 0.2615 - val_mse: 0.2638 - val_rmse: 0.5136 - val_mae: 0.2615 - val_mape: 8.3293 - lr: 1.0000e-05\n",
      "Epoch 472/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2623 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2623 - mape: 8.3132\n",
      "Epoch 472: val_loss improved from 0.26113 to 0.26102, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2617 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2617 - mape: 8.3071 - val_loss: 0.2610 - val_mse: 0.2637 - val_rmse: 0.5135 - val_mae: 0.2610 - val_mape: 8.2947 - lr: 1.0000e-05\n",
      "Epoch 473/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2615 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2615 - mape: 8.3108\n",
      "Epoch 473: val_loss improved from 0.26102 to 0.26101, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2616 - mse: 0.2648 - rmse: 0.5146 - mae: 0.2616 - mape: 8.3134 - val_loss: 0.2610 - val_mse: 0.2626 - val_rmse: 0.5124 - val_mae: 0.2610 - val_mape: 8.2764 - lr: 1.0000e-05\n",
      "Epoch 474/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2596 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2596 - mape: 8.2402\n",
      "Epoch 474: val_loss did not improve from 0.26101\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2618 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2618 - mape: 8.3059 - val_loss: 0.2613 - val_mse: 0.2637 - val_rmse: 0.5136 - val_mae: 0.2613 - val_mape: 8.2909 - lr: 1.0000e-05\n",
      "Epoch 475/1000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2627 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2627 - mape: 8.3303\n",
      "Epoch 475: val_loss did not improve from 0.26101\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2616 - mse: 0.2648 - rmse: 0.5146 - mae: 0.2616 - mape: 8.3034 - val_loss: 0.2629 - val_mse: 0.2647 - val_rmse: 0.5145 - val_mae: 0.2629 - val_mape: 8.3970 - lr: 1.0000e-05\n",
      "Epoch 476/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2625 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2625 - mape: 8.3232\n",
      "Epoch 476: val_loss did not improve from 0.26101\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2618 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2618 - mape: 8.3075 - val_loss: 0.2611 - val_mse: 0.2630 - val_rmse: 0.5128 - val_mae: 0.2611 - val_mape: 8.2836 - lr: 1.0000e-05\n",
      "Epoch 477/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2619 - mse: 0.2648 - rmse: 0.5146 - mae: 0.2619 - mape: 8.3142\n",
      "Epoch 477: val_loss did not improve from 0.26101\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2650 - rmse: 0.5148 - mae: 0.2619 - mape: 8.3197 - val_loss: 0.2621 - val_mse: 0.2655 - val_rmse: 0.5152 - val_mae: 0.2621 - val_mape: 8.2566 - lr: 1.0000e-05\n",
      "Epoch 478/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2615 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2615 - mape: 8.3150\n",
      "Epoch 478: val_loss did not improve from 0.26101\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2619 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2619 - mape: 8.3105 - val_loss: 0.2611 - val_mse: 0.2628 - val_rmse: 0.5127 - val_mae: 0.2611 - val_mape: 8.2562 - lr: 1.0000e-05\n",
      "Epoch 479/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2625 - mse: 0.2655 - rmse: 0.5153 - mae: 0.2625 - mape: 8.3452\n",
      "Epoch 479: val_loss improved from 0.26101 to 0.26100, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2616 - mse: 0.2639 - rmse: 0.5137 - mae: 0.2616 - mape: 8.3055 - val_loss: 0.2610 - val_mse: 0.2630 - val_rmse: 0.5128 - val_mae: 0.2610 - val_mape: 8.2734 - lr: 1.0000e-05\n",
      "Epoch 480/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2606 - mse: 0.2629 - rmse: 0.5127 - mae: 0.2606 - mape: 8.2663\n",
      "Epoch 480: val_loss did not improve from 0.26100\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2615 - mse: 0.2638 - rmse: 0.5136 - mae: 0.2615 - mape: 8.2895 - val_loss: 0.2615 - val_mse: 0.2656 - val_rmse: 0.5153 - val_mae: 0.2615 - val_mape: 8.3339 - lr: 1.0000e-05\n",
      "Epoch 481/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2612 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2612 - mape: 8.2846\n",
      "Epoch 481: val_loss did not improve from 0.26100\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2617 - mse: 0.2650 - rmse: 0.5148 - mae: 0.2617 - mape: 8.3067 - val_loss: 0.2612 - val_mse: 0.2645 - val_rmse: 0.5143 - val_mae: 0.2612 - val_mape: 8.2999 - lr: 1.0000e-05\n",
      "Epoch 482/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2624 - mse: 0.2660 - rmse: 0.5158 - mae: 0.2624 - mape: 8.3261\n",
      "Epoch 482: val_loss improved from 0.26100 to 0.26098, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2616 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2616 - mape: 8.3070 - val_loss: 0.2610 - val_mse: 0.2634 - val_rmse: 0.5132 - val_mae: 0.2610 - val_mape: 8.2661 - lr: 1.0000e-05\n",
      "Epoch 483/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2616 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2616 - mape: 8.3033\n",
      "Epoch 483: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2620 - mse: 0.2648 - rmse: 0.5146 - mae: 0.2620 - mape: 8.3213 - val_loss: 0.2613 - val_mse: 0.2631 - val_rmse: 0.5129 - val_mae: 0.2613 - val_mape: 8.3061 - lr: 1.0000e-05\n",
      "Epoch 484/1000\n",
      "262/318 [=======================>......] - ETA: 0s - loss: 0.2655 - mse: 0.2740 - rmse: 0.5234 - mae: 0.2655 - mape: 8.4217\n",
      "Epoch 484: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2654 - rmse: 0.5152 - mae: 0.2619 - mape: 8.3024 - val_loss: 0.2611 - val_mse: 0.2655 - val_rmse: 0.5153 - val_mae: 0.2611 - val_mape: 8.2920 - lr: 1.0000e-05\n",
      "Epoch 485/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2610 - mse: 0.2639 - rmse: 0.5137 - mae: 0.2610 - mape: 8.3067\n",
      "Epoch 485: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2616 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2616 - mape: 8.3133 - val_loss: 0.2615 - val_mse: 0.2623 - val_rmse: 0.5122 - val_mae: 0.2615 - val_mape: 8.2764 - lr: 1.0000e-05\n",
      "Epoch 486/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2615 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2615 - mape: 8.3076\n",
      "Epoch 486: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2617 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2617 - mape: 8.3029 - val_loss: 0.2611 - val_mse: 0.2658 - val_rmse: 0.5156 - val_mae: 0.2611 - val_mape: 8.3285 - lr: 1.0000e-05\n",
      "Epoch 487/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2628 - mse: 0.2684 - rmse: 0.5180 - mae: 0.2628 - mape: 8.3415\n",
      "Epoch 487: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2614 - mse: 0.2653 - rmse: 0.5151 - mae: 0.2614 - mape: 8.3176 - val_loss: 0.2611 - val_mse: 0.2627 - val_rmse: 0.5126 - val_mae: 0.2611 - val_mape: 8.2736 - lr: 1.0000e-05\n",
      "Epoch 488/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2611 - mse: 0.2619 - rmse: 0.5117 - mae: 0.2611 - mape: 8.2766\n",
      "Epoch 488: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2617 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2617 - mape: 8.2937 - val_loss: 0.2611 - val_mse: 0.2648 - val_rmse: 0.5146 - val_mae: 0.2611 - val_mape: 8.3160 - lr: 1.0000e-05\n",
      "Epoch 489/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2616 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2616 - mape: 8.3062\n",
      "Epoch 489: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2615 - mse: 0.2639 - rmse: 0.5137 - mae: 0.2615 - mape: 8.3051 - val_loss: 0.2627 - val_mse: 0.2655 - val_rmse: 0.5153 - val_mae: 0.2627 - val_mape: 8.3198 - lr: 1.0000e-05\n",
      "Epoch 490/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2617 - mse: 0.2629 - rmse: 0.5128 - mae: 0.2617 - mape: 8.3105\n",
      "Epoch 490: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2618 - mse: 0.2638 - rmse: 0.5136 - mae: 0.2618 - mape: 8.3002 - val_loss: 0.2611 - val_mse: 0.2641 - val_rmse: 0.5139 - val_mae: 0.2611 - val_mape: 8.2700 - lr: 1.0000e-05\n",
      "Epoch 491/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2598 - mape: 8.2468\n",
      "Epoch 491: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2617 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2617 - mape: 8.3131 - val_loss: 0.2615 - val_mse: 0.2644 - val_rmse: 0.5142 - val_mae: 0.2615 - val_mape: 8.3200 - lr: 1.0000e-05\n",
      "Epoch 492/1000\n",
      "261/318 [=======================>......] - ETA: 0s - loss: 0.2606 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2606 - mape: 8.2769\n",
      "Epoch 492: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2614 - mse: 0.2643 - rmse: 0.5141 - mae: 0.2614 - mape: 8.2925 - val_loss: 0.2611 - val_mse: 0.2613 - val_rmse: 0.5112 - val_mae: 0.2611 - val_mape: 8.2692 - lr: 1.0000e-05\n",
      "Epoch 493/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2614 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2614 - mape: 8.3060\n",
      "Epoch 493: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2616 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2616 - mape: 8.3025 - val_loss: 0.2614 - val_mse: 0.2628 - val_rmse: 0.5127 - val_mae: 0.2614 - val_mape: 8.2911 - lr: 1.0000e-05\n",
      "Epoch 494/1000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.2611 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2611 - mape: 8.2829\n",
      "Epoch 494: val_loss did not improve from 0.26098\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2612 - mse: 0.2638 - rmse: 0.5136 - mae: 0.2612 - mape: 8.2837 - val_loss: 0.2630 - val_mse: 0.2687 - val_rmse: 0.5184 - val_mae: 0.2630 - val_mape: 8.4317 - lr: 1.0000e-05\n",
      "Epoch 495/1000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2604 - mse: 0.2632 - rmse: 0.5131 - mae: 0.2604 - mape: 8.2851\n",
      "Epoch 495: val_loss improved from 0.26098 to 0.26090, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2620 - mse: 0.2662 - rmse: 0.5159 - mae: 0.2620 - mape: 8.3217 - val_loss: 0.2609 - val_mse: 0.2654 - val_rmse: 0.5152 - val_mae: 0.2609 - val_mape: 8.3052 - lr: 1.0000e-05\n",
      "Epoch 496/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2598 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2598 - mape: 8.2690\n",
      "Epoch 496: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2643 - rmse: 0.5141 - mae: 0.2613 - mape: 8.3079 - val_loss: 0.2610 - val_mse: 0.2640 - val_rmse: 0.5138 - val_mae: 0.2610 - val_mape: 8.2540 - lr: 1.0000e-05\n",
      "Epoch 497/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2604 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2604 - mape: 8.2516\n",
      "Epoch 497: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2614 - mse: 0.2642 - rmse: 0.5140 - mae: 0.2614 - mape: 8.2958 - val_loss: 0.2618 - val_mse: 0.2645 - val_rmse: 0.5143 - val_mae: 0.2618 - val_mape: 8.3594 - lr: 1.0000e-05\n",
      "Epoch 498/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2605 - mse: 0.2632 - rmse: 0.5130 - mae: 0.2605 - mape: 8.2418\n",
      "Epoch 498: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2613 - mape: 8.2973 - val_loss: 0.2612 - val_mse: 0.2631 - val_rmse: 0.5130 - val_mae: 0.2612 - val_mape: 8.2854 - lr: 1.0000e-05\n",
      "Epoch 499/1000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2622 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2622 - mape: 8.3377\n",
      "Epoch 499: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2615 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2615 - mape: 8.3038 - val_loss: 0.2609 - val_mse: 0.2623 - val_rmse: 0.5122 - val_mae: 0.2609 - val_mape: 8.2547 - lr: 1.0000e-05\n",
      "Epoch 500/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2613 - mse: 0.2632 - rmse: 0.5130 - mae: 0.2613 - mape: 8.2919\n",
      "Epoch 500: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2615 - mse: 0.2642 - rmse: 0.5140 - mae: 0.2615 - mape: 8.2896 - val_loss: 0.2614 - val_mse: 0.2668 - val_rmse: 0.5165 - val_mae: 0.2614 - val_mape: 8.2997 - lr: 1.0000e-05\n",
      "Epoch 501/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2619 - mse: 0.2653 - rmse: 0.5150 - mae: 0.2619 - mape: 8.3155\n",
      "Epoch 501: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2617 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2617 - mape: 8.3092 - val_loss: 0.2617 - val_mse: 0.2617 - val_rmse: 0.5115 - val_mae: 0.2617 - val_mape: 8.3161 - lr: 1.0000e-05\n",
      "Epoch 502/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2621 - rmse: 0.5119 - mae: 0.2610 - mape: 8.2808\n",
      "Epoch 502: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2629 - rmse: 0.5128 - mae: 0.2613 - mape: 8.3019 - val_loss: 0.2620 - val_mse: 0.2670 - val_rmse: 0.5167 - val_mae: 0.2620 - val_mape: 8.3874 - lr: 1.0000e-05\n",
      "Epoch 503/1000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2612 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2612 - mape: 8.3002\n",
      "Epoch 503: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2615 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2615 - mape: 8.3084 - val_loss: 0.2609 - val_mse: 0.2647 - val_rmse: 0.5145 - val_mae: 0.2609 - val_mape: 8.2906 - lr: 1.0000e-05\n",
      "Epoch 504/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2617 - mse: 0.2639 - rmse: 0.5137 - mae: 0.2617 - mape: 8.3124\n",
      "Epoch 504: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2614 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2614 - mape: 8.2917 - val_loss: 0.2625 - val_mse: 0.2669 - val_rmse: 0.5166 - val_mae: 0.2625 - val_mape: 8.2570 - lr: 1.0000e-05\n",
      "Epoch 505/1000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2643 - mse: 0.2719 - rmse: 0.5214 - mae: 0.2643 - mape: 8.3594\n",
      "Epoch 505: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2617 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2617 - mape: 8.3022 - val_loss: 0.2610 - val_mse: 0.2623 - val_rmse: 0.5121 - val_mae: 0.2610 - val_mape: 8.2378 - lr: 1.0000e-05\n",
      "Epoch 506/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2608 - mse: 0.2633 - rmse: 0.5132 - mae: 0.2608 - mape: 8.2745\n",
      "Epoch 506: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2614 - mse: 0.2650 - rmse: 0.5148 - mae: 0.2614 - mape: 8.2918 - val_loss: 0.2611 - val_mse: 0.2627 - val_rmse: 0.5126 - val_mae: 0.2611 - val_mape: 8.2451 - lr: 1.0000e-05\n",
      "Epoch 507/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2599 - mse: 0.2629 - rmse: 0.5128 - mae: 0.2599 - mape: 8.2363\n",
      "Epoch 507: val_loss did not improve from 0.26090\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2616 - mse: 0.2652 - rmse: 0.5150 - mae: 0.2616 - mape: 8.2892 - val_loss: 0.2610 - val_mse: 0.2648 - val_rmse: 0.5146 - val_mae: 0.2610 - val_mape: 8.2821 - lr: 1.0000e-05\n",
      "Epoch 508/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2660 - rmse: 0.5157 - mae: 0.2616 - mape: 8.3094\n",
      "Epoch 508: val_loss improved from 0.26090 to 0.26067, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2614 - mse: 0.2643 - rmse: 0.5141 - mae: 0.2614 - mape: 8.2955 - val_loss: 0.2607 - val_mse: 0.2637 - val_rmse: 0.5135 - val_mae: 0.2607 - val_mape: 8.2856 - lr: 1.0000e-05\n",
      "Epoch 509/1000\n",
      "262/318 [=======================>......] - ETA: 0s - loss: 0.2614 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2614 - mape: 8.3030\n",
      "Epoch 509: val_loss did not improve from 0.26067\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2613 - mape: 8.2908 - val_loss: 0.2623 - val_mse: 0.2671 - val_rmse: 0.5168 - val_mae: 0.2623 - val_mape: 8.2586 - lr: 1.0000e-05\n",
      "Epoch 510/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2619 - mse: 0.2649 - rmse: 0.5146 - mae: 0.2619 - mape: 8.3025\n",
      "Epoch 510: val_loss did not improve from 0.26067\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2616 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2616 - mape: 8.2907 - val_loss: 0.2609 - val_mse: 0.2639 - val_rmse: 0.5137 - val_mae: 0.2609 - val_mape: 8.2960 - lr: 1.0000e-05\n",
      "Epoch 511/1000\n",
      "256/318 [=======================>......] - ETA: 0s - loss: 0.2614 - mse: 0.2628 - rmse: 0.5126 - mae: 0.2614 - mape: 8.2882\n",
      "Epoch 511: val_loss improved from 0.26067 to 0.26064, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2615 - mse: 0.2642 - rmse: 0.5140 - mae: 0.2615 - mape: 8.2988 - val_loss: 0.2606 - val_mse: 0.2629 - val_rmse: 0.5127 - val_mae: 0.2606 - val_mape: 8.2715 - lr: 1.0000e-05\n",
      "Epoch 512/1000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2612 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2612 - mape: 8.2720\n",
      "Epoch 512: val_loss did not improve from 0.26064\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2643 - rmse: 0.5141 - mae: 0.2613 - mape: 8.2962 - val_loss: 0.2610 - val_mse: 0.2623 - val_rmse: 0.5121 - val_mae: 0.2610 - val_mape: 8.3017 - lr: 1.0000e-05\n",
      "Epoch 513/1000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2607 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2607 - mape: 8.2255\n",
      "Epoch 513: val_loss did not improve from 0.26064\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2615 - mse: 0.2638 - rmse: 0.5136 - mae: 0.2615 - mape: 8.2884 - val_loss: 0.2609 - val_mse: 0.2644 - val_rmse: 0.5142 - val_mae: 0.2609 - val_mape: 8.2586 - lr: 1.0000e-05\n",
      "Epoch 514/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2594 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2594 - mape: 8.2345\n",
      "Epoch 514: val_loss did not improve from 0.26064\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2612 - mse: 0.2643 - rmse: 0.5141 - mae: 0.2612 - mape: 8.2884 - val_loss: 0.2610 - val_mse: 0.2639 - val_rmse: 0.5137 - val_mae: 0.2610 - val_mape: 8.2396 - lr: 1.0000e-05\n",
      "Epoch 515/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2606 - mse: 0.2614 - rmse: 0.5112 - mae: 0.2606 - mape: 8.2880\n",
      "Epoch 515: val_loss did not improve from 0.26064\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2614 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2614 - mape: 8.2917 - val_loss: 0.2610 - val_mse: 0.2650 - val_rmse: 0.5147 - val_mae: 0.2610 - val_mape: 8.3233 - lr: 1.0000e-05\n",
      "Epoch 516/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2600 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2600 - mape: 8.2760\n",
      "Epoch 516: val_loss did not improve from 0.26064\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2614 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2614 - mape: 8.3062 - val_loss: 0.2609 - val_mse: 0.2642 - val_rmse: 0.5140 - val_mae: 0.2609 - val_mape: 8.3027 - lr: 1.0000e-05\n",
      "Epoch 517/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2607 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2607 - mape: 8.2968\n",
      "Epoch 517: val_loss did not improve from 0.26064\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2611 - mape: 8.2912 - val_loss: 0.2608 - val_mse: 0.2620 - val_rmse: 0.5118 - val_mae: 0.2608 - val_mape: 8.2686 - lr: 1.0000e-05\n",
      "Epoch 518/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2606 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2606 - mape: 8.2666\n",
      "Epoch 518: val_loss did not improve from 0.26064\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2612 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2612 - mape: 8.2886 - val_loss: 0.2626 - val_mse: 0.2655 - val_rmse: 0.5153 - val_mae: 0.2626 - val_mape: 8.2380 - lr: 1.0000e-05\n",
      "Epoch 519/1000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2648 - mse: 0.2695 - rmse: 0.5191 - mae: 0.2648 - mape: 8.4071\n",
      "Epoch 519: val_loss did not improve from 0.26064\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2614 - mse: 0.2632 - rmse: 0.5131 - mae: 0.2614 - mape: 8.2765 - val_loss: 0.2607 - val_mse: 0.2653 - val_rmse: 0.5151 - val_mae: 0.2607 - val_mape: 8.3020 - lr: 1.0000e-05\n",
      "Epoch 520/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2618 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2618 - mape: 8.2970\n",
      "Epoch 520: val_loss did not improve from 0.26064\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2615 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2615 - mape: 8.2962 - val_loss: 0.2608 - val_mse: 0.2653 - val_rmse: 0.5151 - val_mae: 0.2608 - val_mape: 8.2979 - lr: 1.0000e-05\n",
      "Epoch 521/1000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2591 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2591 - mape: 8.2275\n",
      "Epoch 521: val_loss did not improve from 0.26064\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2615 - mse: 0.2642 - rmse: 0.5140 - mae: 0.2615 - mape: 8.2978 - val_loss: 0.2608 - val_mse: 0.2633 - val_rmse: 0.5131 - val_mae: 0.2608 - val_mape: 8.3021 - lr: 1.0000e-05\n",
      "Epoch 522/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2613 - mse: 0.2640 - rmse: 0.5138 - mae: 0.2613 - mape: 8.2853\n",
      "Epoch 522: val_loss improved from 0.26064 to 0.26058, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2614 - mse: 0.2640 - rmse: 0.5138 - mae: 0.2614 - mape: 8.3010 - val_loss: 0.2606 - val_mse: 0.2629 - val_rmse: 0.5127 - val_mae: 0.2606 - val_mape: 8.2652 - lr: 1.0000e-05\n",
      "Epoch 523/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2623 - mse: 0.2657 - rmse: 0.5155 - mae: 0.2623 - mape: 8.3159\n",
      "Epoch 523: val_loss did not improve from 0.26058\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2614 - mse: 0.2643 - rmse: 0.5141 - mae: 0.2614 - mape: 8.2946 - val_loss: 0.2613 - val_mse: 0.2642 - val_rmse: 0.5140 - val_mae: 0.2613 - val_mape: 8.3258 - lr: 1.0000e-05\n",
      "Epoch 524/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2619 - rmse: 0.5117 - mae: 0.2598 - mape: 8.2432\n",
      "Epoch 524: val_loss did not improve from 0.26058\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2612 - mse: 0.2642 - rmse: 0.5140 - mae: 0.2612 - mape: 8.2969 - val_loss: 0.2613 - val_mse: 0.2625 - val_rmse: 0.5124 - val_mae: 0.2613 - val_mape: 8.3182 - lr: 1.0000e-05\n",
      "Epoch 525/1000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2628 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2628 - mape: 8.3005\n",
      "Epoch 525: val_loss did not improve from 0.26058\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2630 - rmse: 0.5129 - mae: 0.2613 - mape: 8.2881 - val_loss: 0.2616 - val_mse: 0.2664 - val_rmse: 0.5161 - val_mae: 0.2616 - val_mape: 8.2722 - lr: 1.0000e-05\n",
      "Epoch 526/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2628 - mse: 0.2677 - rmse: 0.5174 - mae: 0.2628 - mape: 8.3337\n",
      "Epoch 526: val_loss did not improve from 0.26058\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2613 - mape: 8.2900 - val_loss: 0.2606 - val_mse: 0.2638 - val_rmse: 0.5136 - val_mae: 0.2606 - val_mape: 8.2821 - lr: 1.0000e-05\n",
      "Epoch 527/1000\n",
      "262/318 [=======================>......] - ETA: 0s - loss: 0.2596 - mse: 0.2585 - rmse: 0.5085 - mae: 0.2596 - mape: 8.2399\n",
      "Epoch 527: val_loss did not improve from 0.26058\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2613 - mape: 8.3013 - val_loss: 0.2607 - val_mse: 0.2613 - val_rmse: 0.5112 - val_mae: 0.2607 - val_mape: 8.2386 - lr: 1.0000e-05\n",
      "Epoch 528/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2604 - mse: 0.2628 - rmse: 0.5126 - mae: 0.2604 - mape: 8.2557\n",
      "Epoch 528: val_loss did not improve from 0.26058\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2611 - mse: 0.2633 - rmse: 0.5131 - mae: 0.2611 - mape: 8.2854 - val_loss: 0.2608 - val_mse: 0.2619 - val_rmse: 0.5117 - val_mae: 0.2608 - val_mape: 8.2830 - lr: 1.0000e-05\n",
      "Epoch 529/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2615 - mse: 0.2652 - rmse: 0.5149 - mae: 0.2615 - mape: 8.3099\n",
      "Epoch 529: val_loss improved from 0.26058 to 0.26057, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2614 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2614 - mape: 8.3074 - val_loss: 0.2606 - val_mse: 0.2634 - val_rmse: 0.5132 - val_mae: 0.2606 - val_mape: 8.2634 - lr: 1.0000e-05\n",
      "Epoch 530/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2611 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2611 - mape: 8.3019\n",
      "Epoch 530: val_loss did not improve from 0.26057\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2614 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2614 - mape: 8.3017 - val_loss: 0.2610 - val_mse: 0.2652 - val_rmse: 0.5150 - val_mae: 0.2610 - val_mape: 8.2757 - lr: 1.0000e-05\n",
      "Epoch 531/1000\n",
      "261/318 [=======================>......] - ETA: 0s - loss: 0.2596 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2596 - mape: 8.2405\n",
      "Epoch 531: val_loss did not improve from 0.26057\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2633 - rmse: 0.5131 - mae: 0.2613 - mape: 8.2747 - val_loss: 0.2620 - val_mse: 0.2646 - val_rmse: 0.5144 - val_mae: 0.2620 - val_mape: 8.3743 - lr: 1.0000e-05\n",
      "Epoch 532/1000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2600 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2600 - mape: 8.2459\n",
      "Epoch 532: val_loss did not improve from 0.26057\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2614 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2614 - mape: 8.2917 - val_loss: 0.2607 - val_mse: 0.2608 - val_rmse: 0.5107 - val_mae: 0.2607 - val_mape: 8.2747 - lr: 1.0000e-05\n",
      "Epoch 533/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2609 - mape: 8.2707\n",
      "Epoch 533: val_loss did not improve from 0.26057\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2628 - rmse: 0.5127 - mae: 0.2610 - mape: 8.2886 - val_loss: 0.2615 - val_mse: 0.2674 - val_rmse: 0.5171 - val_mae: 0.2615 - val_mape: 8.3020 - lr: 1.0000e-05\n",
      "Epoch 534/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2617 - mse: 0.2652 - rmse: 0.5150 - mae: 0.2617 - mape: 8.2987\n",
      "Epoch 534: val_loss did not improve from 0.26057\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2613 - mape: 8.3041 - val_loss: 0.2607 - val_mse: 0.2637 - val_rmse: 0.5135 - val_mae: 0.2607 - val_mape: 8.2531 - lr: 1.0000e-05\n",
      "Epoch 535/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2603 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2603 - mape: 8.2534\n",
      "Epoch 535: val_loss improved from 0.26057 to 0.26043, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2613 - mse: 0.2640 - rmse: 0.5138 - mae: 0.2613 - mape: 8.2917 - val_loss: 0.2604 - val_mse: 0.2629 - val_rmse: 0.5127 - val_mae: 0.2604 - val_mape: 8.2802 - lr: 1.0000e-05\n",
      "Epoch 536/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2603 - mse: 0.2627 - rmse: 0.5126 - mae: 0.2603 - mape: 8.2907\n",
      "Epoch 536: val_loss improved from 0.26043 to 0.26042, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2612 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2612 - mape: 8.2883 - val_loss: 0.2604 - val_mse: 0.2627 - val_rmse: 0.5126 - val_mae: 0.2604 - val_mape: 8.2593 - lr: 1.0000e-05\n",
      "Epoch 537/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2613 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2613 - mape: 8.3067\n",
      "Epoch 537: val_loss did not improve from 0.26042\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2615 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2615 - mape: 8.2991 - val_loss: 0.2605 - val_mse: 0.2624 - val_rmse: 0.5122 - val_mae: 0.2605 - val_mape: 8.2442 - lr: 1.0000e-05\n",
      "Epoch 538/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2606 - mse: 0.2643 - rmse: 0.5141 - mae: 0.2606 - mape: 8.2771\n",
      "Epoch 538: val_loss did not improve from 0.26042\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2610 - mse: 0.2643 - rmse: 0.5141 - mae: 0.2610 - mape: 8.2920 - val_loss: 0.2613 - val_mse: 0.2618 - val_rmse: 0.5117 - val_mae: 0.2613 - val_mape: 8.3080 - lr: 1.0000e-05\n",
      "Epoch 539/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2620 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2620 - mape: 8.3223\n",
      "Epoch 539: val_loss did not improve from 0.26042\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2615 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2615 - mape: 8.3015 - val_loss: 0.2611 - val_mse: 0.2631 - val_rmse: 0.5129 - val_mae: 0.2611 - val_mape: 8.3193 - lr: 1.0000e-05\n",
      "Epoch 540/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2616 - mse: 0.2662 - rmse: 0.5160 - mae: 0.2616 - mape: 8.3048\n",
      "Epoch 540: val_loss did not improve from 0.26042\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2610 - mse: 0.2636 - rmse: 0.5135 - mae: 0.2610 - mape: 8.2878 - val_loss: 0.2604 - val_mse: 0.2627 - val_rmse: 0.5125 - val_mae: 0.2604 - val_mape: 8.2631 - lr: 1.0000e-05\n",
      "Epoch 541/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2609 - mse: 0.2624 - rmse: 0.5122 - mae: 0.2609 - mape: 8.2804\n",
      "Epoch 541: val_loss improved from 0.26042 to 0.26041, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2631 - rmse: 0.5130 - mae: 0.2611 - mape: 8.2841 - val_loss: 0.2604 - val_mse: 0.2623 - val_rmse: 0.5121 - val_mae: 0.2604 - val_mape: 8.2517 - lr: 1.0000e-05\n",
      "Epoch 542/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2610 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2610 - mape: 8.2953\n",
      "Epoch 542: val_loss did not improve from 0.26041\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2612 - mse: 0.2648 - rmse: 0.5145 - mae: 0.2612 - mape: 8.2877 - val_loss: 0.2607 - val_mse: 0.2619 - val_rmse: 0.5118 - val_mae: 0.2607 - val_mape: 8.2920 - lr: 1.0000e-05\n",
      "Epoch 543/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2611 - mse: 0.2630 - rmse: 0.5129 - mae: 0.2611 - mape: 8.2840\n",
      "Epoch 543: val_loss did not improve from 0.26041\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2610 - mse: 0.2631 - rmse: 0.5129 - mae: 0.2610 - mape: 8.2873 - val_loss: 0.2605 - val_mse: 0.2623 - val_rmse: 0.5122 - val_mae: 0.2605 - val_mape: 8.2515 - lr: 1.0000e-05\n",
      "Epoch 544/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2595 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2595 - mape: 8.2297\n",
      "Epoch 544: val_loss did not improve from 0.26041\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2611 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2611 - mape: 8.2922 - val_loss: 0.2605 - val_mse: 0.2645 - val_rmse: 0.5143 - val_mae: 0.2605 - val_mape: 8.2685 - lr: 1.0000e-05\n",
      "Epoch 545/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2612 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2612 - mape: 8.2858\n",
      "Epoch 545: val_loss did not improve from 0.26041\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2638 - rmse: 0.5136 - mae: 0.2611 - mape: 8.2881 - val_loss: 0.2614 - val_mse: 0.2658 - val_rmse: 0.5155 - val_mae: 0.2614 - val_mape: 8.2737 - lr: 1.0000e-05\n",
      "Epoch 546/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2616 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2616 - mape: 8.2987\n",
      "Epoch 546: val_loss did not improve from 0.26041\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2613 - mape: 8.2938 - val_loss: 0.2607 - val_mse: 0.2632 - val_rmse: 0.5131 - val_mae: 0.2607 - val_mape: 8.2792 - lr: 1.0000e-05\n",
      "Epoch 547/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2610 - mse: 0.2628 - rmse: 0.5126 - mae: 0.2610 - mape: 8.2908\n",
      "Epoch 547: val_loss did not improve from 0.26041\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2613 - mape: 8.2987 - val_loss: 0.2607 - val_mse: 0.2637 - val_rmse: 0.5135 - val_mae: 0.2607 - val_mape: 8.2725 - lr: 1.0000e-05\n",
      "Epoch 548/1000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2616 - mse: 0.2657 - rmse: 0.5155 - mae: 0.2616 - mape: 8.3046\n",
      "Epoch 548: val_loss did not improve from 0.26041\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2612 - mse: 0.2639 - rmse: 0.5137 - mae: 0.2612 - mape: 8.2936 - val_loss: 0.2608 - val_mse: 0.2647 - val_rmse: 0.5145 - val_mae: 0.2608 - val_mape: 8.2698 - lr: 1.0000e-05\n",
      "Epoch 549/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2613 - mse: 0.2625 - rmse: 0.5124 - mae: 0.2613 - mape: 8.2643\n",
      "Epoch 549: val_loss did not improve from 0.26041\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2632 - rmse: 0.5130 - mae: 0.2613 - mape: 8.2713 - val_loss: 0.2614 - val_mse: 0.2651 - val_rmse: 0.5149 - val_mae: 0.2614 - val_mape: 8.3016 - lr: 1.0000e-05\n",
      "Epoch 550/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2618 - mse: 0.2658 - rmse: 0.5155 - mae: 0.2618 - mape: 8.3194\n",
      "Epoch 550: val_loss did not improve from 0.26041\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2613 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2613 - mape: 8.2939 - val_loss: 0.2604 - val_mse: 0.2644 - val_rmse: 0.5142 - val_mae: 0.2604 - val_mape: 8.2737 - lr: 1.0000e-05\n",
      "Epoch 551/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2613 - mse: 0.2640 - rmse: 0.5138 - mae: 0.2613 - mape: 8.2976\n",
      "Epoch 551: val_loss did not improve from 0.26041\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2611 - mse: 0.2632 - rmse: 0.5130 - mae: 0.2611 - mape: 8.2931 - val_loss: 0.2609 - val_mse: 0.2615 - val_rmse: 0.5114 - val_mae: 0.2609 - val_mape: 8.2459 - lr: 1.0000e-05\n",
      "Epoch 552/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2607 - mape: 8.2806\n",
      "Epoch 552: val_loss did not improve from 0.26041\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2610 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2610 - mape: 8.2813 - val_loss: 0.2604 - val_mse: 0.2613 - val_rmse: 0.5112 - val_mae: 0.2604 - val_mape: 8.2447 - lr: 1.0000e-05\n",
      "Epoch 553/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2614 - mse: 0.2635 - rmse: 0.5134 - mae: 0.2614 - mape: 8.3027\n",
      "Epoch 553: val_loss improved from 0.26041 to 0.26034, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2611 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2611 - mape: 8.2907 - val_loss: 0.2603 - val_mse: 0.2624 - val_rmse: 0.5123 - val_mae: 0.2603 - val_mape: 8.2544 - lr: 1.0000e-05\n",
      "Epoch 554/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2617 - mse: 0.2643 - rmse: 0.5141 - mae: 0.2617 - mape: 8.3022\n",
      "Epoch 554: val_loss did not improve from 0.26034\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2608 - mse: 0.2629 - rmse: 0.5127 - mae: 0.2608 - mape: 8.2766 - val_loss: 0.2608 - val_mse: 0.2631 - val_rmse: 0.5129 - val_mae: 0.2608 - val_mape: 8.2314 - lr: 1.0000e-05\n",
      "Epoch 555/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2604 - mse: 0.2625 - rmse: 0.5124 - mae: 0.2604 - mape: 8.2575\n",
      "Epoch 555: val_loss did not improve from 0.26034\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2612 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2612 - mape: 8.2937 - val_loss: 0.2605 - val_mse: 0.2611 - val_rmse: 0.5110 - val_mae: 0.2605 - val_mape: 8.2695 - lr: 1.0000e-05\n",
      "Epoch 556/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2613 - mse: 0.2632 - rmse: 0.5131 - mae: 0.2613 - mape: 8.2900\n",
      "Epoch 556: val_loss did not improve from 0.26034\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2611 - mse: 0.2627 - rmse: 0.5126 - mae: 0.2611 - mape: 8.2856 - val_loss: 0.2621 - val_mse: 0.2626 - val_rmse: 0.5124 - val_mae: 0.2621 - val_mape: 8.3539 - lr: 1.0000e-05\n",
      "Epoch 557/1000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2644 - mse: 0.2694 - rmse: 0.5190 - mae: 0.2644 - mape: 8.3783\n",
      "Epoch 557: val_loss did not improve from 0.26034\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2611 - mse: 0.2627 - rmse: 0.5125 - mae: 0.2611 - mape: 8.2737 - val_loss: 0.2624 - val_mse: 0.2666 - val_rmse: 0.5164 - val_mae: 0.2624 - val_mape: 8.4005 - lr: 1.0000e-05\n",
      "Epoch 558/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2624 - rmse: 0.5123 - mae: 0.2607 - mape: 8.2660\n",
      "Epoch 558: val_loss improved from 0.26034 to 0.26022, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2608 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2608 - mape: 8.2769 - val_loss: 0.2602 - val_mse: 0.2621 - val_rmse: 0.5120 - val_mae: 0.2602 - val_mape: 8.2664 - lr: 1.0000e-05\n",
      "Epoch 559/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2619 - mse: 0.2653 - rmse: 0.5151 - mae: 0.2619 - mape: 8.2974\n",
      "Epoch 559: val_loss did not improve from 0.26022\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2609 - mse: 0.2627 - rmse: 0.5126 - mae: 0.2609 - mape: 8.2763 - val_loss: 0.2607 - val_mse: 0.2632 - val_rmse: 0.5130 - val_mae: 0.2607 - val_mape: 8.2748 - lr: 1.0000e-05\n",
      "Epoch 560/1000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2618 - mse: 0.2642 - rmse: 0.5140 - mae: 0.2618 - mape: 8.3086\n",
      "Epoch 560: val_loss improved from 0.26022 to 0.26020, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2612 - mse: 0.2634 - rmse: 0.5133 - mae: 0.2612 - mape: 8.2931 - val_loss: 0.2602 - val_mse: 0.2628 - val_rmse: 0.5127 - val_mae: 0.2602 - val_mape: 8.2784 - lr: 1.0000e-05\n",
      "Epoch 561/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2615 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2615 - mape: 8.2963\n",
      "Epoch 561: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2608 - mse: 0.2629 - rmse: 0.5127 - mae: 0.2608 - mape: 8.2904 - val_loss: 0.2614 - val_mse: 0.2657 - val_rmse: 0.5154 - val_mae: 0.2614 - val_mape: 8.2487 - lr: 1.0000e-05\n",
      "Epoch 562/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2624 - rmse: 0.5123 - mae: 0.2608 - mape: 8.2646\n",
      "Epoch 562: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2609 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2609 - mape: 8.2747 - val_loss: 0.2606 - val_mse: 0.2632 - val_rmse: 0.5130 - val_mae: 0.2606 - val_mape: 8.2991 - lr: 1.0000e-05\n",
      "Epoch 563/1000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2605 - mse: 0.2618 - rmse: 0.5116 - mae: 0.2605 - mape: 8.2666\n",
      "Epoch 563: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2612 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2612 - mape: 8.2855 - val_loss: 0.2604 - val_mse: 0.2626 - val_rmse: 0.5124 - val_mae: 0.2604 - val_mape: 8.2943 - lr: 1.0000e-05\n",
      "Epoch 564/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2601 - mse: 0.2612 - rmse: 0.5110 - mae: 0.2601 - mape: 8.2529\n",
      "Epoch 564: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2608 - mse: 0.2638 - rmse: 0.5136 - mae: 0.2608 - mape: 8.2850 - val_loss: 0.2603 - val_mse: 0.2641 - val_rmse: 0.5139 - val_mae: 0.2603 - val_mape: 8.2743 - lr: 1.0000e-05\n",
      "Epoch 565/1000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2601 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2601 - mape: 8.2421\n",
      "Epoch 565: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2609 - mse: 0.2632 - rmse: 0.5131 - mae: 0.2609 - mape: 8.2779 - val_loss: 0.2611 - val_mse: 0.2670 - val_rmse: 0.5167 - val_mae: 0.2611 - val_mape: 8.2928 - lr: 1.0000e-05\n",
      "Epoch 566/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2630 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2630 - mape: 8.3459\n",
      "Epoch 566: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2610 - mse: 0.2634 - rmse: 0.5133 - mae: 0.2610 - mape: 8.2876 - val_loss: 0.2606 - val_mse: 0.2645 - val_rmse: 0.5143 - val_mae: 0.2606 - val_mape: 8.3013 - lr: 1.0000e-05\n",
      "Epoch 567/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2607 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2607 - mape: 8.2858\n",
      "Epoch 567: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2609 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2609 - mape: 8.2855 - val_loss: 0.2605 - val_mse: 0.2629 - val_rmse: 0.5128 - val_mae: 0.2605 - val_mape: 8.2912 - lr: 1.0000e-05\n",
      "Epoch 568/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2616 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2616 - mape: 8.3061\n",
      "Epoch 568: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2610 - mse: 0.2633 - rmse: 0.5132 - mae: 0.2610 - mape: 8.2845 - val_loss: 0.2604 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2604 - val_mape: 8.2544 - lr: 1.0000e-05\n",
      "Epoch 569/1000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2602 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2602 - mape: 8.2465\n",
      "Epoch 569: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2607 - mse: 0.2623 - rmse: 0.5122 - mae: 0.2607 - mape: 8.2760 - val_loss: 0.2609 - val_mse: 0.2609 - val_rmse: 0.5108 - val_mae: 0.2609 - val_mape: 8.2474 - lr: 1.0000e-05\n",
      "Epoch 570/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2600 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2600 - mape: 8.2519\n",
      "Epoch 570: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2609 - mse: 0.2632 - rmse: 0.5131 - mae: 0.2609 - mape: 8.2762 - val_loss: 0.2604 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2604 - val_mape: 8.2194 - lr: 1.0000e-05\n",
      "Epoch 571/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2632 - rmse: 0.5130 - mae: 0.2607 - mape: 8.2624\n",
      "Epoch 571: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2609 - mse: 0.2633 - rmse: 0.5131 - mae: 0.2609 - mape: 8.2673 - val_loss: 0.2621 - val_mse: 0.2626 - val_rmse: 0.5124 - val_mae: 0.2621 - val_mape: 8.3519 - lr: 1.0000e-05\n",
      "Epoch 572/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2602 - mse: 0.2613 - rmse: 0.5111 - mae: 0.2602 - mape: 8.2516\n",
      "Epoch 572: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2612 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2612 - mape: 8.2969 - val_loss: 0.2608 - val_mse: 0.2646 - val_rmse: 0.5144 - val_mae: 0.2608 - val_mape: 8.2770 - lr: 1.0000e-05\n",
      "Epoch 573/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2624 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2624 - mape: 8.3260\n",
      "Epoch 573: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2610 - mse: 0.2632 - rmse: 0.5130 - mae: 0.2610 - mape: 8.2797 - val_loss: 0.2623 - val_mse: 0.2668 - val_rmse: 0.5165 - val_mae: 0.2623 - val_mape: 8.4024 - lr: 1.0000e-05\n",
      "Epoch 574/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2616 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2616 - mape: 8.3061\n",
      "Epoch 574: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2609 - mse: 0.2632 - rmse: 0.5131 - mae: 0.2609 - mape: 8.2855 - val_loss: 0.2607 - val_mse: 0.2649 - val_rmse: 0.5147 - val_mae: 0.2607 - val_mape: 8.2783 - lr: 1.0000e-05\n",
      "Epoch 575/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2624 - mse: 0.2664 - rmse: 0.5162 - mae: 0.2624 - mape: 8.3051\n",
      "Epoch 575: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2610 - mse: 0.2638 - rmse: 0.5136 - mae: 0.2610 - mape: 8.2836 - val_loss: 0.2603 - val_mse: 0.2633 - val_rmse: 0.5131 - val_mae: 0.2603 - val_mape: 8.2671 - lr: 1.0000e-05\n",
      "Epoch 576/1000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2595 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2595 - mape: 8.2641\n",
      "Epoch 576: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2608 - mse: 0.2631 - rmse: 0.5129 - mae: 0.2608 - mape: 8.2667 - val_loss: 0.2611 - val_mse: 0.2626 - val_rmse: 0.5124 - val_mae: 0.2611 - val_mape: 8.2340 - lr: 1.0000e-05\n",
      "Epoch 577/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2602 - mse: 0.2620 - rmse: 0.5119 - mae: 0.2602 - mape: 8.2397\n",
      "Epoch 577: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2606 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2606 - mape: 8.2651 - val_loss: 0.2608 - val_mse: 0.2616 - val_rmse: 0.5115 - val_mae: 0.2608 - val_mape: 8.3024 - lr: 1.0000e-05\n",
      "Epoch 578/1000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2617 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2617 - mape: 8.3293\n",
      "Epoch 578: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2608 - mse: 0.2632 - rmse: 0.5131 - mae: 0.2608 - mape: 8.2741 - val_loss: 0.2609 - val_mse: 0.2635 - val_rmse: 0.5134 - val_mae: 0.2609 - val_mape: 8.3372 - lr: 1.0000e-05\n",
      "Epoch 579/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2618 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2618 - mape: 8.3173\n",
      "Epoch 579: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2609 - mse: 0.2636 - rmse: 0.5135 - mae: 0.2609 - mape: 8.2868 - val_loss: 0.2611 - val_mse: 0.2648 - val_rmse: 0.5146 - val_mae: 0.2611 - val_mape: 8.2843 - lr: 1.0000e-05\n",
      "Epoch 580/1000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2612 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2612 - mape: 8.2955\n",
      "Epoch 580: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2610 - mse: 0.2632 - rmse: 0.5130 - mae: 0.2610 - mape: 8.2798 - val_loss: 0.2604 - val_mse: 0.2624 - val_rmse: 0.5122 - val_mae: 0.2604 - val_mape: 8.2370 - lr: 1.0000e-05\n",
      "Epoch 581/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2607 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2607 - mape: 8.2555\n",
      "Epoch 581: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2608 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2608 - mape: 8.2733 - val_loss: 0.2603 - val_mse: 0.2648 - val_rmse: 0.5146 - val_mae: 0.2603 - val_mape: 8.2906 - lr: 1.0000e-05\n",
      "Epoch 582/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2602 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2602 - mape: 8.2486\n",
      "Epoch 582: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2607 - mse: 0.2639 - rmse: 0.5137 - mae: 0.2607 - mape: 8.2773 - val_loss: 0.2606 - val_mse: 0.2630 - val_rmse: 0.5129 - val_mae: 0.2606 - val_mape: 8.2727 - lr: 1.0000e-05\n",
      "Epoch 583/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2614 - mse: 0.2648 - rmse: 0.5146 - mae: 0.2614 - mape: 8.3064\n",
      "Epoch 583: val_loss did not improve from 0.26020\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2608 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2608 - mape: 8.2794 - val_loss: 0.2608 - val_mse: 0.2614 - val_rmse: 0.5113 - val_mae: 0.2608 - val_mape: 8.2114 - lr: 1.0000e-05\n",
      "Epoch 584/1000\n",
      "255/318 [=======================>......] - ETA: 0s - loss: 0.2615 - mse: 0.2627 - rmse: 0.5126 - mae: 0.2615 - mape: 8.2905\n",
      "Epoch 584: val_loss improved from 0.26020 to 0.26000, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2608 - mse: 0.2631 - rmse: 0.5130 - mae: 0.2608 - mape: 8.2645 - val_loss: 0.2600 - val_mse: 0.2614 - val_rmse: 0.5112 - val_mae: 0.2600 - val_mape: 8.2405 - lr: 1.0000e-05\n",
      "Epoch 585/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2600 - mse: 0.2615 - rmse: 0.5113 - mae: 0.2600 - mape: 8.2426\n",
      "Epoch 585: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2607 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2607 - mape: 8.2595 - val_loss: 0.2602 - val_mse: 0.2611 - val_rmse: 0.5110 - val_mae: 0.2602 - val_mape: 8.2369 - lr: 1.0000e-05\n",
      "Epoch 586/1000\n",
      "259/318 [=======================>......] - ETA: 0s - loss: 0.2590 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2590 - mape: 8.2163\n",
      "Epoch 586: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2610 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2610 - mape: 8.2750 - val_loss: 0.2608 - val_mse: 0.2634 - val_rmse: 0.5132 - val_mae: 0.2608 - val_mape: 8.3320 - lr: 1.0000e-05\n",
      "Epoch 587/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2605 - mse: 0.2620 - rmse: 0.5118 - mae: 0.2605 - mape: 8.2805\n",
      "Epoch 587: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2609 - mse: 0.2624 - rmse: 0.5122 - mae: 0.2609 - mape: 8.2800 - val_loss: 0.2605 - val_mse: 0.2623 - val_rmse: 0.5121 - val_mae: 0.2605 - val_mape: 8.3097 - lr: 1.0000e-05\n",
      "Epoch 588/1000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2618 - mse: 0.2657 - rmse: 0.5155 - mae: 0.2618 - mape: 8.3091\n",
      "Epoch 588: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2609 - mse: 0.2642 - rmse: 0.5140 - mae: 0.2609 - mape: 8.2818 - val_loss: 0.2606 - val_mse: 0.2644 - val_rmse: 0.5142 - val_mae: 0.2606 - val_mape: 8.2529 - lr: 1.0000e-05\n",
      "Epoch 589/1000\n",
      "262/318 [=======================>......] - ETA: 0s - loss: 0.2604 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2604 - mape: 8.2492\n",
      "Epoch 589: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2609 - mse: 0.2633 - rmse: 0.5131 - mae: 0.2609 - mape: 8.2864 - val_loss: 0.2601 - val_mse: 0.2610 - val_rmse: 0.5108 - val_mae: 0.2601 - val_mape: 8.2267 - lr: 1.0000e-05\n",
      "Epoch 590/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2618 - mse: 0.2628 - rmse: 0.5126 - mae: 0.2618 - mape: 8.2963\n",
      "Epoch 590: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2609 - mse: 0.2625 - rmse: 0.5123 - mae: 0.2609 - mape: 8.2722 - val_loss: 0.2603 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2603 - val_mape: 8.2783 - lr: 1.0000e-05\n",
      "Epoch 591/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2607 - mse: 0.2625 - rmse: 0.5124 - mae: 0.2607 - mape: 8.2579\n",
      "Epoch 591: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2605 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2605 - mape: 8.2608 - val_loss: 0.2603 - val_mse: 0.2626 - val_rmse: 0.5124 - val_mae: 0.2603 - val_mape: 8.2679 - lr: 1.0000e-05\n",
      "Epoch 592/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2615 - mse: 0.2631 - rmse: 0.5129 - mae: 0.2615 - mape: 8.2910\n",
      "Epoch 592: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2611 - mse: 0.2628 - rmse: 0.5127 - mae: 0.2611 - mape: 8.2847 - val_loss: 0.2602 - val_mse: 0.2635 - val_rmse: 0.5134 - val_mae: 0.2602 - val_mape: 8.2475 - lr: 1.0000e-05\n",
      "Epoch 593/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2607 - mse: 0.2625 - rmse: 0.5124 - mae: 0.2607 - mape: 8.2739\n",
      "Epoch 593: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2607 - mse: 0.2625 - rmse: 0.5124 - mae: 0.2607 - mape: 8.2739 - val_loss: 0.2604 - val_mse: 0.2628 - val_rmse: 0.5126 - val_mae: 0.2604 - val_mape: 8.2376 - lr: 1.0000e-05\n",
      "Epoch 594/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2615 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2615 - mape: 8.2978\n",
      "Epoch 594: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2606 - mse: 0.2622 - rmse: 0.5121 - mae: 0.2606 - mape: 8.2650 - val_loss: 0.2602 - val_mse: 0.2630 - val_rmse: 0.5128 - val_mae: 0.2602 - val_mape: 8.2317 - lr: 1.0000e-05\n",
      "Epoch 595/1000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2621 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2621 - mape: 8.2936\n",
      "Epoch 595: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2605 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2605 - mape: 8.2570 - val_loss: 0.2614 - val_mse: 0.2682 - val_rmse: 0.5178 - val_mae: 0.2614 - val_mape: 8.3532 - lr: 1.0000e-05\n",
      "Epoch 596/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2620 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2620 - mape: 8.3357\n",
      "Epoch 596: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2610 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2610 - mape: 8.2929 - val_loss: 0.2608 - val_mse: 0.2604 - val_rmse: 0.5103 - val_mae: 0.2608 - val_mape: 8.2971 - lr: 1.0000e-05\n",
      "Epoch 597/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2605 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2605 - mape: 8.3015\n",
      "Epoch 597: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2606 - mse: 0.2625 - rmse: 0.5124 - mae: 0.2606 - mape: 8.2661 - val_loss: 0.2602 - val_mse: 0.2628 - val_rmse: 0.5126 - val_mae: 0.2602 - val_mape: 8.2638 - lr: 1.0000e-05\n",
      "Epoch 598/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2596 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2596 - mape: 8.2386\n",
      "Epoch 598: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2606 - mse: 0.2629 - rmse: 0.5127 - mae: 0.2606 - mape: 8.2563 - val_loss: 0.2606 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2606 - val_mape: 8.2797 - lr: 1.0000e-05\n",
      "Epoch 599/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2602 - mse: 0.2629 - rmse: 0.5127 - mae: 0.2602 - mape: 8.2829\n",
      "Epoch 599: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2607 - mse: 0.2628 - rmse: 0.5126 - mae: 0.2607 - mape: 8.2737 - val_loss: 0.2601 - val_mse: 0.2629 - val_rmse: 0.5127 - val_mae: 0.2601 - val_mape: 8.2547 - lr: 1.0000e-05\n",
      "Epoch 600/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2596 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2596 - mape: 8.2445\n",
      "Epoch 600: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2607 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2607 - mape: 8.2763 - val_loss: 0.2608 - val_mse: 0.2612 - val_rmse: 0.5111 - val_mae: 0.2608 - val_mape: 8.2978 - lr: 1.0000e-05\n",
      "Epoch 601/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2590 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2590 - mape: 8.1980\n",
      "Epoch 601: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2607 - mse: 0.2625 - rmse: 0.5123 - mae: 0.2607 - mape: 8.2757 - val_loss: 0.2600 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2600 - val_mape: 8.2252 - lr: 1.0000e-05\n",
      "Epoch 602/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2626 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2626 - mape: 8.3003\n",
      "Epoch 602: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2605 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2605 - mape: 8.2433 - val_loss: 0.2625 - val_mse: 0.2676 - val_rmse: 0.5173 - val_mae: 0.2625 - val_mape: 8.4338 - lr: 1.0000e-05\n",
      "Epoch 603/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2603 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2603 - mape: 8.2698\n",
      "Epoch 603: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2608 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2608 - mape: 8.2840 - val_loss: 0.2603 - val_mse: 0.2649 - val_rmse: 0.5147 - val_mae: 0.2603 - val_mape: 8.2549 - lr: 1.0000e-05\n",
      "Epoch 604/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2589 - mse: 0.2617 - rmse: 0.5116 - mae: 0.2589 - mape: 8.2332\n",
      "Epoch 604: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2604 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2604 - mape: 8.2671 - val_loss: 0.2601 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2601 - val_mape: 8.2380 - lr: 1.0000e-05\n",
      "Epoch 605/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2612 - mse: 0.2630 - rmse: 0.5129 - mae: 0.2612 - mape: 8.2820\n",
      "Epoch 605: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2604 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2604 - mape: 8.2511 - val_loss: 0.2622 - val_mse: 0.2672 - val_rmse: 0.5169 - val_mae: 0.2622 - val_mape: 8.3301 - lr: 1.0000e-05\n",
      "Epoch 606/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2602 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2602 - mape: 8.2735\n",
      "Epoch 606: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2606 - mse: 0.2627 - rmse: 0.5126 - mae: 0.2606 - mape: 8.2732 - val_loss: 0.2601 - val_mse: 0.2618 - val_rmse: 0.5117 - val_mae: 0.2601 - val_mape: 8.2468 - lr: 1.0000e-05\n",
      "Epoch 607/1000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2595 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2595 - mape: 8.2154\n",
      "Epoch 607: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2604 - mse: 0.2623 - rmse: 0.5122 - mae: 0.2604 - mape: 8.2618 - val_loss: 0.2600 - val_mse: 0.2618 - val_rmse: 0.5117 - val_mae: 0.2600 - val_mape: 8.2233 - lr: 1.0000e-05\n",
      "Epoch 608/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2601 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2601 - mape: 8.2402\n",
      "Epoch 608: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2605 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2605 - mape: 8.2616 - val_loss: 0.2603 - val_mse: 0.2617 - val_rmse: 0.5116 - val_mae: 0.2603 - val_mape: 8.2754 - lr: 1.0000e-05\n",
      "Epoch 609/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2606 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2606 - mape: 8.2605\n",
      "Epoch 609: val_loss did not improve from 0.26000\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2608 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2608 - mape: 8.2682 - val_loss: 0.2603 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2603 - val_mape: 8.2166 - lr: 1.0000e-05\n",
      "Epoch 610/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2602 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2602 - mape: 8.2573\n",
      "Epoch 610: val_loss improved from 0.26000 to 0.25988, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2604 - mse: 0.2622 - rmse: 0.5121 - mae: 0.2604 - mape: 8.2615 - val_loss: 0.2599 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2599 - val_mape: 8.2552 - lr: 1.0000e-05\n",
      "Epoch 611/1000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2619 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2619 - mape: 8.2902\n",
      "Epoch 611: val_loss did not improve from 0.25988\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2609 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2609 - mape: 8.2696 - val_loss: 0.2602 - val_mse: 0.2620 - val_rmse: 0.5118 - val_mae: 0.2602 - val_mape: 8.2333 - lr: 1.0000e-05\n",
      "Epoch 612/1000\n",
      "262/318 [=======================>......] - ETA: 0s - loss: 0.2601 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2601 - mape: 8.2593\n",
      "Epoch 612: val_loss did not improve from 0.25988\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2609 - mse: 0.2627 - rmse: 0.5125 - mae: 0.2609 - mape: 8.2691 - val_loss: 0.2599 - val_mse: 0.2627 - val_rmse: 0.5125 - val_mae: 0.2599 - val_mape: 8.2592 - lr: 1.0000e-05\n",
      "Epoch 613/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2592 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2592 - mape: 8.2317\n",
      "Epoch 613: val_loss did not improve from 0.25988\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2604 - mse: 0.2626 - rmse: 0.5124 - mae: 0.2604 - mape: 8.2637 - val_loss: 0.2614 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2614 - val_mape: 8.2186 - lr: 1.0000e-05\n",
      "Epoch 614/1000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2606 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2606 - mape: 8.2501\n",
      "Epoch 614: val_loss did not improve from 0.25988\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2610 - mse: 0.2628 - rmse: 0.5126 - mae: 0.2610 - mape: 8.2594 - val_loss: 0.2599 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2599 - val_mape: 8.2402 - lr: 1.0000e-05\n",
      "Epoch 615/1000\n",
      "259/318 [=======================>......] - ETA: 0s - loss: 0.2605 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2605 - mape: 8.2688\n",
      "Epoch 615: val_loss did not improve from 0.25988\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2605 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2605 - mape: 8.2601 - val_loss: 0.2599 - val_mse: 0.2635 - val_rmse: 0.5134 - val_mae: 0.2599 - val_mape: 8.2645 - lr: 1.0000e-05\n",
      "Epoch 616/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2604 - mse: 0.2627 - rmse: 0.5125 - mae: 0.2604 - mape: 8.2531\n",
      "Epoch 616: val_loss did not improve from 0.25988\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2606 - mse: 0.2628 - rmse: 0.5127 - mae: 0.2606 - mape: 8.2731 - val_loss: 0.2599 - val_mse: 0.2631 - val_rmse: 0.5129 - val_mae: 0.2599 - val_mape: 8.2654 - lr: 1.0000e-05\n",
      "Epoch 617/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2612 - mse: 0.2632 - rmse: 0.5131 - mae: 0.2612 - mape: 8.2671\n",
      "Epoch 617: val_loss improved from 0.25988 to 0.25985, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2607 - mse: 0.2628 - rmse: 0.5126 - mae: 0.2607 - mape: 8.2625 - val_loss: 0.2598 - val_mse: 0.2613 - val_rmse: 0.5112 - val_mae: 0.2598 - val_mape: 8.2214 - lr: 1.0000e-05\n",
      "Epoch 618/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2612 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2612 - mape: 8.2651\n",
      "Epoch 618: val_loss did not improve from 0.25985\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2607 - mse: 0.2626 - rmse: 0.5124 - mae: 0.2607 - mape: 8.2578 - val_loss: 0.2602 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2602 - val_mape: 8.2587 - lr: 1.0000e-05\n",
      "Epoch 619/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2604 - mse: 0.2628 - rmse: 0.5126 - mae: 0.2604 - mape: 8.2615\n",
      "Epoch 619: val_loss improved from 0.25985 to 0.25973, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2605 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2605 - mape: 8.2686 - val_loss: 0.2597 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2597 - val_mape: 8.2397 - lr: 1.0000e-05\n",
      "Epoch 620/1000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2601 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2601 - mape: 8.2412\n",
      "Epoch 620: val_loss did not improve from 0.25973\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2605 - mse: 0.2624 - rmse: 0.5122 - mae: 0.2605 - mape: 8.2671 - val_loss: 0.2599 - val_mse: 0.2634 - val_rmse: 0.5133 - val_mae: 0.2599 - val_mape: 8.2592 - lr: 1.0000e-05\n",
      "Epoch 621/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2619 - mse: 0.2638 - rmse: 0.5136 - mae: 0.2619 - mape: 8.3117\n",
      "Epoch 621: val_loss did not improve from 0.25973\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2606 - mse: 0.2624 - rmse: 0.5123 - mae: 0.2606 - mape: 8.2681 - val_loss: 0.2604 - val_mse: 0.2656 - val_rmse: 0.5153 - val_mae: 0.2604 - val_mape: 8.3054 - lr: 1.0000e-05\n",
      "Epoch 622/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2625 - rmse: 0.5123 - mae: 0.2604 - mape: 8.2416\n",
      "Epoch 622: val_loss did not improve from 0.25973\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2606 - mse: 0.2628 - rmse: 0.5126 - mae: 0.2606 - mape: 8.2499 - val_loss: 0.2605 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2605 - val_mape: 8.2980 - lr: 1.0000e-05\n",
      "Epoch 623/1000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2607 - mse: 0.2621 - rmse: 0.5119 - mae: 0.2607 - mape: 8.2693\n",
      "Epoch 623: val_loss did not improve from 0.25973\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2606 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2606 - mape: 8.2616 - val_loss: 0.2612 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2612 - val_mape: 8.2482 - lr: 1.0000e-05\n",
      "Epoch 624/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2600 - mse: 0.2609 - rmse: 0.5107 - mae: 0.2600 - mape: 8.2670\n",
      "Epoch 624: val_loss did not improve from 0.25973\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2607 - mse: 0.2624 - rmse: 0.5123 - mae: 0.2607 - mape: 8.2699 - val_loss: 0.2610 - val_mse: 0.2638 - val_rmse: 0.5136 - val_mae: 0.2610 - val_mape: 8.2765 - lr: 1.0000e-05\n",
      "Epoch 625/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2620 - rmse: 0.5118 - mae: 0.2604 - mape: 8.2416\n",
      "Epoch 625: val_loss did not improve from 0.25973\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2605 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2605 - mape: 8.2613 - val_loss: 0.2603 - val_mse: 0.2648 - val_rmse: 0.5146 - val_mae: 0.2603 - val_mape: 8.2711 - lr: 1.0000e-05\n",
      "Epoch 626/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2597 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2597 - mape: 8.2167\n",
      "Epoch 626: val_loss did not improve from 0.25973\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2605 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2605 - mape: 8.2616 - val_loss: 0.2610 - val_mse: 0.2602 - val_rmse: 0.5101 - val_mae: 0.2610 - val_mape: 8.2881 - lr: 1.0000e-05\n",
      "Epoch 627/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2601 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2601 - mape: 8.2463\n",
      "Epoch 627: val_loss did not improve from 0.25973\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2602 - mse: 0.2612 - rmse: 0.5110 - mae: 0.2602 - mape: 8.2583 - val_loss: 0.2602 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2602 - val_mape: 8.2223 - lr: 1.0000e-05\n",
      "Epoch 628/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2583 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2583 - mape: 8.2059\n",
      "Epoch 628: val_loss did not improve from 0.25973\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2602 - mse: 0.2617 - rmse: 0.5115 - mae: 0.2602 - mape: 8.2460 - val_loss: 0.2607 - val_mse: 0.2643 - val_rmse: 0.5141 - val_mae: 0.2607 - val_mape: 8.2450 - lr: 1.0000e-05\n",
      "Epoch 629/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2587 - mse: 0.2560 - rmse: 0.5060 - mae: 0.2587 - mape: 8.1893\n",
      "Epoch 629: val_loss did not improve from 0.25973\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2601 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2601 - mape: 8.2522 - val_loss: 0.2601 - val_mse: 0.2618 - val_rmse: 0.5117 - val_mae: 0.2601 - val_mape: 8.2961 - lr: 1.0000e-05\n",
      "Epoch 630/1000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.2578 - mse: 0.2550 - rmse: 0.5050 - mae: 0.2578 - mape: 8.2228\n",
      "Epoch 630: val_loss improved from 0.25973 to 0.25968, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2604 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2604 - mape: 8.2581 - val_loss: 0.2597 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2597 - val_mape: 8.2261 - lr: 1.0000e-05\n",
      "Epoch 631/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2595 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2595 - mape: 8.2486\n",
      "Epoch 631: val_loss did not improve from 0.25968\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2605 - mse: 0.2623 - rmse: 0.5122 - mae: 0.2605 - mape: 8.2580 - val_loss: 0.2604 - val_mse: 0.2651 - val_rmse: 0.5149 - val_mae: 0.2604 - val_mape: 8.3267 - lr: 1.0000e-05\n",
      "Epoch 632/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2599 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2599 - mape: 8.2429\n",
      "Epoch 632: val_loss did not improve from 0.25968\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2600 - mse: 0.2613 - rmse: 0.5111 - mae: 0.2600 - mape: 8.2628 - val_loss: 0.2615 - val_mse: 0.2651 - val_rmse: 0.5149 - val_mae: 0.2615 - val_mape: 8.2228 - lr: 1.0000e-05\n",
      "Epoch 633/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2595 - mse: 0.2624 - rmse: 0.5123 - mae: 0.2595 - mape: 8.2695\n",
      "Epoch 633: val_loss did not improve from 0.25968\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2604 - mse: 0.2624 - rmse: 0.5123 - mae: 0.2604 - mape: 8.2645 - val_loss: 0.2600 - val_mse: 0.2602 - val_rmse: 0.5101 - val_mae: 0.2600 - val_mape: 8.2489 - lr: 1.0000e-05\n",
      "Epoch 634/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2603 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2603 - mape: 8.2510\n",
      "Epoch 634: val_loss did not improve from 0.25968\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2606 - mse: 0.2628 - rmse: 0.5127 - mae: 0.2606 - mape: 8.2682 - val_loss: 0.2600 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2600 - val_mape: 8.2049 - lr: 1.0000e-05\n",
      "Epoch 635/1000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2615 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2615 - mape: 8.2962\n",
      "Epoch 635: val_loss did not improve from 0.25968\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2602 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2602 - mape: 8.2454 - val_loss: 0.2603 - val_mse: 0.2624 - val_rmse: 0.5123 - val_mae: 0.2603 - val_mape: 8.1997 - lr: 1.0000e-05\n",
      "Epoch 636/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2593 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2593 - mape: 8.2100\n",
      "Epoch 636: val_loss did not improve from 0.25968\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2604 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2604 - mape: 8.2499 - val_loss: 0.2602 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2602 - val_mape: 8.2533 - lr: 1.0000e-05\n",
      "Epoch 637/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2592 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2592 - mape: 8.2387\n",
      "Epoch 637: val_loss did not improve from 0.25968\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2605 - mse: 0.2627 - rmse: 0.5125 - mae: 0.2605 - mape: 8.2629 - val_loss: 0.2606 - val_mse: 0.2627 - val_rmse: 0.5125 - val_mae: 0.2606 - val_mape: 8.2388 - lr: 1.0000e-05\n",
      "Epoch 638/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2610 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2610 - mape: 8.2760\n",
      "Epoch 638: val_loss did not improve from 0.25968\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2604 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2604 - mape: 8.2536 - val_loss: 0.2600 - val_mse: 0.2619 - val_rmse: 0.5118 - val_mae: 0.2600 - val_mape: 8.2626 - lr: 1.0000e-05\n",
      "Epoch 639/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2601 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2601 - mape: 8.2786\n",
      "Epoch 639: val_loss did not improve from 0.25968\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2606 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2606 - mape: 8.2746 - val_loss: 0.2600 - val_mse: 0.2622 - val_rmse: 0.5120 - val_mae: 0.2600 - val_mape: 8.2751 - lr: 1.0000e-05\n",
      "Epoch 640/1000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2607 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2607 - mape: 8.2691\n",
      "Epoch 640: val_loss did not improve from 0.25968\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2604 - mse: 0.2622 - rmse: 0.5121 - mae: 0.2604 - mape: 8.2665 - val_loss: 0.2598 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2598 - val_mape: 8.2263 - lr: 1.0000e-05\n",
      "Epoch 641/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2604 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2604 - mape: 8.2572\n",
      "Epoch 641: val_loss improved from 0.25968 to 0.25949, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2603 - mse: 0.2617 - rmse: 0.5115 - mae: 0.2603 - mape: 8.2530 - val_loss: 0.2595 - val_mse: 0.2623 - val_rmse: 0.5122 - val_mae: 0.2595 - val_mape: 8.2453 - lr: 1.0000e-05\n",
      "Epoch 642/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2606 - mse: 0.2634 - rmse: 0.5133 - mae: 0.2606 - mape: 8.2684\n",
      "Epoch 642: val_loss did not improve from 0.25949\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2602 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2602 - mape: 8.2547 - val_loss: 0.2604 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2604 - val_mape: 8.2079 - lr: 1.0000e-05\n",
      "Epoch 643/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2609 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2609 - mape: 8.2772\n",
      "Epoch 643: val_loss did not improve from 0.25949\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2602 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2602 - mape: 8.2471 - val_loss: 0.2601 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2601 - val_mape: 8.2671 - lr: 1.0000e-05\n",
      "Epoch 644/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2611 - mse: 0.2617 - rmse: 0.5115 - mae: 0.2611 - mape: 8.2653\n",
      "Epoch 644: val_loss did not improve from 0.25949\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2604 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2604 - mape: 8.2550 - val_loss: 0.2597 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2597 - val_mape: 8.2187 - lr: 1.0000e-05\n",
      "Epoch 645/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2613 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2613 - mape: 8.2785\n",
      "Epoch 645: val_loss did not improve from 0.25949\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2604 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2604 - mape: 8.2586 - val_loss: 0.2599 - val_mse: 0.2590 - val_rmse: 0.5090 - val_mae: 0.2599 - val_mape: 8.2394 - lr: 1.0000e-05\n",
      "Epoch 646/1000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2602 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2602 - mape: 8.2732\n",
      "Epoch 646: val_loss did not improve from 0.25949\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2603 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2603 - mape: 8.2526 - val_loss: 0.2605 - val_mse: 0.2668 - val_rmse: 0.5165 - val_mae: 0.2605 - val_mape: 8.3225 - lr: 1.0000e-05\n",
      "Epoch 647/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2599 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2599 - mape: 8.2563\n",
      "Epoch 647: val_loss did not improve from 0.25949\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2602 - mse: 0.2619 - rmse: 0.5117 - mae: 0.2602 - mape: 8.2578 - val_loss: 0.2596 - val_mse: 0.2613 - val_rmse: 0.5112 - val_mae: 0.2596 - val_mape: 8.2606 - lr: 1.0000e-05\n",
      "Epoch 648/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2601 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2601 - mape: 8.2538\n",
      "Epoch 648: val_loss did not improve from 0.25949\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2602 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2602 - mape: 8.2571 - val_loss: 0.2604 - val_mse: 0.2642 - val_rmse: 0.5140 - val_mae: 0.2604 - val_mape: 8.2711 - lr: 1.0000e-05\n",
      "Epoch 649/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2633 - mse: 0.2671 - rmse: 0.5168 - mae: 0.2633 - mape: 8.3536\n",
      "Epoch 649: val_loss did not improve from 0.25949\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2601 - mse: 0.2620 - rmse: 0.5118 - mae: 0.2601 - mape: 8.2540 - val_loss: 0.2596 - val_mse: 0.2599 - val_rmse: 0.5098 - val_mae: 0.2596 - val_mape: 8.1989 - lr: 1.0000e-05\n",
      "Epoch 650/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2605 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2605 - mape: 8.2540\n",
      "Epoch 650: val_loss did not improve from 0.25949\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2603 - mse: 0.2618 - rmse: 0.5116 - mae: 0.2603 - mape: 8.2519 - val_loss: 0.2596 - val_mse: 0.2615 - val_rmse: 0.5114 - val_mae: 0.2596 - val_mape: 8.2459 - lr: 1.0000e-05\n",
      "Epoch 651/1000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2622 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2622 - mape: 8.3261\n",
      "Epoch 651: val_loss did not improve from 0.25949\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2603 - mse: 0.2630 - rmse: 0.5129 - mae: 0.2603 - mape: 8.2653 - val_loss: 0.2602 - val_mse: 0.2578 - val_rmse: 0.5078 - val_mae: 0.2602 - val_mape: 8.2156 - lr: 1.0000e-05\n",
      "Epoch 652/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2605 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2605 - mape: 8.2373\n",
      "Epoch 652: val_loss did not improve from 0.25949\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2603 - mse: 0.2615 - rmse: 0.5113 - mae: 0.2603 - mape: 8.2518 - val_loss: 0.2599 - val_mse: 0.2628 - val_rmse: 0.5127 - val_mae: 0.2599 - val_mape: 8.2833 - lr: 1.0000e-05\n",
      "Epoch 653/1000\n",
      "258/318 [=======================>......] - ETA: 0s - loss: 0.2589 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2589 - mape: 8.2688\n",
      "Epoch 653: val_loss did not improve from 0.25949\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2601 - mse: 0.2617 - rmse: 0.5116 - mae: 0.2601 - mape: 8.2588 - val_loss: 0.2601 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2601 - val_mape: 8.2563 - lr: 1.0000e-05\n",
      "Epoch 654/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2598 - mape: 8.2551\n",
      "Epoch 654: val_loss did not improve from 0.25949\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2600 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2600 - mape: 8.2568 - val_loss: 0.2599 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2599 - val_mape: 8.2141 - lr: 1.0000e-05\n",
      "Epoch 655/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2599 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2599 - mape: 8.2372\n",
      "Epoch 655: val_loss improved from 0.25949 to 0.25941, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2602 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2602 - mape: 8.2521 - val_loss: 0.2594 - val_mse: 0.2612 - val_rmse: 0.5110 - val_mae: 0.2594 - val_mape: 8.2385 - lr: 1.0000e-05\n",
      "Epoch 656/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2603 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2603 - mape: 8.2549\n",
      "Epoch 656: val_loss did not improve from 0.25941\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2601 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2601 - mape: 8.2467 - val_loss: 0.2605 - val_mse: 0.2644 - val_rmse: 0.5142 - val_mae: 0.2605 - val_mape: 8.2348 - lr: 1.0000e-05\n",
      "Epoch 657/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2610 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2610 - mape: 8.2551\n",
      "Epoch 657: val_loss did not improve from 0.25941\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2600 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2600 - mape: 8.2299 - val_loss: 0.2604 - val_mse: 0.2625 - val_rmse: 0.5123 - val_mae: 0.2604 - val_mape: 8.3169 - lr: 1.0000e-05\n",
      "Epoch 658/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2591 - mse: 0.2616 - rmse: 0.5114 - mae: 0.2591 - mape: 8.2311\n",
      "Epoch 658: val_loss did not improve from 0.25941\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2602 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2602 - mape: 8.2515 - val_loss: 0.2610 - val_mse: 0.2624 - val_rmse: 0.5122 - val_mae: 0.2610 - val_mape: 8.3002 - lr: 1.0000e-05\n",
      "Epoch 659/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2624 - rmse: 0.5123 - mae: 0.2608 - mape: 8.2845\n",
      "Epoch 659: val_loss did not improve from 0.25941\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2609 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2609 - mape: 8.2699 - val_loss: 0.2600 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2600 - val_mape: 8.2445 - lr: 1.0000e-05\n",
      "Epoch 660/1000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2600 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2600 - mape: 8.2143\n",
      "Epoch 660: val_loss improved from 0.25941 to 0.25934, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2603 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2603 - mape: 8.2468 - val_loss: 0.2593 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2593 - val_mape: 8.2155 - lr: 1.0000e-05\n",
      "Epoch 661/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2600 - mse: 0.2621 - rmse: 0.5119 - mae: 0.2600 - mape: 8.2366\n",
      "Epoch 661: val_loss did not improve from 0.25934\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2599 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2599 - mape: 8.2384 - val_loss: 0.2594 - val_mse: 0.2616 - val_rmse: 0.5115 - val_mae: 0.2594 - val_mape: 8.2287 - lr: 1.0000e-05\n",
      "Epoch 662/1000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2603 - mse: 0.2621 - rmse: 0.5119 - mae: 0.2603 - mape: 8.2649\n",
      "Epoch 662: val_loss did not improve from 0.25934\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2599 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2599 - mape: 8.2374 - val_loss: 0.2601 - val_mse: 0.2614 - val_rmse: 0.5113 - val_mae: 0.2601 - val_mape: 8.2041 - lr: 1.0000e-05\n",
      "Epoch 663/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2598 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2598 - mape: 8.2345\n",
      "Epoch 663: val_loss did not improve from 0.25934\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2601 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2601 - mape: 8.2466 - val_loss: 0.2599 - val_mse: 0.2634 - val_rmse: 0.5132 - val_mae: 0.2599 - val_mape: 8.2312 - lr: 1.0000e-05\n",
      "Epoch 664/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2588 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2588 - mape: 8.2249\n",
      "Epoch 664: val_loss did not improve from 0.25934\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2604 - mse: 0.2619 - rmse: 0.5117 - mae: 0.2604 - mape: 8.2591 - val_loss: 0.2595 - val_mse: 0.2608 - val_rmse: 0.5106 - val_mae: 0.2595 - val_mape: 8.2052 - lr: 1.0000e-05\n",
      "Epoch 665/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2600 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2600 - mape: 8.2380\n",
      "Epoch 665: val_loss did not improve from 0.25934\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2601 - mse: 0.2613 - rmse: 0.5111 - mae: 0.2601 - mape: 8.2404 - val_loss: 0.2599 - val_mse: 0.2616 - val_rmse: 0.5115 - val_mae: 0.2599 - val_mape: 8.2693 - lr: 1.0000e-05\n",
      "Epoch 666/1000\n",
      "261/318 [=======================>......] - ETA: 0s - loss: 0.2599 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2599 - mape: 8.2273\n",
      "Epoch 666: val_loss did not improve from 0.25934\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2601 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2601 - mape: 8.2507 - val_loss: 0.2601 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2601 - val_mape: 8.1813 - lr: 1.0000e-05\n",
      "Epoch 667/1000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2611 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2611 - mape: 8.2267\n",
      "Epoch 667: val_loss did not improve from 0.25934\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2602 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2602 - mape: 8.2499 - val_loss: 0.2599 - val_mse: 0.2608 - val_rmse: 0.5107 - val_mae: 0.2599 - val_mape: 8.2224 - lr: 1.0000e-05\n",
      "Epoch 668/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2601 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2601 - mape: 8.2295\n",
      "Epoch 668: val_loss did not improve from 0.25934\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2600 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2600 - mape: 8.2376 - val_loss: 0.2595 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2595 - val_mape: 8.1935 - lr: 1.0000e-05\n",
      "Epoch 669/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2602 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2602 - mape: 8.2431\n",
      "Epoch 669: val_loss did not improve from 0.25934\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2604 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2604 - mape: 8.2494 - val_loss: 0.2594 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2594 - val_mape: 8.2193 - lr: 1.0000e-05\n",
      "Epoch 670/1000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2596 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2596 - mape: 8.2055\n",
      "Epoch 670: val_loss improved from 0.25934 to 0.25921, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2600 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2600 - mape: 8.2335 - val_loss: 0.2592 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2592 - val_mape: 8.2346 - lr: 1.0000e-05\n",
      "Epoch 671/1000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2599 - mse: 0.2620 - rmse: 0.5119 - mae: 0.2599 - mape: 8.2073\n",
      "Epoch 671: val_loss did not improve from 0.25921\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2600 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2600 - mape: 8.2288 - val_loss: 0.2595 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2595 - val_mape: 8.2493 - lr: 1.0000e-05\n",
      "Epoch 672/1000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2600 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2600 - mape: 8.2479\n",
      "Epoch 672: val_loss did not improve from 0.25921\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2600 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2600 - mape: 8.2337 - val_loss: 0.2599 - val_mse: 0.2631 - val_rmse: 0.5130 - val_mae: 0.2599 - val_mape: 8.2630 - lr: 1.0000e-05\n",
      "Epoch 673/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2585 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2585 - mape: 8.2165\n",
      "Epoch 673: val_loss did not improve from 0.25921\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2600 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2600 - mape: 8.2491 - val_loss: 0.2594 - val_mse: 0.2594 - val_rmse: 0.5094 - val_mae: 0.2594 - val_mape: 8.2392 - lr: 1.0000e-05\n",
      "Epoch 674/1000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2609 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2609 - mape: 8.2298\n",
      "Epoch 674: val_loss did not improve from 0.25921\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2599 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2599 - mape: 8.2376 - val_loss: 0.2592 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2592 - val_mape: 8.2063 - lr: 1.0000e-05\n",
      "Epoch 675/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2597 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2597 - mape: 8.2488\n",
      "Epoch 675: val_loss did not improve from 0.25921\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2598 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2598 - mape: 8.2350 - val_loss: 0.2599 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2599 - val_mape: 8.1901 - lr: 1.0000e-05\n",
      "Epoch 676/1000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2621 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2621 - mape: 8.3274\n",
      "Epoch 676: val_loss did not improve from 0.25921\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2600 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2600 - mape: 8.2383 - val_loss: 0.2595 - val_mse: 0.2609 - val_rmse: 0.5108 - val_mae: 0.2595 - val_mape: 8.1962 - lr: 1.0000e-05\n",
      "Epoch 677/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2602 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2602 - mape: 8.2328\n",
      "Epoch 677: val_loss did not improve from 0.25921\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2602 - mse: 0.2617 - rmse: 0.5115 - mae: 0.2602 - mape: 8.2512 - val_loss: 0.2593 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2593 - val_mape: 8.2070 - lr: 1.0000e-05\n",
      "Epoch 678/1000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2610 - mse: 0.2638 - rmse: 0.5136 - mae: 0.2610 - mape: 8.2483\n",
      "Epoch 678: val_loss did not improve from 0.25921\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2602 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2602 - mape: 8.2458 - val_loss: 0.2593 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2593 - val_mape: 8.1916 - lr: 1.0000e-05\n",
      "Epoch 679/1000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2596 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2596 - mape: 8.2356\n",
      "Epoch 679: val_loss did not improve from 0.25921\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2601 - mse: 0.2616 - rmse: 0.5114 - mae: 0.2601 - mape: 8.2526 - val_loss: 0.2599 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2599 - val_mape: 8.2113 - lr: 1.0000e-05\n",
      "Epoch 680/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2587 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2587 - mape: 8.1813\n",
      "Epoch 680: val_loss improved from 0.25921 to 0.25918, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2598 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2598 - mape: 8.2341 - val_loss: 0.2592 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2592 - val_mape: 8.2094 - lr: 1.0000e-05\n",
      "Epoch 681/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2603 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2603 - mape: 8.2465\n",
      "Epoch 681: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2602 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2602 - mape: 8.2368 - val_loss: 0.2598 - val_mse: 0.2609 - val_rmse: 0.5108 - val_mae: 0.2598 - val_mape: 8.2626 - lr: 1.0000e-05\n",
      "Epoch 682/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2606 - mse: 0.2625 - rmse: 0.5124 - mae: 0.2606 - mape: 8.2618\n",
      "Epoch 682: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2598 - mse: 0.2607 - rmse: 0.5105 - mae: 0.2598 - mape: 8.2432 - val_loss: 0.2600 - val_mse: 0.2608 - val_rmse: 0.5107 - val_mae: 0.2600 - val_mape: 8.2384 - lr: 1.0000e-05\n",
      "Epoch 683/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2603 - mse: 0.2621 - rmse: 0.5119 - mae: 0.2603 - mape: 8.2422\n",
      "Epoch 683: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2599 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2599 - mape: 8.2280 - val_loss: 0.2594 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2594 - val_mape: 8.2562 - lr: 1.0000e-05\n",
      "Epoch 684/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2601 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2601 - mape: 8.2433\n",
      "Epoch 684: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2598 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2598 - mape: 8.2403 - val_loss: 0.2618 - val_mse: 0.2679 - val_rmse: 0.5176 - val_mae: 0.2618 - val_mape: 8.2862 - lr: 1.0000e-05\n",
      "Epoch 685/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2601 - mse: 0.2627 - rmse: 0.5125 - mae: 0.2601 - mape: 8.2102\n",
      "Epoch 685: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2598 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2598 - mape: 8.2330 - val_loss: 0.2597 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2597 - val_mape: 8.2566 - lr: 1.0000e-05\n",
      "Epoch 686/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2623 - rmse: 0.5122 - mae: 0.2608 - mape: 8.2677\n",
      "Epoch 686: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2595 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2595 - mape: 8.2337 - val_loss: 0.2595 - val_mse: 0.2612 - val_rmse: 0.5111 - val_mae: 0.2595 - val_mape: 8.2671 - lr: 1.0000e-05\n",
      "Epoch 687/1000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2600 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2600 - mape: 8.2641\n",
      "Epoch 687: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2597 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2597 - mape: 8.2458 - val_loss: 0.2594 - val_mse: 0.2614 - val_rmse: 0.5113 - val_mae: 0.2594 - val_mape: 8.2705 - lr: 1.0000e-05\n",
      "Epoch 688/1000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2606 - mse: 0.2617 - rmse: 0.5116 - mae: 0.2606 - mape: 8.2524\n",
      "Epoch 688: val_loss did not improve from 0.25918\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2598 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2598 - mape: 8.2397 - val_loss: 0.2598 - val_mse: 0.2637 - val_rmse: 0.5135 - val_mae: 0.2598 - val_mape: 8.3053 - lr: 1.0000e-05\n",
      "Epoch 689/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2600 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2600 - mape: 8.2423\n",
      "Epoch 689: val_loss improved from 0.25918 to 0.25904, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2600 - mse: 0.2608 - rmse: 0.5106 - mae: 0.2600 - mape: 8.2467 - val_loss: 0.2590 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2590 - val_mape: 8.2125 - lr: 1.0000e-05\n",
      "Epoch 690/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2597 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2597 - mape: 8.2354\n",
      "Epoch 690: val_loss improved from 0.25904 to 0.25897, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2597 - mse: 0.2611 - rmse: 0.5109 - mae: 0.2597 - mape: 8.2274 - val_loss: 0.2590 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2590 - val_mape: 8.2251 - lr: 1.0000e-05\n",
      "Epoch 691/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2595 - mse: 0.2612 - rmse: 0.5110 - mae: 0.2595 - mape: 8.2441\n",
      "Epoch 691: val_loss did not improve from 0.25897\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2597 - mse: 0.2618 - rmse: 0.5116 - mae: 0.2597 - mape: 8.2456 - val_loss: 0.2591 - val_mse: 0.2586 - val_rmse: 0.5086 - val_mae: 0.2591 - val_mape: 8.2078 - lr: 1.0000e-05\n",
      "Epoch 692/1000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2603 - mse: 0.2622 - rmse: 0.5121 - mae: 0.2603 - mape: 8.2276\n",
      "Epoch 692: val_loss did not improve from 0.25897\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2595 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2595 - mape: 8.2323 - val_loss: 0.2596 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2596 - val_mape: 8.1733 - lr: 1.0000e-05\n",
      "Epoch 693/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2592 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2592 - mape: 8.2272\n",
      "Epoch 693: val_loss did not improve from 0.25897\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2597 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2597 - mape: 8.2323 - val_loss: 0.2603 - val_mse: 0.2616 - val_rmse: 0.5115 - val_mae: 0.2603 - val_mape: 8.1936 - lr: 1.0000e-05\n",
      "Epoch 694/1000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2598 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2598 - mape: 8.2248\n",
      "Epoch 694: val_loss did not improve from 0.25897\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2599 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2599 - mape: 8.2340 - val_loss: 0.2593 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2593 - val_mape: 8.2398 - lr: 1.0000e-05\n",
      "Epoch 695/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2594 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2594 - mape: 8.2241\n",
      "Epoch 695: val_loss did not improve from 0.25897\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2597 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2597 - mape: 8.2373 - val_loss: 0.2595 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2595 - val_mape: 8.1826 - lr: 1.0000e-05\n",
      "Epoch 696/1000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2595 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2595 - mape: 8.2354\n",
      "Epoch 696: val_loss did not improve from 0.25897\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2599 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2599 - mape: 8.2369 - val_loss: 0.2600 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2600 - val_mape: 8.2360 - lr: 1.0000e-05\n",
      "Epoch 697/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2577 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2577 - mape: 8.1961\n",
      "Epoch 697: val_loss did not improve from 0.25897\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2598 - mse: 0.2610 - rmse: 0.5108 - mae: 0.2598 - mape: 8.2418 - val_loss: 0.2599 - val_mse: 0.2558 - val_rmse: 0.5057 - val_mae: 0.2599 - val_mape: 8.1994 - lr: 1.0000e-05\n",
      "Epoch 698/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2602 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2602 - mape: 8.2181\n",
      "Epoch 698: val_loss improved from 0.25897 to 0.25896, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2597 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2597 - mape: 8.2174 - val_loss: 0.2590 - val_mse: 0.2603 - val_rmse: 0.5101 - val_mae: 0.2590 - val_mape: 8.2146 - lr: 1.0000e-05\n",
      "Epoch 699/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2619 - mse: 0.2660 - rmse: 0.5157 - mae: 0.2619 - mape: 8.3011\n",
      "Epoch 699: val_loss did not improve from 0.25896\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2597 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2597 - mape: 8.2433 - val_loss: 0.2594 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2594 - val_mape: 8.2034 - lr: 1.0000e-05\n",
      "Epoch 700/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2613 - mse: 0.2655 - rmse: 0.5153 - mae: 0.2613 - mape: 8.2857\n",
      "Epoch 700: val_loss did not improve from 0.25896\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2600 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2600 - mape: 8.2460 - val_loss: 0.2593 - val_mse: 0.2574 - val_rmse: 0.5074 - val_mae: 0.2593 - val_mape: 8.1765 - lr: 1.0000e-05\n",
      "Epoch 701/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2608 - mse: 0.2628 - rmse: 0.5126 - mae: 0.2608 - mape: 8.2530\n",
      "Epoch 701: val_loss improved from 0.25896 to 0.25895, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2596 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2596 - mape: 8.2323 - val_loss: 0.2589 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2589 - val_mape: 8.2349 - lr: 1.0000e-05\n",
      "Epoch 702/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2596 - mape: 8.2452\n",
      "Epoch 702: val_loss improved from 0.25895 to 0.25893, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2597 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2597 - mape: 8.2436 - val_loss: 0.2589 - val_mse: 0.2581 - val_rmse: 0.5081 - val_mae: 0.2589 - val_mape: 8.2058 - lr: 1.0000e-05\n",
      "Epoch 703/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2574 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2574 - mape: 8.1573\n",
      "Epoch 703: val_loss did not improve from 0.25893\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2597 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2597 - mape: 8.2141 - val_loss: 0.2602 - val_mse: 0.2622 - val_rmse: 0.5120 - val_mae: 0.2602 - val_mape: 8.1990 - lr: 1.0000e-05\n",
      "Epoch 704/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2596 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2596 - mape: 8.2370\n",
      "Epoch 704: val_loss improved from 0.25893 to 0.25889, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2598 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2598 - mape: 8.2324 - val_loss: 0.2589 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2589 - val_mape: 8.2099 - lr: 1.0000e-05\n",
      "Epoch 705/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2594 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2594 - mape: 8.2054\n",
      "Epoch 705: val_loss did not improve from 0.25889\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2596 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2596 - mape: 8.2228 - val_loss: 0.2605 - val_mse: 0.2612 - val_rmse: 0.5111 - val_mae: 0.2605 - val_mape: 8.3086 - lr: 1.0000e-05\n",
      "Epoch 706/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2586 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2586 - mape: 8.2068\n",
      "Epoch 706: val_loss did not improve from 0.25889\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2597 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2597 - mape: 8.2393 - val_loss: 0.2594 - val_mse: 0.2617 - val_rmse: 0.5116 - val_mae: 0.2594 - val_mape: 8.1976 - lr: 1.0000e-05\n",
      "Epoch 707/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2593 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2593 - mape: 8.2359\n",
      "Epoch 707: val_loss did not improve from 0.25889\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2596 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2596 - mape: 8.2222 - val_loss: 0.2592 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2592 - val_mape: 8.2020 - lr: 1.0000e-05\n",
      "Epoch 708/1000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2602 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2602 - mape: 8.2302\n",
      "Epoch 708: val_loss did not improve from 0.25889\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2598 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2598 - mape: 8.2323 - val_loss: 0.2594 - val_mse: 0.2619 - val_rmse: 0.5118 - val_mae: 0.2594 - val_mape: 8.2569 - lr: 1.0000e-05\n",
      "Epoch 709/1000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2585 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2585 - mape: 8.1680\n",
      "Epoch 709: val_loss did not improve from 0.25889\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2594 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2594 - mape: 8.2288 - val_loss: 0.2600 - val_mse: 0.2560 - val_rmse: 0.5060 - val_mae: 0.2600 - val_mape: 8.1565 - lr: 1.0000e-05\n",
      "Epoch 710/1000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2595 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2595 - mape: 8.2360\n",
      "Epoch 710: val_loss improved from 0.25889 to 0.25886, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2598 - mse: 0.2595 - rmse: 0.5095 - mae: 0.2598 - mape: 8.2284 - val_loss: 0.2589 - val_mse: 0.2604 - val_rmse: 0.5103 - val_mae: 0.2589 - val_mape: 8.2162 - lr: 1.0000e-05\n",
      "Epoch 711/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2594 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2594 - mape: 8.2250\n",
      "Epoch 711: val_loss did not improve from 0.25886\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2594 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2594 - mape: 8.2250 - val_loss: 0.2605 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2605 - val_mape: 8.2989 - lr: 1.0000e-05\n",
      "Epoch 712/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2604 - mape: 8.2505\n",
      "Epoch 712: val_loss did not improve from 0.25886\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2601 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2601 - mape: 8.2408 - val_loss: 0.2589 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2589 - val_mape: 8.2282 - lr: 1.0000e-05\n",
      "Epoch 713/1000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2582 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2582 - mape: 8.2073\n",
      "Epoch 713: val_loss did not improve from 0.25886\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2598 - mse: 0.2613 - rmse: 0.5111 - mae: 0.2598 - mape: 8.2373 - val_loss: 0.2590 - val_mse: 0.2588 - val_rmse: 0.5088 - val_mae: 0.2590 - val_mape: 8.1837 - lr: 1.0000e-05\n",
      "Epoch 714/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2589 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2589 - mape: 8.2164\n",
      "Epoch 714: val_loss did not improve from 0.25886\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2597 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2597 - mape: 8.2252 - val_loss: 0.2592 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2592 - val_mape: 8.1737 - lr: 1.0000e-05\n",
      "Epoch 715/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2592 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2592 - mape: 8.2041\n",
      "Epoch 715: val_loss did not improve from 0.25886\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2595 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2595 - mape: 8.2169 - val_loss: 0.2604 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2604 - val_mape: 8.2906 - lr: 1.0000e-05\n",
      "Epoch 716/1000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2583 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2583 - mape: 8.1834\n",
      "Epoch 716: val_loss did not improve from 0.25886\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2595 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2595 - mape: 8.2330 - val_loss: 0.2608 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2608 - val_mape: 8.1502 - lr: 1.0000e-05\n",
      "Epoch 717/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2600 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2600 - mape: 8.2309\n",
      "Epoch 717: val_loss improved from 0.25886 to 0.25867, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2599 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2599 - mape: 8.2267 - val_loss: 0.2587 - val_mse: 0.2581 - val_rmse: 0.5081 - val_mae: 0.2587 - val_mape: 8.1957 - lr: 1.0000e-05\n",
      "Epoch 718/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2593 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2593 - mape: 8.2203\n",
      "Epoch 718: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2593 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2593 - mape: 8.2243 - val_loss: 0.2592 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2592 - val_mape: 8.2183 - lr: 1.0000e-05\n",
      "Epoch 719/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2593 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2593 - mape: 8.2208\n",
      "Epoch 719: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2596 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2596 - mape: 8.2250 - val_loss: 0.2587 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2587 - val_mape: 8.2042 - lr: 1.0000e-05\n",
      "Epoch 720/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2595 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2595 - mape: 8.2284\n",
      "Epoch 720: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2595 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2595 - mape: 8.2284 - val_loss: 0.2599 - val_mse: 0.2652 - val_rmse: 0.5150 - val_mae: 0.2599 - val_mape: 8.3084 - lr: 1.0000e-05\n",
      "Epoch 721/1000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2588 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2588 - mape: 8.1855\n",
      "Epoch 721: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2596 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2596 - mape: 8.2152 - val_loss: 0.2589 - val_mse: 0.2614 - val_rmse: 0.5113 - val_mae: 0.2589 - val_mape: 8.2133 - lr: 1.0000e-05\n",
      "Epoch 722/1000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2600 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2600 - mape: 8.2367\n",
      "Epoch 722: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2593 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2593 - mape: 8.2093 - val_loss: 0.2590 - val_mse: 0.2604 - val_rmse: 0.5103 - val_mae: 0.2590 - val_mape: 8.2405 - lr: 1.0000e-05\n",
      "Epoch 723/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2591 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2591 - mape: 8.2119\n",
      "Epoch 723: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2594 - mse: 0.2592 - rmse: 0.5092 - mae: 0.2594 - mape: 8.2294 - val_loss: 0.2589 - val_mse: 0.2577 - val_rmse: 0.5077 - val_mae: 0.2589 - val_mape: 8.1917 - lr: 1.0000e-05\n",
      "Epoch 724/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2607 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2607 - mape: 8.2573\n",
      "Epoch 724: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2593 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2593 - mape: 8.2087 - val_loss: 0.2589 - val_mse: 0.2621 - val_rmse: 0.5119 - val_mae: 0.2589 - val_mape: 8.2439 - lr: 1.0000e-05\n",
      "Epoch 725/1000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2580 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2580 - mape: 8.2154\n",
      "Epoch 725: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2595 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2595 - mape: 8.2240 - val_loss: 0.2588 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2588 - val_mape: 8.2448 - lr: 1.0000e-05\n",
      "Epoch 726/1000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2569 - mse: 0.2545 - rmse: 0.5044 - mae: 0.2569 - mape: 8.1743\n",
      "Epoch 726: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2596 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2596 - mape: 8.2325 - val_loss: 0.2591 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2591 - val_mape: 8.2257 - lr: 1.0000e-05\n",
      "Epoch 727/1000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2593 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2593 - mape: 8.2152\n",
      "Epoch 727: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2594 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2594 - mape: 8.2238 - val_loss: 0.2590 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2590 - val_mape: 8.1587 - lr: 1.0000e-05\n",
      "Epoch 728/1000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.2593 - mse: 0.2593 - rmse: 0.5093 - mae: 0.2593 - mape: 8.2419\n",
      "Epoch 728: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2595 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2595 - mape: 8.2251 - val_loss: 0.2591 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2591 - val_mape: 8.2400 - lr: 1.0000e-05\n",
      "Epoch 729/1000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2610 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2610 - mape: 8.2556\n",
      "Epoch 729: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2593 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2593 - mape: 8.2236 - val_loss: 0.2588 - val_mse: 0.2580 - val_rmse: 0.5080 - val_mae: 0.2588 - val_mape: 8.2012 - lr: 1.0000e-05\n",
      "Epoch 730/1000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2590 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2590 - mape: 8.2201\n",
      "Epoch 730: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2598 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2598 - mape: 8.2317 - val_loss: 0.2588 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2588 - val_mape: 8.2377 - lr: 1.0000e-05\n",
      "Epoch 731/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2582 - mse: 0.2560 - rmse: 0.5060 - mae: 0.2582 - mape: 8.1629\n",
      "Epoch 731: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2589 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2589 - mape: 8.1996 - val_loss: 0.2597 - val_mse: 0.2635 - val_rmse: 0.5133 - val_mae: 0.2597 - val_mape: 8.2997 - lr: 1.0000e-05\n",
      "Epoch 732/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2581 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2581 - mape: 8.1966\n",
      "Epoch 732: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2596 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2596 - mape: 8.2247 - val_loss: 0.2595 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2595 - val_mape: 8.2337 - lr: 1.0000e-05\n",
      "Epoch 733/1000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2591 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2591 - mape: 8.2329\n",
      "Epoch 733: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2597 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2597 - mape: 8.2349 - val_loss: 0.2590 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2590 - val_mape: 8.2423 - lr: 1.0000e-05\n",
      "Epoch 734/1000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2597 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2597 - mape: 8.2002\n",
      "Epoch 734: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2592 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2592 - mape: 8.2160 - val_loss: 0.2617 - val_mse: 0.2636 - val_rmse: 0.5134 - val_mae: 0.2617 - val_mape: 8.1806 - lr: 1.0000e-05\n",
      "Epoch 735/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2589 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2589 - mape: 8.1853\n",
      "Epoch 735: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2587 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2587 - mape: 8.1948 - val_loss: 0.2590 - val_mse: 0.2568 - val_rmse: 0.5068 - val_mae: 0.2590 - val_mape: 8.2023 - lr: 1.0000e-05\n",
      "Epoch 736/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2591 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2591 - mape: 8.2113\n",
      "Epoch 736: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2592 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2592 - mape: 8.2038 - val_loss: 0.2587 - val_mse: 0.2581 - val_rmse: 0.5081 - val_mae: 0.2587 - val_mape: 8.2113 - lr: 1.0000e-05\n",
      "Epoch 737/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2591 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2591 - mape: 8.2107\n",
      "Epoch 737: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2593 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2593 - mape: 8.2155 - val_loss: 0.2588 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2588 - val_mape: 8.1553 - lr: 1.0000e-05\n",
      "Epoch 738/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2609 - rmse: 0.5107 - mae: 0.2598 - mape: 8.2166\n",
      "Epoch 738: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2596 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2596 - mape: 8.2210 - val_loss: 0.2589 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2589 - val_mape: 8.2351 - lr: 1.0000e-05\n",
      "Epoch 739/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2582 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2582 - mape: 8.1821\n",
      "Epoch 739: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2593 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2593 - mape: 8.2138 - val_loss: 0.2594 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2594 - val_mape: 8.2409 - lr: 1.0000e-05\n",
      "Epoch 740/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2599 - mse: 0.2613 - rmse: 0.5111 - mae: 0.2599 - mape: 8.2313\n",
      "Epoch 740: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2592 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2592 - mape: 8.2166 - val_loss: 0.2587 - val_mse: 0.2560 - val_rmse: 0.5060 - val_mae: 0.2587 - val_mape: 8.1603 - lr: 1.0000e-05\n",
      "Epoch 741/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2595 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2595 - mape: 8.2319\n",
      "Epoch 741: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2594 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2594 - mape: 8.2194 - val_loss: 0.2595 - val_mse: 0.2568 - val_rmse: 0.5068 - val_mae: 0.2595 - val_mape: 8.1987 - lr: 1.0000e-05\n",
      "Epoch 742/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2587 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2587 - mape: 8.1925\n",
      "Epoch 742: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2594 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2594 - mape: 8.2135 - val_loss: 0.2593 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2593 - val_mape: 8.2618 - lr: 1.0000e-05\n",
      "Epoch 743/1000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2593 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2593 - mape: 8.2246\n",
      "Epoch 743: val_loss did not improve from 0.25867\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2595 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2595 - mape: 8.2150 - val_loss: 0.2588 - val_mse: 0.2565 - val_rmse: 0.5065 - val_mae: 0.2588 - val_mape: 8.1626 - lr: 1.0000e-05\n",
      "Epoch 744/1000\n",
      "262/318 [=======================>......] - ETA: 0s - loss: 0.2605 - mse: 0.2601 - rmse: 0.5100 - mae: 0.2605 - mape: 8.2427\n",
      "Epoch 744: val_loss improved from 0.25867 to 0.25856, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2592 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2592 - mape: 8.2051 - val_loss: 0.2586 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2586 - val_mape: 8.1624 - lr: 1.0000e-05\n",
      "Epoch 745/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2597 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2597 - mape: 8.2162\n",
      "Epoch 745: val_loss did not improve from 0.25856\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2589 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2589 - mape: 8.1998 - val_loss: 0.2592 - val_mse: 0.2568 - val_rmse: 0.5068 - val_mae: 0.2592 - val_mape: 8.1976 - lr: 1.0000e-05\n",
      "Epoch 746/1000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2599 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2599 - mape: 8.2480\n",
      "Epoch 746: val_loss did not improve from 0.25856\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2592 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2592 - mape: 8.2070 - val_loss: 0.2592 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2592 - val_mape: 8.2201 - lr: 1.0000e-05\n",
      "Epoch 747/1000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2590 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2590 - mape: 8.2014\n",
      "Epoch 747: val_loss did not improve from 0.25856\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2592 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2592 - mape: 8.2009 - val_loss: 0.2603 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2603 - val_mape: 8.2985 - lr: 1.0000e-05\n",
      "Epoch 748/1000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2593 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2593 - mape: 8.2207\n",
      "Epoch 748: val_loss did not improve from 0.25856\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2592 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2592 - mape: 8.2284 - val_loss: 0.2591 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2591 - val_mape: 8.1777 - lr: 1.0000e-05\n",
      "Epoch 749/1000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.2597 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2597 - mape: 8.1974\n",
      "Epoch 749: val_loss improved from 0.25856 to 0.25852, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2593 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2593 - mape: 8.2120 - val_loss: 0.2585 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2585 - val_mape: 8.1876 - lr: 1.0000e-05\n",
      "Epoch 750/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2599 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2599 - mape: 8.2397\n",
      "Epoch 750: val_loss did not improve from 0.25852\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2594 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2594 - mape: 8.2229 - val_loss: 0.2587 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2587 - val_mape: 8.2168 - lr: 1.0000e-05\n",
      "Epoch 751/1000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2589 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2589 - mape: 8.2095\n",
      "Epoch 751: val_loss did not improve from 0.25852\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2593 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2593 - mape: 8.2326 - val_loss: 0.2589 - val_mse: 0.2565 - val_rmse: 0.5065 - val_mae: 0.2589 - val_mape: 8.1558 - lr: 1.0000e-05\n",
      "Epoch 752/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2593 - mse: 0.2590 - rmse: 0.5090 - mae: 0.2593 - mape: 8.2271\n",
      "Epoch 752: val_loss did not improve from 0.25852\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2594 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2594 - mape: 8.2195 - val_loss: 0.2588 - val_mse: 0.2566 - val_rmse: 0.5065 - val_mae: 0.2588 - val_mape: 8.1858 - lr: 1.0000e-05\n",
      "Epoch 753/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2583 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2583 - mape: 8.1761\n",
      "Epoch 753: val_loss did not improve from 0.25852\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2591 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2591 - mape: 8.2098 - val_loss: 0.2588 - val_mse: 0.2559 - val_rmse: 0.5059 - val_mae: 0.2588 - val_mape: 8.1726 - lr: 1.0000e-05\n",
      "Epoch 754/1000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2596 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2596 - mape: 8.2069\n",
      "Epoch 754: val_loss improved from 0.25852 to 0.25851, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2589 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2589 - mape: 8.1995 - val_loss: 0.2585 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2585 - val_mape: 8.1818 - lr: 1.0000e-05\n",
      "Epoch 755/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2592 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2592 - mape: 8.2019\n",
      "Epoch 755: val_loss did not improve from 0.25851\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2591 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2591 - mape: 8.2072 - val_loss: 0.2591 - val_mse: 0.2553 - val_rmse: 0.5053 - val_mae: 0.2591 - val_mape: 8.1799 - lr: 1.0000e-05\n",
      "Epoch 756/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2593 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2593 - mape: 8.2024\n",
      "Epoch 756: val_loss did not improve from 0.25851\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2591 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2591 - mape: 8.2023 - val_loss: 0.2587 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2587 - val_mape: 8.2287 - lr: 1.0000e-05\n",
      "Epoch 757/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2594 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2594 - mape: 8.2312\n",
      "Epoch 757: val_loss did not improve from 0.25851\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2593 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2593 - mape: 8.2258 - val_loss: 0.2592 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2592 - val_mape: 8.1331 - lr: 1.0000e-05\n",
      "Epoch 758/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2589 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2589 - mape: 8.1921\n",
      "Epoch 758: val_loss did not improve from 0.25851\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2592 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2592 - mape: 8.2071 - val_loss: 0.2586 - val_mse: 0.2591 - val_rmse: 0.5090 - val_mae: 0.2586 - val_mape: 8.1857 - lr: 1.0000e-05\n",
      "Epoch 759/1000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2588 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2588 - mape: 8.1672\n",
      "Epoch 759: val_loss did not improve from 0.25851\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2590 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2590 - mape: 8.2098 - val_loss: 0.2592 - val_mse: 0.2559 - val_rmse: 0.5059 - val_mae: 0.2592 - val_mape: 8.1494 - lr: 1.0000e-05\n",
      "Epoch 760/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2579 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2579 - mape: 8.1674\n",
      "Epoch 760: val_loss did not improve from 0.25851\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2591 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2591 - mape: 8.2033 - val_loss: 0.2591 - val_mse: 0.2547 - val_rmse: 0.5047 - val_mae: 0.2591 - val_mape: 8.1464 - lr: 1.0000e-05\n",
      "Epoch 761/1000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2577 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2577 - mape: 8.1482\n",
      "Epoch 761: val_loss did not improve from 0.25851\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2592 - mse: 0.2585 - rmse: 0.5085 - mae: 0.2592 - mape: 8.2095 - val_loss: 0.2587 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2587 - val_mape: 8.1877 - lr: 1.0000e-05\n",
      "Epoch 762/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2590 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2590 - mape: 8.2032\n",
      "Epoch 762: val_loss did not improve from 0.25851\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2591 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2591 - mape: 8.2087 - val_loss: 0.2595 - val_mse: 0.2631 - val_rmse: 0.5129 - val_mae: 0.2595 - val_mape: 8.2338 - lr: 1.0000e-05\n",
      "Epoch 763/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2578 - mse: 0.2551 - rmse: 0.5051 - mae: 0.2578 - mape: 8.1782\n",
      "Epoch 763: val_loss did not improve from 0.25851\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2591 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2591 - mape: 8.2017 - val_loss: 0.2610 - val_mse: 0.2680 - val_rmse: 0.5177 - val_mae: 0.2610 - val_mape: 8.3315 - lr: 1.0000e-05\n",
      "Epoch 764/1000\n",
      "271/318 [========================>.....] - ETA: 0s - loss: 0.2574 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2574 - mape: 8.1449\n",
      "Epoch 764: val_loss improved from 0.25851 to 0.25834, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2593 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2593 - mape: 8.2207 - val_loss: 0.2583 - val_mse: 0.2568 - val_rmse: 0.5068 - val_mae: 0.2583 - val_mape: 8.1785 - lr: 1.0000e-05\n",
      "Epoch 765/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2572 - mse: 0.2543 - rmse: 0.5043 - mae: 0.2572 - mape: 8.1362\n",
      "Epoch 765: val_loss did not improve from 0.25834\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2588 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2588 - mape: 8.1978 - val_loss: 0.2592 - val_mse: 0.2612 - val_rmse: 0.5111 - val_mae: 0.2592 - val_mape: 8.2574 - lr: 1.0000e-05\n",
      "Epoch 766/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2584 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2584 - mape: 8.1844\n",
      "Epoch 766: val_loss did not improve from 0.25834\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2588 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2588 - mape: 8.2076 - val_loss: 0.2595 - val_mse: 0.2625 - val_rmse: 0.5123 - val_mae: 0.2595 - val_mape: 8.2086 - lr: 1.0000e-05\n",
      "Epoch 767/1000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2597 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2597 - mape: 8.1952\n",
      "Epoch 767: val_loss did not improve from 0.25834\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2588 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2588 - mape: 8.1964 - val_loss: 0.2584 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2584 - val_mape: 8.2106 - lr: 1.0000e-05\n",
      "Epoch 768/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2599 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2599 - mape: 8.2369\n",
      "Epoch 768: val_loss did not improve from 0.25834\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2591 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2591 - mape: 8.2123 - val_loss: 0.2584 - val_mse: 0.2552 - val_rmse: 0.5051 - val_mae: 0.2584 - val_mape: 8.1646 - lr: 1.0000e-05\n",
      "Epoch 769/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2589 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2589 - mape: 8.1938\n",
      "Epoch 769: val_loss did not improve from 0.25834\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2590 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2590 - mape: 8.2074 - val_loss: 0.2584 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2584 - val_mape: 8.1780 - lr: 1.0000e-05\n",
      "Epoch 770/1000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2601 - mse: 0.2630 - rmse: 0.5128 - mae: 0.2601 - mape: 8.2596\n",
      "Epoch 770: val_loss did not improve from 0.25834\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2588 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2588 - mape: 8.2124 - val_loss: 0.2583 - val_mse: 0.2557 - val_rmse: 0.5057 - val_mae: 0.2583 - val_mape: 8.1746 - lr: 1.0000e-05\n",
      "Epoch 771/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2587 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2587 - mape: 8.1784\n",
      "Epoch 771: val_loss did not improve from 0.25834\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2591 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2591 - mape: 8.1985 - val_loss: 0.2590 - val_mse: 0.2624 - val_rmse: 0.5123 - val_mae: 0.2590 - val_mape: 8.2353 - lr: 1.0000e-05\n",
      "Epoch 772/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2580 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2580 - mape: 8.1872\n",
      "Epoch 772: val_loss did not improve from 0.25834\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2590 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2590 - mape: 8.2140 - val_loss: 0.2605 - val_mse: 0.2651 - val_rmse: 0.5148 - val_mae: 0.2605 - val_mape: 8.2865 - lr: 1.0000e-05\n",
      "Epoch 773/1000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2586 - mse: 0.2561 - rmse: 0.5060 - mae: 0.2586 - mape: 8.2104\n",
      "Epoch 773: val_loss did not improve from 0.25834\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2593 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2593 - mape: 8.2095 - val_loss: 0.2599 - val_mse: 0.2566 - val_rmse: 0.5065 - val_mae: 0.2599 - val_mape: 8.2265 - lr: 1.0000e-05\n",
      "Epoch 774/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2576 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2576 - mape: 8.1759\n",
      "Epoch 774: val_loss did not improve from 0.25834\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2590 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2590 - mape: 8.2076 - val_loss: 0.2599 - val_mse: 0.2549 - val_rmse: 0.5049 - val_mae: 0.2599 - val_mape: 8.1486 - lr: 1.0000e-05\n",
      "Epoch 775/1000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2600 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2600 - mape: 8.2068\n",
      "Epoch 775: val_loss improved from 0.25834 to 0.25824, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2592 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2592 - mape: 8.1953 - val_loss: 0.2582 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2582 - val_mape: 8.1891 - lr: 1.0000e-05\n",
      "Epoch 776/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2578 - mse: 0.2559 - rmse: 0.5059 - mae: 0.2578 - mape: 8.1642\n",
      "Epoch 776: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2588 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2588 - mape: 8.2002 - val_loss: 0.2584 - val_mse: 0.2579 - val_rmse: 0.5078 - val_mae: 0.2584 - val_mape: 8.2092 - lr: 1.0000e-05\n",
      "Epoch 777/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2581 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2581 - mape: 8.1894\n",
      "Epoch 777: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2591 - mse: 0.2590 - rmse: 0.5090 - mae: 0.2591 - mape: 8.2113 - val_loss: 0.2591 - val_mse: 0.2555 - val_rmse: 0.5055 - val_mae: 0.2591 - val_mape: 8.2155 - lr: 1.0000e-05\n",
      "Epoch 778/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2597 - mse: 0.2617 - rmse: 0.5116 - mae: 0.2597 - mape: 8.2123\n",
      "Epoch 778: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2590 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2590 - mape: 8.2024 - val_loss: 0.2587 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2587 - val_mape: 8.2551 - lr: 1.0000e-05\n",
      "Epoch 779/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2598 - mse: 0.2621 - rmse: 0.5120 - mae: 0.2598 - mape: 8.2326\n",
      "Epoch 779: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2589 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2589 - mape: 8.2153 - val_loss: 0.2586 - val_mse: 0.2559 - val_rmse: 0.5059 - val_mae: 0.2586 - val_mape: 8.1513 - lr: 1.0000e-05\n",
      "Epoch 780/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2584 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2584 - mape: 8.1807\n",
      "Epoch 780: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2589 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2589 - mape: 8.2052 - val_loss: 0.2601 - val_mse: 0.2557 - val_rmse: 0.5057 - val_mae: 0.2601 - val_mape: 8.2423 - lr: 1.0000e-05\n",
      "Epoch 781/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2601 - mse: 0.2607 - rmse: 0.5106 - mae: 0.2601 - mape: 8.2315\n",
      "Epoch 781: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2589 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2589 - mape: 8.2023 - val_loss: 0.2585 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2585 - val_mape: 8.1873 - lr: 1.0000e-05\n",
      "Epoch 782/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2578 - mse: 0.2560 - rmse: 0.5059 - mae: 0.2578 - mape: 8.1771\n",
      "Epoch 782: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2587 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2587 - mape: 8.1976 - val_loss: 0.2586 - val_mse: 0.2573 - val_rmse: 0.5072 - val_mae: 0.2586 - val_mape: 8.2035 - lr: 1.0000e-05\n",
      "Epoch 783/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2587 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2587 - mape: 8.1968\n",
      "Epoch 783: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2587 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2587 - mape: 8.1889 - val_loss: 0.2586 - val_mse: 0.2594 - val_rmse: 0.5094 - val_mae: 0.2586 - val_mape: 8.2461 - lr: 1.0000e-05\n",
      "Epoch 784/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2600 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2600 - mape: 8.2367\n",
      "Epoch 784: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2587 - mse: 0.2589 - rmse: 0.5089 - mae: 0.2587 - mape: 8.2035 - val_loss: 0.2584 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2584 - val_mape: 8.2014 - lr: 1.0000e-05\n",
      "Epoch 785/1000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2597 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2597 - mape: 8.2358\n",
      "Epoch 785: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2591 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2591 - mape: 8.2172 - val_loss: 0.2586 - val_mse: 0.2620 - val_rmse: 0.5118 - val_mae: 0.2586 - val_mape: 8.2161 - lr: 1.0000e-05\n",
      "Epoch 786/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2588 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2588 - mape: 8.2048\n",
      "Epoch 786: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2590 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2590 - mape: 8.2081 - val_loss: 0.2585 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2585 - val_mape: 8.2041 - lr: 1.0000e-05\n",
      "Epoch 787/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2592 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2592 - mape: 8.2068\n",
      "Epoch 787: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2588 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2588 - mape: 8.2045 - val_loss: 0.2585 - val_mse: 0.2568 - val_rmse: 0.5068 - val_mae: 0.2585 - val_mape: 8.2184 - lr: 1.0000e-05\n",
      "Epoch 788/1000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2583 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2583 - mape: 8.2002\n",
      "Epoch 788: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2587 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2587 - mape: 8.2091 - val_loss: 0.2591 - val_mse: 0.2622 - val_rmse: 0.5121 - val_mae: 0.2591 - val_mape: 8.2529 - lr: 1.0000e-05\n",
      "Epoch 789/1000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2577 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2577 - mape: 8.1485\n",
      "Epoch 789: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2591 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2591 - mape: 8.2126 - val_loss: 0.2599 - val_mse: 0.2544 - val_rmse: 0.5044 - val_mae: 0.2599 - val_mape: 8.1983 - lr: 1.0000e-05\n",
      "Epoch 790/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2585 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2585 - mape: 8.1750\n",
      "Epoch 790: val_loss did not improve from 0.25824\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2587 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2587 - mape: 8.1860 - val_loss: 0.2591 - val_mse: 0.2633 - val_rmse: 0.5131 - val_mae: 0.2591 - val_mape: 8.2816 - lr: 1.0000e-05\n",
      "Epoch 791/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2586 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2586 - mape: 8.1949\n",
      "Epoch 791: val_loss improved from 0.25824 to 0.25807, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2590 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2590 - mape: 8.2153 - val_loss: 0.2581 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2581 - val_mape: 8.1669 - lr: 1.0000e-05\n",
      "Epoch 792/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2594 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2594 - mape: 8.2104\n",
      "Epoch 792: val_loss did not improve from 0.25807\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2589 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2589 - mape: 8.1971 - val_loss: 0.2583 - val_mse: 0.2572 - val_rmse: 0.5071 - val_mae: 0.2583 - val_mape: 8.1884 - lr: 1.0000e-05\n",
      "Epoch 793/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2584 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2584 - mape: 8.1846\n",
      "Epoch 793: val_loss did not improve from 0.25807\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2586 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2586 - mape: 8.1886 - val_loss: 0.2587 - val_mse: 0.2629 - val_rmse: 0.5128 - val_mae: 0.2587 - val_mape: 8.2595 - lr: 1.0000e-05\n",
      "Epoch 794/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2584 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2584 - mape: 8.1800\n",
      "Epoch 794: val_loss did not improve from 0.25807\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2588 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2588 - mape: 8.2096 - val_loss: 0.2594 - val_mse: 0.2548 - val_rmse: 0.5048 - val_mae: 0.2594 - val_mape: 8.1547 - lr: 1.0000e-05\n",
      "Epoch 795/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2596 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2596 - mape: 8.2227\n",
      "Epoch 795: val_loss improved from 0.25807 to 0.25804, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2589 - mse: 0.2597 - rmse: 0.5097 - mae: 0.2589 - mape: 8.2065 - val_loss: 0.2580 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2580 - val_mape: 8.1455 - lr: 1.0000e-05\n",
      "Epoch 796/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2584 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2584 - mape: 8.1915\n",
      "Epoch 796: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2585 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2585 - mape: 8.1907 - val_loss: 0.2581 - val_mse: 0.2573 - val_rmse: 0.5073 - val_mae: 0.2581 - val_mape: 8.1605 - lr: 1.0000e-05\n",
      "Epoch 797/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2587 - mse: 0.2566 - rmse: 0.5065 - mae: 0.2587 - mape: 8.2093\n",
      "Epoch 797: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2590 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2590 - mape: 8.2030 - val_loss: 0.2583 - val_mse: 0.2562 - val_rmse: 0.5062 - val_mae: 0.2583 - val_mape: 8.1495 - lr: 1.0000e-05\n",
      "Epoch 798/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2587 - mse: 0.2561 - rmse: 0.5061 - mae: 0.2587 - mape: 8.1902\n",
      "Epoch 798: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2587 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2587 - mape: 8.1991 - val_loss: 0.2590 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2590 - val_mape: 8.1319 - lr: 1.0000e-05\n",
      "Epoch 799/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2595 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2595 - mape: 8.2170\n",
      "Epoch 799: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2594 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2594 - mape: 8.2105 - val_loss: 0.2589 - val_mse: 0.2600 - val_rmse: 0.5099 - val_mae: 0.2589 - val_mape: 8.1684 - lr: 1.0000e-05\n",
      "Epoch 800/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2585 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2585 - mape: 8.1899\n",
      "Epoch 800: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2589 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2589 - mape: 8.2025 - val_loss: 0.2598 - val_mse: 0.2632 - val_rmse: 0.5130 - val_mae: 0.2598 - val_mape: 8.2066 - lr: 1.0000e-05\n",
      "Epoch 801/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2592 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2592 - mape: 8.2051\n",
      "Epoch 801: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2586 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2586 - mape: 8.1960 - val_loss: 0.2582 - val_mse: 0.2602 - val_rmse: 0.5101 - val_mae: 0.2582 - val_mape: 8.1854 - lr: 1.0000e-05\n",
      "Epoch 802/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2578 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2578 - mape: 8.1578\n",
      "Epoch 802: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2587 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2587 - mape: 8.2010 - val_loss: 0.2583 - val_mse: 0.2557 - val_rmse: 0.5057 - val_mae: 0.2583 - val_mape: 8.1520 - lr: 1.0000e-05\n",
      "Epoch 803/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2601 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2601 - mape: 8.2401\n",
      "Epoch 803: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2585 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2585 - mape: 8.1838 - val_loss: 0.2581 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2581 - val_mape: 8.2042 - lr: 1.0000e-05\n",
      "Epoch 804/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2584 - mse: 0.2565 - rmse: 0.5065 - mae: 0.2584 - mape: 8.1955\n",
      "Epoch 804: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2584 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2584 - mape: 8.1930 - val_loss: 0.2601 - val_mse: 0.2607 - val_rmse: 0.5106 - val_mae: 0.2601 - val_mape: 8.1758 - lr: 1.0000e-05\n",
      "Epoch 805/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2587 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2587 - mape: 8.1900\n",
      "Epoch 805: val_loss did not improve from 0.25804\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2586 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2586 - mape: 8.1916 - val_loss: 0.2590 - val_mse: 0.2544 - val_rmse: 0.5043 - val_mae: 0.2590 - val_mape: 8.1600 - lr: 1.0000e-05\n",
      "Epoch 806/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2587 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2587 - mape: 8.1932\n",
      "Epoch 806: val_loss improved from 0.25804 to 0.25787, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2588 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2588 - mape: 8.2007 - val_loss: 0.2579 - val_mse: 0.2576 - val_rmse: 0.5076 - val_mae: 0.2579 - val_mape: 8.1803 - lr: 1.0000e-05\n",
      "Epoch 807/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2595 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2595 - mape: 8.2368\n",
      "Epoch 807: val_loss did not improve from 0.25787\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2585 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2585 - mape: 8.1981 - val_loss: 0.2588 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2588 - val_mape: 8.2513 - lr: 1.0000e-05\n",
      "Epoch 808/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2587 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2587 - mape: 8.2105\n",
      "Epoch 808: val_loss did not improve from 0.25787\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2585 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2585 - mape: 8.2029 - val_loss: 0.2583 - val_mse: 0.2585 - val_rmse: 0.5084 - val_mae: 0.2583 - val_mape: 8.2243 - lr: 1.0000e-05\n",
      "Epoch 809/1000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2582 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2582 - mape: 8.1816\n",
      "Epoch 809: val_loss did not improve from 0.25787\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2588 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2588 - mape: 8.1985 - val_loss: 0.2582 - val_mse: 0.2556 - val_rmse: 0.5056 - val_mae: 0.2582 - val_mape: 8.1287 - lr: 1.0000e-05\n",
      "Epoch 810/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2585 - mse: 0.2553 - rmse: 0.5053 - mae: 0.2585 - mape: 8.2040\n",
      "Epoch 810: val_loss improved from 0.25787 to 0.25776, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2587 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2587 - mape: 8.1907 - val_loss: 0.2578 - val_mse: 0.2567 - val_rmse: 0.5067 - val_mae: 0.2578 - val_mape: 8.1616 - lr: 1.0000e-05\n",
      "Epoch 811/1000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2597 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2597 - mape: 8.2307\n",
      "Epoch 811: val_loss did not improve from 0.25776\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2588 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2588 - mape: 8.2020 - val_loss: 0.2580 - val_mse: 0.2556 - val_rmse: 0.5056 - val_mae: 0.2580 - val_mape: 8.1402 - lr: 1.0000e-05\n",
      "Epoch 812/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2587 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2587 - mape: 8.1968\n",
      "Epoch 812: val_loss improved from 0.25776 to 0.25774, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2587 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2587 - mape: 8.1937 - val_loss: 0.2577 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2577 - val_mape: 8.1726 - lr: 1.0000e-05\n",
      "Epoch 813/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2588 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2588 - mape: 8.2152\n",
      "Epoch 813: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2586 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2586 - mape: 8.2067 - val_loss: 0.2582 - val_mse: 0.2565 - val_rmse: 0.5064 - val_mae: 0.2582 - val_mape: 8.1409 - lr: 1.0000e-05\n",
      "Epoch 814/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2586 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2586 - mape: 8.2005\n",
      "Epoch 814: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2586 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2586 - mape: 8.1956 - val_loss: 0.2582 - val_mse: 0.2576 - val_rmse: 0.5076 - val_mae: 0.2582 - val_mape: 8.1364 - lr: 1.0000e-05\n",
      "Epoch 815/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2588 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2588 - mape: 8.1893\n",
      "Epoch 815: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2587 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2587 - mape: 8.1936 - val_loss: 0.2578 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2578 - val_mape: 8.1810 - lr: 1.0000e-05\n",
      "Epoch 816/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2583 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2583 - mape: 8.1873\n",
      "Epoch 816: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2587 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2587 - mape: 8.1987 - val_loss: 0.2582 - val_mse: 0.2553 - val_rmse: 0.5053 - val_mae: 0.2582 - val_mape: 8.1344 - lr: 1.0000e-05\n",
      "Epoch 817/1000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2596 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2596 - mape: 8.2016\n",
      "Epoch 817: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2584 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2584 - mape: 8.1892 - val_loss: 0.2592 - val_mse: 0.2638 - val_rmse: 0.5137 - val_mae: 0.2592 - val_mape: 8.2661 - lr: 1.0000e-05\n",
      "Epoch 818/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2583 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2583 - mape: 8.1693\n",
      "Epoch 818: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2587 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2587 - mape: 8.1932 - val_loss: 0.2583 - val_mse: 0.2553 - val_rmse: 0.5053 - val_mae: 0.2583 - val_mape: 8.1473 - lr: 1.0000e-05\n",
      "Epoch 819/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2594 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2594 - mape: 8.1986\n",
      "Epoch 819: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2586 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2586 - mape: 8.1968 - val_loss: 0.2591 - val_mse: 0.2634 - val_rmse: 0.5132 - val_mae: 0.2591 - val_mape: 8.2415 - lr: 1.0000e-05\n",
      "Epoch 820/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2576 - mse: 0.2562 - rmse: 0.5062 - mae: 0.2576 - mape: 8.1683\n",
      "Epoch 820: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2586 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2586 - mape: 8.2019 - val_loss: 0.2589 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2589 - val_mape: 8.1383 - lr: 1.0000e-05\n",
      "Epoch 821/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2576 - mse: 0.2554 - rmse: 0.5054 - mae: 0.2576 - mape: 8.1467\n",
      "Epoch 821: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2586 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2586 - mape: 8.1895 - val_loss: 0.2578 - val_mse: 0.2556 - val_rmse: 0.5056 - val_mae: 0.2578 - val_mape: 8.1437 - lr: 1.0000e-05\n",
      "Epoch 822/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2579 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2579 - mape: 8.1613\n",
      "Epoch 822: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2583 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2583 - mape: 8.1886 - val_loss: 0.2579 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2579 - val_mape: 8.1927 - lr: 1.0000e-05\n",
      "Epoch 823/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2583 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2583 - mape: 8.1788\n",
      "Epoch 823: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2582 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2582 - mape: 8.1788 - val_loss: 0.2585 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2585 - val_mape: 8.1471 - lr: 1.0000e-05\n",
      "Epoch 824/1000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2555 - mse: 0.2516 - rmse: 0.5016 - mae: 0.2555 - mape: 8.0522\n",
      "Epoch 824: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2582 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2582 - mape: 8.1853 - val_loss: 0.2587 - val_mse: 0.2570 - val_rmse: 0.5069 - val_mae: 0.2587 - val_mape: 8.1196 - lr: 1.0000e-05\n",
      "Epoch 825/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2591 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2591 - mape: 8.2207\n",
      "Epoch 825: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2584 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2584 - mape: 8.1880 - val_loss: 0.2582 - val_mse: 0.2570 - val_rmse: 0.5070 - val_mae: 0.2582 - val_mape: 8.1847 - lr: 1.0000e-05\n",
      "Epoch 826/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2584 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2584 - mape: 8.1819\n",
      "Epoch 826: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2583 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2583 - mape: 8.1783 - val_loss: 0.2579 - val_mse: 0.2569 - val_rmse: 0.5068 - val_mae: 0.2579 - val_mape: 8.2003 - lr: 1.0000e-05\n",
      "Epoch 827/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2582 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2582 - mape: 8.1861\n",
      "Epoch 827: val_loss did not improve from 0.25774\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2584 - mse: 0.2590 - rmse: 0.5090 - mae: 0.2584 - mape: 8.1846 - val_loss: 0.2587 - val_mse: 0.2560 - val_rmse: 0.5059 - val_mae: 0.2587 - val_mape: 8.1875 - lr: 1.0000e-05\n",
      "Epoch 828/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2572 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2572 - mape: 8.1635\n",
      "Epoch 828: val_loss improved from 0.25774 to 0.25769, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2583 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2583 - mape: 8.1919 - val_loss: 0.2577 - val_mse: 0.2565 - val_rmse: 0.5064 - val_mae: 0.2577 - val_mape: 8.1774 - lr: 1.0000e-05\n",
      "Epoch 829/1000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2593 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2593 - mape: 8.2260\n",
      "Epoch 829: val_loss did not improve from 0.25769\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2582 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2582 - mape: 8.1841 - val_loss: 0.2580 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2580 - val_mape: 8.1363 - lr: 1.0000e-05\n",
      "Epoch 830/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2587 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2587 - mape: 8.2067\n",
      "Epoch 830: val_loss did not improve from 0.25769\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2583 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2583 - mape: 8.1904 - val_loss: 0.2585 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2585 - val_mape: 8.1434 - lr: 1.0000e-05\n",
      "Epoch 831/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2584 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2584 - mape: 8.1884\n",
      "Epoch 831: val_loss did not improve from 0.25769\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2583 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2583 - mape: 8.1808 - val_loss: 0.2577 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2577 - val_mape: 8.1891 - lr: 1.0000e-05\n",
      "Epoch 832/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2588 - mse: 0.2598 - rmse: 0.5098 - mae: 0.2588 - mape: 8.2010\n",
      "Epoch 832: val_loss did not improve from 0.25769\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2584 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2584 - mape: 8.1891 - val_loss: 0.2577 - val_mse: 0.2577 - val_rmse: 0.5077 - val_mae: 0.2577 - val_mape: 8.1761 - lr: 1.0000e-05\n",
      "Epoch 833/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2586 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2586 - mape: 8.1887\n",
      "Epoch 833: val_loss did not improve from 0.25769\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2583 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2583 - mape: 8.1877 - val_loss: 0.2585 - val_mse: 0.2552 - val_rmse: 0.5052 - val_mae: 0.2585 - val_mape: 8.1578 - lr: 1.0000e-05\n",
      "Epoch 834/1000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2586 - mse: 0.2589 - rmse: 0.5089 - mae: 0.2586 - mape: 8.1938\n",
      "Epoch 834: val_loss did not improve from 0.25769\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2582 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2582 - mape: 8.1848 - val_loss: 0.2579 - val_mse: 0.2566 - val_rmse: 0.5065 - val_mae: 0.2579 - val_mape: 8.1991 - lr: 1.0000e-05\n",
      "Epoch 835/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2579 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2579 - mape: 8.1734\n",
      "Epoch 835: val_loss did not improve from 0.25769\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2585 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2585 - mape: 8.1819 - val_loss: 0.2583 - val_mse: 0.2614 - val_rmse: 0.5112 - val_mae: 0.2583 - val_mape: 8.2496 - lr: 1.0000e-05\n",
      "Epoch 836/1000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2582 - mse: 0.2591 - rmse: 0.5091 - mae: 0.2582 - mape: 8.1834\n",
      "Epoch 836: val_loss did not improve from 0.25769\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2582 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2582 - mape: 8.1822 - val_loss: 0.2579 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2579 - val_mape: 8.1569 - lr: 1.0000e-05\n",
      "Epoch 837/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2571 - mse: 0.2555 - rmse: 0.5054 - mae: 0.2571 - mape: 8.1658\n",
      "Epoch 837: val_loss did not improve from 0.25769\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2585 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2585 - mape: 8.1814 - val_loss: 0.2579 - val_mse: 0.2602 - val_rmse: 0.5101 - val_mae: 0.2579 - val_mape: 8.1876 - lr: 1.0000e-05\n",
      "Epoch 838/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2586 - mse: 0.2571 - rmse: 0.5071 - mae: 0.2586 - mape: 8.1795\n",
      "Epoch 838: val_loss did not improve from 0.25769\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2566 - rmse: 0.5066 - mae: 0.2579 - mape: 8.1648 - val_loss: 0.2599 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2599 - val_mape: 8.2941 - lr: 1.0000e-05\n",
      "Epoch 839/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2586 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2586 - mape: 8.1921\n",
      "Epoch 839: val_loss did not improve from 0.25769\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2580 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2580 - mape: 8.1695 - val_loss: 0.2585 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2585 - val_mape: 8.2525 - lr: 1.0000e-05\n",
      "Epoch 840/1000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2593 - mse: 0.2593 - rmse: 0.5093 - mae: 0.2593 - mape: 8.2053\n",
      "Epoch 840: val_loss improved from 0.25769 to 0.25757, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2584 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2584 - mape: 8.1991 - val_loss: 0.2576 - val_mse: 0.2562 - val_rmse: 0.5062 - val_mae: 0.2576 - val_mape: 8.1790 - lr: 1.0000e-05\n",
      "Epoch 841/1000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2587 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2587 - mape: 8.1996\n",
      "Epoch 841: val_loss did not improve from 0.25757\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2580 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2580 - mape: 8.1869 - val_loss: 0.2586 - val_mse: 0.2535 - val_rmse: 0.5034 - val_mae: 0.2586 - val_mape: 8.1579 - lr: 1.0000e-05\n",
      "Epoch 842/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2582 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2582 - mape: 8.1829\n",
      "Epoch 842: val_loss did not improve from 0.25757\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2585 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2585 - mape: 8.1880 - val_loss: 0.2580 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2580 - val_mape: 8.2259 - lr: 1.0000e-05\n",
      "Epoch 843/1000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2588 - mse: 0.2590 - rmse: 0.5090 - mae: 0.2588 - mape: 8.2056\n",
      "Epoch 843: val_loss did not improve from 0.25757\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2583 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2583 - mape: 8.1937 - val_loss: 0.2577 - val_mse: 0.2560 - val_rmse: 0.5060 - val_mae: 0.2577 - val_mape: 8.1506 - lr: 1.0000e-05\n",
      "Epoch 844/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2590 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2590 - mape: 8.2078\n",
      "Epoch 844: val_loss did not improve from 0.25757\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2584 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2584 - mape: 8.1946 - val_loss: 0.2576 - val_mse: 0.2567 - val_rmse: 0.5067 - val_mae: 0.2576 - val_mape: 8.1330 - lr: 1.0000e-05\n",
      "Epoch 845/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2593 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2593 - mape: 8.2216\n",
      "Epoch 845: val_loss did not improve from 0.25757\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2588 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2588 - mape: 8.2007 - val_loss: 0.2579 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2579 - val_mape: 8.2066 - lr: 1.0000e-05\n",
      "Epoch 846/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2593 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2593 - mape: 8.2360\n",
      "Epoch 846: val_loss improved from 0.25757 to 0.25756, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2587 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2587 - mape: 8.1989 - val_loss: 0.2576 - val_mse: 0.2557 - val_rmse: 0.5056 - val_mae: 0.2576 - val_mape: 8.1306 - lr: 1.0000e-05\n",
      "Epoch 847/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2591 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2591 - mape: 8.2240\n",
      "Epoch 847: val_loss did not improve from 0.25756\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2582 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2582 - mape: 8.1914 - val_loss: 0.2577 - val_mse: 0.2562 - val_rmse: 0.5062 - val_mae: 0.2577 - val_mape: 8.1592 - lr: 1.0000e-05\n",
      "Epoch 848/1000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2596 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2596 - mape: 8.2391\n",
      "Epoch 848: val_loss did not improve from 0.25756\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2582 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2582 - mape: 8.1781 - val_loss: 0.2579 - val_mse: 0.2614 - val_rmse: 0.5113 - val_mae: 0.2579 - val_mape: 8.2337 - lr: 1.0000e-05\n",
      "Epoch 849/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2587 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2587 - mape: 8.2029\n",
      "Epoch 849: val_loss did not improve from 0.25756\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2585 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2585 - mape: 8.1979 - val_loss: 0.2578 - val_mse: 0.2565 - val_rmse: 0.5065 - val_mae: 0.2578 - val_mape: 8.1317 - lr: 1.0000e-05\n",
      "Epoch 850/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2569 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2569 - mape: 8.1579\n",
      "Epoch 850: val_loss improved from 0.25756 to 0.25743, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2581 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2581 - mape: 8.1912 - val_loss: 0.2574 - val_mse: 0.2565 - val_rmse: 0.5064 - val_mae: 0.2574 - val_mape: 8.1445 - lr: 1.0000e-05\n",
      "Epoch 851/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2575 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2575 - mape: 8.1601\n",
      "Epoch 851: val_loss improved from 0.25743 to 0.25739, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2582 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2582 - mape: 8.1863 - val_loss: 0.2574 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2574 - val_mape: 8.1594 - lr: 1.0000e-05\n",
      "Epoch 852/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2579 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2579 - mape: 8.1586\n",
      "Epoch 852: val_loss did not improve from 0.25739\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2582 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2582 - mape: 8.1723 - val_loss: 0.2574 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2574 - val_mape: 8.1609 - lr: 1.0000e-05\n",
      "Epoch 853/1000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2600 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2600 - mape: 8.2276\n",
      "Epoch 853: val_loss did not improve from 0.25739\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2582 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2582 - mape: 8.1822 - val_loss: 0.2575 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2575 - val_mape: 8.1677 - lr: 1.0000e-05\n",
      "Epoch 854/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2578 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2578 - mape: 8.1862\n",
      "Epoch 854: val_loss did not improve from 0.25739\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2581 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2581 - mape: 8.1853 - val_loss: 0.2576 - val_mse: 0.2548 - val_rmse: 0.5048 - val_mae: 0.2576 - val_mape: 8.1665 - lr: 1.0000e-05\n",
      "Epoch 855/1000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2599 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2599 - mape: 8.2531\n",
      "Epoch 855: val_loss did not improve from 0.25739\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2582 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2582 - mape: 8.1847 - val_loss: 0.2574 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2574 - val_mape: 8.1819 - lr: 1.0000e-05\n",
      "Epoch 856/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2577 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2577 - mape: 8.1702\n",
      "Epoch 856: val_loss did not improve from 0.25739\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2577 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2577 - mape: 8.1705 - val_loss: 0.2577 - val_mse: 0.2553 - val_rmse: 0.5053 - val_mae: 0.2577 - val_mape: 8.1931 - lr: 1.0000e-05\n",
      "Epoch 857/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2586 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2586 - mape: 8.1960\n",
      "Epoch 857: val_loss did not improve from 0.25739\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2585 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2585 - mape: 8.1936 - val_loss: 0.2583 - val_mse: 0.2553 - val_rmse: 0.5053 - val_mae: 0.2583 - val_mape: 8.2032 - lr: 1.0000e-05\n",
      "Epoch 858/1000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.2599 - mse: 0.2596 - rmse: 0.5095 - mae: 0.2599 - mape: 8.2162\n",
      "Epoch 858: val_loss did not improve from 0.25739\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2584 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2584 - mape: 8.1934 - val_loss: 0.2578 - val_mse: 0.2559 - val_rmse: 0.5059 - val_mae: 0.2578 - val_mape: 8.1911 - lr: 1.0000e-05\n",
      "Epoch 859/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2581 - mse: 0.2570 - rmse: 0.5070 - mae: 0.2581 - mape: 8.1683\n",
      "Epoch 859: val_loss improved from 0.25739 to 0.25735, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2580 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2580 - mape: 8.1724 - val_loss: 0.2573 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2573 - val_mape: 8.1763 - lr: 1.0000e-05\n",
      "Epoch 860/1000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.2561 - mse: 0.2562 - rmse: 0.5061 - mae: 0.2561 - mape: 8.0893\n",
      "Epoch 860: val_loss did not improve from 0.25735\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2583 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2583 - mape: 8.1844 - val_loss: 0.2577 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2577 - val_mape: 8.1733 - lr: 1.0000e-05\n",
      "Epoch 861/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2584 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2584 - mape: 8.2028\n",
      "Epoch 861: val_loss did not improve from 0.25735\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2577 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2577 - mape: 8.1787 - val_loss: 0.2579 - val_mse: 0.2599 - val_rmse: 0.5098 - val_mae: 0.2579 - val_mape: 8.1827 - lr: 1.0000e-05\n",
      "Epoch 862/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2573 - mse: 0.2561 - rmse: 0.5060 - mae: 0.2573 - mape: 8.1652\n",
      "Epoch 862: val_loss did not improve from 0.25735\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2578 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2578 - mape: 8.1781 - val_loss: 0.2577 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2577 - val_mape: 8.1440 - lr: 1.0000e-05\n",
      "Epoch 863/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2586 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2586 - mape: 8.2049\n",
      "Epoch 863: val_loss did not improve from 0.25735\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2580 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2580 - mape: 8.1916 - val_loss: 0.2581 - val_mse: 0.2564 - val_rmse: 0.5063 - val_mae: 0.2581 - val_mape: 8.1528 - lr: 1.0000e-05\n",
      "Epoch 864/1000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2596 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2596 - mape: 8.2261\n",
      "Epoch 864: val_loss did not improve from 0.25735\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2583 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2583 - mape: 8.1749 - val_loss: 0.2577 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2577 - val_mape: 8.2223 - lr: 1.0000e-05\n",
      "Epoch 865/1000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2598 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2598 - mape: 8.2431\n",
      "Epoch 865: val_loss improved from 0.25735 to 0.25733, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2581 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2581 - mape: 8.1812 - val_loss: 0.2573 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2573 - val_mape: 8.1911 - lr: 1.0000e-05\n",
      "Epoch 866/1000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2559 - mse: 0.2543 - rmse: 0.5043 - mae: 0.2559 - mape: 8.1167\n",
      "Epoch 866: val_loss did not improve from 0.25733\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2579 - mape: 8.1849 - val_loss: 0.2586 - val_mse: 0.2553 - val_rmse: 0.5053 - val_mae: 0.2586 - val_mape: 8.1850 - lr: 1.0000e-05\n",
      "Epoch 867/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2583 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2583 - mape: 8.1878\n",
      "Epoch 867: val_loss did not improve from 0.25733\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2582 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2582 - mape: 8.1812 - val_loss: 0.2579 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2579 - val_mape: 8.2029 - lr: 1.0000e-05\n",
      "Epoch 868/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2581 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2581 - mape: 8.1766\n",
      "Epoch 868: val_loss did not improve from 0.25733\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2581 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2581 - mape: 8.1766 - val_loss: 0.2577 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2577 - val_mape: 8.1342 - lr: 1.0000e-05\n",
      "Epoch 869/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2582 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2582 - mape: 8.1760\n",
      "Epoch 869: val_loss did not improve from 0.25733\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2582 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2582 - mape: 8.1710 - val_loss: 0.2575 - val_mse: 0.2584 - val_rmse: 0.5083 - val_mae: 0.2575 - val_mape: 8.1524 - lr: 1.0000e-05\n",
      "Epoch 870/1000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2574 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2574 - mape: 8.1679\n",
      "Epoch 870: val_loss improved from 0.25733 to 0.25717, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2582 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2582 - mape: 8.1914 - val_loss: 0.2572 - val_mse: 0.2565 - val_rmse: 0.5065 - val_mae: 0.2572 - val_mape: 8.1542 - lr: 1.0000e-05\n",
      "Epoch 871/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2578 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2578 - mape: 8.1483\n",
      "Epoch 871: val_loss did not improve from 0.25717\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2578 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2578 - mape: 8.1676 - val_loss: 0.2578 - val_mse: 0.2576 - val_rmse: 0.5076 - val_mae: 0.2578 - val_mape: 8.1915 - lr: 1.0000e-05\n",
      "Epoch 872/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2584 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2584 - mape: 8.1825\n",
      "Epoch 872: val_loss did not improve from 0.25717\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2579 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2579 - mape: 8.1720 - val_loss: 0.2580 - val_mse: 0.2625 - val_rmse: 0.5123 - val_mae: 0.2580 - val_mape: 8.2452 - lr: 1.0000e-05\n",
      "Epoch 873/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2585 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2585 - mape: 8.1804\n",
      "Epoch 873: val_loss did not improve from 0.25717\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2584 - mse: 0.2585 - rmse: 0.5085 - mae: 0.2584 - mape: 8.1766 - val_loss: 0.2572 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2572 - val_mape: 8.1720 - lr: 1.0000e-05\n",
      "Epoch 874/1000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2589 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2589 - mape: 8.2117\n",
      "Epoch 874: val_loss did not improve from 0.25717\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2579 - mape: 8.1691 - val_loss: 0.2574 - val_mse: 0.2566 - val_rmse: 0.5066 - val_mae: 0.2574 - val_mape: 8.1618 - lr: 1.0000e-05\n",
      "Epoch 875/1000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2567 - mse: 0.2549 - rmse: 0.5049 - mae: 0.2567 - mape: 8.1296\n",
      "Epoch 875: val_loss did not improve from 0.25717\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2579 - mape: 8.1759 - val_loss: 0.2583 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2583 - val_mape: 8.1371 - lr: 1.0000e-05\n",
      "Epoch 876/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2577 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2577 - mape: 8.1824\n",
      "Epoch 876: val_loss did not improve from 0.25717\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2580 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2580 - mape: 8.1796 - val_loss: 0.2580 - val_mse: 0.2538 - val_rmse: 0.5038 - val_mae: 0.2580 - val_mape: 8.1131 - lr: 1.0000e-05\n",
      "Epoch 877/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2581 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2581 - mape: 8.1630\n",
      "Epoch 877: val_loss did not improve from 0.25717\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2581 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2581 - mape: 8.1630 - val_loss: 0.2581 - val_mse: 0.2589 - val_rmse: 0.5089 - val_mae: 0.2581 - val_mape: 8.2446 - lr: 1.0000e-05\n",
      "Epoch 878/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2581 - mse: 0.2590 - rmse: 0.5090 - mae: 0.2581 - mape: 8.1828\n",
      "Epoch 878: val_loss did not improve from 0.25717\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2580 - mse: 0.2585 - rmse: 0.5085 - mae: 0.2580 - mape: 8.1794 - val_loss: 0.2575 - val_mse: 0.2565 - val_rmse: 0.5064 - val_mae: 0.2575 - val_mape: 8.1410 - lr: 1.0000e-05\n",
      "Epoch 879/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2579 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2579 - mape: 8.1529\n",
      "Epoch 879: val_loss did not improve from 0.25717\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2579 - mape: 8.1529 - val_loss: 0.2575 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2575 - val_mape: 8.2151 - lr: 1.0000e-05\n",
      "Epoch 880/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2576 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2576 - mape: 8.1704\n",
      "Epoch 880: val_loss improved from 0.25717 to 0.25716, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2585 - rmse: 0.5085 - mae: 0.2579 - mape: 8.1754 - val_loss: 0.2572 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2572 - val_mape: 8.1641 - lr: 1.0000e-05\n",
      "Epoch 881/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2572 - mse: 0.2583 - rmse: 0.5083 - mae: 0.2572 - mape: 8.1719\n",
      "Epoch 881: val_loss did not improve from 0.25716\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2580 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2580 - mape: 8.1825 - val_loss: 0.2575 - val_mse: 0.2604 - val_rmse: 0.5103 - val_mae: 0.2575 - val_mape: 8.2146 - lr: 1.0000e-05\n",
      "Epoch 882/1000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2597 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2597 - mape: 8.2342\n",
      "Epoch 882: val_loss did not improve from 0.25716\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2579 - mape: 8.1787 - val_loss: 0.2580 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2580 - val_mape: 8.2250 - lr: 1.0000e-05\n",
      "Epoch 883/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2572 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2572 - mape: 8.1638\n",
      "Epoch 883: val_loss did not improve from 0.25716\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2580 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2580 - mape: 8.1751 - val_loss: 0.2574 - val_mse: 0.2574 - val_rmse: 0.5073 - val_mae: 0.2574 - val_mape: 8.1861 - lr: 1.0000e-05\n",
      "Epoch 884/1000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2582 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2582 - mape: 8.1958\n",
      "Epoch 884: val_loss did not improve from 0.25716\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2577 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2577 - mape: 8.1666 - val_loss: 0.2589 - val_mse: 0.2532 - val_rmse: 0.5032 - val_mae: 0.2589 - val_mape: 8.1447 - lr: 1.0000e-05\n",
      "Epoch 885/1000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2581 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2581 - mape: 8.1736\n",
      "Epoch 885: val_loss did not improve from 0.25716\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2578 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2578 - mape: 8.1630 - val_loss: 0.2582 - val_mse: 0.2578 - val_rmse: 0.5078 - val_mae: 0.2582 - val_mape: 8.2291 - lr: 1.0000e-05\n",
      "Epoch 886/1000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2579 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2579 - mape: 8.1647\n",
      "Epoch 886: val_loss did not improve from 0.25716\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2579 - mape: 8.1802 - val_loss: 0.2576 - val_mse: 0.2575 - val_rmse: 0.5075 - val_mae: 0.2576 - val_mape: 8.1282 - lr: 1.0000e-05\n",
      "Epoch 887/1000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2587 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2587 - mape: 8.2047\n",
      "Epoch 887: val_loss did not improve from 0.25716\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2581 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2581 - mape: 8.1877 - val_loss: 0.2575 - val_mse: 0.2560 - val_rmse: 0.5060 - val_mae: 0.2575 - val_mape: 8.1355 - lr: 1.0000e-05\n",
      "Epoch 888/1000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2592 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2592 - mape: 8.2072\n",
      "Epoch 888: val_loss did not improve from 0.25716\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2588 - rmse: 0.5088 - mae: 0.2579 - mape: 8.1784 - val_loss: 0.2575 - val_mse: 0.2545 - val_rmse: 0.5045 - val_mae: 0.2575 - val_mape: 8.1317 - lr: 1.0000e-05\n",
      "Epoch 889/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2562 - mse: 0.2542 - rmse: 0.5042 - mae: 0.2562 - mape: 8.1119\n",
      "Epoch 889: val_loss did not improve from 0.25716\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2578 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2578 - mape: 8.1716 - val_loss: 0.2580 - val_mse: 0.2542 - val_rmse: 0.5042 - val_mae: 0.2580 - val_mape: 8.1371 - lr: 1.0000e-05\n",
      "Epoch 890/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2579 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2579 - mape: 8.1658\n",
      "Epoch 890: val_loss did not improve from 0.25716\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2579 - mape: 8.1809 - val_loss: 0.2584 - val_mse: 0.2552 - val_rmse: 0.5051 - val_mae: 0.2584 - val_mape: 8.1687 - lr: 1.0000e-05\n",
      "Epoch 891/1000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2567 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2567 - mape: 8.1378\n",
      "Epoch 891: val_loss did not improve from 0.25716\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2576 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2576 - mape: 8.1658 - val_loss: 0.2583 - val_mse: 0.2634 - val_rmse: 0.5132 - val_mae: 0.2583 - val_mape: 8.2748 - lr: 1.0000e-05\n",
      "Epoch 892/1000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2580 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2580 - mape: 8.1881\n",
      "Epoch 892: val_loss did not improve from 0.25716\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2579 - mape: 8.1814 - val_loss: 0.2576 - val_mse: 0.2617 - val_rmse: 0.5116 - val_mae: 0.2576 - val_mape: 8.2060 - lr: 1.0000e-05\n",
      "Epoch 893/1000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2572 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2572 - mape: 8.1634\n",
      "Epoch 893: val_loss did not improve from 0.25716\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2579 - mape: 8.1728 - val_loss: 0.2575 - val_mse: 0.2552 - val_rmse: 0.5052 - val_mae: 0.2575 - val_mape: 8.1538 - lr: 1.0000e-05\n",
      "Epoch 894/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2586 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2586 - mape: 8.1899\n",
      "Epoch 894: val_loss improved from 0.25716 to 0.25706, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2577 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2577 - mape: 8.1638 - val_loss: 0.2571 - val_mse: 0.2590 - val_rmse: 0.5089 - val_mae: 0.2571 - val_mape: 8.1789 - lr: 1.0000e-05\n",
      "Epoch 895/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2575 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2575 - mape: 8.1544\n",
      "Epoch 895: val_loss did not improve from 0.25706\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2577 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2577 - mape: 8.1667 - val_loss: 0.2573 - val_mse: 0.2574 - val_rmse: 0.5074 - val_mae: 0.2573 - val_mape: 8.1721 - lr: 1.0000e-05\n",
      "Epoch 896/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2569 - mse: 0.2561 - rmse: 0.5060 - mae: 0.2569 - mape: 8.1297\n",
      "Epoch 896: val_loss did not improve from 0.25706\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2576 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2576 - mape: 8.1641 - val_loss: 0.2576 - val_mse: 0.2599 - val_rmse: 0.5098 - val_mae: 0.2576 - val_mape: 8.1973 - lr: 1.0000e-05\n",
      "Epoch 897/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2573 - mse: 0.2570 - rmse: 0.5070 - mae: 0.2573 - mape: 8.1618\n",
      "Epoch 897: val_loss did not improve from 0.25706\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2577 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2577 - mape: 8.1612 - val_loss: 0.2572 - val_mse: 0.2556 - val_rmse: 0.5056 - val_mae: 0.2572 - val_mape: 8.1577 - lr: 1.0000e-05\n",
      "Epoch 898/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2562 - mse: 0.2555 - rmse: 0.5054 - mae: 0.2562 - mape: 8.1310\n",
      "Epoch 898: val_loss did not improve from 0.25706\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2577 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2577 - mape: 8.1774 - val_loss: 0.2574 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2574 - val_mape: 8.1209 - lr: 1.0000e-05\n",
      "Epoch 899/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2585 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2585 - mape: 8.1944\n",
      "Epoch 899: val_loss did not improve from 0.25706\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2579 - mape: 8.1696 - val_loss: 0.2571 - val_mse: 0.2579 - val_rmse: 0.5079 - val_mae: 0.2571 - val_mape: 8.1755 - lr: 1.0000e-05\n",
      "Epoch 900/1000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2565 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2565 - mape: 8.1275\n",
      "Epoch 900: val_loss improved from 0.25706 to 0.25701, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2578 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2578 - mape: 8.1744 - val_loss: 0.2570 - val_mse: 0.2562 - val_rmse: 0.5062 - val_mae: 0.2570 - val_mape: 8.1194 - lr: 1.0000e-05\n",
      "Epoch 901/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2567 - mse: 0.2558 - rmse: 0.5058 - mae: 0.2567 - mape: 8.1427\n",
      "Epoch 901: val_loss did not improve from 0.25701\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2580 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2580 - mape: 8.1797 - val_loss: 0.2571 - val_mse: 0.2584 - val_rmse: 0.5084 - val_mae: 0.2571 - val_mape: 8.1856 - lr: 1.0000e-05\n",
      "Epoch 902/1000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2564 - mse: 0.2545 - rmse: 0.5045 - mae: 0.2564 - mape: 8.1345\n",
      "Epoch 902: val_loss did not improve from 0.25701\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2578 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2578 - mape: 8.1785 - val_loss: 0.2570 - val_mse: 0.2548 - val_rmse: 0.5048 - val_mae: 0.2570 - val_mape: 8.1542 - lr: 1.0000e-05\n",
      "Epoch 903/1000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2570 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2570 - mape: 8.1548\n",
      "Epoch 903: val_loss did not improve from 0.25701\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2576 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2576 - mape: 8.1709 - val_loss: 0.2581 - val_mse: 0.2568 - val_rmse: 0.5067 - val_mae: 0.2581 - val_mape: 8.1206 - lr: 1.0000e-05\n",
      "Epoch 904/1000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2573 - mse: 0.2557 - rmse: 0.5056 - mae: 0.2573 - mape: 8.1569\n",
      "Epoch 904: val_loss improved from 0.25701 to 0.25701, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2578 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2578 - mape: 8.1669 - val_loss: 0.2570 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2570 - val_mape: 8.1410 - lr: 1.0000e-05\n",
      "Epoch 905/1000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2580 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2580 - mape: 8.1970\n",
      "Epoch 905: val_loss did not improve from 0.25701\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2577 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2577 - mape: 8.1709 - val_loss: 0.2571 - val_mse: 0.2587 - val_rmse: 0.5087 - val_mae: 0.2571 - val_mape: 8.1654 - lr: 1.0000e-05\n",
      "Epoch 906/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2569 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2569 - mape: 8.1679\n",
      "Epoch 906: val_loss did not improve from 0.25701\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2579 - mape: 8.1804 - val_loss: 0.2570 - val_mse: 0.2574 - val_rmse: 0.5074 - val_mae: 0.2570 - val_mape: 8.1437 - lr: 1.0000e-05\n",
      "Epoch 907/1000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2554 - mse: 0.2546 - rmse: 0.5046 - mae: 0.2554 - mape: 8.1262\n",
      "Epoch 907: val_loss improved from 0.25701 to 0.25692, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2576 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2576 - mape: 8.1730 - val_loss: 0.2569 - val_mse: 0.2556 - val_rmse: 0.5055 - val_mae: 0.2569 - val_mape: 8.1385 - lr: 1.0000e-05\n",
      "Epoch 908/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2593 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2593 - mape: 8.2098\n",
      "Epoch 908: val_loss did not improve from 0.25692\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2578 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2578 - mape: 8.1656 - val_loss: 0.2573 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2573 - val_mape: 8.2036 - lr: 1.0000e-05\n",
      "Epoch 909/1000\n",
      "271/318 [========================>.....] - ETA: 0s - loss: 0.2562 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2562 - mape: 8.1486\n",
      "Epoch 909: val_loss did not improve from 0.25692\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2577 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2577 - mape: 8.1642 - val_loss: 0.2569 - val_mse: 0.2566 - val_rmse: 0.5066 - val_mae: 0.2569 - val_mape: 8.1217 - lr: 1.0000e-05\n",
      "Epoch 910/1000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2576 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2576 - mape: 8.1606\n",
      "Epoch 910: val_loss improved from 0.25692 to 0.25688, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2579 - mape: 8.1712 - val_loss: 0.2569 - val_mse: 0.2583 - val_rmse: 0.5083 - val_mae: 0.2569 - val_mape: 8.1487 - lr: 1.0000e-05\n",
      "Epoch 911/1000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2574 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2574 - mape: 8.1828\n",
      "Epoch 911: val_loss did not improve from 0.25688\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2575 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2575 - mape: 8.1569 - val_loss: 0.2570 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2570 - val_mape: 8.1792 - lr: 1.0000e-05\n",
      "Epoch 912/1000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.2577 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2577 - mape: 8.1583\n",
      "Epoch 912: val_loss did not improve from 0.25688\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2578 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2578 - mape: 8.1806 - val_loss: 0.2571 - val_mse: 0.2540 - val_rmse: 0.5040 - val_mae: 0.2571 - val_mape: 8.1082 - lr: 1.0000e-05\n",
      "Epoch 913/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2574 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2574 - mape: 8.1450\n",
      "Epoch 913: val_loss did not improve from 0.25688\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2577 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2577 - mape: 8.1590 - val_loss: 0.2573 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2573 - val_mape: 8.2144 - lr: 1.0000e-05\n",
      "Epoch 914/1000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2581 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2581 - mape: 8.1842\n",
      "Epoch 914: val_loss improved from 0.25688 to 0.25680, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2577 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2577 - mape: 8.1656 - val_loss: 0.2568 - val_mse: 0.2559 - val_rmse: 0.5059 - val_mae: 0.2568 - val_mape: 8.1398 - lr: 1.0000e-05\n",
      "Epoch 915/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2571 - mse: 0.2553 - rmse: 0.5052 - mae: 0.2571 - mape: 8.1505\n",
      "Epoch 915: val_loss did not improve from 0.25680\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2576 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2576 - mape: 8.1778 - val_loss: 0.2579 - val_mse: 0.2540 - val_rmse: 0.5040 - val_mae: 0.2579 - val_mape: 8.1015 - lr: 1.0000e-05\n",
      "Epoch 916/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2589 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2589 - mape: 8.2014\n",
      "Epoch 916: val_loss did not improve from 0.25680\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2575 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2575 - mape: 8.1569 - val_loss: 0.2569 - val_mse: 0.2580 - val_rmse: 0.5080 - val_mae: 0.2569 - val_mape: 8.1404 - lr: 1.0000e-05\n",
      "Epoch 917/1000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2585 - mse: 0.2598 - rmse: 0.5098 - mae: 0.2585 - mape: 8.1680\n",
      "Epoch 917: val_loss improved from 0.25680 to 0.25677, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2576 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2576 - mape: 8.1619 - val_loss: 0.2568 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2568 - val_mape: 8.1530 - lr: 1.0000e-05\n",
      "Epoch 918/1000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.2595 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2595 - mape: 8.2039\n",
      "Epoch 918: val_loss did not improve from 0.25677\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2575 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2575 - mape: 8.1632 - val_loss: 0.2569 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2569 - val_mape: 8.1791 - lr: 1.0000e-05\n",
      "Epoch 919/1000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2581 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2581 - mape: 8.1765\n",
      "Epoch 919: val_loss did not improve from 0.25677\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2575 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2575 - mape: 8.1722 - val_loss: 0.2572 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2572 - val_mape: 8.1686 - lr: 1.0000e-05\n",
      "Epoch 920/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2581 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2581 - mape: 8.1805\n",
      "Epoch 920: val_loss did not improve from 0.25677\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2574 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2574 - mape: 8.1536 - val_loss: 0.2569 - val_mse: 0.2567 - val_rmse: 0.5067 - val_mae: 0.2569 - val_mape: 8.1595 - lr: 1.0000e-05\n",
      "Epoch 921/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2576 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2576 - mape: 8.1808\n",
      "Epoch 921: val_loss did not improve from 0.25677\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2573 - mse: 0.2558 - rmse: 0.5058 - mae: 0.2573 - mape: 8.1543 - val_loss: 0.2586 - val_mse: 0.2566 - val_rmse: 0.5066 - val_mae: 0.2586 - val_mape: 8.2281 - lr: 1.0000e-05\n",
      "Epoch 922/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2587 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2587 - mape: 8.2046\n",
      "Epoch 922: val_loss improved from 0.25677 to 0.25671, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2579 - mape: 8.1780 - val_loss: 0.2567 - val_mse: 0.2557 - val_rmse: 0.5057 - val_mae: 0.2567 - val_mape: 8.1571 - lr: 1.0000e-05\n",
      "Epoch 923/1000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2585 - mse: 0.2595 - rmse: 0.5095 - mae: 0.2585 - mape: 8.2071\n",
      "Epoch 923: val_loss did not improve from 0.25671\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2574 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2574 - mape: 8.1638 - val_loss: 0.2573 - val_mse: 0.2575 - val_rmse: 0.5074 - val_mae: 0.2573 - val_mape: 8.1890 - lr: 1.0000e-05\n",
      "Epoch 924/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2576 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2576 - mape: 8.1676\n",
      "Epoch 924: val_loss did not improve from 0.25671\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2576 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2576 - mape: 8.1667 - val_loss: 0.2569 - val_mse: 0.2555 - val_rmse: 0.5055 - val_mae: 0.2569 - val_mape: 8.1398 - lr: 1.0000e-05\n",
      "Epoch 925/1000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2585 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2585 - mape: 8.1898\n",
      "Epoch 925: val_loss did not improve from 0.25671\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2574 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2574 - mape: 8.1548 - val_loss: 0.2570 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2570 - val_mape: 8.1863 - lr: 1.0000e-05\n",
      "Epoch 926/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2585 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2585 - mape: 8.1960\n",
      "Epoch 926: val_loss did not improve from 0.25671\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2575 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2575 - mape: 8.1604 - val_loss: 0.2572 - val_mse: 0.2570 - val_rmse: 0.5070 - val_mae: 0.2572 - val_mape: 8.1622 - lr: 1.0000e-05\n",
      "Epoch 927/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2580 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2580 - mape: 8.2003\n",
      "Epoch 927: val_loss did not improve from 0.25671\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2573 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2573 - mape: 8.1616 - val_loss: 0.2571 - val_mse: 0.2542 - val_rmse: 0.5042 - val_mae: 0.2571 - val_mape: 8.1152 - lr: 1.0000e-05\n",
      "Epoch 928/1000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2568 - mse: 0.2555 - rmse: 0.5055 - mae: 0.2568 - mape: 8.1419\n",
      "Epoch 928: val_loss did not improve from 0.25671\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2575 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2575 - mape: 8.1635 - val_loss: 0.2567 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2567 - val_mape: 8.1503 - lr: 1.0000e-05\n",
      "Epoch 929/1000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2573 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2573 - mape: 8.1726\n",
      "Epoch 929: val_loss did not improve from 0.25671\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2572 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2572 - mape: 8.1622 - val_loss: 0.2567 - val_mse: 0.2554 - val_rmse: 0.5054 - val_mae: 0.2567 - val_mape: 8.1550 - lr: 1.0000e-05\n",
      "Epoch 930/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2576 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2576 - mape: 8.1674\n",
      "Epoch 930: val_loss did not improve from 0.25671\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2576 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2576 - mape: 8.1674 - val_loss: 0.2579 - val_mse: 0.2533 - val_rmse: 0.5033 - val_mae: 0.2579 - val_mape: 8.1242 - lr: 1.0000e-05\n",
      "Epoch 931/1000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2566 - mse: 0.2559 - rmse: 0.5059 - mae: 0.2566 - mape: 8.0863\n",
      "Epoch 931: val_loss did not improve from 0.25671\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2573 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2573 - mape: 8.1471 - val_loss: 0.2571 - val_mse: 0.2597 - val_rmse: 0.5096 - val_mae: 0.2571 - val_mape: 8.1962 - lr: 1.0000e-05\n",
      "Epoch 932/1000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2578 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2578 - mape: 8.1720\n",
      "Epoch 932: val_loss did not improve from 0.25671\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2579 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2579 - mape: 8.1750 - val_loss: 0.2576 - val_mse: 0.2545 - val_rmse: 0.5045 - val_mae: 0.2576 - val_mape: 8.1769 - lr: 1.0000e-05\n",
      "Epoch 933/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2571 - mse: 0.2566 - rmse: 0.5066 - mae: 0.2571 - mape: 8.1515\n",
      "Epoch 933: val_loss did not improve from 0.25671\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2572 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2572 - mape: 8.1538 - val_loss: 0.2577 - val_mse: 0.2580 - val_rmse: 0.5080 - val_mae: 0.2577 - val_mape: 8.2065 - lr: 1.0000e-05\n",
      "Epoch 934/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2586 - mse: 0.2585 - rmse: 0.5085 - mae: 0.2586 - mape: 8.2132\n",
      "Epoch 934: val_loss did not improve from 0.25671\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2574 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2574 - mape: 8.1656 - val_loss: 0.2567 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.2567 - val_mape: 8.1763 - lr: 1.0000e-05\n",
      "Epoch 935/1000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2577 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2577 - mape: 8.1945\n",
      "Epoch 935: val_loss did not improve from 0.25671\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2573 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2573 - mape: 8.1557 - val_loss: 0.2581 - val_mse: 0.2623 - val_rmse: 0.5122 - val_mae: 0.2581 - val_mape: 8.2773 - lr: 1.0000e-05\n",
      "Epoch 936/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2564 - mse: 0.2562 - rmse: 0.5062 - mae: 0.2564 - mape: 8.1441\n",
      "Epoch 936: val_loss improved from 0.25671 to 0.25662, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2572 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2572 - mape: 8.1653 - val_loss: 0.2566 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2566 - val_mape: 8.1432 - lr: 1.0000e-05\n",
      "Epoch 937/1000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2573 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2573 - mape: 8.1563\n",
      "Epoch 937: val_loss did not improve from 0.25662\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2574 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2574 - mape: 8.1595 - val_loss: 0.2571 - val_mse: 0.2564 - val_rmse: 0.5063 - val_mae: 0.2571 - val_mape: 8.0929 - lr: 1.0000e-05\n",
      "Epoch 938/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2576 - mse: 0.2568 - rmse: 0.5067 - mae: 0.2576 - mape: 8.1641\n",
      "Epoch 938: val_loss did not improve from 0.25662\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2573 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2573 - mape: 8.1578 - val_loss: 0.2575 - val_mse: 0.2553 - val_rmse: 0.5052 - val_mae: 0.2575 - val_mape: 8.0877 - lr: 1.0000e-05\n",
      "Epoch 939/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2577 - mse: 0.2578 - rmse: 0.5078 - mae: 0.2577 - mape: 8.1574\n",
      "Epoch 939: val_loss did not improve from 0.25662\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2575 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2575 - mape: 8.1598 - val_loss: 0.2574 - val_mse: 0.2611 - val_rmse: 0.5110 - val_mae: 0.2574 - val_mape: 8.2232 - lr: 1.0000e-05\n",
      "Epoch 940/1000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2582 - mse: 0.2589 - rmse: 0.5088 - mae: 0.2582 - mape: 8.1873\n",
      "Epoch 940: val_loss did not improve from 0.25662\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2576 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2576 - mape: 8.1677 - val_loss: 0.2571 - val_mse: 0.2593 - val_rmse: 0.5092 - val_mae: 0.2571 - val_mape: 8.1819 - lr: 1.0000e-05\n",
      "Epoch 941/1000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2581 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2581 - mape: 8.2110\n",
      "Epoch 941: val_loss did not improve from 0.25662\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2572 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2572 - mape: 8.1584 - val_loss: 0.2568 - val_mse: 0.2565 - val_rmse: 0.5065 - val_mae: 0.2568 - val_mape: 8.1015 - lr: 1.0000e-05\n",
      "Epoch 942/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2571 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2571 - mape: 8.1388\n",
      "Epoch 942: val_loss did not improve from 0.25662\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2572 - mse: 0.2568 - rmse: 0.5067 - mae: 0.2572 - mape: 8.1533 - val_loss: 0.2571 - val_mse: 0.2542 - val_rmse: 0.5042 - val_mae: 0.2571 - val_mape: 8.0908 - lr: 1.0000e-05\n",
      "Epoch 943/1000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2573 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2573 - mape: 8.1810\n",
      "Epoch 943: val_loss did not improve from 0.25662\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2574 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2574 - mape: 8.1595 - val_loss: 0.2570 - val_mse: 0.2531 - val_rmse: 0.5031 - val_mae: 0.2570 - val_mape: 8.1318 - lr: 1.0000e-05\n",
      "Epoch 944/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2562 - mse: 0.2562 - rmse: 0.5061 - mae: 0.2562 - mape: 8.1499\n",
      "Epoch 944: val_loss did not improve from 0.25662\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2571 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2571 - mape: 8.1445 - val_loss: 0.2568 - val_mse: 0.2546 - val_rmse: 0.5046 - val_mae: 0.2568 - val_mape: 8.1570 - lr: 1.0000e-05\n",
      "Epoch 945/1000\n",
      "271/318 [========================>.....] - ETA: 0s - loss: 0.2584 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2584 - mape: 8.1771\n",
      "Epoch 945: val_loss improved from 0.25662 to 0.25645, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2577 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2577 - mape: 8.1731 - val_loss: 0.2564 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2564 - val_mape: 8.1316 - lr: 1.0000e-05\n",
      "Epoch 946/1000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2579 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2579 - mape: 8.1738\n",
      "Epoch 946: val_loss did not improve from 0.25645\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2572 - mse: 0.2566 - rmse: 0.5066 - mae: 0.2572 - mape: 8.1484 - val_loss: 0.2568 - val_mse: 0.2559 - val_rmse: 0.5059 - val_mae: 0.2568 - val_mape: 8.1396 - lr: 1.0000e-05\n",
      "Epoch 947/1000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2569 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2569 - mape: 8.1541\n",
      "Epoch 947: val_loss did not improve from 0.25645\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2573 - mse: 0.2580 - rmse: 0.5079 - mae: 0.2573 - mape: 8.1613 - val_loss: 0.2572 - val_mse: 0.2576 - val_rmse: 0.5075 - val_mae: 0.2572 - val_mape: 8.1148 - lr: 1.0000e-05\n",
      "Epoch 948/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2575 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2575 - mape: 8.1540\n",
      "Epoch 948: val_loss did not improve from 0.25645\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2575 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2575 - mape: 8.1540 - val_loss: 0.2581 - val_mse: 0.2643 - val_rmse: 0.5141 - val_mae: 0.2581 - val_mape: 8.2599 - lr: 1.0000e-05\n",
      "Epoch 949/1000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2564 - mse: 0.2549 - rmse: 0.5049 - mae: 0.2564 - mape: 8.1301\n",
      "Epoch 949: val_loss did not improve from 0.25645\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2573 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2573 - mape: 8.1609 - val_loss: 0.2567 - val_mse: 0.2557 - val_rmse: 0.5057 - val_mae: 0.2567 - val_mape: 8.0946 - lr: 1.0000e-05\n",
      "Epoch 950/1000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.2560 - mse: 0.2540 - rmse: 0.5040 - mae: 0.2560 - mape: 8.1436\n",
      "Epoch 950: val_loss did not improve from 0.25645\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2571 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2571 - mape: 8.1367 - val_loss: 0.2573 - val_mse: 0.2556 - val_rmse: 0.5056 - val_mae: 0.2573 - val_mape: 8.1506 - lr: 1.0000e-05\n",
      "Epoch 951/1000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2585 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2585 - mape: 8.1720\n",
      "Epoch 951: val_loss did not improve from 0.25645\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2571 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2571 - mape: 8.1415 - val_loss: 0.2577 - val_mse: 0.2635 - val_rmse: 0.5133 - val_mae: 0.2577 - val_mape: 8.2585 - lr: 1.0000e-05\n",
      "Epoch 952/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2575 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2575 - mape: 8.1657\n",
      "Epoch 952: val_loss did not improve from 0.25645\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2572 - mse: 0.2571 - rmse: 0.5071 - mae: 0.2572 - mape: 8.1511 - val_loss: 0.2568 - val_mse: 0.2580 - val_rmse: 0.5079 - val_mae: 0.2568 - val_mape: 8.1441 - lr: 1.0000e-05\n",
      "Epoch 953/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2572 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2572 - mape: 8.1647\n",
      "Epoch 953: val_loss did not improve from 0.25645\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2570 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2570 - mape: 8.1567 - val_loss: 0.2573 - val_mse: 0.2534 - val_rmse: 0.5034 - val_mae: 0.2573 - val_mape: 8.0973 - lr: 1.0000e-05\n",
      "Epoch 954/1000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2577 - mse: 0.2593 - rmse: 0.5093 - mae: 0.2577 - mape: 8.1974\n",
      "Epoch 954: val_loss did not improve from 0.25645\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2574 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2574 - mape: 8.1575 - val_loss: 0.2568 - val_mse: 0.2558 - val_rmse: 0.5057 - val_mae: 0.2568 - val_mape: 8.1587 - lr: 1.0000e-05\n",
      "Epoch 955/1000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2572 - mse: 0.2608 - rmse: 0.5107 - mae: 0.2572 - mape: 8.1584\n",
      "Epoch 955: val_loss did not improve from 0.25645\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2574 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2574 - mape: 8.1667 - val_loss: 0.2569 - val_mse: 0.2540 - val_rmse: 0.5040 - val_mae: 0.2569 - val_mape: 8.0893 - lr: 1.0000e-05\n",
      "Epoch 956/1000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2576 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2576 - mape: 8.1586\n",
      "Epoch 956: val_loss did not improve from 0.25645\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2570 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2570 - mape: 8.1341 - val_loss: 0.2568 - val_mse: 0.2543 - val_rmse: 0.5043 - val_mae: 0.2568 - val_mape: 8.1305 - lr: 1.0000e-05\n",
      "Epoch 957/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2557 - mse: 0.2561 - rmse: 0.5061 - mae: 0.2557 - mape: 8.1161\n",
      "Epoch 957: val_loss improved from 0.25645 to 0.25632, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2572 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2572 - mape: 8.1595 - val_loss: 0.2563 - val_mse: 0.2556 - val_rmse: 0.5055 - val_mae: 0.2563 - val_mape: 8.1034 - lr: 1.0000e-05\n",
      "Epoch 958/1000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2557 - mse: 0.2556 - rmse: 0.5056 - mae: 0.2557 - mape: 8.1080\n",
      "Epoch 958: val_loss did not improve from 0.25632\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2569 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2569 - mape: 8.1476 - val_loss: 0.2566 - val_mse: 0.2546 - val_rmse: 0.5046 - val_mae: 0.2566 - val_mape: 8.0984 - lr: 1.0000e-05\n",
      "Epoch 959/1000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2562 - mse: 0.2558 - rmse: 0.5058 - mae: 0.2562 - mape: 8.1519\n",
      "Epoch 959: val_loss did not improve from 0.25632\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2569 - mse: 0.2568 - rmse: 0.5067 - mae: 0.2569 - mape: 8.1332 - val_loss: 0.2571 - val_mse: 0.2611 - val_rmse: 0.5109 - val_mae: 0.2571 - val_mape: 8.2160 - lr: 1.0000e-05\n",
      "Epoch 960/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2588 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2588 - mape: 8.2149\n",
      "Epoch 960: val_loss did not improve from 0.25632\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2571 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2571 - mape: 8.1583 - val_loss: 0.2565 - val_mse: 0.2545 - val_rmse: 0.5045 - val_mae: 0.2565 - val_mape: 8.1134 - lr: 1.0000e-05\n",
      "Epoch 961/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2569 - mse: 0.2558 - rmse: 0.5058 - mae: 0.2569 - mape: 8.1547\n",
      "Epoch 961: val_loss did not improve from 0.25632\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2570 - mse: 0.2565 - rmse: 0.5065 - mae: 0.2570 - mape: 8.1444 - val_loss: 0.2572 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2572 - val_mape: 8.1918 - lr: 1.0000e-05\n",
      "Epoch 962/1000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2571 - mse: 0.2565 - rmse: 0.5064 - mae: 0.2571 - mape: 8.1568\n",
      "Epoch 962: val_loss did not improve from 0.25632\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2572 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2572 - mape: 8.1496 - val_loss: 0.2565 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2565 - val_mape: 8.1757 - lr: 1.0000e-05\n",
      "Epoch 963/1000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2572 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2572 - mape: 8.1576\n",
      "Epoch 963: val_loss did not improve from 0.25632\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2572 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2572 - mape: 8.1585 - val_loss: 0.2567 - val_mse: 0.2545 - val_rmse: 0.5044 - val_mae: 0.2567 - val_mape: 8.0785 - lr: 1.0000e-05\n",
      "Epoch 964/1000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2577 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2577 - mape: 8.1562\n",
      "Epoch 964: val_loss did not improve from 0.25632\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2571 - mse: 0.2571 - rmse: 0.5071 - mae: 0.2571 - mape: 8.1421 - val_loss: 0.2568 - val_mse: 0.2580 - val_rmse: 0.5080 - val_mae: 0.2568 - val_mape: 8.1565 - lr: 1.0000e-05\n",
      "Epoch 965/1000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2577 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2577 - mape: 8.1779\n",
      "Epoch 965: val_loss did not improve from 0.25632\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2572 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2572 - mape: 8.1466 - val_loss: 0.2565 - val_mse: 0.2537 - val_rmse: 0.5037 - val_mae: 0.2565 - val_mape: 8.1017 - lr: 1.0000e-05\n",
      "Epoch 966/1000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2552 - mse: 0.2508 - rmse: 0.5008 - mae: 0.2552 - mape: 8.0755\n",
      "Epoch 966: val_loss did not improve from 0.25632\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2570 - mse: 0.2566 - rmse: 0.5066 - mae: 0.2570 - mape: 8.1448 - val_loss: 0.2565 - val_mse: 0.2560 - val_rmse: 0.5060 - val_mae: 0.2565 - val_mape: 8.1245 - lr: 1.0000e-05\n",
      "Epoch 967/1000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2571 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2571 - mape: 8.1438\n",
      "Epoch 967: val_loss did not improve from 0.25632\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2571 - mse: 0.2568 - rmse: 0.5067 - mae: 0.2571 - mape: 8.1470 - val_loss: 0.2565 - val_mse: 0.2547 - val_rmse: 0.5047 - val_mae: 0.2565 - val_mape: 8.0998 - lr: 1.0000e-05\n",
      "Epoch 968/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2574 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2574 - mape: 8.1350\n",
      "Epoch 968: val_loss improved from 0.25632 to 0.25629, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2570 - mse: 0.2568 - rmse: 0.5068 - mae: 0.2570 - mape: 8.1452 - val_loss: 0.2563 - val_mse: 0.2552 - val_rmse: 0.5051 - val_mae: 0.2563 - val_mape: 8.1228 - lr: 1.0000e-05\n",
      "Epoch 969/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2571 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2571 - mape: 8.1286\n",
      "Epoch 969: val_loss did not improve from 0.25629\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2570 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2570 - mape: 8.1372 - val_loss: 0.2564 - val_mse: 0.2537 - val_rmse: 0.5037 - val_mae: 0.2564 - val_mape: 8.1130 - lr: 1.0000e-05\n",
      "Epoch 970/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2572 - mse: 0.2557 - rmse: 0.5056 - mae: 0.2572 - mape: 8.1314\n",
      "Epoch 970: val_loss did not improve from 0.25629\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2574 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2574 - mape: 8.1537 - val_loss: 0.2565 - val_mse: 0.2553 - val_rmse: 0.5053 - val_mae: 0.2565 - val_mape: 8.0935 - lr: 1.0000e-05\n",
      "Epoch 971/1000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2549 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2549 - mape: 8.0711\n",
      "Epoch 971: val_loss did not improve from 0.25629\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2567 - mse: 0.2571 - rmse: 0.5071 - mae: 0.2567 - mape: 8.1363 - val_loss: 0.2581 - val_mse: 0.2548 - val_rmse: 0.5047 - val_mae: 0.2581 - val_mape: 8.0764 - lr: 1.0000e-05\n",
      "Epoch 972/1000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2569 - mse: 0.2558 - rmse: 0.5057 - mae: 0.2569 - mape: 8.1404\n",
      "Epoch 972: val_loss did not improve from 0.25629\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2570 - mse: 0.2561 - rmse: 0.5061 - mae: 0.2570 - mape: 8.1425 - val_loss: 0.2569 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2569 - val_mape: 8.0916 - lr: 1.0000e-05\n",
      "Epoch 973/1000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2573 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2573 - mape: 8.1449\n",
      "Epoch 973: val_loss did not improve from 0.25629\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2569 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2569 - mape: 8.1458 - val_loss: 0.2563 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2563 - val_mape: 8.1641 - lr: 1.0000e-05\n",
      "Epoch 974/1000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2572 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2572 - mape: 8.1615\n",
      "Epoch 974: val_loss did not improve from 0.25629\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2571 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2571 - mape: 8.1589 - val_loss: 0.2577 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2577 - val_mape: 8.1654 - lr: 1.0000e-05\n",
      "Epoch 975/1000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2575 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2575 - mape: 8.1494\n",
      "Epoch 975: val_loss did not improve from 0.25629\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2569 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2569 - mape: 8.1362 - val_loss: 0.2565 - val_mse: 0.2549 - val_rmse: 0.5048 - val_mae: 0.2565 - val_mape: 8.0973 - lr: 1.0000e-05\n",
      "Epoch 976/1000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2557 - mse: 0.2522 - rmse: 0.5022 - mae: 0.2557 - mape: 8.0930\n",
      "Epoch 976: val_loss did not improve from 0.25629\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2570 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2570 - mape: 8.1438 - val_loss: 0.2563 - val_mse: 0.2551 - val_rmse: 0.5051 - val_mae: 0.2563 - val_mape: 8.1401 - lr: 1.0000e-05\n",
      "Epoch 977/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2556 - mse: 0.2542 - rmse: 0.5042 - mae: 0.2556 - mape: 8.0913\n",
      "Epoch 977: val_loss did not improve from 0.25629\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2568 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2568 - mape: 8.1322 - val_loss: 0.2578 - val_mse: 0.2529 - val_rmse: 0.5029 - val_mae: 0.2578 - val_mape: 8.1055 - lr: 1.0000e-05\n",
      "Epoch 978/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2578 - mse: 0.2565 - rmse: 0.5064 - mae: 0.2578 - mape: 8.1384\n",
      "Epoch 978: val_loss did not improve from 0.25629\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2570 - mse: 0.2559 - rmse: 0.5058 - mae: 0.2570 - mape: 8.1241 - val_loss: 0.2570 - val_mse: 0.2609 - val_rmse: 0.5108 - val_mae: 0.2570 - val_mape: 8.1777 - lr: 1.0000e-05\n",
      "Epoch 979/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2561 - mse: 0.2560 - rmse: 0.5060 - mae: 0.2561 - mape: 8.1176\n",
      "Epoch 979: val_loss did not improve from 0.25629\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2575 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2575 - mape: 8.1596 - val_loss: 0.2563 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2563 - val_mape: 8.1193 - lr: 1.0000e-05\n",
      "Epoch 980/1000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2573 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2573 - mape: 8.1220\n",
      "Epoch 980: val_loss improved from 0.25629 to 0.25622, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2569 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2569 - mape: 8.1374 - val_loss: 0.2562 - val_mse: 0.2589 - val_rmse: 0.5088 - val_mae: 0.2562 - val_mape: 8.1526 - lr: 1.0000e-05\n",
      "Epoch 981/1000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2570 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2570 - mape: 8.1291\n",
      "Epoch 981: val_loss improved from 0.25622 to 0.25613, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2569 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2569 - mape: 8.1416 - val_loss: 0.2561 - val_mse: 0.2581 - val_rmse: 0.5081 - val_mae: 0.2561 - val_mape: 8.1569 - lr: 1.0000e-05\n",
      "Epoch 982/1000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2554 - mse: 0.2535 - rmse: 0.5034 - mae: 0.2554 - mape: 8.0847\n",
      "Epoch 982: val_loss did not improve from 0.25613\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2569 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2569 - mape: 8.1319 - val_loss: 0.2565 - val_mse: 0.2543 - val_rmse: 0.5042 - val_mae: 0.2565 - val_mape: 8.1000 - lr: 1.0000e-05\n",
      "Epoch 983/1000\n",
      "271/318 [========================>.....] - ETA: 0s - loss: 0.2553 - mse: 0.2519 - rmse: 0.5019 - mae: 0.2553 - mape: 8.0988\n",
      "Epoch 983: val_loss did not improve from 0.25613\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2569 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2569 - mape: 8.1519 - val_loss: 0.2561 - val_mse: 0.2560 - val_rmse: 0.5060 - val_mae: 0.2561 - val_mape: 8.1080 - lr: 1.0000e-05\n",
      "Epoch 984/1000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2580 - mse: 0.2619 - rmse: 0.5118 - mae: 0.2580 - mape: 8.1857\n",
      "Epoch 984: val_loss did not improve from 0.25613\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2568 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2568 - mape: 8.1366 - val_loss: 0.2572 - val_mse: 0.2614 - val_rmse: 0.5113 - val_mae: 0.2572 - val_mape: 8.2244 - lr: 1.0000e-05\n",
      "Epoch 985/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2572 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2572 - mape: 8.1398\n",
      "Epoch 985: val_loss did not improve from 0.25613\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2569 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2569 - mape: 8.1343 - val_loss: 0.2566 - val_mse: 0.2537 - val_rmse: 0.5037 - val_mae: 0.2566 - val_mape: 8.1046 - lr: 1.0000e-05\n",
      "Epoch 986/1000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2571 - mse: 0.2549 - rmse: 0.5049 - mae: 0.2571 - mape: 8.1163\n",
      "Epoch 986: val_loss improved from 0.25613 to 0.25610, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2569 - mse: 0.2561 - rmse: 0.5060 - mae: 0.2569 - mape: 8.1393 - val_loss: 0.2561 - val_mse: 0.2545 - val_rmse: 0.5045 - val_mae: 0.2561 - val_mape: 8.1020 - lr: 1.0000e-05\n",
      "Epoch 987/1000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2560 - mse: 0.2581 - rmse: 0.5081 - mae: 0.2560 - mape: 8.1328\n",
      "Epoch 987: val_loss did not improve from 0.25610\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2570 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2570 - mape: 8.1411 - val_loss: 0.2562 - val_mse: 0.2574 - val_rmse: 0.5074 - val_mae: 0.2562 - val_mape: 8.1404 - lr: 1.0000e-05\n",
      "Epoch 988/1000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2574 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2574 - mape: 8.1506\n",
      "Epoch 988: val_loss did not improve from 0.25610\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2568 - mse: 0.2575 - rmse: 0.5074 - mae: 0.2568 - mape: 8.1465 - val_loss: 0.2584 - val_mse: 0.2611 - val_rmse: 0.5110 - val_mae: 0.2584 - val_mape: 8.1493 - lr: 1.0000e-05\n",
      "Epoch 989/1000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2562 - mse: 0.2555 - rmse: 0.5055 - mae: 0.2562 - mape: 8.1047\n",
      "Epoch 989: val_loss did not improve from 0.25610\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2570 - mse: 0.2572 - rmse: 0.5072 - mae: 0.2570 - mape: 8.1405 - val_loss: 0.2564 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2564 - val_mape: 8.1609 - lr: 1.0000e-05\n",
      "Epoch 990/1000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2566 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2566 - mape: 8.1533\n",
      "Epoch 990: val_loss did not improve from 0.25610\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2567 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2567 - mape: 8.1426 - val_loss: 0.2568 - val_mse: 0.2562 - val_rmse: 0.5062 - val_mae: 0.2568 - val_mape: 8.0907 - lr: 1.0000e-05\n",
      "Epoch 991/1000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2560 - mse: 0.2551 - rmse: 0.5051 - mae: 0.2560 - mape: 8.1053\n",
      "Epoch 991: val_loss did not improve from 0.25610\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2565 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2565 - mape: 8.1249 - val_loss: 0.2570 - val_mse: 0.2581 - val_rmse: 0.5080 - val_mae: 0.2570 - val_mape: 8.0870 - lr: 1.0000e-05\n",
      "Epoch 992/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2569 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2569 - mape: 8.1327\n",
      "Epoch 992: val_loss did not improve from 0.25610\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2569 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2569 - mape: 8.1327 - val_loss: 0.2570 - val_mse: 0.2551 - val_rmse: 0.5050 - val_mae: 0.2570 - val_mape: 8.1877 - lr: 1.0000e-05\n",
      "Epoch 993/1000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2565 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2565 - mape: 8.1237\n",
      "Epoch 993: val_loss did not improve from 0.25610\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2566 - mse: 0.2563 - rmse: 0.5063 - mae: 0.2566 - mape: 8.1344 - val_loss: 0.2562 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2562 - val_mape: 8.1690 - lr: 1.0000e-05\n",
      "Epoch 994/1000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2569 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2569 - mape: 8.1358\n",
      "Epoch 994: val_loss did not improve from 0.25610\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2568 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2568 - mape: 8.1370 - val_loss: 0.2568 - val_mse: 0.2596 - val_rmse: 0.5095 - val_mae: 0.2568 - val_mape: 8.2116 - lr: 1.0000e-05\n",
      "Epoch 995/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2566 - mse: 0.2562 - rmse: 0.5062 - mae: 0.2566 - mape: 8.1340\n",
      "Epoch 995: val_loss did not improve from 0.25610\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2567 - mse: 0.2565 - rmse: 0.5065 - mae: 0.2567 - mape: 8.1408 - val_loss: 0.2562 - val_mse: 0.2564 - val_rmse: 0.5064 - val_mae: 0.2562 - val_mape: 8.1331 - lr: 1.0000e-05\n",
      "Epoch 996/1000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2577 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2577 - mape: 8.1662\n",
      "Epoch 996: val_loss did not improve from 0.25610\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2566 - mse: 0.2556 - rmse: 0.5055 - mae: 0.2566 - mape: 8.1350 - val_loss: 0.2566 - val_mse: 0.2560 - val_rmse: 0.5059 - val_mae: 0.2566 - val_mape: 8.1000 - lr: 1.0000e-05\n",
      "Epoch 997/1000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2558 - mse: 0.2537 - rmse: 0.5036 - mae: 0.2558 - mape: 8.1343\n",
      "Epoch 997: val_loss improved from 0.25610 to 0.25598, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2568 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2568 - mape: 8.1421 - val_loss: 0.2560 - val_mse: 0.2554 - val_rmse: 0.5054 - val_mae: 0.2560 - val_mape: 8.1227 - lr: 1.0000e-05\n",
      "Epoch 998/1000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2561 - mse: 0.2552 - rmse: 0.5052 - mae: 0.2561 - mape: 8.1113\n",
      "Epoch 998: val_loss did not improve from 0.25598\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2564 - mse: 0.2562 - rmse: 0.5061 - mae: 0.2564 - mape: 8.1292 - val_loss: 0.2570 - val_mse: 0.2529 - val_rmse: 0.5029 - val_mae: 0.2570 - val_mape: 8.0610 - lr: 1.0000e-05\n",
      "Epoch 999/1000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2568 - mse: 0.2566 - rmse: 0.5065 - mae: 0.2568 - mape: 8.1321\n",
      "Epoch 999: val_loss improved from 0.25598 to 0.25596, saving model to model_weights/20221123-211518_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2568 - mse: 0.2566 - rmse: 0.5065 - mae: 0.2568 - mape: 8.1321 - val_loss: 0.2560 - val_mse: 0.2552 - val_rmse: 0.5052 - val_mae: 0.2560 - val_mape: 8.1082 - lr: 1.0000e-05\n",
      "Epoch 1000/1000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2566 - mse: 0.2551 - rmse: 0.5050 - mae: 0.2566 - mape: 8.1265\n",
      "Epoch 1000: val_loss did not improve from 0.25596\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2568 - mse: 0.2560 - rmse: 0.5060 - mae: 0.2568 - mape: 8.1322 - val_loss: 0.2563 - val_mse: 0.2528 - val_rmse: 0.5028 - val_mae: 0.2563 - val_mape: 8.0898 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f089dcb1f30>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 1000, batch_size = 64, validation_data = (X_val, y_val), callbacks=[checkpoint_callback, es_callback, tf.keras.callbacks.ReduceLROnPlateau(patience=40)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20221123-211518\n"
     ]
    }
   ],
   "source": [
    "print(date_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models.load_model(f'model_weights/{date_actual}_mlp_best_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 557us/step - loss: 0.3429 - mse: 0.3815 - rmse: 0.6177 - mae: 0.3429 - mape: 12.1199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34287169575691223,\n",
       " 0.3815343976020813,\n",
       " 0.6176847219467163,\n",
       " 0.34287169575691223,\n",
       " 12.119946479797363]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 444us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.02351544519270865\n",
      "mse:  0.381534390933214\n",
      "rmse:  0.6176847018772716\n",
      "mae:  0.34287172744600436\n",
      "mape:  0.12119945810086158\n",
      "Error estandar:  0.6081122558299572\n"
     ]
    }
   ],
   "source": [
    "print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "print(\"mse: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"rmse: \", mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"mae: \", mean_absolute_error(y_test, y_pred))\n",
    "print(\"mape: \", mean_absolute_percentage_error(y_test, y_pred))\n",
    "print(\"Error estandar: \", stde(y_test.squeeze(),\n",
    "      y_pred.squeeze(), ddof=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x640 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residuals = y_test - y_pred\n",
    "residuals_std = residuals/residuals.std()\n",
    "\n",
    "y_real_stage = np.array([i[0] for i in y_test])\n",
    "residual_stage = np.array([i[0] for i in residuals])\n",
    "\n",
    "y_real_discharge = np.array([i[-1] for i in y_test])\n",
    "residual_discharge = np.array([i[-1] for i in residuals])\n",
    "\n",
    "\n",
    "figure, ax = plt.subplots(ncols=2, figsize=(20, 8), dpi=80)\n",
    "\n",
    "ax[0].scatter(y_real_stage, residual_stage / residual_stage.std(), label=\"stage residuals\")\n",
    "ax[1].scatter(y_real_discharge, residual_discharge / residual_discharge.std(), label=\"discharge residuals\")\n",
    "ax[0].axhline(y=0.0, color='r', linestyle='-')\n",
    "ax[1].axhline(y=0.0, color='r', linestyle='-')\n",
    "\n",
    "ax[0].set_title(\"Stage residuals\")\n",
    "ax[1].set_title(\"Discharge residuals\")\n",
    "\n",
    "ax[1].set_xlabel(\"Fitted values\")\n",
    "ax[0].set_xlabel(\"Fitted values\")\n",
    "ax[1].set_ylabel(\"Standarized residuals\")\n",
    "ax[0].set_ylabel(\"Standarized residuals\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import normal_ad\n",
    "\n",
    "#figure = sm.qqplot(residual_stage / residual_stage.std(), line ='45', label='stage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = sm.qqplot(residual_discharge / residual_discharge.std(), line='45', label='discharge')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(residual_stage / residual_stage.std(), density=True, bins = 60)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Residual stage');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(residual_discharge / residual_discharge.std(), density=True, bins = 60)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Residual discharge');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.0\n",
      "Hay evidencia de que los residuos no provienen de una distribución normal.\n"
     ]
    }
   ],
   "source": [
    "stat, pval = normal_ad(residual_discharge / residual_discharge.std())\n",
    "print(\"p-value:\", pval)\n",
    "\n",
    "if pval < 0.05:\n",
    "    print(\"Hay evidencia de que los residuos no provienen de una distribución normal.\")\n",
    "else:\n",
    "    print(\"No hay evidencia para rechazar la hipótesis de que los residuos vienen de una distribución normal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x640 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure, ax = plt.subplots(ncols=2, figsize=(20, 8), dpi=80)\n",
    "\n",
    "ax[0].plot(np.arange(len(y_test)), y_test, label=\"Stage real\")\n",
    "ax[0].plot(np.arange(len(y_test)), y_pred, label=\"Stage pred\")\n",
    "\n",
    "ax[0].set_title(\"Stage residuals\")\n",
    "ax[1].set_title(\"Discharge residuals\")\n",
    "\n",
    "ax[1].set_ylabel(\"Values\")\n",
    "ax[0].set_ylabel(\"Values\")\n",
    "ax[1].set_xlabel(\"Time\")\n",
    "ax[0].set_xlabel(\"Time\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79f576286c1276b480d6696ed40f6607e18214e4a2875a618cb5be817ff26007"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
