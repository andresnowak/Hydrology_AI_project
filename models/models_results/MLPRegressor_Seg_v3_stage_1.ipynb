{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tools.eval_measures import stde\n",
    "\n",
    "import kerastuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the etl info results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>remove_time_features</th>\n",
       "      <th>generic_features</th>\n",
       "      <th>remove_atypical_values</th>\n",
       "      <th>feature_combination</th>\n",
       "      <th>remove_feature_selection</th>\n",
       "      <th>remove_invalid_correlated_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   remove_time_features  generic_features  remove_atypical_values  \\\n",
       "0                 False             False                   False   \n",
       "\n",
       "   feature_combination  remove_feature_selection  \\\n",
       "0                False                     False   \n",
       "\n",
       "   remove_invalid_correlated_features  \n",
       "0                               False  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info = pd.read_csv('../dataset_clean/options_csv_v1_etl.csv')\n",
    "df_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SensorTime</th>\n",
       "      <th>CaptureTime</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Agency</th>\n",
       "      <th>SiteNumber</th>\n",
       "      <th>TimeZone</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Discharge</th>\n",
       "      <th>...</th>\n",
       "      <th>WwRawLineMin</th>\n",
       "      <th>WwRawLineMax</th>\n",
       "      <th>WwRawLineMean</th>\n",
       "      <th>WwRawLineSigma</th>\n",
       "      <th>WwCurveLineMin</th>\n",
       "      <th>WwCurveLineMax</th>\n",
       "      <th>WwCurveLineMean</th>\n",
       "      <th>WwCurveLineSigma</th>\n",
       "      <th>RiverArea</th>\n",
       "      <th>RiverWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2012-06-09T13:09:07</td>\n",
       "      <td>statelineweir_20120609_farrell_001.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49975.0</td>\n",
       "      <td>207.508733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2012-06-09T13:10:29</td>\n",
       "      <td>statelineweir_20120609_farrell_002.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50184.0</td>\n",
       "      <td>208.663145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-06-09 13:45:00</td>\n",
       "      <td>2012-06-09T13:44:01</td>\n",
       "      <td>statelineweir_20120609_farrell_003.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.96</td>\n",
       "      <td>873.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50543.0</td>\n",
       "      <td>209.445067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2012-06-09 14:45:00</td>\n",
       "      <td>2012-06-09T14:44:30</td>\n",
       "      <td>statelineweir_20120609_farrell_004.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50856.0</td>\n",
       "      <td>211.265690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-06-09 15:45:00</td>\n",
       "      <td>2012-06-09T15:44:59</td>\n",
       "      <td>statelineweir_20120609_farrell_005.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51004.0</td>\n",
       "      <td>211.250274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42054</th>\n",
       "      <td>42054</td>\n",
       "      <td>42054</td>\n",
       "      <td>2019-10-11 09:00:00</td>\n",
       "      <td>2019-10-11T08:59:53</td>\n",
       "      <td>statelineweir_20191011_farrell_409.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9284.0</td>\n",
       "      <td>77521.0</td>\n",
       "      <td>38385.370066</td>\n",
       "      <td>15952.029728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70085.0</td>\n",
       "      <td>37550.894823</td>\n",
       "      <td>16444.401209</td>\n",
       "      <td>45842.0</td>\n",
       "      <td>194.934605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42055</th>\n",
       "      <td>42055</td>\n",
       "      <td>42055</td>\n",
       "      <td>2019-10-11 10:00:00</td>\n",
       "      <td>2019-10-11T09:59:52</td>\n",
       "      <td>statelineweir_20191011_farrell_410.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10092.0</td>\n",
       "      <td>74614.0</td>\n",
       "      <td>40162.989292</td>\n",
       "      <td>15467.708856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70061.0</td>\n",
       "      <td>39397.339095</td>\n",
       "      <td>16009.008049</td>\n",
       "      <td>42300.0</td>\n",
       "      <td>194.762264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42056</th>\n",
       "      <td>42056</td>\n",
       "      <td>42056</td>\n",
       "      <td>2019-10-11 11:00:00</td>\n",
       "      <td>2019-10-11T10:59:52</td>\n",
       "      <td>statelineweir_20191011_farrell_411.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7067.0</td>\n",
       "      <td>83260.0</td>\n",
       "      <td>42095.946590</td>\n",
       "      <td>16770.357949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76335.0</td>\n",
       "      <td>41350.006568</td>\n",
       "      <td>17489.374617</td>\n",
       "      <td>41080.0</td>\n",
       "      <td>196.480105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42057</th>\n",
       "      <td>42057</td>\n",
       "      <td>42057</td>\n",
       "      <td>2019-10-11 12:00:00</td>\n",
       "      <td>2019-10-11T11:59:53</td>\n",
       "      <td>statelineweir_20191011_farrell_412.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6283.0</td>\n",
       "      <td>83045.0</td>\n",
       "      <td>45345.490954</td>\n",
       "      <td>17498.432849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78882.0</td>\n",
       "      <td>44553.920296</td>\n",
       "      <td>18268.294896</td>\n",
       "      <td>40976.0</td>\n",
       "      <td>193.595245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42058</th>\n",
       "      <td>42058</td>\n",
       "      <td>42058</td>\n",
       "      <td>2019-10-11 12:45:00</td>\n",
       "      <td>2019-10-11T12:59:52</td>\n",
       "      <td>statelineweir_20191011_farrell_413.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7375.0</td>\n",
       "      <td>89813.0</td>\n",
       "      <td>47877.870782</td>\n",
       "      <td>19963.166359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82630.0</td>\n",
       "      <td>47280.270559</td>\n",
       "      <td>20559.358767</td>\n",
       "      <td>41435.0</td>\n",
       "      <td>196.801994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42059 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0           SensorTime          CaptureTime  \\\n",
       "0                 0           0  2012-06-09 13:15:00  2012-06-09T13:09:07   \n",
       "1                 1           1  2012-06-09 13:15:00  2012-06-09T13:10:29   \n",
       "2                 2           2  2012-06-09 13:45:00  2012-06-09T13:44:01   \n",
       "3                 3           3  2012-06-09 14:45:00  2012-06-09T14:44:30   \n",
       "4                 4           4  2012-06-09 15:45:00  2012-06-09T15:44:59   \n",
       "...             ...         ...                  ...                  ...   \n",
       "42054         42054       42054  2019-10-11 09:00:00  2019-10-11T08:59:53   \n",
       "42055         42055       42055  2019-10-11 10:00:00  2019-10-11T09:59:52   \n",
       "42056         42056       42056  2019-10-11 11:00:00  2019-10-11T10:59:52   \n",
       "42057         42057       42057  2019-10-11 12:00:00  2019-10-11T11:59:53   \n",
       "42058         42058       42058  2019-10-11 12:45:00  2019-10-11T12:59:52   \n",
       "\n",
       "                                     Filename Agency  SiteNumber TimeZone  \\\n",
       "0      statelineweir_20120609_farrell_001.jpg   USGS     6674500      MDT   \n",
       "1      statelineweir_20120609_farrell_002.jpg   USGS     6674500      MDT   \n",
       "2      statelineweir_20120609_farrell_003.jpg   USGS     6674500      MDT   \n",
       "3      statelineweir_20120609_farrell_004.jpg   USGS     6674500      MDT   \n",
       "4      statelineweir_20120609_farrell_005.jpg   USGS     6674500      MDT   \n",
       "...                                       ...    ...         ...      ...   \n",
       "42054  statelineweir_20191011_farrell_409.jpg   USGS     6674500      MDT   \n",
       "42055  statelineweir_20191011_farrell_410.jpg   USGS     6674500      MDT   \n",
       "42056  statelineweir_20191011_farrell_411.jpg   USGS     6674500      MDT   \n",
       "42057  statelineweir_20191011_farrell_412.jpg   USGS     6674500      MDT   \n",
       "42058  statelineweir_20191011_farrell_413.jpg   USGS     6674500      MDT   \n",
       "\n",
       "       Stage  Discharge  ... WwRawLineMin  WwRawLineMax  WwRawLineMean  \\\n",
       "0       2.99      916.0  ...          0.0           0.0       0.000000   \n",
       "1       2.99      916.0  ...          0.0           0.0       0.000000   \n",
       "2       2.96      873.0  ...          0.0           0.0       0.000000   \n",
       "3       2.94      846.0  ...          0.0           0.0       0.000000   \n",
       "4       2.94      846.0  ...          0.0           0.0       0.000000   \n",
       "...      ...        ...  ...          ...           ...            ...   \n",
       "42054   2.54      434.0  ...       9284.0       77521.0   38385.370066   \n",
       "42055   2.54      434.0  ...      10092.0       74614.0   40162.989292   \n",
       "42056   2.54      434.0  ...       7067.0       83260.0   42095.946590   \n",
       "42057   2.54      434.0  ...       6283.0       83045.0   45345.490954   \n",
       "42058   2.54      434.0  ...       7375.0       89813.0   47877.870782   \n",
       "\n",
       "       WwRawLineSigma  WwCurveLineMin  WwCurveLineMax  WwCurveLineMean  \\\n",
       "0            0.000000             0.0             0.0         0.000000   \n",
       "1            0.000000             0.0             0.0         0.000000   \n",
       "2            0.000000             0.0             0.0         0.000000   \n",
       "3            0.000000             0.0             0.0         0.000000   \n",
       "4            0.000000             0.0             0.0         0.000000   \n",
       "...               ...             ...             ...              ...   \n",
       "42054    15952.029728             0.0         70085.0     37550.894823   \n",
       "42055    15467.708856             0.0         70061.0     39397.339095   \n",
       "42056    16770.357949             0.0         76335.0     41350.006568   \n",
       "42057    17498.432849             0.0         78882.0     44553.920296   \n",
       "42058    19963.166359             0.0         82630.0     47280.270559   \n",
       "\n",
       "       WwCurveLineSigma  RiverArea  RiverWidth  \n",
       "0              0.000000    49975.0  207.508733  \n",
       "1              0.000000    50184.0  208.663145  \n",
       "2              0.000000    50543.0  209.445067  \n",
       "3              0.000000    50856.0  211.265690  \n",
       "4              0.000000    51004.0  211.250274  \n",
       "...                 ...        ...         ...  \n",
       "42054      16444.401209    45842.0  194.934605  \n",
       "42055      16009.008049    42300.0  194.762264  \n",
       "42056      17489.374617    41080.0  196.480105  \n",
       "42057      18268.294896    40976.0  193.595245  \n",
       "42058      20559.358767    41435.0  196.801994  \n",
       "\n",
       "[42059 rows x 63 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset/V2_PlatteRiverWeir_features_merged_all.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SensorTime'] = pd.to_datetime(df['SensorTime'])\n",
    "df['Year'] = df['SensorTime'].dt.year\n",
    "df['Month'] = df['SensorTime'].dt.month\n",
    "df['date_offset'] = (df.SensorTime.dt.month * 100 + df.SensorTime.dt.day - 320)%1300\n",
    "\n",
    "df['Season'] = pd.cut(df['date_offset'], [0, 300, 602, 900, 1300], \n",
    "                      labels=['spring', 'summer', 'autumn', 'winter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CaptureTime</th>\n",
       "      <th>SensorTime</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Discharge</th>\n",
       "      <th>RiverArea</th>\n",
       "      <th>RiverWidth</th>\n",
       "      <th>Month</th>\n",
       "      <th>Season</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-06-09T13:09:07</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>49975.0</td>\n",
       "      <td>207.508733</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-06-09T13:10:29</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>50184.0</td>\n",
       "      <td>208.663145</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-06-09T13:44:01</td>\n",
       "      <td>2012-06-09 13:45:00</td>\n",
       "      <td>2.96</td>\n",
       "      <td>873.0</td>\n",
       "      <td>50543.0</td>\n",
       "      <td>209.445067</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-06-09T14:44:30</td>\n",
       "      <td>2012-06-09 14:45:00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>50856.0</td>\n",
       "      <td>211.265690</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-06-09T15:44:59</td>\n",
       "      <td>2012-06-09 15:45:00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>51004.0</td>\n",
       "      <td>211.250274</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42054</th>\n",
       "      <td>2019-10-11T08:59:53</td>\n",
       "      <td>2019-10-11 09:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>45842.0</td>\n",
       "      <td>194.934605</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42055</th>\n",
       "      <td>2019-10-11T09:59:52</td>\n",
       "      <td>2019-10-11 10:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>42300.0</td>\n",
       "      <td>194.762264</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42056</th>\n",
       "      <td>2019-10-11T10:59:52</td>\n",
       "      <td>2019-10-11 11:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>41080.0</td>\n",
       "      <td>196.480105</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42057</th>\n",
       "      <td>2019-10-11T11:59:53</td>\n",
       "      <td>2019-10-11 12:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>40976.0</td>\n",
       "      <td>193.595245</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42058</th>\n",
       "      <td>2019-10-11T12:59:52</td>\n",
       "      <td>2019-10-11 12:45:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>41435.0</td>\n",
       "      <td>196.801994</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42059 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               CaptureTime          SensorTime  Stage  Discharge  RiverArea  \\\n",
       "0      2012-06-09T13:09:07 2012-06-09 13:15:00   2.99      916.0    49975.0   \n",
       "1      2012-06-09T13:10:29 2012-06-09 13:15:00   2.99      916.0    50184.0   \n",
       "2      2012-06-09T13:44:01 2012-06-09 13:45:00   2.96      873.0    50543.0   \n",
       "3      2012-06-09T14:44:30 2012-06-09 14:45:00   2.94      846.0    50856.0   \n",
       "4      2012-06-09T15:44:59 2012-06-09 15:45:00   2.94      846.0    51004.0   \n",
       "...                    ...                 ...    ...        ...        ...   \n",
       "42054  2019-10-11T08:59:53 2019-10-11 09:00:00   2.54      434.0    45842.0   \n",
       "42055  2019-10-11T09:59:52 2019-10-11 10:00:00   2.54      434.0    42300.0   \n",
       "42056  2019-10-11T10:59:52 2019-10-11 11:00:00   2.54      434.0    41080.0   \n",
       "42057  2019-10-11T11:59:53 2019-10-11 12:00:00   2.54      434.0    40976.0   \n",
       "42058  2019-10-11T12:59:52 2019-10-11 12:45:00   2.54      434.0    41435.0   \n",
       "\n",
       "       RiverWidth  Month  Season  Year  \n",
       "0      207.508733      6  spring  2012  \n",
       "1      208.663145      6  spring  2012  \n",
       "2      209.445067      6  spring  2012  \n",
       "3      211.265690      6  spring  2012  \n",
       "4      211.250274      6  spring  2012  \n",
       "...           ...    ...     ...   ...  \n",
       "42054  194.934605     10  autumn  2019  \n",
       "42055  194.762264     10  autumn  2019  \n",
       "42056  196.480105     10  autumn  2019  \n",
       "42057  193.595245     10  autumn  2019  \n",
       "42058  196.801994     10  autumn  2019  \n",
       "\n",
       "[42059 rows x 9 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"CaptureTime\", \"SensorTime\", \"Stage\", \"Discharge\", \"RiverArea\", \"RiverWidth\", \"Month\", \"Season\", \"Year\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaptureTime            object\n",
       "SensorTime     datetime64[ns]\n",
       "Stage                 float64\n",
       "Discharge             float64\n",
       "RiverArea             float64\n",
       "RiverWidth            float64\n",
       "Month                   int64\n",
       "Season               category\n",
       "Year                    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40148, 9)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.Stage > 0]\n",
    "df = df[df.Discharge > 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40142, 9)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.RiverWidth > 0]\n",
    "#df = df[df.Discharge > 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.Season != \"winter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaptureTime      0\n",
       "SensorTime       0\n",
       "Stage            0\n",
       "Discharge        0\n",
       "RiverArea        0\n",
       "RiverWidth       0\n",
       "Month            0\n",
       "Season         126\n",
       "Year             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide dataset to X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "df_train = df[(df.Year >= 2012) & (df.Year <= 2016)]\n",
    "df_train = df_train.iloc[np.random.permutation(len(df_train))]\n",
    "\n",
    "df_val = df[(df.Year >= 2017) & (df.Year <= 2017)]\n",
    "df_val = df_val.iloc[np.random.permutation(len(df_val))]\n",
    "\n",
    "df_test = df[(df.Year >= 2018) & (df.Year <= 2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])\n",
    "df_val = df_val.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])\n",
    "df_test = df_test.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = df_train[[\"Stage\"]].values\n",
    "X_train = df_train[[\"RiverWidth\", \"Month\"]].values\n",
    "\n",
    "y_val = df_train[[\"Stage\"]].values\n",
    "X_val = df_train[[\"RiverWidth\", \"Month\"]].values\n",
    "\n",
    "y_test = df_test[[\"Stage\"]].values\n",
    "X_test = df_test[[\"RiverWidth\", \"Month\"]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20304, 2)\n",
      "(20304, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "print(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(lr):\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.Input(shape=input_shape))\n",
    "  \n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "  model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "  \"\"\"model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(512, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(512, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(128, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(32, activation=\"tanh\"))\"\"\"\n",
    "\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(output_shape, activation = 'linear'))\n",
    "\n",
    "  \n",
    "  model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr), loss = 'mae', metrics = ['mse', tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mae', 'mape'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_builder(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date_actual = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/fit/\" + date_actual\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"model_weights/{date_actual}_mlp_best_weights.hdf5\",\n",
    "                               monitor='val_loss',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.9355 - mse: 1.8455 - rmse: 1.3585 - mae: 0.9355 - mape: 31.7379\n",
      "Epoch 1: val_loss improved from inf to 0.67933, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.9203 - mse: 1.7892 - rmse: 1.3376 - mae: 0.9203 - mape: 31.1113 - val_loss: 0.6793 - val_mse: 0.7825 - val_rmse: 0.8846 - val_mae: 0.6793 - val_mape: 23.8905 - lr: 0.0010\n",
      "Epoch 2/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.7348 - mse: 1.0381 - rmse: 1.0189 - mae: 0.7348 - mape: 23.8783\n",
      "Epoch 2: val_loss did not improve from 0.67933\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.7301 - mse: 1.0282 - rmse: 1.0140 - mae: 0.7301 - mape: 23.6984 - val_loss: 0.6893 - val_mse: 0.7934 - val_rmse: 0.8907 - val_mae: 0.6893 - val_mape: 24.6079 - lr: 0.0010\n",
      "Epoch 3/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.7220 - mse: 1.0010 - rmse: 1.0005 - mae: 0.7220 - mape: 23.3378\n",
      "Epoch 3: val_loss improved from 0.67933 to 0.64661, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.7209 - mse: 0.9971 - rmse: 0.9986 - mae: 0.7209 - mape: 23.3442 - val_loss: 0.6466 - val_mse: 0.7789 - val_rmse: 0.8826 - val_mae: 0.6466 - val_mape: 21.1963 - lr: 0.0010\n",
      "Epoch 4/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.6368 - mse: 0.8672 - rmse: 0.9313 - mae: 0.6368 - mape: 19.6581\n",
      "Epoch 4: val_loss improved from 0.64661 to 0.60459, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.6375 - mse: 0.8667 - rmse: 0.9310 - mae: 0.6375 - mape: 19.6721 - val_loss: 0.6046 - val_mse: 0.8767 - val_rmse: 0.9363 - val_mae: 0.6046 - val_mape: 17.6092 - lr: 0.0010\n",
      "Epoch 5/2000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.5812 - mse: 0.7811 - rmse: 0.8838 - mae: 0.5812 - mape: 17.8541\n",
      "Epoch 5: val_loss improved from 0.60459 to 0.59620, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.5723 - mse: 0.7651 - rmse: 0.8747 - mae: 0.5723 - mape: 17.6313 - val_loss: 0.5962 - val_mse: 0.6852 - val_rmse: 0.8278 - val_mae: 0.5962 - val_mape: 19.8224 - lr: 0.0010\n",
      "Epoch 6/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.5109 - mse: 0.6682 - rmse: 0.8174 - mae: 0.5109 - mape: 15.9812\n",
      "Epoch 6: val_loss improved from 0.59620 to 0.51496, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.5099 - mse: 0.6694 - rmse: 0.8182 - mae: 0.5099 - mape: 15.9313 - val_loss: 0.5150 - val_mse: 0.7338 - val_rmse: 0.8566 - val_mae: 0.5150 - val_mape: 15.1182 - lr: 0.0010\n",
      "Epoch 7/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.4938 - mse: 0.6491 - rmse: 0.8057 - mae: 0.4938 - mape: 15.4061\n",
      "Epoch 7: val_loss improved from 0.51496 to 0.45020, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.4884 - mse: 0.6430 - rmse: 0.8019 - mae: 0.4884 - mape: 15.1900 - val_loss: 0.4502 - val_mse: 0.6057 - val_rmse: 0.7783 - val_mae: 0.4502 - val_mape: 13.6133 - lr: 0.0010\n",
      "Epoch 8/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.4650 - mse: 0.6317 - rmse: 0.7948 - mae: 0.4650 - mape: 14.1542\n",
      "Epoch 8: val_loss improved from 0.45020 to 0.42974, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.4650 - mse: 0.6317 - rmse: 0.7948 - mae: 0.4650 - mape: 14.1542 - val_loss: 0.4297 - val_mse: 0.5869 - val_rmse: 0.7661 - val_mae: 0.4297 - val_mape: 12.8660 - lr: 0.0010\n",
      "Epoch 9/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.4483 - mse: 0.6055 - rmse: 0.7781 - mae: 0.4483 - mape: 13.5897\n",
      "Epoch 9: val_loss did not improve from 0.42974\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.4481 - mse: 0.6054 - rmse: 0.7781 - mae: 0.4481 - mape: 13.5796 - val_loss: 0.4462 - val_mse: 0.7121 - val_rmse: 0.8439 - val_mae: 0.4462 - val_mape: 12.5547 - lr: 0.0010\n",
      "Epoch 10/2000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.4518 - mse: 0.6057 - rmse: 0.7783 - mae: 0.4518 - mape: 13.7582\n",
      "Epoch 10: val_loss did not improve from 0.42974\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4503 - mse: 0.6044 - rmse: 0.7774 - mae: 0.4503 - mape: 13.7093 - val_loss: 0.5079 - val_mse: 0.7714 - val_rmse: 0.8783 - val_mae: 0.5079 - val_mape: 14.3428 - lr: 0.0010\n",
      "Epoch 11/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.4373 - mse: 0.6073 - rmse: 0.7793 - mae: 0.4373 - mape: 13.1044\n",
      "Epoch 11: val_loss improved from 0.42974 to 0.41707, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.4359 - mse: 0.6037 - rmse: 0.7770 - mae: 0.4359 - mape: 13.0354 - val_loss: 0.4171 - val_mse: 0.5486 - val_rmse: 0.7407 - val_mae: 0.4171 - val_mape: 12.6307 - lr: 0.0010\n",
      "Epoch 12/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.4291 - mse: 0.5853 - rmse: 0.7651 - mae: 0.4291 - mape: 12.8828\n",
      "Epoch 12: val_loss did not improve from 0.41707\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.4292 - mse: 0.5850 - rmse: 0.7648 - mae: 0.4292 - mape: 12.8908 - val_loss: 0.4772 - val_mse: 0.5154 - val_rmse: 0.7179 - val_mae: 0.4772 - val_mape: 16.2468 - lr: 0.0010\n",
      "Epoch 13/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.4262 - mse: 0.5810 - rmse: 0.7622 - mae: 0.4262 - mape: 12.8217\n",
      "Epoch 13: val_loss did not improve from 0.41707\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4268 - mse: 0.5796 - rmse: 0.7613 - mae: 0.4268 - mape: 12.8856 - val_loss: 0.4406 - val_mse: 0.5694 - val_rmse: 0.7546 - val_mae: 0.4406 - val_mape: 13.5248 - lr: 0.0010\n",
      "Epoch 14/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.4273 - mse: 0.5815 - rmse: 0.7626 - mae: 0.4273 - mape: 12.8906\n",
      "Epoch 14: val_loss improved from 0.41707 to 0.41490, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4263 - mse: 0.5794 - rmse: 0.7612 - mae: 0.4263 - mape: 12.8582 - val_loss: 0.4149 - val_mse: 0.6048 - val_rmse: 0.7777 - val_mae: 0.4149 - val_mape: 12.0971 - lr: 0.0010\n",
      "Epoch 15/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.4164 - mse: 0.5646 - rmse: 0.7514 - mae: 0.4164 - mape: 12.5702\n",
      "Epoch 15: val_loss did not improve from 0.41490\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4194 - mse: 0.5707 - rmse: 0.7554 - mae: 0.4194 - mape: 12.6491 - val_loss: 0.4496 - val_mse: 0.5010 - val_rmse: 0.7078 - val_mae: 0.4496 - val_mape: 15.0352 - lr: 0.0010\n",
      "Epoch 16/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.4171 - mse: 0.5600 - rmse: 0.7483 - mae: 0.4171 - mape: 12.6245\n",
      "Epoch 16: val_loss improved from 0.41490 to 0.39849, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4167 - mse: 0.5581 - rmse: 0.7471 - mae: 0.4167 - mape: 12.6318 - val_loss: 0.3985 - val_mse: 0.5703 - val_rmse: 0.7552 - val_mae: 0.3985 - val_mape: 11.6439 - lr: 0.0010\n",
      "Epoch 17/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.4144 - mse: 0.5657 - rmse: 0.7521 - mae: 0.4144 - mape: 12.4692\n",
      "Epoch 17: val_loss did not improve from 0.39849\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4127 - mse: 0.5615 - rmse: 0.7493 - mae: 0.4127 - mape: 12.4197 - val_loss: 0.4257 - val_mse: 0.6115 - val_rmse: 0.7820 - val_mae: 0.4257 - val_mape: 12.4161 - lr: 0.0010\n",
      "Epoch 18/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.4161 - mse: 0.5634 - rmse: 0.7506 - mae: 0.4161 - mape: 12.5800\n",
      "Epoch 18: val_loss improved from 0.39849 to 0.39295, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4165 - mse: 0.5637 - rmse: 0.7508 - mae: 0.4165 - mape: 12.6063 - val_loss: 0.3929 - val_mse: 0.5192 - val_rmse: 0.7205 - val_mae: 0.3929 - val_mape: 11.8826 - lr: 0.0010\n",
      "Epoch 19/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.4063 - mse: 0.5558 - rmse: 0.7455 - mae: 0.4063 - mape: 12.1770\n",
      "Epoch 19: val_loss did not improve from 0.39295\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4078 - mse: 0.5566 - rmse: 0.7461 - mae: 0.4078 - mape: 12.2206 - val_loss: 0.4005 - val_mse: 0.4983 - val_rmse: 0.7059 - val_mae: 0.4005 - val_mape: 12.3752 - lr: 0.0010\n",
      "Epoch 20/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.4020 - mse: 0.5425 - rmse: 0.7366 - mae: 0.4020 - mape: 12.1161\n",
      "Epoch 20: val_loss did not improve from 0.39295\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4034 - mse: 0.5469 - rmse: 0.7395 - mae: 0.4034 - mape: 12.1078 - val_loss: 0.4035 - val_mse: 0.4978 - val_rmse: 0.7055 - val_mae: 0.4035 - val_mape: 12.6487 - lr: 0.0010\n",
      "Epoch 21/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.4095 - mse: 0.5435 - rmse: 0.7373 - mae: 0.4095 - mape: 12.3798\n",
      "Epoch 21: val_loss did not improve from 0.39295\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4089 - mse: 0.5434 - rmse: 0.7371 - mae: 0.4089 - mape: 12.3979 - val_loss: 0.3966 - val_mse: 0.5713 - val_rmse: 0.7559 - val_mae: 0.3966 - val_mape: 11.6285 - lr: 0.0010\n",
      "Epoch 22/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.4199 - mse: 0.5862 - rmse: 0.7656 - mae: 0.4199 - mape: 12.5972\n",
      "Epoch 22: val_loss did not improve from 0.39295\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4183 - mse: 0.5794 - rmse: 0.7612 - mae: 0.4183 - mape: 12.5877 - val_loss: 0.4463 - val_mse: 0.5553 - val_rmse: 0.7452 - val_mae: 0.4463 - val_mape: 13.6456 - lr: 0.0010\n",
      "Epoch 23/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.4016 - mse: 0.5457 - rmse: 0.7387 - mae: 0.4016 - mape: 12.0347\n",
      "Epoch 23: val_loss did not improve from 0.39295\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4025 - mse: 0.5474 - rmse: 0.7398 - mae: 0.4025 - mape: 12.0654 - val_loss: 0.4185 - val_mse: 0.5835 - val_rmse: 0.7639 - val_mae: 0.4185 - val_mape: 12.6409 - lr: 0.0010\n",
      "Epoch 24/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.3930 - mse: 0.5158 - rmse: 0.7182 - mae: 0.3930 - mape: 11.9117\n",
      "Epoch 24: val_loss improved from 0.39295 to 0.39017, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3920 - mse: 0.5132 - rmse: 0.7164 - mae: 0.3920 - mape: 11.9038 - val_loss: 0.3902 - val_mse: 0.4741 - val_rmse: 0.6885 - val_mae: 0.3902 - val_mape: 12.1553 - lr: 0.0010\n",
      "Epoch 25/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.3953 - mse: 0.5217 - rmse: 0.7223 - mae: 0.3953 - mape: 12.0080\n",
      "Epoch 25: val_loss did not improve from 0.39017\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3954 - mse: 0.5225 - rmse: 0.7228 - mae: 0.3954 - mape: 12.0021 - val_loss: 0.4336 - val_mse: 0.6269 - val_rmse: 0.7918 - val_mae: 0.4336 - val_mape: 12.8466 - lr: 0.0010\n",
      "Epoch 26/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.4037 - mse: 0.5433 - rmse: 0.7371 - mae: 0.4037 - mape: 12.1365\n",
      "Epoch 26: val_loss did not improve from 0.39017\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4028 - mse: 0.5436 - rmse: 0.7373 - mae: 0.4028 - mape: 12.1264 - val_loss: 0.4612 - val_mse: 0.7441 - val_rmse: 0.8626 - val_mae: 0.4612 - val_mape: 13.2072 - lr: 0.0010\n",
      "Epoch 27/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.4043 - mse: 0.5367 - rmse: 0.7326 - mae: 0.4043 - mape: 12.2673\n",
      "Epoch 27: val_loss did not improve from 0.39017\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.4075 - mse: 0.5445 - rmse: 0.7379 - mae: 0.4075 - mape: 12.3444 - val_loss: 0.4189 - val_mse: 0.6240 - val_rmse: 0.7899 - val_mae: 0.4189 - val_mape: 12.2872 - lr: 0.0010\n",
      "Epoch 28/2000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.3890 - mse: 0.5053 - rmse: 0.7108 - mae: 0.3890 - mape: 11.8824\n",
      "Epoch 28: val_loss improved from 0.39017 to 0.38269, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3860 - mse: 0.4960 - rmse: 0.7043 - mae: 0.3860 - mape: 11.8130 - val_loss: 0.3827 - val_mse: 0.5200 - val_rmse: 0.7211 - val_mae: 0.3827 - val_mape: 11.3926 - lr: 0.0010\n",
      "Epoch 29/2000\n",
      "271/318 [========================>.....] - ETA: 0s - loss: 0.3969 - mse: 0.5230 - rmse: 0.7232 - mae: 0.3969 - mape: 12.1024\n",
      "Epoch 29: val_loss did not improve from 0.38269\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3951 - mse: 0.5163 - rmse: 0.7185 - mae: 0.3951 - mape: 12.0783 - val_loss: 0.3915 - val_mse: 0.4990 - val_rmse: 0.7064 - val_mae: 0.3915 - val_mape: 11.8105 - lr: 0.0010\n",
      "Epoch 30/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3869 - mse: 0.5028 - rmse: 0.7091 - mae: 0.3869 - mape: 11.8209\n",
      "Epoch 30: val_loss did not improve from 0.38269\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3869 - mse: 0.5034 - rmse: 0.7095 - mae: 0.3869 - mape: 11.8196 - val_loss: 0.4310 - val_mse: 0.7168 - val_rmse: 0.8466 - val_mae: 0.4310 - val_mape: 11.9200 - lr: 0.0010\n",
      "Epoch 31/2000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.3858 - mse: 0.4906 - rmse: 0.7005 - mae: 0.3858 - mape: 11.8666\n",
      "Epoch 31: val_loss did not improve from 0.38269\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3880 - mse: 0.4934 - rmse: 0.7024 - mae: 0.3880 - mape: 11.9222 - val_loss: 0.4537 - val_mse: 0.7203 - val_rmse: 0.8487 - val_mae: 0.4537 - val_mape: 13.3532 - lr: 0.0010\n",
      "Epoch 32/2000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.3812 - mse: 0.4828 - rmse: 0.6948 - mae: 0.3812 - mape: 11.7502\n",
      "Epoch 32: val_loss did not improve from 0.38269\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3814 - mse: 0.4788 - rmse: 0.6919 - mae: 0.3814 - mape: 11.7752 - val_loss: 0.4090 - val_mse: 0.4353 - val_rmse: 0.6598 - val_mae: 0.4090 - val_mape: 13.5530 - lr: 0.0010\n",
      "Epoch 33/2000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.3763 - mse: 0.4658 - rmse: 0.6825 - mae: 0.3763 - mape: 11.6580\n",
      "Epoch 33: val_loss improved from 0.38269 to 0.37046, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3774 - mse: 0.4708 - rmse: 0.6862 - mae: 0.3774 - mape: 11.6565 - val_loss: 0.3705 - val_mse: 0.4600 - val_rmse: 0.6783 - val_mae: 0.3705 - val_mape: 11.3585 - lr: 0.0010\n",
      "Epoch 34/2000\n",
      "271/318 [========================>.....] - ETA: 0s - loss: 0.3831 - mse: 0.5006 - rmse: 0.7075 - mae: 0.3831 - mape: 11.6768\n",
      "Epoch 34: val_loss did not improve from 0.37046\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3861 - mse: 0.5041 - rmse: 0.7100 - mae: 0.3861 - mape: 11.7833 - val_loss: 0.3938 - val_mse: 0.5003 - val_rmse: 0.7073 - val_mae: 0.3938 - val_mape: 12.2329 - lr: 0.0010\n",
      "Epoch 35/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.3796 - mse: 0.4828 - rmse: 0.6949 - mae: 0.3796 - mape: 11.6169\n",
      "Epoch 35: val_loss did not improve from 0.37046\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3776 - mse: 0.4787 - rmse: 0.6919 - mae: 0.3776 - mape: 11.5893 - val_loss: 0.3853 - val_mse: 0.4813 - val_rmse: 0.6937 - val_mae: 0.3853 - val_mape: 11.9809 - lr: 0.0010\n",
      "Epoch 36/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.3819 - mse: 0.5017 - rmse: 0.7083 - mae: 0.3819 - mape: 11.6325\n",
      "Epoch 36: val_loss did not improve from 0.37046\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3833 - mse: 0.5014 - rmse: 0.7081 - mae: 0.3833 - mape: 11.6805 - val_loss: 0.3861 - val_mse: 0.5618 - val_rmse: 0.7496 - val_mae: 0.3861 - val_mape: 11.4726 - lr: 0.0010\n",
      "Epoch 37/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.3919 - mse: 0.5263 - rmse: 0.7255 - mae: 0.3919 - mape: 11.8227\n",
      "Epoch 37: val_loss did not improve from 0.37046\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3912 - mse: 0.5351 - rmse: 0.7315 - mae: 0.3912 - mape: 11.7178 - val_loss: 0.4027 - val_mse: 0.6277 - val_rmse: 0.7923 - val_mae: 0.4027 - val_mape: 11.2631 - lr: 0.0010\n",
      "Epoch 38/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.3918 - mse: 0.5223 - rmse: 0.7227 - mae: 0.3918 - mape: 11.8428\n",
      "Epoch 38: val_loss did not improve from 0.37046\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3893 - mse: 0.5152 - rmse: 0.7178 - mae: 0.3893 - mape: 11.7930 - val_loss: 0.3731 - val_mse: 0.4752 - val_rmse: 0.6893 - val_mae: 0.3731 - val_mape: 11.4145 - lr: 0.0010\n",
      "Epoch 39/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.3733 - mse: 0.4653 - rmse: 0.6821 - mae: 0.3733 - mape: 11.5458\n",
      "Epoch 39: val_loss improved from 0.37046 to 0.36373, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3755 - mse: 0.4678 - rmse: 0.6839 - mae: 0.3755 - mape: 11.6219 - val_loss: 0.3637 - val_mse: 0.4446 - val_rmse: 0.6668 - val_mae: 0.3637 - val_mape: 11.2631 - lr: 0.0010\n",
      "Epoch 40/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.3985 - mse: 0.5525 - rmse: 0.7433 - mae: 0.3985 - mape: 11.9609\n",
      "Epoch 40: val_loss did not improve from 0.36373\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3929 - mse: 0.5349 - rmse: 0.7314 - mae: 0.3929 - mape: 11.8416 - val_loss: 0.3854 - val_mse: 0.4398 - val_rmse: 0.6632 - val_mae: 0.3854 - val_mape: 12.4710 - lr: 0.0010\n",
      "Epoch 41/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.3734 - mse: 0.4621 - rmse: 0.6798 - mae: 0.3734 - mape: 11.5546\n",
      "Epoch 41: val_loss did not improve from 0.36373\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3719 - mse: 0.4626 - rmse: 0.6801 - mae: 0.3719 - mape: 11.5036 - val_loss: 0.3728 - val_mse: 0.4642 - val_rmse: 0.6813 - val_mae: 0.3728 - val_mape: 11.3743 - lr: 0.0010\n",
      "Epoch 42/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.3698 - mse: 0.4535 - rmse: 0.6735 - mae: 0.3698 - mape: 11.4913\n",
      "Epoch 42: val_loss did not improve from 0.36373\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3738 - mse: 0.4648 - rmse: 0.6818 - mae: 0.3738 - mape: 11.5921 - val_loss: 0.4383 - val_mse: 0.7336 - val_rmse: 0.8565 - val_mae: 0.4383 - val_mape: 12.3814 - lr: 0.0010\n",
      "Epoch 43/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.3762 - mse: 0.4808 - rmse: 0.6934 - mae: 0.3762 - mape: 11.5385\n",
      "Epoch 43: val_loss did not improve from 0.36373\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3794 - mse: 0.4865 - rmse: 0.6975 - mae: 0.3794 - mape: 11.6423 - val_loss: 0.3883 - val_mse: 0.5491 - val_rmse: 0.7410 - val_mae: 0.3883 - val_mape: 11.5569 - lr: 0.0010\n",
      "Epoch 44/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.3908 - mse: 0.5402 - rmse: 0.7350 - mae: 0.3908 - mape: 11.6451\n",
      "Epoch 44: val_loss did not improve from 0.36373\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3899 - mse: 0.5336 - rmse: 0.7304 - mae: 0.3899 - mape: 11.6674 - val_loss: 0.3766 - val_mse: 0.4757 - val_rmse: 0.6897 - val_mae: 0.3766 - val_mape: 11.4655 - lr: 0.0010\n",
      "Epoch 45/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.3852 - mse: 0.5080 - rmse: 0.7128 - mae: 0.3852 - mape: 11.7003\n",
      "Epoch 45: val_loss did not improve from 0.36373\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3845 - mse: 0.5048 - rmse: 0.7105 - mae: 0.3845 - mape: 11.7002 - val_loss: 0.4012 - val_mse: 0.4930 - val_rmse: 0.7021 - val_mae: 0.4012 - val_mape: 12.4515 - lr: 0.0010\n",
      "Epoch 46/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.3790 - mse: 0.4727 - rmse: 0.6875 - mae: 0.3790 - mape: 11.8156\n",
      "Epoch 46: val_loss did not improve from 0.36373\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3776 - mse: 0.4702 - rmse: 0.6857 - mae: 0.3776 - mape: 11.7670 - val_loss: 0.3797 - val_mse: 0.4295 - val_rmse: 0.6554 - val_mae: 0.3797 - val_mape: 12.1105 - lr: 0.0010\n",
      "Epoch 47/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.3609 - mse: 0.4327 - rmse: 0.6578 - mae: 0.3609 - mape: 11.3462\n",
      "Epoch 47: val_loss did not improve from 0.36373\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3609 - mse: 0.4332 - rmse: 0.6582 - mae: 0.3609 - mape: 11.3129 - val_loss: 0.3662 - val_mse: 0.4092 - val_rmse: 0.6397 - val_mae: 0.3662 - val_mape: 11.8139 - lr: 0.0010\n",
      "Epoch 48/2000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.3739 - mse: 0.4672 - rmse: 0.6835 - mae: 0.3739 - mape: 11.6092\n",
      "Epoch 48: val_loss did not improve from 0.36373\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3730 - mse: 0.4623 - rmse: 0.6800 - mae: 0.3730 - mape: 11.6280 - val_loss: 0.3796 - val_mse: 0.4746 - val_rmse: 0.6889 - val_mae: 0.3796 - val_mape: 12.0368 - lr: 0.0010\n",
      "Epoch 49/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.3696 - mse: 0.4515 - rmse: 0.6719 - mae: 0.3696 - mape: 11.5861\n",
      "Epoch 49: val_loss improved from 0.36373 to 0.34942, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3669 - mse: 0.4451 - rmse: 0.6671 - mae: 0.3669 - mape: 11.5206 - val_loss: 0.3494 - val_mse: 0.4055 - val_rmse: 0.6368 - val_mae: 0.3494 - val_mape: 10.9486 - lr: 0.0010\n",
      "Epoch 50/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.3608 - mse: 0.4341 - rmse: 0.6589 - mae: 0.3608 - mape: 11.3089\n",
      "Epoch 50: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3608 - mse: 0.4331 - rmse: 0.6581 - mae: 0.3608 - mape: 11.3175 - val_loss: 0.3535 - val_mse: 0.3839 - val_rmse: 0.6196 - val_mae: 0.3535 - val_mape: 11.4883 - lr: 0.0010\n",
      "Epoch 51/2000\n",
      "271/318 [========================>.....] - ETA: 0s - loss: 0.3532 - mse: 0.4132 - rmse: 0.6428 - mae: 0.3532 - mape: 11.1678\n",
      "Epoch 51: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3610 - mse: 0.4399 - rmse: 0.6632 - mae: 0.3610 - mape: 11.3136 - val_loss: 0.4243 - val_mse: 0.7246 - val_rmse: 0.8512 - val_mae: 0.4243 - val_mape: 11.5844 - lr: 0.0010\n",
      "Epoch 52/2000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.3720 - mse: 0.4642 - rmse: 0.6813 - mae: 0.3720 - mape: 11.6426\n",
      "Epoch 52: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3707 - mse: 0.4627 - rmse: 0.6802 - mae: 0.3707 - mape: 11.5706 - val_loss: 0.3842 - val_mse: 0.5801 - val_rmse: 0.7616 - val_mae: 0.3842 - val_mape: 11.3419 - lr: 0.0010\n",
      "Epoch 53/2000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.3568 - mse: 0.4284 - rmse: 0.6545 - mae: 0.3568 - mape: 11.2321\n",
      "Epoch 53: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3573 - mse: 0.4297 - rmse: 0.6555 - mae: 0.3573 - mape: 11.2402 - val_loss: 0.3755 - val_mse: 0.4268 - val_rmse: 0.6533 - val_mae: 0.3755 - val_mape: 11.5763 - lr: 0.0010\n",
      "Epoch 54/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.3579 - mse: 0.4279 - rmse: 0.6542 - mae: 0.3579 - mape: 11.1051\n",
      "Epoch 54: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3640 - mse: 0.4564 - rmse: 0.6755 - mae: 0.3640 - mape: 11.1252 - val_loss: 0.4169 - val_mse: 0.6325 - val_rmse: 0.7953 - val_mae: 0.4169 - val_mape: 12.6343 - lr: 0.0010\n",
      "Epoch 55/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.3729 - mse: 0.4652 - rmse: 0.6821 - mae: 0.3729 - mape: 11.6456\n",
      "Epoch 55: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3737 - mse: 0.4645 - rmse: 0.6816 - mae: 0.3737 - mape: 11.6808 - val_loss: 0.3562 - val_mse: 0.4343 - val_rmse: 0.6590 - val_mae: 0.3562 - val_mape: 11.0780 - lr: 0.0010\n",
      "Epoch 56/2000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.3735 - mse: 0.4719 - rmse: 0.6870 - mae: 0.3735 - mape: 11.6650\n",
      "Epoch 56: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3780 - mse: 0.4891 - rmse: 0.6994 - mae: 0.3780 - mape: 11.6418 - val_loss: 0.3963 - val_mse: 0.5638 - val_rmse: 0.7509 - val_mae: 0.3963 - val_mape: 11.7644 - lr: 0.0010\n",
      "Epoch 57/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.3694 - mse: 0.4692 - rmse: 0.6850 - mae: 0.3694 - mape: 11.4968\n",
      "Epoch 57: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3682 - mse: 0.4653 - rmse: 0.6821 - mae: 0.3682 - mape: 11.4606 - val_loss: 0.3685 - val_mse: 0.4140 - val_rmse: 0.6434 - val_mae: 0.3685 - val_mape: 12.0785 - lr: 0.0010\n",
      "Epoch 58/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.3597 - mse: 0.4215 - rmse: 0.6493 - mae: 0.3597 - mape: 11.4458\n",
      "Epoch 58: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3606 - mse: 0.4321 - rmse: 0.6573 - mae: 0.3606 - mape: 11.4123 - val_loss: 0.4252 - val_mse: 0.5636 - val_rmse: 0.7507 - val_mae: 0.4252 - val_mape: 13.1040 - lr: 0.0010\n",
      "Epoch 59/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.3834 - mse: 0.5241 - rmse: 0.7239 - mae: 0.3834 - mape: 11.6402\n",
      "Epoch 59: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3871 - mse: 0.5265 - rmse: 0.7256 - mae: 0.3871 - mape: 11.7915 - val_loss: 0.3866 - val_mse: 0.4651 - val_rmse: 0.6820 - val_mae: 0.3866 - val_mape: 12.7831 - lr: 0.0010\n",
      "Epoch 60/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.3840 - mse: 0.5072 - rmse: 0.7122 - mae: 0.3840 - mape: 11.9667\n",
      "Epoch 60: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3854 - mse: 0.5106 - rmse: 0.7146 - mae: 0.3854 - mape: 12.0199 - val_loss: 0.4222 - val_mse: 0.5124 - val_rmse: 0.7158 - val_mae: 0.4222 - val_mape: 14.4078 - lr: 0.0010\n",
      "Epoch 61/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.3811 - mse: 0.4998 - rmse: 0.7069 - mae: 0.3811 - mape: 11.9080\n",
      "Epoch 61: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3777 - mse: 0.4902 - rmse: 0.7002 - mae: 0.3777 - mape: 11.8305 - val_loss: 0.3571 - val_mse: 0.4429 - val_rmse: 0.6655 - val_mae: 0.3571 - val_mape: 11.2607 - lr: 0.0010\n",
      "Epoch 62/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.3767 - mse: 0.5183 - rmse: 0.7199 - mae: 0.3767 - mape: 11.5215\n",
      "Epoch 62: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3766 - mse: 0.5157 - rmse: 0.7181 - mae: 0.3766 - mape: 11.5382 - val_loss: 0.3778 - val_mse: 0.4861 - val_rmse: 0.6972 - val_mae: 0.3778 - val_mape: 11.5432 - lr: 0.0010\n",
      "Epoch 63/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.3659 - mse: 0.4599 - rmse: 0.6782 - mae: 0.3659 - mape: 11.5536\n",
      "Epoch 63: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3658 - mse: 0.4600 - rmse: 0.6783 - mae: 0.3658 - mape: 11.5404 - val_loss: 0.3626 - val_mse: 0.4293 - val_rmse: 0.6552 - val_mae: 0.3626 - val_mape: 11.7844 - lr: 0.0010\n",
      "Epoch 64/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.3620 - mse: 0.4499 - rmse: 0.6708 - mae: 0.3620 - mape: 11.4269\n",
      "Epoch 64: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3621 - mse: 0.4485 - rmse: 0.6697 - mae: 0.3621 - mape: 11.4362 - val_loss: 0.3518 - val_mse: 0.4168 - val_rmse: 0.6456 - val_mae: 0.3518 - val_mape: 11.4054 - lr: 0.0010\n",
      "Epoch 65/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.3624 - mse: 0.4414 - rmse: 0.6644 - mae: 0.3624 - mape: 11.4483\n",
      "Epoch 65: val_loss did not improve from 0.34942\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3620 - mse: 0.4411 - rmse: 0.6641 - mae: 0.3620 - mape: 11.4338 - val_loss: 0.3569 - val_mse: 0.4311 - val_rmse: 0.6566 - val_mae: 0.3569 - val_mape: 11.3541 - lr: 0.0010\n",
      "Epoch 66/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.3592 - mse: 0.4331 - rmse: 0.6581 - mae: 0.3592 - mape: 11.3282\n",
      "Epoch 66: val_loss improved from 0.34942 to 0.34400, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3563 - mse: 0.4262 - rmse: 0.6529 - mae: 0.3563 - mape: 11.2762 - val_loss: 0.3440 - val_mse: 0.3975 - val_rmse: 0.6305 - val_mae: 0.3440 - val_mape: 10.8348 - lr: 0.0010\n",
      "Epoch 67/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.3593 - mse: 0.4266 - rmse: 0.6532 - mae: 0.3593 - mape: 11.2948\n",
      "Epoch 67: val_loss did not improve from 0.34400\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3572 - mse: 0.4206 - rmse: 0.6485 - mae: 0.3572 - mape: 11.2284 - val_loss: 0.3514 - val_mse: 0.4030 - val_rmse: 0.6349 - val_mae: 0.3514 - val_mape: 11.0764 - lr: 0.0010\n",
      "Epoch 68/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.3426 - mse: 0.3908 - rmse: 0.6252 - mae: 0.3426 - mape: 10.7502\n",
      "Epoch 68: val_loss did not improve from 0.34400\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3436 - mse: 0.3932 - rmse: 0.6271 - mae: 0.3436 - mape: 10.7727 - val_loss: 0.3557 - val_mse: 0.4094 - val_rmse: 0.6398 - val_mae: 0.3557 - val_mape: 10.7363 - lr: 0.0010\n",
      "Epoch 69/2000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.3481 - mse: 0.3974 - rmse: 0.6304 - mae: 0.3481 - mape: 10.8807\n",
      "Epoch 69: val_loss improved from 0.34400 to 0.32969, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3467 - mse: 0.3975 - rmse: 0.6304 - mae: 0.3467 - mape: 10.7987 - val_loss: 0.3297 - val_mse: 0.3663 - val_rmse: 0.6052 - val_mae: 0.3297 - val_mape: 9.6970 - lr: 0.0010\n",
      "Epoch 70/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3556 - mse: 0.4031 - rmse: 0.6349 - mae: 0.3556 - mape: 10.9638\n",
      "Epoch 70: val_loss did not improve from 0.32969\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3556 - mse: 0.4026 - rmse: 0.6345 - mae: 0.3556 - mape: 10.9698 - val_loss: 0.3547 - val_mse: 0.3979 - val_rmse: 0.6308 - val_mae: 0.3547 - val_mape: 11.4647 - lr: 0.0010\n",
      "Epoch 71/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.3373 - mse: 0.3534 - rmse: 0.5945 - mae: 0.3373 - mape: 10.3325\n",
      "Epoch 71: val_loss did not improve from 0.32969\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3395 - mse: 0.3624 - rmse: 0.6020 - mae: 0.3395 - mape: 10.3783 - val_loss: 0.4006 - val_mse: 0.6311 - val_rmse: 0.7944 - val_mae: 0.4006 - val_mape: 11.5481 - lr: 0.0010\n",
      "Epoch 72/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.3374 - mse: 0.3643 - rmse: 0.6036 - mae: 0.3374 - mape: 10.3796\n",
      "Epoch 72: val_loss improved from 0.32969 to 0.30251, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3365 - mse: 0.3621 - rmse: 0.6017 - mae: 0.3365 - mape: 10.3367 - val_loss: 0.3025 - val_mse: 0.3068 - val_rmse: 0.5539 - val_mae: 0.3025 - val_mape: 8.8518 - lr: 0.0010\n",
      "Epoch 73/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3241 - mse: 0.3355 - rmse: 0.5792 - mae: 0.3241 - mape: 9.9974\n",
      "Epoch 73: val_loss did not improve from 0.30251\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3240 - mse: 0.3354 - rmse: 0.5791 - mae: 0.3240 - mape: 9.9967 - val_loss: 0.3040 - val_mse: 0.3024 - val_rmse: 0.5499 - val_mae: 0.3040 - val_mape: 9.7701 - lr: 0.0010\n",
      "Epoch 74/2000\n",
      "271/318 [========================>.....] - ETA: 0s - loss: 0.3262 - mse: 0.3424 - rmse: 0.5852 - mae: 0.3262 - mape: 10.0804\n",
      "Epoch 74: val_loss did not improve from 0.30251\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3241 - mse: 0.3361 - rmse: 0.5797 - mae: 0.3241 - mape: 10.0141 - val_loss: 0.3354 - val_mse: 0.3609 - val_rmse: 0.6008 - val_mae: 0.3354 - val_mape: 10.1767 - lr: 0.0010\n",
      "Epoch 75/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.3175 - mse: 0.3147 - rmse: 0.5610 - mae: 0.3175 - mape: 9.8720\n",
      "Epoch 75: val_loss improved from 0.30251 to 0.29135, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3187 - mse: 0.3173 - rmse: 0.5633 - mae: 0.3187 - mape: 9.9088 - val_loss: 0.2913 - val_mse: 0.2808 - val_rmse: 0.5299 - val_mae: 0.2913 - val_mape: 8.9902 - lr: 0.0010\n",
      "Epoch 76/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.3344 - mse: 0.3820 - rmse: 0.6181 - mae: 0.3344 - mape: 10.2920\n",
      "Epoch 76: val_loss did not improve from 0.29135\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3347 - mse: 0.3832 - rmse: 0.6190 - mae: 0.3347 - mape: 10.3061 - val_loss: 0.4013 - val_mse: 0.6517 - val_rmse: 0.8073 - val_mae: 0.4013 - val_mape: 11.6764 - lr: 0.0010\n",
      "Epoch 77/2000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.3481 - mse: 0.4169 - rmse: 0.6457 - mae: 0.3481 - mape: 10.6645\n",
      "Epoch 77: val_loss did not improve from 0.29135\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3457 - mse: 0.4113 - rmse: 0.6414 - mae: 0.3457 - mape: 10.6277 - val_loss: 0.3202 - val_mse: 0.3531 - val_rmse: 0.5942 - val_mae: 0.3202 - val_mape: 9.7378 - lr: 0.0010\n",
      "Epoch 78/2000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.3473 - mse: 0.4297 - rmse: 0.6555 - mae: 0.3473 - mape: 10.5713\n",
      "Epoch 78: val_loss did not improve from 0.29135\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3453 - mse: 0.4238 - rmse: 0.6510 - mae: 0.3453 - mape: 10.4877 - val_loss: 0.3366 - val_mse: 0.3537 - val_rmse: 0.5947 - val_mae: 0.3366 - val_mape: 10.7114 - lr: 0.0010\n",
      "Epoch 79/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.3459 - mse: 0.4267 - rmse: 0.6532 - mae: 0.3459 - mape: 10.4752\n",
      "Epoch 79: val_loss did not improve from 0.29135\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3488 - mse: 0.4364 - rmse: 0.6606 - mae: 0.3488 - mape: 10.5725 - val_loss: 0.3775 - val_mse: 0.5772 - val_rmse: 0.7598 - val_mae: 0.3775 - val_mape: 11.2049 - lr: 0.0010\n",
      "Epoch 80/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.3813 - mse: 0.5486 - rmse: 0.7407 - mae: 0.3813 - mape: 11.3572\n",
      "Epoch 80: val_loss did not improve from 0.29135\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3803 - mse: 0.5459 - rmse: 0.7388 - mae: 0.3803 - mape: 11.3302 - val_loss: 0.3772 - val_mse: 0.4660 - val_rmse: 0.6826 - val_mae: 0.3772 - val_mape: 11.7831 - lr: 0.0010\n",
      "Epoch 81/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.3632 - mse: 0.4707 - rmse: 0.6860 - mae: 0.3632 - mape: 11.1257\n",
      "Epoch 81: val_loss did not improve from 0.29135\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3611 - mse: 0.4651 - rmse: 0.6820 - mae: 0.3611 - mape: 11.0396 - val_loss: 0.3516 - val_mse: 0.4270 - val_rmse: 0.6534 - val_mae: 0.3516 - val_mape: 10.9410 - lr: 0.0010\n",
      "Epoch 82/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.3548 - mse: 0.4342 - rmse: 0.6589 - mae: 0.3548 - mape: 10.9797\n",
      "Epoch 82: val_loss did not improve from 0.29135\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3538 - mse: 0.4327 - rmse: 0.6578 - mae: 0.3538 - mape: 10.9328 - val_loss: 0.3531 - val_mse: 0.4438 - val_rmse: 0.6662 - val_mae: 0.3531 - val_mape: 11.6282 - lr: 0.0010\n",
      "Epoch 83/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.3366 - mse: 0.3917 - rmse: 0.6259 - mae: 0.3366 - mape: 10.3350\n",
      "Epoch 83: val_loss did not improve from 0.29135\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3384 - mse: 0.3950 - rmse: 0.6285 - mae: 0.3384 - mape: 10.3560 - val_loss: 0.3260 - val_mse: 0.3509 - val_rmse: 0.5923 - val_mae: 0.3260 - val_mape: 10.2136 - lr: 0.0010\n",
      "Epoch 84/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.3380 - mse: 0.3768 - rmse: 0.6138 - mae: 0.3380 - mape: 10.5025\n",
      "Epoch 84: val_loss did not improve from 0.29135\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3380 - mse: 0.3766 - rmse: 0.6136 - mae: 0.3380 - mape: 10.5151 - val_loss: 0.3141 - val_mse: 0.3321 - val_rmse: 0.5763 - val_mae: 0.3141 - val_mape: 9.2847 - lr: 0.0010\n",
      "Epoch 85/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.3375 - mse: 0.3746 - rmse: 0.6121 - mae: 0.3375 - mape: 10.5034\n",
      "Epoch 85: val_loss did not improve from 0.29135\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3401 - mse: 0.3768 - rmse: 0.6138 - mae: 0.3401 - mape: 10.5788 - val_loss: 0.3463 - val_mse: 0.3329 - val_rmse: 0.5770 - val_mae: 0.3463 - val_mape: 11.4382 - lr: 0.0010\n",
      "Epoch 86/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.3369 - mse: 0.3742 - rmse: 0.6117 - mae: 0.3369 - mape: 10.4861\n",
      "Epoch 86: val_loss did not improve from 0.29135\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3375 - mse: 0.3713 - rmse: 0.6093 - mae: 0.3375 - mape: 10.5225 - val_loss: 0.3184 - val_mse: 0.3350 - val_rmse: 0.5788 - val_mae: 0.3184 - val_mape: 9.6193 - lr: 0.0010\n",
      "Epoch 87/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.3364 - mse: 0.3751 - rmse: 0.6125 - mae: 0.3364 - mape: 10.3439\n",
      "Epoch 87: val_loss did not improve from 0.29135\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3364 - mse: 0.3729 - rmse: 0.6107 - mae: 0.3364 - mape: 10.3837 - val_loss: 0.2992 - val_mse: 0.2816 - val_rmse: 0.5307 - val_mae: 0.2992 - val_mape: 8.9987 - lr: 0.0010\n",
      "Epoch 88/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.3097 - mse: 0.3074 - rmse: 0.5544 - mae: 0.3097 - mape: 9.7205\n",
      "Epoch 88: val_loss did not improve from 0.29135\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3104 - mse: 0.3106 - rmse: 0.5573 - mae: 0.3104 - mape: 9.7020 - val_loss: 0.2958 - val_mse: 0.2625 - val_rmse: 0.5124 - val_mae: 0.2958 - val_mape: 9.6117 - lr: 0.0010\n",
      "Epoch 89/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.3113 - mse: 0.3113 - rmse: 0.5579 - mae: 0.3113 - mape: 9.7307\n",
      "Epoch 89: val_loss improved from 0.29135 to 0.28459, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3113 - mse: 0.3107 - rmse: 0.5574 - mae: 0.3113 - mape: 9.7371 - val_loss: 0.2846 - val_mse: 0.2632 - val_rmse: 0.5130 - val_mae: 0.2846 - val_mape: 8.9404 - lr: 0.0010\n",
      "Epoch 90/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.3019 - mse: 0.2944 - rmse: 0.5426 - mae: 0.3019 - mape: 9.5703\n",
      "Epoch 90: val_loss did not improve from 0.28459\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3030 - mse: 0.2972 - rmse: 0.5452 - mae: 0.3030 - mape: 9.5752 - val_loss: 0.3038 - val_mse: 0.2972 - val_rmse: 0.5451 - val_mae: 0.3038 - val_mape: 9.6611 - lr: 0.0010\n",
      "Epoch 91/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.3116 - mse: 0.3207 - rmse: 0.5663 - mae: 0.3116 - mape: 9.7706\n",
      "Epoch 91: val_loss did not improve from 0.28459\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3126 - mse: 0.3208 - rmse: 0.5664 - mae: 0.3126 - mape: 9.7888 - val_loss: 0.2999 - val_mse: 0.2727 - val_rmse: 0.5222 - val_mae: 0.2999 - val_mape: 9.4776 - lr: 0.0010\n",
      "Epoch 92/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2973 - mse: 0.2875 - rmse: 0.5361 - mae: 0.2973 - mape: 9.3560\n",
      "Epoch 92: val_loss did not improve from 0.28459\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2975 - mse: 0.2876 - rmse: 0.5363 - mae: 0.2975 - mape: 9.3617 - val_loss: 0.2865 - val_mse: 0.2693 - val_rmse: 0.5189 - val_mae: 0.2865 - val_mape: 9.4604 - lr: 0.0010\n",
      "Epoch 93/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.3061 - mse: 0.3087 - rmse: 0.5556 - mae: 0.3061 - mape: 9.6529\n",
      "Epoch 93: val_loss did not improve from 0.28459\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3067 - mse: 0.3092 - rmse: 0.5560 - mae: 0.3067 - mape: 9.6068 - val_loss: 0.2967 - val_mse: 0.2955 - val_rmse: 0.5436 - val_mae: 0.2967 - val_mape: 9.2404 - lr: 0.0010\n",
      "Epoch 94/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.3059 - mse: 0.3036 - rmse: 0.5510 - mae: 0.3059 - mape: 9.4784\n",
      "Epoch 94: val_loss did not improve from 0.28459\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3044 - mse: 0.3004 - rmse: 0.5481 - mae: 0.3044 - mape: 9.4669 - val_loss: 0.2920 - val_mse: 0.2879 - val_rmse: 0.5366 - val_mae: 0.2920 - val_mape: 8.6182 - lr: 0.0010\n",
      "Epoch 95/2000\n",
      "271/318 [========================>.....] - ETA: 0s - loss: 0.2970 - mse: 0.2927 - rmse: 0.5410 - mae: 0.2970 - mape: 9.2233\n",
      "Epoch 95: val_loss did not improve from 0.28459\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3003 - mse: 0.2985 - rmse: 0.5463 - mae: 0.3003 - mape: 9.3400 - val_loss: 0.3541 - val_mse: 0.3719 - val_rmse: 0.6098 - val_mae: 0.3541 - val_mape: 12.3791 - lr: 0.0010\n",
      "Epoch 96/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2958 - mse: 0.2849 - rmse: 0.5338 - mae: 0.2958 - mape: 9.2450\n",
      "Epoch 96: val_loss did not improve from 0.28459\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2958 - mse: 0.2849 - rmse: 0.5338 - mae: 0.2958 - mape: 9.2450 - val_loss: 0.2885 - val_mse: 0.2752 - val_rmse: 0.5246 - val_mae: 0.2885 - val_mape: 8.7430 - lr: 0.0010\n",
      "Epoch 97/2000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2975 - mse: 0.2928 - rmse: 0.5411 - mae: 0.2975 - mape: 9.3536\n",
      "Epoch 97: val_loss did not improve from 0.28459\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3097 - mse: 0.3211 - rmse: 0.5666 - mae: 0.3097 - mape: 9.6540 - val_loss: 0.3379 - val_mse: 0.3884 - val_rmse: 0.6232 - val_mae: 0.3379 - val_mape: 9.4490 - lr: 0.0010\n",
      "Epoch 98/2000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.3235 - mse: 0.3628 - rmse: 0.6024 - mae: 0.3235 - mape: 9.7678\n",
      "Epoch 98: val_loss did not improve from 0.28459\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3207 - mse: 0.3549 - rmse: 0.5957 - mae: 0.3207 - mape: 9.7068 - val_loss: 0.3178 - val_mse: 0.3138 - val_rmse: 0.5601 - val_mae: 0.3178 - val_mape: 10.3513 - lr: 0.0010\n",
      "Epoch 99/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2934 - mse: 0.2873 - rmse: 0.5360 - mae: 0.2934 - mape: 9.1538\n",
      "Epoch 99: val_loss improved from 0.28459 to 0.28407, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2938 - mse: 0.2879 - rmse: 0.5366 - mae: 0.2938 - mape: 9.1527 - val_loss: 0.2841 - val_mse: 0.2716 - val_rmse: 0.5212 - val_mae: 0.2841 - val_mape: 9.3026 - lr: 0.0010\n",
      "Epoch 100/2000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2916 - mse: 0.2811 - rmse: 0.5302 - mae: 0.2916 - mape: 9.1308\n",
      "Epoch 100: val_loss did not improve from 0.28407\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2908 - mse: 0.2804 - rmse: 0.5295 - mae: 0.2908 - mape: 9.1205 - val_loss: 0.2953 - val_mse: 0.2891 - val_rmse: 0.5377 - val_mae: 0.2953 - val_mape: 9.7069 - lr: 0.0010\n",
      "Epoch 101/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2921 - mse: 0.2827 - rmse: 0.5317 - mae: 0.2921 - mape: 9.1463\n",
      "Epoch 101: val_loss did not improve from 0.28407\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2919 - mse: 0.2827 - rmse: 0.5317 - mae: 0.2919 - mape: 9.1387 - val_loss: 0.3402 - val_mse: 0.4554 - val_rmse: 0.6748 - val_mae: 0.3402 - val_mape: 9.3631 - lr: 0.0010\n",
      "Epoch 102/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2926 - mse: 0.2847 - rmse: 0.5336 - mae: 0.2926 - mape: 9.1780\n",
      "Epoch 102: val_loss improved from 0.28407 to 0.27608, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2923 - mse: 0.2835 - rmse: 0.5324 - mae: 0.2923 - mape: 9.1809 - val_loss: 0.2761 - val_mse: 0.2525 - val_rmse: 0.5025 - val_mae: 0.2761 - val_mape: 8.8206 - lr: 0.0010\n",
      "Epoch 103/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2913 - mse: 0.2873 - rmse: 0.5360 - mae: 0.2913 - mape: 9.1144\n",
      "Epoch 103: val_loss improved from 0.27608 to 0.27188, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2898 - mse: 0.2843 - rmse: 0.5332 - mae: 0.2898 - mape: 9.0795 - val_loss: 0.2719 - val_mse: 0.2529 - val_rmse: 0.5028 - val_mae: 0.2719 - val_mape: 8.7810 - lr: 0.0010\n",
      "Epoch 104/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2876 - mse: 0.2774 - rmse: 0.5267 - mae: 0.2876 - mape: 9.0391\n",
      "Epoch 104: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2885 - mse: 0.2782 - rmse: 0.5274 - mae: 0.2885 - mape: 9.0585 - val_loss: 0.2783 - val_mse: 0.2796 - val_rmse: 0.5288 - val_mae: 0.2783 - val_mape: 9.0049 - lr: 0.0010\n",
      "Epoch 105/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2942 - mse: 0.2929 - rmse: 0.5412 - mae: 0.2942 - mape: 9.2365\n",
      "Epoch 105: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2924 - mse: 0.2888 - rmse: 0.5374 - mae: 0.2924 - mape: 9.1673 - val_loss: 0.2924 - val_mse: 0.2608 - val_rmse: 0.5106 - val_mae: 0.2924 - val_mape: 9.3433 - lr: 0.0010\n",
      "Epoch 106/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2852 - mse: 0.2735 - rmse: 0.5230 - mae: 0.2852 - mape: 8.9626\n",
      "Epoch 106: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2855 - mse: 0.2738 - rmse: 0.5233 - mae: 0.2855 - mape: 8.9607 - val_loss: 0.3057 - val_mse: 0.3255 - val_rmse: 0.5706 - val_mae: 0.3057 - val_mape: 9.3010 - lr: 0.0010\n",
      "Epoch 107/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2880 - mse: 0.2832 - rmse: 0.5322 - mae: 0.2880 - mape: 9.0490\n",
      "Epoch 107: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2886 - mse: 0.2831 - rmse: 0.5321 - mae: 0.2886 - mape: 9.0873 - val_loss: 0.2904 - val_mse: 0.2853 - val_rmse: 0.5341 - val_mae: 0.2904 - val_mape: 8.8606 - lr: 0.0010\n",
      "Epoch 108/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2938 - mse: 0.2874 - rmse: 0.5361 - mae: 0.2938 - mape: 9.2031\n",
      "Epoch 108: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2931 - mse: 0.2875 - rmse: 0.5362 - mae: 0.2931 - mape: 9.1892 - val_loss: 0.3361 - val_mse: 0.4008 - val_rmse: 0.6331 - val_mae: 0.3361 - val_mape: 9.8339 - lr: 0.0010\n",
      "Epoch 109/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2821 - mse: 0.2716 - rmse: 0.5211 - mae: 0.2821 - mape: 8.8869\n",
      "Epoch 109: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2823 - mse: 0.2712 - rmse: 0.5208 - mae: 0.2823 - mape: 8.8869 - val_loss: 0.3035 - val_mse: 0.3222 - val_rmse: 0.5676 - val_mae: 0.3035 - val_mape: 9.1581 - lr: 0.0010\n",
      "Epoch 110/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.3485 - mse: 0.4300 - rmse: 0.6558 - mae: 0.3485 - mape: 10.6939\n",
      "Epoch 110: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3447 - mse: 0.4210 - rmse: 0.6488 - mae: 0.3447 - mape: 10.5857 - val_loss: 0.2749 - val_mse: 0.2641 - val_rmse: 0.5139 - val_mae: 0.2749 - val_mape: 8.7736 - lr: 0.0010\n",
      "Epoch 111/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2910 - mse: 0.2837 - rmse: 0.5327 - mae: 0.2910 - mape: 9.1434\n",
      "Epoch 111: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2904 - mse: 0.2837 - rmse: 0.5327 - mae: 0.2904 - mape: 9.1223 - val_loss: 0.2729 - val_mse: 0.2492 - val_rmse: 0.4992 - val_mae: 0.2729 - val_mape: 8.6291 - lr: 0.0010\n",
      "Epoch 112/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2877 - mse: 0.2797 - rmse: 0.5289 - mae: 0.2877 - mape: 9.0112\n",
      "Epoch 112: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2861 - mse: 0.2759 - rmse: 0.5253 - mae: 0.2861 - mape: 8.9705 - val_loss: 0.2829 - val_mse: 0.2664 - val_rmse: 0.5161 - val_mae: 0.2829 - val_mape: 8.7453 - lr: 0.0010\n",
      "Epoch 113/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2923 - mse: 0.2912 - rmse: 0.5396 - mae: 0.2923 - mape: 9.1341\n",
      "Epoch 113: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2893 - mse: 0.2841 - rmse: 0.5330 - mae: 0.2893 - mape: 9.0498 - val_loss: 0.2846 - val_mse: 0.2778 - val_rmse: 0.5270 - val_mae: 0.2846 - val_mape: 8.7092 - lr: 0.0010\n",
      "Epoch 114/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2910 - mse: 0.2910 - rmse: 0.5394 - mae: 0.2910 - mape: 9.1044\n",
      "Epoch 114: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2912 - mse: 0.2911 - rmse: 0.5395 - mae: 0.2912 - mape: 9.0997 - val_loss: 0.3011 - val_mse: 0.2889 - val_rmse: 0.5375 - val_mae: 0.3011 - val_mape: 9.5554 - lr: 0.0010\n",
      "Epoch 115/2000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2904 - mse: 0.2854 - rmse: 0.5342 - mae: 0.2904 - mape: 8.9797\n",
      "Epoch 115: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2925 - mse: 0.2893 - rmse: 0.5379 - mae: 0.2925 - mape: 9.0631 - val_loss: 0.2820 - val_mse: 0.2691 - val_rmse: 0.5188 - val_mae: 0.2820 - val_mape: 8.4383 - lr: 0.0010\n",
      "Epoch 116/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2847 - mse: 0.2779 - rmse: 0.5271 - mae: 0.2847 - mape: 8.8818\n",
      "Epoch 116: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2846 - mse: 0.2778 - rmse: 0.5271 - mae: 0.2846 - mape: 8.8797 - val_loss: 0.2909 - val_mse: 0.2938 - val_rmse: 0.5420 - val_mae: 0.2909 - val_mape: 8.3928 - lr: 0.0010\n",
      "Epoch 117/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2936 - mse: 0.2944 - rmse: 0.5426 - mae: 0.2936 - mape: 9.0658\n",
      "Epoch 117: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2942 - mse: 0.2947 - rmse: 0.5429 - mae: 0.2942 - mape: 9.1020 - val_loss: 0.2730 - val_mse: 0.2608 - val_rmse: 0.5107 - val_mae: 0.2730 - val_mape: 8.6610 - lr: 0.0010\n",
      "Epoch 118/2000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2882 - mse: 0.2802 - rmse: 0.5293 - mae: 0.2882 - mape: 9.0022\n",
      "Epoch 118: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2879 - mse: 0.2781 - rmse: 0.5274 - mae: 0.2879 - mape: 8.9938 - val_loss: 0.2887 - val_mse: 0.2852 - val_rmse: 0.5340 - val_mae: 0.2887 - val_mape: 8.4197 - lr: 0.0010\n",
      "Epoch 119/2000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2868 - mse: 0.2776 - rmse: 0.5269 - mae: 0.2868 - mape: 8.8863\n",
      "Epoch 119: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2870 - mse: 0.2784 - rmse: 0.5277 - mae: 0.2870 - mape: 8.9068 - val_loss: 0.2822 - val_mse: 0.2620 - val_rmse: 0.5118 - val_mae: 0.2822 - val_mape: 9.1921 - lr: 0.0010\n",
      "Epoch 120/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2915 - mse: 0.2897 - rmse: 0.5383 - mae: 0.2915 - mape: 9.0305\n",
      "Epoch 120: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2923 - mse: 0.2936 - rmse: 0.5419 - mae: 0.2923 - mape: 9.0624 - val_loss: 0.2726 - val_mse: 0.2586 - val_rmse: 0.5085 - val_mae: 0.2726 - val_mape: 8.7561 - lr: 0.0010\n",
      "Epoch 121/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2901 - mse: 0.2823 - rmse: 0.5313 - mae: 0.2901 - mape: 9.0379\n",
      "Epoch 121: val_loss did not improve from 0.27188\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2910 - mse: 0.2859 - rmse: 0.5347 - mae: 0.2910 - mape: 9.0704 - val_loss: 0.2874 - val_mse: 0.2921 - val_rmse: 0.5404 - val_mae: 0.2874 - val_mape: 9.3633 - lr: 0.0010\n",
      "Epoch 122/2000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.2864 - mse: 0.2840 - rmse: 0.5329 - mae: 0.2864 - mape: 8.9272\n",
      "Epoch 122: val_loss improved from 0.27188 to 0.27155, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2864 - mse: 0.2814 - rmse: 0.5305 - mae: 0.2864 - mape: 8.9431 - val_loss: 0.2715 - val_mse: 0.2483 - val_rmse: 0.4983 - val_mae: 0.2715 - val_mape: 8.3926 - lr: 0.0010\n",
      "Epoch 123/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2899 - mse: 0.2835 - rmse: 0.5325 - mae: 0.2899 - mape: 9.0144\n",
      "Epoch 123: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2895 - mse: 0.2834 - rmse: 0.5323 - mae: 0.2895 - mape: 8.9855 - val_loss: 0.2761 - val_mse: 0.2662 - val_rmse: 0.5159 - val_mae: 0.2761 - val_mape: 8.8431 - lr: 0.0010\n",
      "Epoch 124/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2883 - mse: 0.2796 - rmse: 0.5288 - mae: 0.2883 - mape: 8.8912\n",
      "Epoch 124: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2876 - mse: 0.2772 - rmse: 0.5265 - mae: 0.2876 - mape: 8.8912 - val_loss: 0.2975 - val_mse: 0.2902 - val_rmse: 0.5387 - val_mae: 0.2975 - val_mape: 9.8143 - lr: 0.0010\n",
      "Epoch 125/2000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.2863 - mse: 0.2784 - rmse: 0.5276 - mae: 0.2863 - mape: 8.9053\n",
      "Epoch 125: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2878 - mse: 0.2790 - rmse: 0.5282 - mae: 0.2878 - mape: 8.9526 - val_loss: 0.2731 - val_mse: 0.2441 - val_rmse: 0.4941 - val_mae: 0.2731 - val_mape: 8.5857 - lr: 0.0010\n",
      "Epoch 126/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2838 - mse: 0.2721 - rmse: 0.5216 - mae: 0.2838 - mape: 8.8300\n",
      "Epoch 126: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2855 - mse: 0.2772 - rmse: 0.5265 - mae: 0.2855 - mape: 8.8955 - val_loss: 0.2892 - val_mse: 0.2648 - val_rmse: 0.5146 - val_mae: 0.2892 - val_mape: 9.2972 - lr: 0.0010\n",
      "Epoch 127/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2840 - mse: 0.2728 - rmse: 0.5223 - mae: 0.2840 - mape: 8.7781\n",
      "Epoch 127: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2828 - mse: 0.2708 - rmse: 0.5204 - mae: 0.2828 - mape: 8.7539 - val_loss: 0.3015 - val_mse: 0.3275 - val_rmse: 0.5723 - val_mae: 0.3015 - val_mape: 9.9890 - lr: 0.0010\n",
      "Epoch 128/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2848 - mse: 0.2713 - rmse: 0.5209 - mae: 0.2848 - mape: 8.8546\n",
      "Epoch 128: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2849 - mse: 0.2719 - rmse: 0.5215 - mae: 0.2849 - mape: 8.8591 - val_loss: 0.2730 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2730 - val_mape: 8.4398 - lr: 0.0010\n",
      "Epoch 129/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2870 - mse: 0.2833 - rmse: 0.5323 - mae: 0.2870 - mape: 8.9324\n",
      "Epoch 129: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2866 - mse: 0.2825 - rmse: 0.5315 - mae: 0.2866 - mape: 8.9232 - val_loss: 0.3344 - val_mse: 0.4197 - val_rmse: 0.6478 - val_mae: 0.3344 - val_mape: 9.1904 - lr: 0.0010\n",
      "Epoch 130/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.3175 - mse: 0.3584 - rmse: 0.5986 - mae: 0.3175 - mape: 9.7924\n",
      "Epoch 130: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3158 - mse: 0.3517 - rmse: 0.5930 - mae: 0.3158 - mape: 9.7268 - val_loss: 0.2829 - val_mse: 0.2602 - val_rmse: 0.5101 - val_mae: 0.2829 - val_mape: 8.5657 - lr: 0.0010\n",
      "Epoch 131/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2915 - mse: 0.2806 - rmse: 0.5297 - mae: 0.2915 - mape: 9.0382\n",
      "Epoch 131: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2900 - mse: 0.2782 - rmse: 0.5274 - mae: 0.2900 - mape: 8.9980 - val_loss: 0.2848 - val_mse: 0.2679 - val_rmse: 0.5176 - val_mae: 0.2848 - val_mape: 8.4231 - lr: 0.0010\n",
      "Epoch 132/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2883 - mse: 0.2734 - rmse: 0.5228 - mae: 0.2883 - mape: 8.9020\n",
      "Epoch 132: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2873 - mse: 0.2715 - rmse: 0.5210 - mae: 0.2873 - mape: 8.8713 - val_loss: 0.2810 - val_mse: 0.2573 - val_rmse: 0.5072 - val_mae: 0.2810 - val_mape: 8.9525 - lr: 0.0010\n",
      "Epoch 133/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2918 - mse: 0.2827 - rmse: 0.5317 - mae: 0.2918 - mape: 9.0522\n",
      "Epoch 133: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2911 - mse: 0.2813 - rmse: 0.5303 - mae: 0.2911 - mape: 9.0241 - val_loss: 0.2755 - val_mse: 0.2533 - val_rmse: 0.5033 - val_mae: 0.2755 - val_mape: 8.4581 - lr: 0.0010\n",
      "Epoch 134/2000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2855 - mse: 0.2742 - rmse: 0.5236 - mae: 0.2855 - mape: 8.8601\n",
      "Epoch 134: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2845 - mse: 0.2732 - rmse: 0.5227 - mae: 0.2845 - mape: 8.8352 - val_loss: 0.2782 - val_mse: 0.2640 - val_rmse: 0.5138 - val_mae: 0.2782 - val_mape: 8.1392 - lr: 0.0010\n",
      "Epoch 135/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2874 - mse: 0.2734 - rmse: 0.5229 - mae: 0.2874 - mape: 8.9010\n",
      "Epoch 135: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2899 - mse: 0.2820 - rmse: 0.5311 - mae: 0.2899 - mape: 8.9613 - val_loss: 0.2937 - val_mse: 0.2946 - val_rmse: 0.5428 - val_mae: 0.2937 - val_mape: 9.7139 - lr: 0.0010\n",
      "Epoch 136/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2836 - mse: 0.2752 - rmse: 0.5246 - mae: 0.2836 - mape: 8.7840\n",
      "Epoch 136: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2870 - mse: 0.2799 - rmse: 0.5291 - mae: 0.2870 - mape: 8.8676 - val_loss: 0.2819 - val_mse: 0.2628 - val_rmse: 0.5126 - val_mae: 0.2819 - val_mape: 8.3996 - lr: 0.0010\n",
      "Epoch 137/2000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2892 - mse: 0.2834 - rmse: 0.5323 - mae: 0.2892 - mape: 8.8965\n",
      "Epoch 137: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2907 - mse: 0.2850 - rmse: 0.5339 - mae: 0.2907 - mape: 8.9285 - val_loss: 0.2855 - val_mse: 0.2560 - val_rmse: 0.5059 - val_mae: 0.2855 - val_mape: 8.8532 - lr: 0.0010\n",
      "Epoch 138/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2840 - mse: 0.2726 - rmse: 0.5221 - mae: 0.2840 - mape: 8.7979\n",
      "Epoch 138: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2831 - mse: 0.2702 - rmse: 0.5198 - mae: 0.2831 - mape: 8.7703 - val_loss: 0.2816 - val_mse: 0.2642 - val_rmse: 0.5140 - val_mae: 0.2816 - val_mape: 8.3970 - lr: 0.0010\n",
      "Epoch 139/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2873 - mse: 0.2790 - rmse: 0.5282 - mae: 0.2873 - mape: 8.9136\n",
      "Epoch 139: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2874 - mse: 0.2790 - rmse: 0.5282 - mae: 0.2874 - mape: 8.9144 - val_loss: 0.2833 - val_mse: 0.2645 - val_rmse: 0.5143 - val_mae: 0.2833 - val_mape: 9.2012 - lr: 0.0010\n",
      "Epoch 140/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2863 - mse: 0.2772 - rmse: 0.5265 - mae: 0.2863 - mape: 8.8302\n",
      "Epoch 140: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2859 - mse: 0.2773 - rmse: 0.5266 - mae: 0.2859 - mape: 8.8264 - val_loss: 0.2902 - val_mse: 0.3196 - val_rmse: 0.5653 - val_mae: 0.2902 - val_mape: 8.1116 - lr: 0.0010\n",
      "Epoch 141/2000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2837 - mse: 0.2716 - rmse: 0.5212 - mae: 0.2837 - mape: 8.7766\n",
      "Epoch 141: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2827 - mse: 0.2731 - rmse: 0.5226 - mae: 0.2827 - mape: 8.7378 - val_loss: 0.2735 - val_mse: 0.2452 - val_rmse: 0.4952 - val_mae: 0.2735 - val_mape: 8.3121 - lr: 0.0010\n",
      "Epoch 142/2000\n",
      "271/318 [========================>.....] - ETA: 0s - loss: 0.2844 - mse: 0.2750 - rmse: 0.5244 - mae: 0.2844 - mape: 8.8178\n",
      "Epoch 142: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2848 - mse: 0.2760 - rmse: 0.5254 - mae: 0.2848 - mape: 8.8335 - val_loss: 0.2839 - val_mse: 0.2673 - val_rmse: 0.5170 - val_mae: 0.2839 - val_mape: 9.2010 - lr: 0.0010\n",
      "Epoch 143/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2864 - mse: 0.2795 - rmse: 0.5287 - mae: 0.2864 - mape: 8.8396\n",
      "Epoch 143: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2857 - mse: 0.2784 - rmse: 0.5277 - mae: 0.2857 - mape: 8.8201 - val_loss: 0.2725 - val_mse: 0.2459 - val_rmse: 0.4958 - val_mae: 0.2725 - val_mape: 8.4861 - lr: 0.0010\n",
      "Epoch 144/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2760 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2760 - mape: 8.5736\n",
      "Epoch 144: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2754 - mse: 0.2591 - rmse: 0.5091 - mae: 0.2754 - mape: 8.5431 - val_loss: 0.2756 - val_mse: 0.2527 - val_rmse: 0.5027 - val_mae: 0.2756 - val_mape: 8.6159 - lr: 0.0010\n",
      "Epoch 145/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2840 - mse: 0.2773 - rmse: 0.5266 - mae: 0.2840 - mape: 8.7796\n",
      "Epoch 145: val_loss did not improve from 0.27155\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2846 - mse: 0.2792 - rmse: 0.5284 - mae: 0.2846 - mape: 8.8114 - val_loss: 0.2792 - val_mse: 0.2563 - val_rmse: 0.5063 - val_mae: 0.2792 - val_mape: 8.5911 - lr: 0.0010\n",
      "Epoch 146/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2850 - mse: 0.2786 - rmse: 0.5278 - mae: 0.2850 - mape: 8.8708\n",
      "Epoch 146: val_loss improved from 0.27155 to 0.27041, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2843 - mse: 0.2765 - rmse: 0.5259 - mae: 0.2843 - mape: 8.8354 - val_loss: 0.2704 - val_mse: 0.2467 - val_rmse: 0.4967 - val_mae: 0.2704 - val_mape: 8.4994 - lr: 0.0010\n",
      "Epoch 147/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2834 - mse: 0.2722 - rmse: 0.5217 - mae: 0.2834 - mape: 8.7731\n",
      "Epoch 147: val_loss improved from 0.27041 to 0.26801, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2820 - mse: 0.2692 - rmse: 0.5189 - mae: 0.2820 - mape: 8.7159 - val_loss: 0.2680 - val_mse: 0.2489 - val_rmse: 0.4989 - val_mae: 0.2680 - val_mape: 8.2129 - lr: 0.0010\n",
      "Epoch 148/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2752 - mse: 0.2616 - rmse: 0.5114 - mae: 0.2752 - mape: 8.5030\n",
      "Epoch 148: val_loss did not improve from 0.26801\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2769 - mse: 0.2635 - rmse: 0.5134 - mae: 0.2769 - mape: 8.5722 - val_loss: 0.2701 - val_mse: 0.2478 - val_rmse: 0.4978 - val_mae: 0.2701 - val_mape: 8.1350 - lr: 0.0010\n",
      "Epoch 149/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2817 - mse: 0.2681 - rmse: 0.5178 - mae: 0.2817 - mape: 8.7195\n",
      "Epoch 149: val_loss did not improve from 0.26801\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2819 - mse: 0.2691 - rmse: 0.5188 - mae: 0.2819 - mape: 8.7242 - val_loss: 0.2759 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2759 - val_mape: 8.8298 - lr: 0.0010\n",
      "Epoch 150/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2771 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2771 - mape: 8.6013\n",
      "Epoch 150: val_loss did not improve from 0.26801\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2770 - mse: 0.2620 - rmse: 0.5119 - mae: 0.2770 - mape: 8.5929 - val_loss: 0.3085 - val_mse: 0.3223 - val_rmse: 0.5677 - val_mae: 0.3085 - val_mape: 8.7778 - lr: 0.0010\n",
      "Epoch 151/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2869 - mse: 0.2841 - rmse: 0.5330 - mae: 0.2869 - mape: 8.8688\n",
      "Epoch 151: val_loss did not improve from 0.26801\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2870 - mse: 0.2851 - rmse: 0.5340 - mae: 0.2870 - mape: 8.8774 - val_loss: 0.2729 - val_mse: 0.2565 - val_rmse: 0.5064 - val_mae: 0.2729 - val_mape: 8.3581 - lr: 0.0010\n",
      "Epoch 152/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2772 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2772 - mape: 8.6035\n",
      "Epoch 152: val_loss did not improve from 0.26801\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2772 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2772 - mape: 8.5952 - val_loss: 0.2748 - val_mse: 0.2654 - val_rmse: 0.5152 - val_mae: 0.2748 - val_mape: 8.9160 - lr: 0.0010\n",
      "Epoch 153/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2805 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2805 - mape: 8.6518\n",
      "Epoch 153: val_loss improved from 0.26801 to 0.26647, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2805 - mse: 0.2672 - rmse: 0.5170 - mae: 0.2805 - mape: 8.6457 - val_loss: 0.2665 - val_mse: 0.2447 - val_rmse: 0.4946 - val_mae: 0.2665 - val_mape: 8.2311 - lr: 0.0010\n",
      "Epoch 154/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2782 - mse: 0.2638 - rmse: 0.5137 - mae: 0.2782 - mape: 8.5705\n",
      "Epoch 154: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2767 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2767 - mape: 8.5524 - val_loss: 0.2711 - val_mse: 0.2589 - val_rmse: 0.5089 - val_mae: 0.2711 - val_mape: 8.7362 - lr: 0.0010\n",
      "Epoch 155/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2718 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2718 - mape: 8.4541\n",
      "Epoch 155: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2736 - mse: 0.2637 - rmse: 0.5135 - mae: 0.2736 - mape: 8.4851 - val_loss: 0.3929 - val_mse: 0.7050 - val_rmse: 0.8397 - val_mae: 0.3929 - val_mape: 10.2932 - lr: 0.0010\n",
      "Epoch 156/2000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.3309 - mse: 0.4421 - rmse: 0.6649 - mae: 0.3309 - mape: 9.7251\n",
      "Epoch 156: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.3376 - mse: 0.4620 - rmse: 0.6797 - mae: 0.3376 - mape: 9.8376 - val_loss: 0.3735 - val_mse: 0.5709 - val_rmse: 0.7556 - val_mae: 0.3735 - val_mape: 10.6038 - lr: 0.0010\n",
      "Epoch 157/2000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.3588 - mse: 0.5118 - rmse: 0.7154 - mae: 0.3588 - mape: 10.3159\n",
      "Epoch 157: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3540 - mse: 0.4893 - rmse: 0.6995 - mae: 0.3540 - mape: 10.2530 - val_loss: 0.3115 - val_mse: 0.3342 - val_rmse: 0.5781 - val_mae: 0.3115 - val_mape: 9.4463 - lr: 0.0010\n",
      "Epoch 158/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3142 - mse: 0.3222 - rmse: 0.5676 - mae: 0.3142 - mape: 9.6861\n",
      "Epoch 158: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.3141 - mse: 0.3220 - rmse: 0.5675 - mae: 0.3141 - mape: 9.6849 - val_loss: 0.3672 - val_mse: 0.3890 - val_rmse: 0.6237 - val_mae: 0.3672 - val_mape: 11.3592 - lr: 0.0010\n",
      "Epoch 159/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2891 - mse: 0.2725 - rmse: 0.5220 - mae: 0.2891 - mape: 8.9534\n",
      "Epoch 159: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2901 - mse: 0.2743 - rmse: 0.5237 - mae: 0.2901 - mape: 8.9750 - val_loss: 0.3532 - val_mse: 0.3912 - val_rmse: 0.6255 - val_mae: 0.3532 - val_mape: 11.2929 - lr: 0.0010\n",
      "Epoch 160/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2829 - mse: 0.2661 - rmse: 0.5158 - mae: 0.2829 - mape: 8.7778\n",
      "Epoch 160: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2858 - mse: 0.2731 - rmse: 0.5226 - mae: 0.2858 - mape: 8.8809 - val_loss: 0.3196 - val_mse: 0.3442 - val_rmse: 0.5867 - val_mae: 0.3196 - val_mape: 9.3205 - lr: 0.0010\n",
      "Epoch 161/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2863 - mse: 0.2747 - rmse: 0.5242 - mae: 0.2863 - mape: 8.8904\n",
      "Epoch 161: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2847 - mse: 0.2725 - rmse: 0.5220 - mae: 0.2847 - mape: 8.8433 - val_loss: 0.2686 - val_mse: 0.2595 - val_rmse: 0.5094 - val_mae: 0.2686 - val_mape: 8.4511 - lr: 0.0010\n",
      "Epoch 162/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2827 - mse: 0.2683 - rmse: 0.5180 - mae: 0.2827 - mape: 8.7656\n",
      "Epoch 162: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2816 - mse: 0.2670 - rmse: 0.5167 - mae: 0.2816 - mape: 8.7415 - val_loss: 0.2667 - val_mse: 0.2483 - val_rmse: 0.4983 - val_mae: 0.2667 - val_mape: 8.3736 - lr: 0.0010\n",
      "Epoch 163/2000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2923 - mse: 0.2931 - rmse: 0.5414 - mae: 0.2923 - mape: 9.1272\n",
      "Epoch 163: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2938 - mse: 0.2996 - rmse: 0.5474 - mae: 0.2938 - mape: 9.1330 - val_loss: 0.2831 - val_mse: 0.2878 - val_rmse: 0.5365 - val_mae: 0.2831 - val_mape: 9.2769 - lr: 0.0010\n",
      "Epoch 164/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2903 - mse: 0.2898 - rmse: 0.5383 - mae: 0.2903 - mape: 8.9780\n",
      "Epoch 164: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2888 - mse: 0.2873 - rmse: 0.5360 - mae: 0.2888 - mape: 8.9264 - val_loss: 0.2702 - val_mse: 0.2569 - val_rmse: 0.5069 - val_mae: 0.2702 - val_mape: 8.3558 - lr: 0.0010\n",
      "Epoch 165/2000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2868 - mse: 0.2785 - rmse: 0.5277 - mae: 0.2868 - mape: 8.9028\n",
      "Epoch 165: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2864 - mse: 0.2771 - rmse: 0.5264 - mae: 0.2864 - mape: 8.8805 - val_loss: 0.2754 - val_mse: 0.2598 - val_rmse: 0.5097 - val_mae: 0.2754 - val_mape: 8.4854 - lr: 0.0010\n",
      "Epoch 166/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2808 - mse: 0.2762 - rmse: 0.5256 - mae: 0.2808 - mape: 8.7326\n",
      "Epoch 166: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2807 - mse: 0.2753 - rmse: 0.5247 - mae: 0.2807 - mape: 8.7070 - val_loss: 0.2786 - val_mse: 0.2587 - val_rmse: 0.5086 - val_mae: 0.2786 - val_mape: 8.3904 - lr: 0.0010\n",
      "Epoch 167/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2815 - mse: 0.2758 - rmse: 0.5251 - mae: 0.2815 - mape: 8.7747\n",
      "Epoch 167: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2812 - mse: 0.2741 - rmse: 0.5236 - mae: 0.2812 - mape: 8.7353 - val_loss: 0.2742 - val_mse: 0.2599 - val_rmse: 0.5098 - val_mae: 0.2742 - val_mape: 8.3858 - lr: 0.0010\n",
      "Epoch 168/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2810 - mse: 0.2737 - rmse: 0.5231 - mae: 0.2810 - mape: 8.7065\n",
      "Epoch 168: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2826 - mse: 0.2746 - rmse: 0.5240 - mae: 0.2826 - mape: 8.7483 - val_loss: 0.3035 - val_mse: 0.3161 - val_rmse: 0.5622 - val_mae: 0.3035 - val_mape: 10.1616 - lr: 0.0010\n",
      "Epoch 169/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2907 - mse: 0.2930 - rmse: 0.5413 - mae: 0.2907 - mape: 9.0305\n",
      "Epoch 169: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2903 - mse: 0.2924 - rmse: 0.5407 - mae: 0.2903 - mape: 9.0272 - val_loss: 0.2864 - val_mse: 0.2765 - val_rmse: 0.5258 - val_mae: 0.2864 - val_mape: 8.7132 - lr: 0.0010\n",
      "Epoch 170/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2810 - mse: 0.2736 - rmse: 0.5230 - mae: 0.2810 - mape: 8.6541\n",
      "Epoch 170: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2787 - mse: 0.2697 - rmse: 0.5194 - mae: 0.2787 - mape: 8.6114 - val_loss: 0.2707 - val_mse: 0.2676 - val_rmse: 0.5173 - val_mae: 0.2707 - val_mape: 8.6831 - lr: 0.0010\n",
      "Epoch 171/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2832 - mse: 0.2823 - rmse: 0.5314 - mae: 0.2832 - mape: 8.7652\n",
      "Epoch 171: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2845 - mse: 0.2865 - rmse: 0.5352 - mae: 0.2845 - mape: 8.8130 - val_loss: 0.2727 - val_mse: 0.2522 - val_rmse: 0.5022 - val_mae: 0.2727 - val_mape: 8.3892 - lr: 0.0010\n",
      "Epoch 172/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2765 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2765 - mape: 8.5732\n",
      "Epoch 172: val_loss did not improve from 0.26647\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2773 - mse: 0.2690 - rmse: 0.5186 - mae: 0.2773 - mape: 8.5898 - val_loss: 0.2831 - val_mse: 0.2881 - val_rmse: 0.5368 - val_mae: 0.2831 - val_mape: 8.1185 - lr: 0.0010\n",
      "Epoch 173/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2782 - mse: 0.2715 - rmse: 0.5211 - mae: 0.2782 - mape: 8.5955\n",
      "Epoch 173: val_loss improved from 0.26647 to 0.26526, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2769 - mse: 0.2679 - rmse: 0.5176 - mae: 0.2769 - mape: 8.5586 - val_loss: 0.2653 - val_mse: 0.2425 - val_rmse: 0.4925 - val_mae: 0.2653 - val_mape: 8.1982 - lr: 0.0010\n",
      "Epoch 174/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2777 - mse: 0.2683 - rmse: 0.5180 - mae: 0.2777 - mape: 8.5672\n",
      "Epoch 174: val_loss did not improve from 0.26526\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2767 - mse: 0.2657 - rmse: 0.5155 - mae: 0.2767 - mape: 8.5479 - val_loss: 0.2661 - val_mse: 0.2517 - val_rmse: 0.5017 - val_mae: 0.2661 - val_mape: 8.4832 - lr: 0.0010\n",
      "Epoch 175/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2820 - mse: 0.2747 - rmse: 0.5242 - mae: 0.2820 - mape: 8.7668\n",
      "Epoch 175: val_loss did not improve from 0.26526\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2822 - mse: 0.2753 - rmse: 0.5246 - mae: 0.2822 - mape: 8.7640 - val_loss: 0.2711 - val_mse: 0.2528 - val_rmse: 0.5028 - val_mae: 0.2711 - val_mape: 8.5875 - lr: 0.0010\n",
      "Epoch 176/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2823 - mse: 0.2767 - rmse: 0.5261 - mae: 0.2823 - mape: 8.6867\n",
      "Epoch 176: val_loss did not improve from 0.26526\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2817 - mse: 0.2762 - rmse: 0.5256 - mae: 0.2817 - mape: 8.6660 - val_loss: 0.2691 - val_mse: 0.2478 - val_rmse: 0.4978 - val_mae: 0.2691 - val_mape: 8.2995 - lr: 0.0010\n",
      "Epoch 177/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2765 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2765 - mape: 8.5370\n",
      "Epoch 177: val_loss improved from 0.26526 to 0.26469, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2763 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2763 - mape: 8.5281 - val_loss: 0.2647 - val_mse: 0.2431 - val_rmse: 0.4931 - val_mae: 0.2647 - val_mape: 8.0254 - lr: 0.0010\n",
      "Epoch 178/2000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2792 - mse: 0.2783 - rmse: 0.5275 - mae: 0.2792 - mape: 8.5835\n",
      "Epoch 178: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2769 - mse: 0.2721 - rmse: 0.5217 - mae: 0.2769 - mape: 8.5274 - val_loss: 0.2888 - val_mse: 0.2858 - val_rmse: 0.5346 - val_mae: 0.2888 - val_mape: 8.6738 - lr: 0.0010\n",
      "Epoch 179/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2851 - mse: 0.2849 - rmse: 0.5338 - mae: 0.2851 - mape: 8.7744\n",
      "Epoch 179: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2848 - mse: 0.2844 - rmse: 0.5333 - mae: 0.2848 - mape: 8.7648 - val_loss: 0.2664 - val_mse: 0.2540 - val_rmse: 0.5039 - val_mae: 0.2664 - val_mape: 8.1457 - lr: 0.0010\n",
      "Epoch 180/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2766 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2766 - mape: 8.5442\n",
      "Epoch 180: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2763 - mse: 0.2659 - rmse: 0.5157 - mae: 0.2763 - mape: 8.5374 - val_loss: 0.2870 - val_mse: 0.2833 - val_rmse: 0.5323 - val_mae: 0.2870 - val_mape: 8.5942 - lr: 0.0010\n",
      "Epoch 181/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2739 - mse: 0.2615 - rmse: 0.5113 - mae: 0.2739 - mape: 8.5224\n",
      "Epoch 181: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2737 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2737 - mape: 8.5376 - val_loss: 0.2735 - val_mse: 0.2592 - val_rmse: 0.5091 - val_mae: 0.2735 - val_mape: 8.1616 - lr: 0.0010\n",
      "Epoch 182/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2775 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2775 - mape: 8.5717\n",
      "Epoch 182: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2777 - mse: 0.2684 - rmse: 0.5181 - mae: 0.2777 - mape: 8.5807 - val_loss: 0.2657 - val_mse: 0.2606 - val_rmse: 0.5105 - val_mae: 0.2657 - val_mape: 8.2363 - lr: 0.0010\n",
      "Epoch 183/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2783 - mse: 0.2712 - rmse: 0.5208 - mae: 0.2783 - mape: 8.6313\n",
      "Epoch 183: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2783 - mse: 0.2706 - rmse: 0.5202 - mae: 0.2783 - mape: 8.6324 - val_loss: 0.2738 - val_mse: 0.2443 - val_rmse: 0.4943 - val_mae: 0.2738 - val_mape: 8.6046 - lr: 0.0010\n",
      "Epoch 184/2000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2808 - mse: 0.2718 - rmse: 0.5213 - mae: 0.2808 - mape: 8.7247\n",
      "Epoch 184: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2793 - mse: 0.2681 - rmse: 0.5178 - mae: 0.2793 - mape: 8.6689 - val_loss: 0.2686 - val_mse: 0.2492 - val_rmse: 0.4992 - val_mae: 0.2686 - val_mape: 8.4460 - lr: 0.0010\n",
      "Epoch 185/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.3002 - mse: 0.3041 - rmse: 0.5514 - mae: 0.3002 - mape: 9.1972\n",
      "Epoch 185: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.3002 - mse: 0.3038 - rmse: 0.5512 - mae: 0.3002 - mape: 9.2094 - val_loss: 0.3037 - val_mse: 0.3035 - val_rmse: 0.5510 - val_mae: 0.3037 - val_mape: 8.9051 - lr: 0.0010\n",
      "Epoch 186/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2925 - mse: 0.2812 - rmse: 0.5303 - mae: 0.2925 - mape: 8.8521\n",
      "Epoch 186: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2918 - mse: 0.2787 - rmse: 0.5280 - mae: 0.2918 - mape: 8.8495 - val_loss: 0.2988 - val_mse: 0.2811 - val_rmse: 0.5302 - val_mae: 0.2988 - val_mape: 9.6500 - lr: 0.0010\n",
      "Epoch 187/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2809 - mse: 0.2620 - rmse: 0.5118 - mae: 0.2809 - mape: 8.5205\n",
      "Epoch 187: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2805 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2805 - mape: 8.5081 - val_loss: 0.2692 - val_mse: 0.2542 - val_rmse: 0.5042 - val_mae: 0.2692 - val_mape: 8.0228 - lr: 0.0010\n",
      "Epoch 188/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2793 - mse: 0.2619 - rmse: 0.5117 - mae: 0.2793 - mape: 8.4800\n",
      "Epoch 188: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2796 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2796 - mape: 8.4877 - val_loss: 0.2880 - val_mse: 0.2680 - val_rmse: 0.5177 - val_mae: 0.2880 - val_mape: 9.2524 - lr: 0.0010\n",
      "Epoch 189/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2842 - mse: 0.2759 - rmse: 0.5253 - mae: 0.2842 - mape: 8.6635\n",
      "Epoch 189: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2862 - mse: 0.2769 - rmse: 0.5262 - mae: 0.2862 - mape: 8.6909 - val_loss: 0.2766 - val_mse: 0.2489 - val_rmse: 0.4989 - val_mae: 0.2766 - val_mape: 8.4794 - lr: 0.0010\n",
      "Epoch 190/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2862 - mse: 0.2761 - rmse: 0.5255 - mae: 0.2862 - mape: 8.6659\n",
      "Epoch 190: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2860 - mse: 0.2757 - rmse: 0.5251 - mae: 0.2860 - mape: 8.6561 - val_loss: 0.3021 - val_mse: 0.2999 - val_rmse: 0.5477 - val_mae: 0.3021 - val_mape: 9.7946 - lr: 0.0010\n",
      "Epoch 191/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2802 - mse: 0.2694 - rmse: 0.5191 - mae: 0.2802 - mape: 8.4960\n",
      "Epoch 191: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2803 - mse: 0.2695 - rmse: 0.5192 - mae: 0.2803 - mape: 8.4883 - val_loss: 0.3028 - val_mse: 0.2981 - val_rmse: 0.5459 - val_mae: 0.3028 - val_mape: 9.9264 - lr: 0.0010\n",
      "Epoch 192/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2802 - mse: 0.2672 - rmse: 0.5169 - mae: 0.2802 - mape: 8.4912\n",
      "Epoch 192: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2800 - mse: 0.2677 - rmse: 0.5174 - mae: 0.2800 - mape: 8.4910 - val_loss: 0.3119 - val_mse: 0.3182 - val_rmse: 0.5641 - val_mae: 0.3119 - val_mape: 10.3813 - lr: 0.0010\n",
      "Epoch 193/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2821 - mse: 0.2718 - rmse: 0.5214 - mae: 0.2821 - mape: 8.5726\n",
      "Epoch 193: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2823 - mse: 0.2710 - rmse: 0.5206 - mae: 0.2823 - mape: 8.5746 - val_loss: 0.2702 - val_mse: 0.2548 - val_rmse: 0.5048 - val_mae: 0.2702 - val_mape: 8.1976 - lr: 0.0010\n",
      "Epoch 194/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2859 - mse: 0.2757 - rmse: 0.5251 - mae: 0.2859 - mape: 8.7269\n",
      "Epoch 194: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2861 - mse: 0.2767 - rmse: 0.5260 - mae: 0.2861 - mape: 8.7276 - val_loss: 0.2737 - val_mse: 0.2494 - val_rmse: 0.4994 - val_mae: 0.2737 - val_mape: 8.2866 - lr: 0.0010\n",
      "Epoch 195/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2792 - mse: 0.2625 - rmse: 0.5124 - mae: 0.2792 - mape: 8.4788\n",
      "Epoch 195: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2789 - mse: 0.2645 - rmse: 0.5143 - mae: 0.2789 - mape: 8.4731 - val_loss: 0.2699 - val_mse: 0.2480 - val_rmse: 0.4980 - val_mae: 0.2699 - val_mape: 8.3830 - lr: 0.0010\n",
      "Epoch 196/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2763 - mse: 0.2562 - rmse: 0.5061 - mae: 0.2763 - mape: 8.4729\n",
      "Epoch 196: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2778 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2778 - mape: 8.5079 - val_loss: 0.3000 - val_mse: 0.3010 - val_rmse: 0.5486 - val_mae: 0.3000 - val_mape: 9.5695 - lr: 0.0010\n",
      "Epoch 197/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2842 - mse: 0.2806 - rmse: 0.5297 - mae: 0.2842 - mape: 8.6909\n",
      "Epoch 197: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2837 - mse: 0.2804 - rmse: 0.5296 - mae: 0.2837 - mape: 8.6729 - val_loss: 0.2744 - val_mse: 0.2788 - val_rmse: 0.5280 - val_mae: 0.2744 - val_mape: 7.8932 - lr: 0.0010\n",
      "Epoch 198/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2765 - mse: 0.2665 - rmse: 0.5163 - mae: 0.2765 - mape: 8.4031\n",
      "Epoch 198: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2766 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2766 - mape: 8.4097 - val_loss: 0.2694 - val_mse: 0.2515 - val_rmse: 0.5015 - val_mae: 0.2694 - val_mape: 8.1419 - lr: 0.0010\n",
      "Epoch 199/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2805 - mse: 0.2727 - rmse: 0.5222 - mae: 0.2805 - mape: 8.5634\n",
      "Epoch 199: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2803 - mse: 0.2723 - rmse: 0.5218 - mae: 0.2803 - mape: 8.5565 - val_loss: 0.3113 - val_mse: 0.3645 - val_rmse: 0.6037 - val_mae: 0.3113 - val_mape: 8.6159 - lr: 0.0010\n",
      "Epoch 200/2000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2850 - mse: 0.2741 - rmse: 0.5235 - mae: 0.2850 - mape: 8.7204\n",
      "Epoch 200: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2823 - mse: 0.2697 - rmse: 0.5193 - mae: 0.2823 - mape: 8.6357 - val_loss: 0.2745 - val_mse: 0.2533 - val_rmse: 0.5033 - val_mae: 0.2745 - val_mape: 8.4696 - lr: 0.0010\n",
      "Epoch 201/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2779 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2779 - mape: 8.5254\n",
      "Epoch 201: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2779 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2779 - mape: 8.5220 - val_loss: 0.2702 - val_mse: 0.2440 - val_rmse: 0.4940 - val_mae: 0.2702 - val_mape: 8.3042 - lr: 0.0010\n",
      "Epoch 202/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2806 - mse: 0.2700 - rmse: 0.5196 - mae: 0.2806 - mape: 8.6029\n",
      "Epoch 202: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2796 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2796 - mape: 8.5724 - val_loss: 0.2745 - val_mse: 0.2525 - val_rmse: 0.5025 - val_mae: 0.2745 - val_mape: 8.6870 - lr: 0.0010\n",
      "Epoch 203/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2760 - mse: 0.2664 - rmse: 0.5162 - mae: 0.2760 - mape: 8.4648\n",
      "Epoch 203: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2761 - mse: 0.2662 - rmse: 0.5159 - mae: 0.2761 - mape: 8.4666 - val_loss: 0.2855 - val_mse: 0.2789 - val_rmse: 0.5281 - val_mae: 0.2855 - val_mape: 9.1856 - lr: 0.0010\n",
      "Epoch 204/2000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2753 - mse: 0.2651 - rmse: 0.5148 - mae: 0.2753 - mape: 8.3964\n",
      "Epoch 204: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2759 - mse: 0.2660 - rmse: 0.5157 - mae: 0.2759 - mape: 8.4244 - val_loss: 0.2852 - val_mse: 0.2851 - val_rmse: 0.5339 - val_mae: 0.2852 - val_mape: 8.2528 - lr: 0.0010\n",
      "Epoch 205/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2780 - mse: 0.2663 - rmse: 0.5160 - mae: 0.2780 - mape: 8.4450\n",
      "Epoch 205: val_loss did not improve from 0.26469\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2773 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2773 - mape: 8.4330 - val_loss: 0.2677 - val_mse: 0.2516 - val_rmse: 0.5016 - val_mae: 0.2677 - val_mape: 8.0385 - lr: 0.0010\n",
      "Epoch 206/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2757 - mse: 0.2623 - rmse: 0.5122 - mae: 0.2757 - mape: 8.4598\n",
      "Epoch 206: val_loss improved from 0.26469 to 0.26440, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2750 - mse: 0.2619 - rmse: 0.5117 - mae: 0.2750 - mape: 8.4456 - val_loss: 0.2644 - val_mse: 0.2456 - val_rmse: 0.4956 - val_mae: 0.2644 - val_mape: 8.0397 - lr: 0.0010\n",
      "Epoch 207/2000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2842 - mse: 0.2771 - rmse: 0.5264 - mae: 0.2842 - mape: 8.7287\n",
      "Epoch 207: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2842 - mse: 0.2766 - rmse: 0.5260 - mae: 0.2842 - mape: 8.7097 - val_loss: 0.3216 - val_mse: 0.3870 - val_rmse: 0.6221 - val_mae: 0.3216 - val_mape: 8.8489 - lr: 0.0010\n",
      "Epoch 208/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2759 - mse: 0.2636 - rmse: 0.5134 - mae: 0.2759 - mape: 8.4280\n",
      "Epoch 208: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2771 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2771 - mape: 8.4713 - val_loss: 0.2903 - val_mse: 0.2839 - val_rmse: 0.5328 - val_mae: 0.2903 - val_mape: 9.4143 - lr: 0.0010\n",
      "Epoch 209/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2818 - mse: 0.2729 - rmse: 0.5224 - mae: 0.2818 - mape: 8.6210\n",
      "Epoch 209: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2805 - mse: 0.2702 - rmse: 0.5198 - mae: 0.2805 - mape: 8.5926 - val_loss: 0.2679 - val_mse: 0.2583 - val_rmse: 0.5082 - val_mae: 0.2679 - val_mape: 8.4369 - lr: 0.0010\n",
      "Epoch 210/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2765 - mse: 0.2658 - rmse: 0.5156 - mae: 0.2765 - mape: 8.4562\n",
      "Epoch 210: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2761 - mse: 0.2648 - rmse: 0.5146 - mae: 0.2761 - mape: 8.4549 - val_loss: 0.2711 - val_mse: 0.2452 - val_rmse: 0.4952 - val_mae: 0.2711 - val_mape: 8.3274 - lr: 0.0010\n",
      "Epoch 211/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2740 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2740 - mape: 8.4118\n",
      "Epoch 211: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2767 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2767 - mape: 8.4712 - val_loss: 0.2694 - val_mse: 0.2527 - val_rmse: 0.5027 - val_mae: 0.2694 - val_mape: 8.0945 - lr: 0.0010\n",
      "Epoch 212/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2734 - mse: 0.2563 - rmse: 0.5063 - mae: 0.2734 - mape: 8.3928\n",
      "Epoch 212: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2733 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2733 - mape: 8.3923 - val_loss: 0.2801 - val_mse: 0.2686 - val_rmse: 0.5183 - val_mae: 0.2801 - val_mape: 8.9924 - lr: 0.0010\n",
      "Epoch 213/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2760 - mse: 0.2628 - rmse: 0.5127 - mae: 0.2760 - mape: 8.4848\n",
      "Epoch 213: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2753 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2753 - mape: 8.4735 - val_loss: 0.2738 - val_mse: 0.2622 - val_rmse: 0.5121 - val_mae: 0.2738 - val_mape: 8.1771 - lr: 0.0010\n",
      "Epoch 214/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2727 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2727 - mape: 8.4451\n",
      "Epoch 214: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2740 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2740 - mape: 8.4534 - val_loss: 0.2678 - val_mse: 0.2497 - val_rmse: 0.4997 - val_mae: 0.2678 - val_mape: 8.3484 - lr: 0.0010\n",
      "Epoch 215/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2723 - mse: 0.2560 - rmse: 0.5060 - mae: 0.2723 - mape: 8.3667\n",
      "Epoch 215: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2734 - mse: 0.2591 - rmse: 0.5091 - mae: 0.2734 - mape: 8.4089 - val_loss: 0.2655 - val_mse: 0.2485 - val_rmse: 0.4985 - val_mae: 0.2655 - val_mape: 8.2219 - lr: 0.0010\n",
      "Epoch 216/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2736 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2736 - mape: 8.3820\n",
      "Epoch 216: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2734 - mse: 0.2624 - rmse: 0.5123 - mae: 0.2734 - mape: 8.3725 - val_loss: 0.2798 - val_mse: 0.2590 - val_rmse: 0.5090 - val_mae: 0.2798 - val_mape: 8.7375 - lr: 0.0010\n",
      "Epoch 217/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2785 - mse: 0.2687 - rmse: 0.5184 - mae: 0.2785 - mape: 8.5388\n",
      "Epoch 217: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2777 - mse: 0.2666 - rmse: 0.5163 - mae: 0.2777 - mape: 8.5072 - val_loss: 0.2713 - val_mse: 0.2653 - val_rmse: 0.5151 - val_mae: 0.2713 - val_mape: 8.6307 - lr: 0.0010\n",
      "Epoch 218/2000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2823 - mse: 0.2778 - rmse: 0.5271 - mae: 0.2823 - mape: 8.6377\n",
      "Epoch 218: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2775 - mse: 0.2680 - rmse: 0.5177 - mae: 0.2775 - mape: 8.5117 - val_loss: 0.2800 - val_mse: 0.2872 - val_rmse: 0.5359 - val_mae: 0.2800 - val_mape: 9.0871 - lr: 0.0010\n",
      "Epoch 219/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2773 - mse: 0.2752 - rmse: 0.5246 - mae: 0.2773 - mape: 8.5384\n",
      "Epoch 219: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2777 - mse: 0.2755 - rmse: 0.5249 - mae: 0.2777 - mape: 8.5498 - val_loss: 0.2677 - val_mse: 0.2468 - val_rmse: 0.4968 - val_mae: 0.2677 - val_mape: 8.3062 - lr: 0.0010\n",
      "Epoch 220/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2739 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2739 - mape: 8.4185\n",
      "Epoch 220: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2738 - mse: 0.2613 - rmse: 0.5112 - mae: 0.2738 - mape: 8.4129 - val_loss: 0.2947 - val_mse: 0.3127 - val_rmse: 0.5592 - val_mae: 0.2947 - val_mape: 8.3223 - lr: 0.0010\n",
      "Epoch 221/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2754 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2754 - mape: 8.4487\n",
      "Epoch 221: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2749 - mse: 0.2652 - rmse: 0.5150 - mae: 0.2749 - mape: 8.4313 - val_loss: 0.2679 - val_mse: 0.2475 - val_rmse: 0.4975 - val_mae: 0.2679 - val_mape: 8.3432 - lr: 0.0010\n",
      "Epoch 222/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2735 - mse: 0.2618 - rmse: 0.5116 - mae: 0.2735 - mape: 8.4165\n",
      "Epoch 222: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2743 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2743 - mape: 8.4319 - val_loss: 0.2869 - val_mse: 0.2806 - val_rmse: 0.5297 - val_mae: 0.2869 - val_mape: 9.1774 - lr: 0.0010\n",
      "Epoch 223/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2803 - mse: 0.2733 - rmse: 0.5228 - mae: 0.2803 - mape: 8.6161\n",
      "Epoch 223: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2798 - mse: 0.2725 - rmse: 0.5220 - mae: 0.2798 - mape: 8.6013 - val_loss: 0.2873 - val_mse: 0.2848 - val_rmse: 0.5337 - val_mae: 0.2873 - val_mape: 8.3659 - lr: 0.0010\n",
      "Epoch 224/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2736 - mse: 0.2622 - rmse: 0.5121 - mae: 0.2736 - mape: 8.4001\n",
      "Epoch 224: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2743 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2743 - mape: 8.4237 - val_loss: 0.2698 - val_mse: 0.2570 - val_rmse: 0.5070 - val_mae: 0.2698 - val_mape: 8.0780 - lr: 0.0010\n",
      "Epoch 225/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2740 - mse: 0.2650 - rmse: 0.5148 - mae: 0.2740 - mape: 8.4035\n",
      "Epoch 225: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2735 - mse: 0.2640 - rmse: 0.5138 - mae: 0.2735 - mape: 8.3924 - val_loss: 0.2800 - val_mse: 0.2779 - val_rmse: 0.5272 - val_mae: 0.2800 - val_mape: 8.1901 - lr: 0.0010\n",
      "Epoch 226/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2750 - mse: 0.2673 - rmse: 0.5170 - mae: 0.2750 - mape: 8.4706\n",
      "Epoch 226: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2745 - mse: 0.2675 - rmse: 0.5172 - mae: 0.2745 - mape: 8.4605 - val_loss: 0.2652 - val_mse: 0.2582 - val_rmse: 0.5081 - val_mae: 0.2652 - val_mape: 8.2478 - lr: 0.0010\n",
      "Epoch 227/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2794 - mse: 0.2737 - rmse: 0.5232 - mae: 0.2794 - mape: 8.5520\n",
      "Epoch 227: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2795 - mse: 0.2735 - rmse: 0.5229 - mae: 0.2795 - mape: 8.5617 - val_loss: 0.2700 - val_mse: 0.2514 - val_rmse: 0.5014 - val_mae: 0.2700 - val_mape: 8.6975 - lr: 0.0010\n",
      "Epoch 228/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2710 - mse: 0.2638 - rmse: 0.5136 - mae: 0.2710 - mape: 8.3747\n",
      "Epoch 228: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2701 - mse: 0.2626 - rmse: 0.5124 - mae: 0.2701 - mape: 8.3487 - val_loss: 0.2719 - val_mse: 0.2556 - val_rmse: 0.5056 - val_mae: 0.2719 - val_mape: 8.1071 - lr: 0.0010\n",
      "Epoch 229/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2708 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2708 - mape: 8.3079\n",
      "Epoch 229: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2721 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2721 - mape: 8.3581 - val_loss: 0.2666 - val_mse: 0.2536 - val_rmse: 0.5036 - val_mae: 0.2666 - val_mape: 7.9104 - lr: 0.0010\n",
      "Epoch 230/2000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2708 - mse: 0.2576 - rmse: 0.5076 - mae: 0.2708 - mape: 8.3110\n",
      "Epoch 230: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2734 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2734 - mape: 8.3786 - val_loss: 0.3022 - val_mse: 0.3038 - val_rmse: 0.5512 - val_mae: 0.3022 - val_mape: 10.1154 - lr: 0.0010\n",
      "Epoch 231/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2764 - mse: 0.2720 - rmse: 0.5215 - mae: 0.2764 - mape: 8.5098\n",
      "Epoch 231: val_loss did not improve from 0.26440\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2764 - mse: 0.2720 - rmse: 0.5216 - mae: 0.2764 - mape: 8.5074 - val_loss: 0.2711 - val_mse: 0.2551 - val_rmse: 0.5051 - val_mae: 0.2711 - val_mape: 8.2432 - lr: 0.0010\n",
      "Epoch 232/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2694 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2694 - mape: 8.3313\n",
      "Epoch 232: val_loss improved from 0.26440 to 0.26404, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2712 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2712 - mape: 8.3700 - val_loss: 0.2640 - val_mse: 0.2437 - val_rmse: 0.4937 - val_mae: 0.2640 - val_mape: 8.1156 - lr: 0.0010\n",
      "Epoch 233/2000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2741 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2741 - mape: 8.4515\n",
      "Epoch 233: val_loss did not improve from 0.26404\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2749 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2749 - mape: 8.4499 - val_loss: 0.2729 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2729 - val_mape: 8.2622 - lr: 0.0010\n",
      "Epoch 234/2000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.2738 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2738 - mape: 8.4464\n",
      "Epoch 234: val_loss did not improve from 0.26404\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2734 - mse: 0.2642 - rmse: 0.5140 - mae: 0.2734 - mape: 8.4520 - val_loss: 0.2751 - val_mse: 0.2528 - val_rmse: 0.5028 - val_mae: 0.2751 - val_mape: 8.6667 - lr: 0.0010\n",
      "Epoch 235/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2751 - mse: 0.2641 - rmse: 0.5139 - mae: 0.2751 - mape: 8.4601\n",
      "Epoch 235: val_loss did not improve from 0.26404\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2731 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2731 - mape: 8.4079 - val_loss: 0.2649 - val_mse: 0.2530 - val_rmse: 0.5030 - val_mae: 0.2649 - val_mape: 8.0594 - lr: 0.0010\n",
      "Epoch 236/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2709 - mse: 0.2631 - rmse: 0.5129 - mae: 0.2709 - mape: 8.3060\n",
      "Epoch 236: val_loss improved from 0.26404 to 0.26227, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2718 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2718 - mape: 8.3299 - val_loss: 0.2623 - val_mse: 0.2488 - val_rmse: 0.4988 - val_mae: 0.2623 - val_mape: 8.1572 - lr: 0.0010\n",
      "Epoch 237/2000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2775 - mse: 0.2678 - rmse: 0.5175 - mae: 0.2775 - mape: 8.5313\n",
      "Epoch 237: val_loss did not improve from 0.26227\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2760 - mse: 0.2646 - rmse: 0.5144 - mae: 0.2760 - mape: 8.4726 - val_loss: 0.2699 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2699 - val_mape: 8.0463 - lr: 0.0010\n",
      "Epoch 238/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2762 - mse: 0.2698 - rmse: 0.5195 - mae: 0.2762 - mape: 8.4807\n",
      "Epoch 238: val_loss did not improve from 0.26227\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2741 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2741 - mape: 8.4209 - val_loss: 0.2627 - val_mse: 0.2519 - val_rmse: 0.5019 - val_mae: 0.2627 - val_mape: 8.2047 - lr: 0.0010\n",
      "Epoch 239/2000\n",
      "261/318 [=======================>......] - ETA: 0s - loss: 0.2735 - mse: 0.2626 - rmse: 0.5124 - mae: 0.2735 - mape: 8.4209\n",
      "Epoch 239: val_loss did not improve from 0.26227\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2736 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2736 - mape: 8.4092 - val_loss: 0.2654 - val_mse: 0.2499 - val_rmse: 0.4999 - val_mae: 0.2654 - val_mape: 8.2645 - lr: 0.0010\n",
      "Epoch 240/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2716 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2716 - mape: 8.3290\n",
      "Epoch 240: val_loss did not improve from 0.26227\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2710 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2710 - mape: 8.3010 - val_loss: 0.2698 - val_mse: 0.2628 - val_rmse: 0.5126 - val_mae: 0.2698 - val_mape: 8.0164 - lr: 0.0010\n",
      "Epoch 241/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2784 - mse: 0.2744 - rmse: 0.5239 - mae: 0.2784 - mape: 8.5603\n",
      "Epoch 241: val_loss improved from 0.26227 to 0.26217, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2773 - mse: 0.2715 - rmse: 0.5211 - mae: 0.2773 - mape: 8.5222 - val_loss: 0.2622 - val_mse: 0.2502 - val_rmse: 0.5002 - val_mae: 0.2622 - val_mape: 8.0829 - lr: 0.0010\n",
      "Epoch 242/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2697 - mse: 0.2629 - rmse: 0.5127 - mae: 0.2697 - mape: 8.3255\n",
      "Epoch 242: val_loss did not improve from 0.26217\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2700 - mse: 0.2627 - rmse: 0.5126 - mae: 0.2700 - mape: 8.3267 - val_loss: 0.2704 - val_mse: 0.2515 - val_rmse: 0.5015 - val_mae: 0.2704 - val_mape: 8.6426 - lr: 0.0010\n",
      "Epoch 243/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2694 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2694 - mape: 8.2583\n",
      "Epoch 243: val_loss did not improve from 0.26217\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2698 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2698 - mape: 8.2778 - val_loss: 0.2636 - val_mse: 0.2501 - val_rmse: 0.5001 - val_mae: 0.2636 - val_mape: 8.1175 - lr: 0.0010\n",
      "Epoch 244/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2717 - mse: 0.2620 - rmse: 0.5119 - mae: 0.2717 - mape: 8.3649\n",
      "Epoch 244: val_loss improved from 0.26217 to 0.26153, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2719 - mse: 0.2612 - rmse: 0.5111 - mae: 0.2719 - mape: 8.3658 - val_loss: 0.2615 - val_mse: 0.2435 - val_rmse: 0.4934 - val_mae: 0.2615 - val_mape: 7.9434 - lr: 0.0010\n",
      "Epoch 245/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2705 - mse: 0.2600 - rmse: 0.5099 - mae: 0.2705 - mape: 8.2867\n",
      "Epoch 245: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2718 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2718 - mape: 8.3216 - val_loss: 0.2824 - val_mse: 0.2817 - val_rmse: 0.5308 - val_mae: 0.2824 - val_mape: 8.1801 - lr: 0.0010\n",
      "Epoch 246/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2756 - mse: 0.2686 - rmse: 0.5183 - mae: 0.2756 - mape: 8.4630\n",
      "Epoch 246: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2753 - mse: 0.2676 - rmse: 0.5173 - mae: 0.2753 - mape: 8.4479 - val_loss: 0.2708 - val_mse: 0.2454 - val_rmse: 0.4954 - val_mae: 0.2708 - val_mape: 8.3857 - lr: 0.0010\n",
      "Epoch 247/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2723 - mse: 0.2622 - rmse: 0.5120 - mae: 0.2723 - mape: 8.3914\n",
      "Epoch 247: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2721 - mse: 0.2617 - rmse: 0.5116 - mae: 0.2721 - mape: 8.3931 - val_loss: 0.2742 - val_mse: 0.2692 - val_rmse: 0.5188 - val_mae: 0.2742 - val_mape: 8.0991 - lr: 0.0010\n",
      "Epoch 248/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2745 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2745 - mape: 8.4412\n",
      "Epoch 248: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2745 - mse: 0.2666 - rmse: 0.5163 - mae: 0.2745 - mape: 8.4415 - val_loss: 0.2860 - val_mse: 0.2979 - val_rmse: 0.5458 - val_mae: 0.2860 - val_mape: 9.1857 - lr: 0.0010\n",
      "Epoch 249/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2724 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2724 - mape: 8.4031\n",
      "Epoch 249: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2717 - mse: 0.2618 - rmse: 0.5116 - mae: 0.2717 - mape: 8.3651 - val_loss: 0.2826 - val_mse: 0.2678 - val_rmse: 0.5175 - val_mae: 0.2826 - val_mape: 8.3527 - lr: 0.0010\n",
      "Epoch 250/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2728 - mse: 0.2614 - rmse: 0.5113 - mae: 0.2728 - mape: 8.4180\n",
      "Epoch 250: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2747 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2747 - mape: 8.4834 - val_loss: 0.2647 - val_mse: 0.2455 - val_rmse: 0.4954 - val_mae: 0.2647 - val_mape: 7.9163 - lr: 0.0010\n",
      "Epoch 251/2000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2723 - mse: 0.2644 - rmse: 0.5142 - mae: 0.2723 - mape: 8.3963\n",
      "Epoch 251: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2737 - mse: 0.2666 - rmse: 0.5164 - mae: 0.2737 - mape: 8.4118 - val_loss: 0.2696 - val_mse: 0.2558 - val_rmse: 0.5058 - val_mae: 0.2696 - val_mape: 8.3886 - lr: 0.0010\n",
      "Epoch 252/2000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2756 - mse: 0.2647 - rmse: 0.5145 - mae: 0.2756 - mape: 8.4835\n",
      "Epoch 252: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2771 - mse: 0.2674 - rmse: 0.5171 - mae: 0.2771 - mape: 8.5191 - val_loss: 0.2843 - val_mse: 0.2829 - val_rmse: 0.5319 - val_mae: 0.2843 - val_mape: 8.2830 - lr: 0.0010\n",
      "Epoch 253/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2718 - mse: 0.2622 - rmse: 0.5121 - mae: 0.2718 - mape: 8.3567\n",
      "Epoch 253: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2717 - mse: 0.2615 - rmse: 0.5113 - mae: 0.2717 - mape: 8.3771 - val_loss: 0.2653 - val_mse: 0.2552 - val_rmse: 0.5051 - val_mae: 0.2653 - val_mape: 8.2265 - lr: 0.0010\n",
      "Epoch 254/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2739 - mse: 0.2621 - rmse: 0.5119 - mae: 0.2739 - mape: 8.4875\n",
      "Epoch 254: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2711 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2711 - mape: 8.3903 - val_loss: 0.2761 - val_mse: 0.2990 - val_rmse: 0.5468 - val_mae: 0.2761 - val_mape: 7.8109 - lr: 0.0010\n",
      "Epoch 255/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2742 - mse: 0.2673 - rmse: 0.5171 - mae: 0.2742 - mape: 8.4108\n",
      "Epoch 255: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2741 - mse: 0.2668 - rmse: 0.5165 - mae: 0.2741 - mape: 8.4235 - val_loss: 0.2641 - val_mse: 0.2507 - val_rmse: 0.5007 - val_mae: 0.2641 - val_mape: 8.1481 - lr: 0.0010\n",
      "Epoch 256/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2722 - mse: 0.2619 - rmse: 0.5117 - mae: 0.2722 - mape: 8.3795\n",
      "Epoch 256: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2722 - mse: 0.2621 - rmse: 0.5119 - mae: 0.2722 - mape: 8.3846 - val_loss: 0.2997 - val_mse: 0.3225 - val_rmse: 0.5679 - val_mae: 0.2997 - val_mape: 9.7083 - lr: 0.0010\n",
      "Epoch 257/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2818 - mse: 0.2779 - rmse: 0.5271 - mae: 0.2818 - mape: 8.7366\n",
      "Epoch 257: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2813 - mse: 0.2766 - rmse: 0.5259 - mae: 0.2813 - mape: 8.7129 - val_loss: 0.2779 - val_mse: 0.2748 - val_rmse: 0.5242 - val_mae: 0.2779 - val_mape: 8.9678 - lr: 0.0010\n",
      "Epoch 258/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2679 - mse: 0.2511 - rmse: 0.5011 - mae: 0.2679 - mape: 8.2598\n",
      "Epoch 258: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2687 - mse: 0.2521 - rmse: 0.5021 - mae: 0.2687 - mape: 8.2883 - val_loss: 0.2882 - val_mse: 0.3071 - val_rmse: 0.5541 - val_mae: 0.2882 - val_mape: 9.3862 - lr: 0.0010\n",
      "Epoch 259/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2717 - mse: 0.2615 - rmse: 0.5114 - mae: 0.2717 - mape: 8.3919\n",
      "Epoch 259: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2721 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2721 - mape: 8.3903 - val_loss: 0.2795 - val_mse: 0.2714 - val_rmse: 0.5209 - val_mae: 0.2795 - val_mape: 9.0046 - lr: 0.0010\n",
      "Epoch 260/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2714 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2714 - mape: 8.3802\n",
      "Epoch 260: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2715 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2715 - mape: 8.3898 - val_loss: 0.2629 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.2629 - val_mape: 8.1566 - lr: 0.0010\n",
      "Epoch 261/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2738 - mse: 0.2664 - rmse: 0.5161 - mae: 0.2738 - mape: 8.4961\n",
      "Epoch 261: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2738 - mse: 0.2667 - rmse: 0.5164 - mae: 0.2738 - mape: 8.4921 - val_loss: 0.2787 - val_mse: 0.2761 - val_rmse: 0.5255 - val_mae: 0.2787 - val_mape: 8.9761 - lr: 0.0010\n",
      "Epoch 262/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2704 - mse: 0.2572 - rmse: 0.5071 - mae: 0.2704 - mape: 8.3456\n",
      "Epoch 262: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2704 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2704 - mape: 8.3398 - val_loss: 0.2867 - val_mse: 0.2807 - val_rmse: 0.5298 - val_mae: 0.2867 - val_mape: 8.4692 - lr: 0.0010\n",
      "Epoch 263/2000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2670 - mse: 0.2477 - rmse: 0.4977 - mae: 0.2670 - mape: 8.2566\n",
      "Epoch 263: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2706 - mse: 0.2565 - rmse: 0.5065 - mae: 0.2706 - mape: 8.3554 - val_loss: 0.2733 - val_mse: 0.2633 - val_rmse: 0.5131 - val_mae: 0.2733 - val_mape: 8.2020 - lr: 0.0010\n",
      "Epoch 264/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2711 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2711 - mape: 8.3915\n",
      "Epoch 264: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2715 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2715 - mape: 8.4056 - val_loss: 0.2635 - val_mse: 0.2432 - val_rmse: 0.4932 - val_mae: 0.2635 - val_mape: 8.2153 - lr: 0.0010\n",
      "Epoch 265/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2723 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2723 - mape: 8.4130\n",
      "Epoch 265: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2716 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2716 - mape: 8.3868 - val_loss: 0.2637 - val_mse: 0.2484 - val_rmse: 0.4984 - val_mae: 0.2637 - val_mape: 7.9694 - lr: 0.0010\n",
      "Epoch 266/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2724 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2724 - mape: 8.4244\n",
      "Epoch 266: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2724 - mse: 0.2611 - rmse: 0.5110 - mae: 0.2724 - mape: 8.4244 - val_loss: 0.2658 - val_mse: 0.2516 - val_rmse: 0.5016 - val_mae: 0.2658 - val_mape: 7.9340 - lr: 0.0010\n",
      "Epoch 267/2000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2688 - mse: 0.2558 - rmse: 0.5058 - mae: 0.2688 - mape: 8.2706\n",
      "Epoch 267: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2703 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2703 - mape: 8.3028 - val_loss: 0.2648 - val_mse: 0.2554 - val_rmse: 0.5054 - val_mae: 0.2648 - val_mape: 8.3857 - lr: 0.0010\n",
      "Epoch 268/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2741 - mse: 0.2663 - rmse: 0.5161 - mae: 0.2741 - mape: 8.4282\n",
      "Epoch 268: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2723 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2723 - mape: 8.3834 - val_loss: 0.2711 - val_mse: 0.2740 - val_rmse: 0.5235 - val_mae: 0.2711 - val_mape: 7.8512 - lr: 0.0010\n",
      "Epoch 269/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2725 - mse: 0.2612 - rmse: 0.5110 - mae: 0.2725 - mape: 8.3479\n",
      "Epoch 269: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2724 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2724 - mape: 8.3638 - val_loss: 0.2781 - val_mse: 0.2792 - val_rmse: 0.5284 - val_mae: 0.2781 - val_mape: 8.9799 - lr: 0.0010\n",
      "Epoch 270/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2714 - mse: 0.2640 - rmse: 0.5138 - mae: 0.2714 - mape: 8.3122\n",
      "Epoch 270: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2709 - mse: 0.2626 - rmse: 0.5124 - mae: 0.2709 - mape: 8.3175 - val_loss: 0.2643 - val_mse: 0.2472 - val_rmse: 0.4972 - val_mae: 0.2643 - val_mape: 8.3156 - lr: 0.0010\n",
      "Epoch 271/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2715 - mse: 0.2633 - rmse: 0.5131 - mae: 0.2715 - mape: 8.4198\n",
      "Epoch 271: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2702 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2702 - mape: 8.3605 - val_loss: 0.2625 - val_mse: 0.2461 - val_rmse: 0.4961 - val_mae: 0.2625 - val_mape: 8.0210 - lr: 0.0010\n",
      "Epoch 272/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2701 - mse: 0.2613 - rmse: 0.5111 - mae: 0.2701 - mape: 8.3363\n",
      "Epoch 272: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2704 - mse: 0.2618 - rmse: 0.5117 - mae: 0.2704 - mape: 8.3440 - val_loss: 0.2759 - val_mse: 0.2644 - val_rmse: 0.5142 - val_mae: 0.2759 - val_mape: 8.4090 - lr: 0.0010\n",
      "Epoch 273/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2744 - mse: 0.2634 - rmse: 0.5132 - mae: 0.2744 - mape: 8.4842\n",
      "Epoch 273: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2733 - mse: 0.2630 - rmse: 0.5129 - mae: 0.2733 - mape: 8.4480 - val_loss: 0.2662 - val_mse: 0.2522 - val_rmse: 0.5022 - val_mae: 0.2662 - val_mape: 7.9173 - lr: 0.0010\n",
      "Epoch 274/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2708 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2708 - mape: 8.3542\n",
      "Epoch 274: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2700 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2700 - mape: 8.3261 - val_loss: 0.2624 - val_mse: 0.2526 - val_rmse: 0.5026 - val_mae: 0.2624 - val_mape: 8.0759 - lr: 0.0010\n",
      "Epoch 275/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2707 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2707 - mape: 8.3656\n",
      "Epoch 275: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2709 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2709 - mape: 8.3547 - val_loss: 0.2624 - val_mse: 0.2512 - val_rmse: 0.5012 - val_mae: 0.2624 - val_mape: 7.8576 - lr: 0.0010\n",
      "Epoch 276/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2746 - mse: 0.2710 - rmse: 0.5206 - mae: 0.2746 - mape: 8.4369\n",
      "Epoch 276: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2742 - mse: 0.2701 - rmse: 0.5197 - mae: 0.2742 - mape: 8.4297 - val_loss: 0.2779 - val_mse: 0.2609 - val_rmse: 0.5108 - val_mae: 0.2779 - val_mape: 8.3403 - lr: 0.0010\n",
      "Epoch 277/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2707 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2707 - mape: 8.3684\n",
      "Epoch 277: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2705 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2705 - mape: 8.3621 - val_loss: 0.2699 - val_mse: 0.2568 - val_rmse: 0.5067 - val_mae: 0.2699 - val_mape: 8.5426 - lr: 0.0010\n",
      "Epoch 278/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2686 - mse: 0.2551 - rmse: 0.5051 - mae: 0.2686 - mape: 8.3146\n",
      "Epoch 278: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2693 - mse: 0.2565 - rmse: 0.5065 - mae: 0.2693 - mape: 8.3265 - val_loss: 0.2619 - val_mse: 0.2418 - val_rmse: 0.4918 - val_mae: 0.2619 - val_mape: 8.0293 - lr: 0.0010\n",
      "Epoch 279/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2712 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2712 - mape: 8.3603\n",
      "Epoch 279: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2704 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2704 - mape: 8.3511 - val_loss: 0.2662 - val_mse: 0.2511 - val_rmse: 0.5011 - val_mae: 0.2662 - val_mape: 8.4885 - lr: 0.0010\n",
      "Epoch 280/2000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2691 - mse: 0.2558 - rmse: 0.5058 - mae: 0.2691 - mape: 8.2700\n",
      "Epoch 280: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2708 - mse: 0.2597 - rmse: 0.5097 - mae: 0.2708 - mape: 8.3249 - val_loss: 0.2676 - val_mse: 0.2474 - val_rmse: 0.4974 - val_mae: 0.2676 - val_mape: 8.4874 - lr: 0.0010\n",
      "Epoch 281/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2702 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2702 - mape: 8.3272\n",
      "Epoch 281: val_loss did not improve from 0.26153\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2704 - mse: 0.2581 - rmse: 0.5080 - mae: 0.2704 - mape: 8.3377 - val_loss: 0.2782 - val_mse: 0.2663 - val_rmse: 0.5161 - val_mae: 0.2782 - val_mape: 9.0629 - lr: 0.0010\n",
      "Epoch 282/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2714 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2714 - mape: 8.4080\n",
      "Epoch 282: val_loss improved from 0.26153 to 0.26140, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2706 - mse: 0.2594 - rmse: 0.5093 - mae: 0.2706 - mape: 8.3695 - val_loss: 0.2614 - val_mse: 0.2373 - val_rmse: 0.4872 - val_mae: 0.2614 - val_mape: 7.9844 - lr: 0.0010\n",
      "Epoch 283/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2721 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2721 - mape: 8.4099\n",
      "Epoch 283: val_loss did not improve from 0.26140\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2718 - mse: 0.2597 - rmse: 0.5096 - mae: 0.2718 - mape: 8.4143 - val_loss: 0.2616 - val_mse: 0.2472 - val_rmse: 0.4972 - val_mae: 0.2616 - val_mape: 8.2080 - lr: 0.0010\n",
      "Epoch 284/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2705 - mse: 0.2604 - rmse: 0.5103 - mae: 0.2705 - mape: 8.3687\n",
      "Epoch 284: val_loss did not improve from 0.26140\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2709 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2709 - mape: 8.3787 - val_loss: 0.2652 - val_mse: 0.2408 - val_rmse: 0.4907 - val_mae: 0.2652 - val_mape: 8.1742 - lr: 0.0010\n",
      "Epoch 285/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2682 - mse: 0.2550 - rmse: 0.5049 - mae: 0.2682 - mape: 8.3061\n",
      "Epoch 285: val_loss did not improve from 0.26140\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2681 - mse: 0.2548 - rmse: 0.5048 - mae: 0.2681 - mape: 8.3034 - val_loss: 0.2697 - val_mse: 0.2445 - val_rmse: 0.4945 - val_mae: 0.2697 - val_mape: 8.1372 - lr: 0.0010\n",
      "Epoch 286/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2699 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2699 - mape: 8.3436\n",
      "Epoch 286: val_loss did not improve from 0.26140\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2696 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2696 - mape: 8.3459 - val_loss: 0.2646 - val_mse: 0.2466 - val_rmse: 0.4966 - val_mae: 0.2646 - val_mape: 8.3556 - lr: 0.0010\n",
      "Epoch 287/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2661 - mse: 0.2534 - rmse: 0.5034 - mae: 0.2661 - mape: 8.2030\n",
      "Epoch 287: val_loss did not improve from 0.26140\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2537 - rmse: 0.5037 - mae: 0.2666 - mape: 8.2154 - val_loss: 0.2672 - val_mse: 0.2603 - val_rmse: 0.5102 - val_mae: 0.2672 - val_mape: 7.9460 - lr: 0.0010\n",
      "Epoch 288/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2765 - mse: 0.2714 - rmse: 0.5210 - mae: 0.2765 - mape: 8.5645\n",
      "Epoch 288: val_loss did not improve from 0.26140\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2769 - mse: 0.2716 - rmse: 0.5211 - mae: 0.2769 - mape: 8.5662 - val_loss: 0.2674 - val_mse: 0.2403 - val_rmse: 0.4903 - val_mae: 0.2674 - val_mape: 8.2939 - lr: 0.0010\n",
      "Epoch 289/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2664 - mse: 0.2558 - rmse: 0.5057 - mae: 0.2664 - mape: 8.2429\n",
      "Epoch 289: val_loss did not improve from 0.26140\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2669 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2669 - mape: 8.2565 - val_loss: 0.2643 - val_mse: 0.2511 - val_rmse: 0.5011 - val_mae: 0.2643 - val_mape: 8.4444 - lr: 0.0010\n",
      "Epoch 290/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2692 - mse: 0.2579 - rmse: 0.5079 - mae: 0.2692 - mape: 8.3812\n",
      "Epoch 290: val_loss did not improve from 0.26140\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2703 - mse: 0.2595 - rmse: 0.5095 - mae: 0.2703 - mape: 8.4096 - val_loss: 0.2751 - val_mse: 0.2654 - val_rmse: 0.5152 - val_mae: 0.2751 - val_mape: 8.8727 - lr: 0.0010\n",
      "Epoch 291/2000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2659 - mse: 0.2538 - rmse: 0.5038 - mae: 0.2659 - mape: 8.2228\n",
      "Epoch 291: val_loss improved from 0.26140 to 0.25980, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2666 - mape: 8.2466 - val_loss: 0.2598 - val_mse: 0.2429 - val_rmse: 0.4928 - val_mae: 0.2598 - val_mape: 8.1150 - lr: 0.0010\n",
      "Epoch 292/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2664 - mse: 0.2565 - rmse: 0.5064 - mae: 0.2664 - mape: 8.2352\n",
      "Epoch 292: val_loss did not improve from 0.25980\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2666 - mse: 0.2573 - rmse: 0.5072 - mae: 0.2666 - mape: 8.2297 - val_loss: 0.2629 - val_mse: 0.2471 - val_rmse: 0.4971 - val_mae: 0.2629 - val_mape: 8.2235 - lr: 0.0010\n",
      "Epoch 293/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2698 - mse: 0.2561 - rmse: 0.5061 - mae: 0.2698 - mape: 8.3295\n",
      "Epoch 293: val_loss improved from 0.25980 to 0.25902, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2688 - mse: 0.2551 - rmse: 0.5050 - mae: 0.2688 - mape: 8.2971 - val_loss: 0.2590 - val_mse: 0.2469 - val_rmse: 0.4969 - val_mae: 0.2590 - val_mape: 8.0865 - lr: 0.0010\n",
      "Epoch 294/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2727 - mse: 0.2649 - rmse: 0.5147 - mae: 0.2727 - mape: 8.4226\n",
      "Epoch 294: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2708 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2708 - mape: 8.3751 - val_loss: 0.2591 - val_mse: 0.2426 - val_rmse: 0.4925 - val_mae: 0.2591 - val_mape: 7.9995 - lr: 0.0010\n",
      "Epoch 295/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2672 - mse: 0.2532 - rmse: 0.5032 - mae: 0.2672 - mape: 8.2808\n",
      "Epoch 295: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2670 - mse: 0.2525 - rmse: 0.5025 - mae: 0.2670 - mape: 8.2652 - val_loss: 0.2964 - val_mse: 0.3173 - val_rmse: 0.5633 - val_mae: 0.2964 - val_mape: 9.5244 - lr: 0.0010\n",
      "Epoch 296/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2673 - mse: 0.2550 - rmse: 0.5050 - mae: 0.2673 - mape: 8.2514\n",
      "Epoch 296: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2666 - mse: 0.2540 - rmse: 0.5040 - mae: 0.2666 - mape: 8.2451 - val_loss: 0.2751 - val_mse: 0.2612 - val_rmse: 0.5111 - val_mae: 0.2751 - val_mape: 8.0802 - lr: 0.0010\n",
      "Epoch 297/2000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2689 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2689 - mape: 8.2940\n",
      "Epoch 297: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2693 - mse: 0.2592 - rmse: 0.5091 - mae: 0.2693 - mape: 8.3094 - val_loss: 0.2632 - val_mse: 0.2545 - val_rmse: 0.5045 - val_mae: 0.2632 - val_mape: 8.0976 - lr: 0.0010\n",
      "Epoch 298/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2676 - mse: 0.2535 - rmse: 0.5035 - mae: 0.2676 - mape: 8.2594\n",
      "Epoch 298: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2687 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2687 - mape: 8.3151 - val_loss: 0.2596 - val_mse: 0.2437 - val_rmse: 0.4937 - val_mae: 0.2596 - val_mape: 8.1309 - lr: 0.0010\n",
      "Epoch 299/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2688 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2688 - mape: 8.3281\n",
      "Epoch 299: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2672 - mse: 0.2552 - rmse: 0.5052 - mae: 0.2672 - mape: 8.2761 - val_loss: 0.2627 - val_mse: 0.2490 - val_rmse: 0.4990 - val_mae: 0.2627 - val_mape: 8.2874 - lr: 0.0010\n",
      "Epoch 300/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2653 - mse: 0.2538 - rmse: 0.5038 - mae: 0.2653 - mape: 8.1578\n",
      "Epoch 300: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2652 - mse: 0.2539 - rmse: 0.5039 - mae: 0.2652 - mape: 8.1591 - val_loss: 0.2656 - val_mse: 0.2510 - val_rmse: 0.5010 - val_mae: 0.2656 - val_mape: 8.0014 - lr: 0.0010\n",
      "Epoch 301/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2653 - mse: 0.2546 - rmse: 0.5046 - mae: 0.2653 - mape: 8.1657\n",
      "Epoch 301: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2654 - mse: 0.2545 - rmse: 0.5045 - mae: 0.2654 - mape: 8.1676 - val_loss: 0.2596 - val_mse: 0.2432 - val_rmse: 0.4932 - val_mae: 0.2596 - val_mape: 8.0966 - lr: 0.0010\n",
      "Epoch 302/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2698 - mse: 0.2568 - rmse: 0.5067 - mae: 0.2698 - mape: 8.3208\n",
      "Epoch 302: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2699 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2699 - mape: 8.3247 - val_loss: 0.2712 - val_mse: 0.2447 - val_rmse: 0.4947 - val_mae: 0.2712 - val_mape: 8.6554 - lr: 0.0010\n",
      "Epoch 303/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2646 - mse: 0.2505 - rmse: 0.5005 - mae: 0.2646 - mape: 8.1501\n",
      "Epoch 303: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2643 - mse: 0.2508 - rmse: 0.5008 - mae: 0.2643 - mape: 8.1534 - val_loss: 0.2612 - val_mse: 0.2443 - val_rmse: 0.4943 - val_mae: 0.2612 - val_mape: 8.0442 - lr: 0.0010\n",
      "Epoch 304/2000\n",
      "261/318 [=======================>......] - ETA: 0s - loss: 0.2697 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2697 - mape: 8.3508\n",
      "Epoch 304: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2705 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2705 - mape: 8.3662 - val_loss: 0.2634 - val_mse: 0.2440 - val_rmse: 0.4940 - val_mae: 0.2634 - val_mape: 8.2160 - lr: 0.0010\n",
      "Epoch 305/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2663 - mse: 0.2509 - rmse: 0.5009 - mae: 0.2663 - mape: 8.2678\n",
      "Epoch 305: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2666 - mse: 0.2505 - rmse: 0.5005 - mae: 0.2666 - mape: 8.2770 - val_loss: 0.4077 - val_mse: 0.7552 - val_rmse: 0.8691 - val_mae: 0.4077 - val_mape: 10.4445 - lr: 0.0010\n",
      "Epoch 306/2000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.2737 - mse: 0.2653 - rmse: 0.5150 - mae: 0.2737 - mape: 8.4986\n",
      "Epoch 306: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2705 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2705 - mape: 8.4023 - val_loss: 0.2786 - val_mse: 0.2767 - val_rmse: 0.5260 - val_mae: 0.2786 - val_mape: 8.9017 - lr: 0.0010\n",
      "Epoch 307/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2723 - mse: 0.2627 - rmse: 0.5125 - mae: 0.2723 - mape: 8.4136\n",
      "Epoch 307: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2725 - mse: 0.2626 - rmse: 0.5125 - mae: 0.2725 - mape: 8.4181 - val_loss: 0.2780 - val_mse: 0.2660 - val_rmse: 0.5158 - val_mae: 0.2780 - val_mape: 8.3206 - lr: 0.0010\n",
      "Epoch 308/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2709 - mse: 0.2629 - rmse: 0.5127 - mae: 0.2709 - mape: 8.3645\n",
      "Epoch 308: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2703 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2703 - mape: 8.3511 - val_loss: 0.2600 - val_mse: 0.2427 - val_rmse: 0.4926 - val_mae: 0.2600 - val_mape: 8.0185 - lr: 0.0010\n",
      "Epoch 309/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2646 - mse: 0.2518 - rmse: 0.5018 - mae: 0.2646 - mape: 8.1729\n",
      "Epoch 309: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2646 - mse: 0.2517 - rmse: 0.5017 - mae: 0.2646 - mape: 8.1725 - val_loss: 0.2595 - val_mse: 0.2452 - val_rmse: 0.4952 - val_mae: 0.2595 - val_mape: 8.1421 - lr: 0.0010\n",
      "Epoch 310/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2702 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2702 - mape: 8.3653\n",
      "Epoch 310: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2702 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2702 - mape: 8.3597 - val_loss: 0.2663 - val_mse: 0.2496 - val_rmse: 0.4996 - val_mae: 0.2663 - val_mape: 7.9802 - lr: 0.0010\n",
      "Epoch 311/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2671 - mse: 0.2561 - rmse: 0.5060 - mae: 0.2671 - mape: 8.2829\n",
      "Epoch 311: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2684 - mse: 0.2569 - rmse: 0.5068 - mae: 0.2684 - mape: 8.3212 - val_loss: 0.3031 - val_mse: 0.3129 - val_rmse: 0.5594 - val_mae: 0.3031 - val_mape: 8.7222 - lr: 0.0010\n",
      "Epoch 312/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2673 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2673 - mape: 8.2792\n",
      "Epoch 312: val_loss did not improve from 0.25902\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2660 - mse: 0.2547 - rmse: 0.5047 - mae: 0.2660 - mape: 8.2302 - val_loss: 0.2609 - val_mse: 0.2416 - val_rmse: 0.4916 - val_mae: 0.2609 - val_mape: 8.1394 - lr: 0.0010\n",
      "Epoch 313/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2698 - mse: 0.2587 - rmse: 0.5087 - mae: 0.2698 - mape: 8.2869\n",
      "Epoch 313: val_loss improved from 0.25902 to 0.25855, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2681 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2681 - mape: 8.2524 - val_loss: 0.2586 - val_mse: 0.2453 - val_rmse: 0.4953 - val_mae: 0.2586 - val_mape: 7.9726 - lr: 0.0010\n",
      "Epoch 314/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2713 - mse: 0.2622 - rmse: 0.5121 - mae: 0.2713 - mape: 8.3883\n",
      "Epoch 314: val_loss did not improve from 0.25855\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2692 - mse: 0.2586 - rmse: 0.5086 - mae: 0.2692 - mape: 8.3396 - val_loss: 0.2676 - val_mse: 0.2389 - val_rmse: 0.4888 - val_mae: 0.2676 - val_mape: 8.4326 - lr: 0.0010\n",
      "Epoch 315/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2730 - mse: 0.2651 - rmse: 0.5149 - mae: 0.2730 - mape: 8.4235\n",
      "Epoch 315: val_loss did not improve from 0.25855\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2707 - mse: 0.2609 - rmse: 0.5108 - mae: 0.2707 - mape: 8.3645 - val_loss: 0.2591 - val_mse: 0.2405 - val_rmse: 0.4904 - val_mae: 0.2591 - val_mape: 7.9661 - lr: 0.0010\n",
      "Epoch 316/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2645 - mse: 0.2533 - rmse: 0.5032 - mae: 0.2645 - mape: 8.1729\n",
      "Epoch 316: val_loss did not improve from 0.25855\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2653 - mse: 0.2534 - rmse: 0.5033 - mae: 0.2653 - mape: 8.1862 - val_loss: 0.2587 - val_mse: 0.2434 - val_rmse: 0.4933 - val_mae: 0.2587 - val_mape: 8.0051 - lr: 0.0010\n",
      "Epoch 317/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2670 - mse: 0.2551 - rmse: 0.5050 - mae: 0.2670 - mape: 8.2472\n",
      "Epoch 317: val_loss did not improve from 0.25855\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2682 - mse: 0.2569 - rmse: 0.5069 - mae: 0.2682 - mape: 8.2731 - val_loss: 0.2909 - val_mse: 0.2990 - val_rmse: 0.5468 - val_mae: 0.2909 - val_mape: 8.4029 - lr: 0.0010\n",
      "Epoch 318/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2679 - mse: 0.2571 - rmse: 0.5070 - mae: 0.2679 - mape: 8.3174\n",
      "Epoch 318: val_loss did not improve from 0.25855\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2682 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2682 - mape: 8.3210 - val_loss: 0.2597 - val_mse: 0.2437 - val_rmse: 0.4937 - val_mae: 0.2597 - val_mape: 8.0274 - lr: 0.0010\n",
      "Epoch 319/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2677 - mse: 0.2629 - rmse: 0.5127 - mae: 0.2677 - mape: 8.2637\n",
      "Epoch 319: val_loss improved from 0.25855 to 0.25842, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2657 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2657 - mape: 8.2025 - val_loss: 0.2584 - val_mse: 0.2443 - val_rmse: 0.4943 - val_mae: 0.2584 - val_mape: 8.0883 - lr: 0.0010\n",
      "Epoch 320/2000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2657 - mse: 0.2533 - rmse: 0.5033 - mae: 0.2657 - mape: 8.1677\n",
      "Epoch 320: val_loss did not improve from 0.25842\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2644 - mse: 0.2512 - rmse: 0.5012 - mae: 0.2644 - mape: 8.1390 - val_loss: 0.2686 - val_mse: 0.2642 - val_rmse: 0.5140 - val_mae: 0.2686 - val_mape: 8.7267 - lr: 0.0010\n",
      "Epoch 321/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2661 - mse: 0.2558 - rmse: 0.5058 - mae: 0.2661 - mape: 8.2200\n",
      "Epoch 321: val_loss did not improve from 0.25842\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2661 - mse: 0.2535 - rmse: 0.5035 - mae: 0.2661 - mape: 8.2404 - val_loss: 0.2777 - val_mse: 0.2618 - val_rmse: 0.5116 - val_mae: 0.2777 - val_mape: 8.9766 - lr: 0.0010\n",
      "Epoch 322/2000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2751 - mse: 0.2694 - rmse: 0.5191 - mae: 0.2751 - mape: 8.4792\n",
      "Epoch 322: val_loss did not improve from 0.25842\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2738 - mse: 0.2662 - rmse: 0.5159 - mae: 0.2738 - mape: 8.4404 - val_loss: 0.2666 - val_mse: 0.2432 - val_rmse: 0.4931 - val_mae: 0.2666 - val_mape: 8.3370 - lr: 0.0010\n",
      "Epoch 323/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2664 - mse: 0.2560 - rmse: 0.5060 - mae: 0.2664 - mape: 8.1811\n",
      "Epoch 323: val_loss did not improve from 0.25842\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2662 - mse: 0.2549 - rmse: 0.5048 - mae: 0.2662 - mape: 8.1682 - val_loss: 0.2644 - val_mse: 0.2432 - val_rmse: 0.4932 - val_mae: 0.2644 - val_mape: 8.4081 - lr: 0.0010\n",
      "Epoch 324/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2637 - mse: 0.2503 - rmse: 0.5003 - mae: 0.2637 - mape: 8.1537\n",
      "Epoch 324: val_loss did not improve from 0.25842\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2633 - mse: 0.2492 - rmse: 0.4992 - mae: 0.2633 - mape: 8.1385 - val_loss: 0.2671 - val_mse: 0.2588 - val_rmse: 0.5087 - val_mae: 0.2671 - val_mape: 7.9519 - lr: 0.0010\n",
      "Epoch 325/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2644 - mse: 0.2520 - rmse: 0.5020 - mae: 0.2644 - mape: 8.2035\n",
      "Epoch 325: val_loss did not improve from 0.25842\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2642 - mse: 0.2518 - rmse: 0.5018 - mae: 0.2642 - mape: 8.2000 - val_loss: 0.2927 - val_mse: 0.3444 - val_rmse: 0.5868 - val_mae: 0.2927 - val_mape: 8.2672 - lr: 0.0010\n",
      "Epoch 326/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2661 - mse: 0.2545 - rmse: 0.5045 - mae: 0.2661 - mape: 8.2390\n",
      "Epoch 326: val_loss did not improve from 0.25842\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2661 - mse: 0.2544 - rmse: 0.5043 - mae: 0.2661 - mape: 8.2329 - val_loss: 0.2657 - val_mse: 0.2580 - val_rmse: 0.5080 - val_mae: 0.2657 - val_mape: 8.4115 - lr: 0.0010\n",
      "Epoch 327/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2671 - mse: 0.2550 - rmse: 0.5050 - mae: 0.2671 - mape: 8.2753\n",
      "Epoch 327: val_loss did not improve from 0.25842\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2671 - mse: 0.2551 - rmse: 0.5050 - mae: 0.2671 - mape: 8.2724 - val_loss: 0.2616 - val_mse: 0.2429 - val_rmse: 0.4928 - val_mae: 0.2616 - val_mape: 8.0516 - lr: 0.0010\n",
      "Epoch 328/2000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2649 - mse: 0.2540 - rmse: 0.5040 - mae: 0.2649 - mape: 8.1842\n",
      "Epoch 328: val_loss improved from 0.25842 to 0.25829, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2653 - mse: 0.2527 - rmse: 0.5027 - mae: 0.2653 - mape: 8.1933 - val_loss: 0.2583 - val_mse: 0.2473 - val_rmse: 0.4973 - val_mae: 0.2583 - val_mape: 8.0409 - lr: 0.0010\n",
      "Epoch 329/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2667 - mse: 0.2539 - rmse: 0.5038 - mae: 0.2667 - mape: 8.2391\n",
      "Epoch 329: val_loss did not improve from 0.25829\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2661 - mse: 0.2533 - rmse: 0.5033 - mae: 0.2661 - mape: 8.2357 - val_loss: 0.2672 - val_mse: 0.2565 - val_rmse: 0.5064 - val_mae: 0.2672 - val_mape: 8.0065 - lr: 0.0010\n",
      "Epoch 330/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2695 - mse: 0.2605 - rmse: 0.5104 - mae: 0.2695 - mape: 8.3156\n",
      "Epoch 330: val_loss did not improve from 0.25829\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2708 - mse: 0.2630 - rmse: 0.5129 - mae: 0.2708 - mape: 8.3438 - val_loss: 0.2587 - val_mse: 0.2378 - val_rmse: 0.4877 - val_mae: 0.2587 - val_mape: 8.0844 - lr: 0.0010\n",
      "Epoch 331/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2724 - mse: 0.2593 - rmse: 0.5092 - mae: 0.2724 - mape: 8.3971\n",
      "Epoch 331: val_loss did not improve from 0.25829\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2735 - mse: 0.2619 - rmse: 0.5117 - mae: 0.2735 - mape: 8.4663 - val_loss: 0.2647 - val_mse: 0.2408 - val_rmse: 0.4907 - val_mae: 0.2647 - val_mape: 8.0975 - lr: 0.0010\n",
      "Epoch 332/2000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2671 - mse: 0.2534 - rmse: 0.5034 - mae: 0.2671 - mape: 8.2633\n",
      "Epoch 332: val_loss did not improve from 0.25829\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2665 - mse: 0.2522 - rmse: 0.5022 - mae: 0.2665 - mape: 8.2366 - val_loss: 0.2653 - val_mse: 0.2515 - val_rmse: 0.5015 - val_mae: 0.2653 - val_mape: 8.4501 - lr: 0.0010\n",
      "Epoch 333/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2656 - mse: 0.2532 - rmse: 0.5032 - mae: 0.2656 - mape: 8.1531\n",
      "Epoch 333: val_loss did not improve from 0.25829\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2662 - mse: 0.2539 - rmse: 0.5039 - mae: 0.2662 - mape: 8.1825 - val_loss: 0.2968 - val_mse: 0.3213 - val_rmse: 0.5668 - val_mae: 0.2968 - val_mape: 9.8302 - lr: 0.0010\n",
      "Epoch 334/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2669 - mse: 0.2582 - rmse: 0.5082 - mae: 0.2669 - mape: 8.2298\n",
      "Epoch 334: val_loss did not improve from 0.25829\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2666 - mse: 0.2585 - rmse: 0.5084 - mae: 0.2666 - mape: 8.2207 - val_loss: 0.2645 - val_mse: 0.2539 - val_rmse: 0.5039 - val_mae: 0.2645 - val_mape: 7.9109 - lr: 0.0010\n",
      "Epoch 335/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2656 - mse: 0.2512 - rmse: 0.5012 - mae: 0.2656 - mape: 8.1731\n",
      "Epoch 335: val_loss did not improve from 0.25829\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2660 - mse: 0.2520 - rmse: 0.5020 - mae: 0.2660 - mape: 8.1968 - val_loss: 0.2603 - val_mse: 0.2425 - val_rmse: 0.4925 - val_mae: 0.2603 - val_mape: 8.1058 - lr: 0.0010\n",
      "Epoch 336/2000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.2636 - mse: 0.2515 - rmse: 0.5015 - mae: 0.2636 - mape: 8.0999\n",
      "Epoch 336: val_loss did not improve from 0.25829\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2646 - mse: 0.2541 - rmse: 0.5041 - mae: 0.2646 - mape: 8.1382 - val_loss: 0.2657 - val_mse: 0.2424 - val_rmse: 0.4924 - val_mae: 0.2657 - val_mape: 8.2604 - lr: 0.0010\n",
      "Epoch 337/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2668 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2668 - mape: 8.2543\n",
      "Epoch 337: val_loss did not improve from 0.25829\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2661 - mse: 0.2538 - rmse: 0.5038 - mae: 0.2661 - mape: 8.2483 - val_loss: 0.2637 - val_mse: 0.2507 - val_rmse: 0.5007 - val_mae: 0.2637 - val_mape: 7.9065 - lr: 0.0010\n",
      "Epoch 338/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2652 - mse: 0.2514 - rmse: 0.5014 - mae: 0.2652 - mape: 8.2085\n",
      "Epoch 338: val_loss did not improve from 0.25829\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2667 - mse: 0.2539 - rmse: 0.5039 - mae: 0.2667 - mape: 8.2536 - val_loss: 0.2605 - val_mse: 0.2458 - val_rmse: 0.4958 - val_mae: 0.2605 - val_mape: 8.0266 - lr: 0.0010\n",
      "Epoch 339/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2664 - mse: 0.2518 - rmse: 0.5018 - mae: 0.2664 - mape: 8.2368\n",
      "Epoch 339: val_loss did not improve from 0.25829\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2664 - mse: 0.2523 - rmse: 0.5023 - mae: 0.2664 - mape: 8.2508 - val_loss: 0.2588 - val_mse: 0.2433 - val_rmse: 0.4932 - val_mae: 0.2588 - val_mape: 8.0040 - lr: 0.0010\n",
      "Epoch 340/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2693 - mse: 0.2560 - rmse: 0.5060 - mae: 0.2693 - mape: 8.3144\n",
      "Epoch 340: val_loss improved from 0.25829 to 0.25813, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2687 - mse: 0.2549 - rmse: 0.5049 - mae: 0.2687 - mape: 8.3045 - val_loss: 0.2581 - val_mse: 0.2454 - val_rmse: 0.4954 - val_mae: 0.2581 - val_mape: 7.9412 - lr: 0.0010\n",
      "Epoch 341/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2684 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2684 - mape: 8.3067\n",
      "Epoch 341: val_loss did not improve from 0.25813\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2684 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2684 - mape: 8.3053 - val_loss: 0.2748 - val_mse: 0.2601 - val_rmse: 0.5100 - val_mae: 0.2748 - val_mape: 8.1179 - lr: 0.0010\n",
      "Epoch 342/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2682 - mse: 0.2560 - rmse: 0.5060 - mae: 0.2682 - mape: 8.2546\n",
      "Epoch 342: val_loss did not improve from 0.25813\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2684 - mse: 0.2566 - rmse: 0.5066 - mae: 0.2684 - mape: 8.2601 - val_loss: 0.2603 - val_mse: 0.2448 - val_rmse: 0.4948 - val_mae: 0.2603 - val_mape: 7.8566 - lr: 0.0010\n",
      "Epoch 343/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2642 - mse: 0.2497 - rmse: 0.4997 - mae: 0.2642 - mape: 8.1883\n",
      "Epoch 343: val_loss did not improve from 0.25813\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2647 - mse: 0.2504 - rmse: 0.5004 - mae: 0.2647 - mape: 8.2070 - val_loss: 0.2649 - val_mse: 0.2516 - val_rmse: 0.5016 - val_mae: 0.2649 - val_mape: 7.9399 - lr: 0.0010\n",
      "Epoch 344/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2667 - mse: 0.2548 - rmse: 0.5048 - mae: 0.2667 - mape: 8.2694\n",
      "Epoch 344: val_loss did not improve from 0.25813\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2667 - mse: 0.2548 - rmse: 0.5048 - mae: 0.2667 - mape: 8.2694 - val_loss: 0.2751 - val_mse: 0.2610 - val_rmse: 0.5109 - val_mae: 0.2751 - val_mape: 8.1429 - lr: 0.0010\n",
      "Epoch 345/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2669 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2669 - mape: 8.2635\n",
      "Epoch 345: val_loss did not improve from 0.25813\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2680 - mse: 0.2590 - rmse: 0.5089 - mae: 0.2680 - mape: 8.2824 - val_loss: 0.2658 - val_mse: 0.2379 - val_rmse: 0.4878 - val_mae: 0.2658 - val_mape: 8.1566 - lr: 0.0010\n",
      "Epoch 346/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2651 - mse: 0.2523 - rmse: 0.5023 - mae: 0.2651 - mape: 8.1954\n",
      "Epoch 346: val_loss did not improve from 0.25813\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2651 - mse: 0.2523 - rmse: 0.5023 - mae: 0.2651 - mape: 8.1954 - val_loss: 0.2603 - val_mse: 0.2448 - val_rmse: 0.4948 - val_mae: 0.2603 - val_mape: 8.1777 - lr: 0.0010\n",
      "Epoch 347/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2663 - mse: 0.2534 - rmse: 0.5034 - mae: 0.2663 - mape: 8.2803\n",
      "Epoch 347: val_loss did not improve from 0.25813\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2667 - mse: 0.2536 - rmse: 0.5036 - mae: 0.2667 - mape: 8.2808 - val_loss: 0.2595 - val_mse: 0.2454 - val_rmse: 0.4954 - val_mae: 0.2595 - val_mape: 8.2001 - lr: 0.0010\n",
      "Epoch 348/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2687 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2687 - mape: 8.3101\n",
      "Epoch 348: val_loss did not improve from 0.25813\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2684 - mse: 0.2616 - rmse: 0.5115 - mae: 0.2684 - mape: 8.2996 - val_loss: 0.2963 - val_mse: 0.3181 - val_rmse: 0.5640 - val_mae: 0.2963 - val_mape: 9.7327 - lr: 0.0010\n",
      "Epoch 349/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2652 - mse: 0.2553 - rmse: 0.5053 - mae: 0.2652 - mape: 8.1660\n",
      "Epoch 349: val_loss did not improve from 0.25813\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2655 - mse: 0.2560 - rmse: 0.5060 - mae: 0.2655 - mape: 8.1787 - val_loss: 0.2878 - val_mse: 0.2939 - val_rmse: 0.5421 - val_mae: 0.2878 - val_mape: 9.4475 - lr: 0.0010\n",
      "Epoch 350/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2654 - mse: 0.2560 - rmse: 0.5060 - mae: 0.2654 - mape: 8.2241\n",
      "Epoch 350: val_loss did not improve from 0.25813\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2654 - mse: 0.2562 - rmse: 0.5062 - mae: 0.2654 - mape: 8.2259 - val_loss: 0.2704 - val_mse: 0.2582 - val_rmse: 0.5082 - val_mae: 0.2704 - val_mape: 8.7026 - lr: 0.0010\n",
      "Epoch 351/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2676 - mse: 0.2574 - rmse: 0.5074 - mae: 0.2676 - mape: 8.2743\n",
      "Epoch 351: val_loss did not improve from 0.25813\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2676 - mse: 0.2588 - rmse: 0.5087 - mae: 0.2676 - mape: 8.2575 - val_loss: 0.2625 - val_mse: 0.2514 - val_rmse: 0.5014 - val_mae: 0.2625 - val_mape: 8.3213 - lr: 0.0010\n",
      "Epoch 352/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2641 - mse: 0.2533 - rmse: 0.5033 - mae: 0.2641 - mape: 8.1826\n",
      "Epoch 352: val_loss improved from 0.25813 to 0.25744, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2660 - mse: 0.2555 - rmse: 0.5054 - mae: 0.2660 - mape: 8.2404 - val_loss: 0.2574 - val_mse: 0.2441 - val_rmse: 0.4941 - val_mae: 0.2574 - val_mape: 7.9901 - lr: 0.0010\n",
      "Epoch 353/2000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.2677 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2677 - mape: 8.2453\n",
      "Epoch 353: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2682 - mse: 0.2599 - rmse: 0.5098 - mae: 0.2682 - mape: 8.2799 - val_loss: 0.2606 - val_mse: 0.2484 - val_rmse: 0.4984 - val_mae: 0.2606 - val_mape: 8.1841 - lr: 0.0010\n",
      "Epoch 354/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2718 - mse: 0.2610 - rmse: 0.5109 - mae: 0.2718 - mape: 8.4354\n",
      "Epoch 354: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2725 - mse: 0.2613 - rmse: 0.5111 - mae: 0.2725 - mape: 8.4473 - val_loss: 0.2702 - val_mse: 0.2697 - val_rmse: 0.5193 - val_mae: 0.2702 - val_mape: 8.7229 - lr: 0.0010\n",
      "Epoch 355/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2670 - mse: 0.2559 - rmse: 0.5059 - mae: 0.2670 - mape: 8.2782\n",
      "Epoch 355: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2661 - mse: 0.2539 - rmse: 0.5039 - mae: 0.2661 - mape: 8.2445 - val_loss: 0.2680 - val_mse: 0.2596 - val_rmse: 0.5096 - val_mae: 0.2680 - val_mape: 8.6279 - lr: 0.0010\n",
      "Epoch 356/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2663 - mse: 0.2577 - rmse: 0.5076 - mae: 0.2663 - mape: 8.2125\n",
      "Epoch 356: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2651 - mse: 0.2556 - rmse: 0.5056 - mae: 0.2651 - mape: 8.1735 - val_loss: 0.2599 - val_mse: 0.2473 - val_rmse: 0.4973 - val_mae: 0.2599 - val_mape: 8.1699 - lr: 0.0010\n",
      "Epoch 357/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2631 - mse: 0.2519 - rmse: 0.5019 - mae: 0.2631 - mape: 8.1110\n",
      "Epoch 357: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2639 - mse: 0.2507 - rmse: 0.5007 - mae: 0.2639 - mape: 8.1584 - val_loss: 0.3139 - val_mse: 0.3629 - val_rmse: 0.6024 - val_mae: 0.3139 - val_mape: 8.7687 - lr: 0.0010\n",
      "Epoch 358/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2748 - mse: 0.2711 - rmse: 0.5206 - mae: 0.2748 - mape: 8.4572\n",
      "Epoch 358: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2740 - mse: 0.2700 - rmse: 0.5196 - mae: 0.2740 - mape: 8.4348 - val_loss: 0.2665 - val_mse: 0.2501 - val_rmse: 0.5001 - val_mae: 0.2665 - val_mape: 8.4645 - lr: 0.0010\n",
      "Epoch 359/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2658 - mse: 0.2545 - rmse: 0.5044 - mae: 0.2658 - mape: 8.2016\n",
      "Epoch 359: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2643 - mse: 0.2514 - rmse: 0.5014 - mae: 0.2643 - mape: 8.1427 - val_loss: 0.2842 - val_mse: 0.2864 - val_rmse: 0.5352 - val_mae: 0.2842 - val_mape: 8.2349 - lr: 0.0010\n",
      "Epoch 360/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2634 - mse: 0.2516 - rmse: 0.5016 - mae: 0.2634 - mape: 8.1448\n",
      "Epoch 360: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2629 - mse: 0.2509 - rmse: 0.5009 - mae: 0.2629 - mape: 8.1334 - val_loss: 0.2586 - val_mse: 0.2421 - val_rmse: 0.4920 - val_mae: 0.2586 - val_mape: 8.0515 - lr: 0.0010\n",
      "Epoch 361/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2684 - mse: 0.2606 - rmse: 0.5105 - mae: 0.2684 - mape: 8.2912\n",
      "Epoch 361: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2674 - mse: 0.2598 - rmse: 0.5097 - mae: 0.2674 - mape: 8.2708 - val_loss: 0.2707 - val_mse: 0.2476 - val_rmse: 0.4976 - val_mae: 0.2707 - val_mape: 8.1509 - lr: 0.0010\n",
      "Epoch 362/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2645 - mse: 0.2514 - rmse: 0.5014 - mae: 0.2645 - mape: 8.1499\n",
      "Epoch 362: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2645 - mse: 0.2514 - rmse: 0.5014 - mae: 0.2645 - mape: 8.1499 - val_loss: 0.2576 - val_mse: 0.2454 - val_rmse: 0.4954 - val_mae: 0.2576 - val_mape: 8.1071 - lr: 0.0010\n",
      "Epoch 363/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2639 - mse: 0.2538 - rmse: 0.5038 - mae: 0.2639 - mape: 8.1519\n",
      "Epoch 363: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2647 - mse: 0.2545 - rmse: 0.5045 - mae: 0.2647 - mape: 8.1787 - val_loss: 0.2576 - val_mse: 0.2405 - val_rmse: 0.4904 - val_mae: 0.2576 - val_mape: 8.0213 - lr: 0.0010\n",
      "Epoch 364/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2692 - mse: 0.2626 - rmse: 0.5124 - mae: 0.2692 - mape: 8.2581\n",
      "Epoch 364: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2687 - mse: 0.2617 - rmse: 0.5116 - mae: 0.2687 - mape: 8.2355 - val_loss: 0.2753 - val_mse: 0.2824 - val_rmse: 0.5314 - val_mae: 0.2753 - val_mape: 8.9146 - lr: 0.0010\n",
      "Epoch 365/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2674 - mse: 0.2578 - rmse: 0.5077 - mae: 0.2674 - mape: 8.2347\n",
      "Epoch 365: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2674 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2674 - mape: 8.2353 - val_loss: 0.2605 - val_mse: 0.2446 - val_rmse: 0.4945 - val_mae: 0.2605 - val_mape: 8.3002 - lr: 0.0010\n",
      "Epoch 366/2000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2671 - mse: 0.2525 - rmse: 0.5025 - mae: 0.2671 - mape: 8.2550\n",
      "Epoch 366: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2674 - mse: 0.2552 - rmse: 0.5051 - mae: 0.2674 - mape: 8.2698 - val_loss: 0.2588 - val_mse: 0.2405 - val_rmse: 0.4904 - val_mae: 0.2588 - val_mape: 8.0216 - lr: 0.0010\n",
      "Epoch 367/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2653 - mse: 0.2523 - rmse: 0.5023 - mae: 0.2653 - mape: 8.1891\n",
      "Epoch 367: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2648 - mse: 0.2513 - rmse: 0.5013 - mae: 0.2648 - mape: 8.1784 - val_loss: 0.2627 - val_mse: 0.2436 - val_rmse: 0.4936 - val_mae: 0.2627 - val_mape: 8.0209 - lr: 0.0010\n",
      "Epoch 368/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2672 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2672 - mape: 8.2674\n",
      "Epoch 368: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2672 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2672 - mape: 8.2685 - val_loss: 0.2996 - val_mse: 0.3225 - val_rmse: 0.5679 - val_mae: 0.2996 - val_mape: 9.5393 - lr: 0.0010\n",
      "Epoch 369/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2658 - mse: 0.2538 - rmse: 0.5037 - mae: 0.2658 - mape: 8.1959\n",
      "Epoch 369: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2649 - mse: 0.2529 - rmse: 0.5028 - mae: 0.2649 - mape: 8.1746 - val_loss: 0.2628 - val_mse: 0.2443 - val_rmse: 0.4942 - val_mae: 0.2628 - val_mape: 8.3731 - lr: 0.0010\n",
      "Epoch 370/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2648 - mse: 0.2507 - rmse: 0.5007 - mae: 0.2648 - mape: 8.1802\n",
      "Epoch 370: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2646 - mse: 0.2501 - rmse: 0.5001 - mae: 0.2646 - mape: 8.1738 - val_loss: 0.2782 - val_mse: 0.2791 - val_rmse: 0.5283 - val_mae: 0.2782 - val_mape: 9.0072 - lr: 0.0010\n",
      "Epoch 371/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2669 - mse: 0.2576 - rmse: 0.5075 - mae: 0.2669 - mape: 8.2733\n",
      "Epoch 371: val_loss did not improve from 0.25744\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2663 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2663 - mape: 8.2506 - val_loss: 0.2597 - val_mse: 0.2455 - val_rmse: 0.4955 - val_mae: 0.2597 - val_mape: 7.8957 - lr: 0.0010\n",
      "Epoch 372/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2668 - mse: 0.2549 - rmse: 0.5049 - mae: 0.2668 - mape: 8.2363\n",
      "Epoch 372: val_loss improved from 0.25744 to 0.25634, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2662 - mse: 0.2540 - rmse: 0.5040 - mae: 0.2662 - mape: 8.2171 - val_loss: 0.2563 - val_mse: 0.2415 - val_rmse: 0.4914 - val_mae: 0.2563 - val_mape: 7.9585 - lr: 0.0010\n",
      "Epoch 373/2000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.2736 - mse: 0.2656 - rmse: 0.5154 - mae: 0.2736 - mape: 8.4879\n",
      "Epoch 373: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2703 - mse: 0.2602 - rmse: 0.5101 - mae: 0.2703 - mape: 8.3629 - val_loss: 0.2695 - val_mse: 0.2642 - val_rmse: 0.5140 - val_mae: 0.2695 - val_mape: 8.0560 - lr: 0.0010\n",
      "Epoch 374/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2706 - mse: 0.2656 - rmse: 0.5153 - mae: 0.2706 - mape: 8.4018\n",
      "Epoch 374: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2697 - mse: 0.2630 - rmse: 0.5129 - mae: 0.2697 - mape: 8.3695 - val_loss: 0.2630 - val_mse: 0.2502 - val_rmse: 0.5002 - val_mae: 0.2630 - val_mape: 8.3028 - lr: 0.0010\n",
      "Epoch 375/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2618 - mse: 0.2485 - rmse: 0.4985 - mae: 0.2618 - mape: 8.0609\n",
      "Epoch 375: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2630 - mse: 0.2504 - rmse: 0.5004 - mae: 0.2630 - mape: 8.0860 - val_loss: 0.2636 - val_mse: 0.2572 - val_rmse: 0.5072 - val_mae: 0.2636 - val_mape: 7.7604 - lr: 0.0010\n",
      "Epoch 376/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2665 - mse: 0.2526 - rmse: 0.5026 - mae: 0.2665 - mape: 8.2196\n",
      "Epoch 376: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2656 - mse: 0.2523 - rmse: 0.5023 - mae: 0.2656 - mape: 8.2197 - val_loss: 0.2587 - val_mse: 0.2467 - val_rmse: 0.4967 - val_mae: 0.2587 - val_mape: 7.8926 - lr: 0.0010\n",
      "Epoch 377/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2633 - mse: 0.2471 - rmse: 0.4971 - mae: 0.2633 - mape: 8.1417\n",
      "Epoch 377: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2636 - mse: 0.2488 - rmse: 0.4988 - mae: 0.2636 - mape: 8.1387 - val_loss: 0.2608 - val_mse: 0.2474 - val_rmse: 0.4974 - val_mae: 0.2608 - val_mape: 8.1849 - lr: 0.0010\n",
      "Epoch 378/2000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.2640 - mse: 0.2518 - rmse: 0.5018 - mae: 0.2640 - mape: 8.1836\n",
      "Epoch 378: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2639 - mse: 0.2512 - rmse: 0.5012 - mae: 0.2639 - mape: 8.1771 - val_loss: 0.2691 - val_mse: 0.2525 - val_rmse: 0.5024 - val_mae: 0.2691 - val_mape: 8.1383 - lr: 0.0010\n",
      "Epoch 379/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2659 - mse: 0.2552 - rmse: 0.5052 - mae: 0.2659 - mape: 8.1769\n",
      "Epoch 379: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2648 - mse: 0.2532 - rmse: 0.5032 - mae: 0.2648 - mape: 8.1443 - val_loss: 0.2639 - val_mse: 0.2532 - val_rmse: 0.5032 - val_mae: 0.2639 - val_mape: 8.3920 - lr: 0.0010\n",
      "Epoch 380/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2685 - mse: 0.2580 - rmse: 0.5080 - mae: 0.2685 - mape: 8.3252\n",
      "Epoch 380: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2678 - mse: 0.2570 - rmse: 0.5070 - mae: 0.2678 - mape: 8.3023 - val_loss: 0.2680 - val_mse: 0.2553 - val_rmse: 0.5052 - val_mae: 0.2680 - val_mape: 8.5420 - lr: 0.0010\n",
      "Epoch 381/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2646 - mse: 0.2517 - rmse: 0.5017 - mae: 0.2646 - mape: 8.1593\n",
      "Epoch 381: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2635 - mse: 0.2499 - rmse: 0.4999 - mae: 0.2635 - mape: 8.1221 - val_loss: 0.2590 - val_mse: 0.2517 - val_rmse: 0.5017 - val_mae: 0.2590 - val_mape: 8.1417 - lr: 0.0010\n",
      "Epoch 382/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2642 - mse: 0.2539 - rmse: 0.5038 - mae: 0.2642 - mape: 8.1365\n",
      "Epoch 382: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2657 - mse: 0.2579 - rmse: 0.5078 - mae: 0.2657 - mape: 8.1631 - val_loss: 0.2658 - val_mse: 0.2571 - val_rmse: 0.5071 - val_mae: 0.2658 - val_mape: 8.4444 - lr: 0.0010\n",
      "Epoch 383/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2655 - mse: 0.2537 - rmse: 0.5037 - mae: 0.2655 - mape: 8.2035\n",
      "Epoch 383: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2655 - mse: 0.2537 - rmse: 0.5037 - mae: 0.2655 - mape: 8.2035 - val_loss: 0.2595 - val_mse: 0.2519 - val_rmse: 0.5019 - val_mae: 0.2595 - val_mape: 8.1450 - lr: 0.0010\n",
      "Epoch 384/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2695 - mse: 0.2623 - rmse: 0.5121 - mae: 0.2695 - mape: 8.3390\n",
      "Epoch 384: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2691 - mse: 0.2614 - rmse: 0.5112 - mae: 0.2691 - mape: 8.3271 - val_loss: 0.2623 - val_mse: 0.2534 - val_rmse: 0.5034 - val_mae: 0.2623 - val_mape: 8.2809 - lr: 0.0010\n",
      "Epoch 385/2000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2659 - mse: 0.2586 - rmse: 0.5085 - mae: 0.2659 - mape: 8.2163\n",
      "Epoch 385: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2656 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2656 - mape: 8.1752 - val_loss: 0.2635 - val_mse: 0.2571 - val_rmse: 0.5070 - val_mae: 0.2635 - val_mape: 7.9102 - lr: 0.0010\n",
      "Epoch 386/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2622 - mse: 0.2519 - rmse: 0.5019 - mae: 0.2622 - mape: 8.1071\n",
      "Epoch 386: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2628 - mse: 0.2533 - rmse: 0.5033 - mae: 0.2628 - mape: 8.1324 - val_loss: 0.2705 - val_mse: 0.2661 - val_rmse: 0.5158 - val_mae: 0.2705 - val_mape: 7.9372 - lr: 0.0010\n",
      "Epoch 387/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2632 - mse: 0.2502 - rmse: 0.5002 - mae: 0.2632 - mape: 8.1213\n",
      "Epoch 387: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2627 - mse: 0.2494 - rmse: 0.4994 - mae: 0.2627 - mape: 8.1075 - val_loss: 0.2564 - val_mse: 0.2450 - val_rmse: 0.4949 - val_mae: 0.2564 - val_mape: 8.0254 - lr: 0.0010\n",
      "Epoch 388/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2647 - mse: 0.2530 - rmse: 0.5030 - mae: 0.2647 - mape: 8.1744\n",
      "Epoch 388: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2646 - mse: 0.2533 - rmse: 0.5033 - mae: 0.2646 - mape: 8.1627 - val_loss: 0.2589 - val_mse: 0.2457 - val_rmse: 0.4957 - val_mae: 0.2589 - val_mape: 8.1183 - lr: 0.0010\n",
      "Epoch 389/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2632 - mse: 0.2523 - rmse: 0.5023 - mae: 0.2632 - mape: 8.1426\n",
      "Epoch 389: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2630 - mse: 0.2512 - rmse: 0.5012 - mae: 0.2630 - mape: 8.1329 - val_loss: 0.2643 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2643 - val_mape: 8.3110 - lr: 0.0010\n",
      "Epoch 390/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2642 - mse: 0.2502 - rmse: 0.5002 - mae: 0.2642 - mape: 8.1857\n",
      "Epoch 390: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2646 - mse: 0.2514 - rmse: 0.5014 - mae: 0.2646 - mape: 8.2051 - val_loss: 0.2780 - val_mse: 0.2729 - val_rmse: 0.5224 - val_mae: 0.2780 - val_mape: 8.1424 - lr: 0.0010\n",
      "Epoch 391/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2668 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2668 - mape: 8.2482\n",
      "Epoch 391: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2659 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2659 - mape: 8.2258 - val_loss: 0.2601 - val_mse: 0.2386 - val_rmse: 0.4884 - val_mae: 0.2601 - val_mape: 7.9839 - lr: 0.0010\n",
      "Epoch 392/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2653 - mse: 0.2575 - rmse: 0.5075 - mae: 0.2653 - mape: 8.1742\n",
      "Epoch 392: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2670 - mse: 0.2603 - rmse: 0.5102 - mae: 0.2670 - mape: 8.2148 - val_loss: 0.3607 - val_mse: 0.4492 - val_rmse: 0.6703 - val_mae: 0.3607 - val_mape: 10.1177 - lr: 0.0010\n",
      "Epoch 393/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2651 - mse: 0.2546 - rmse: 0.5046 - mae: 0.2651 - mape: 8.2091\n",
      "Epoch 393: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2656 - mse: 0.2551 - rmse: 0.5050 - mae: 0.2656 - mape: 8.2165 - val_loss: 0.2608 - val_mse: 0.2416 - val_rmse: 0.4915 - val_mae: 0.2608 - val_mape: 7.9598 - lr: 0.0010\n",
      "Epoch 394/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2648 - mse: 0.2536 - rmse: 0.5036 - mae: 0.2648 - mape: 8.1475\n",
      "Epoch 394: val_loss did not improve from 0.25634\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2647 - mse: 0.2549 - rmse: 0.5049 - mae: 0.2647 - mape: 8.1481 - val_loss: 0.2577 - val_mse: 0.2409 - val_rmse: 0.4909 - val_mae: 0.2577 - val_mape: 8.0376 - lr: 0.0010\n",
      "Epoch 395/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2631 - mse: 0.2507 - rmse: 0.5007 - mae: 0.2631 - mape: 8.1381\n",
      "Epoch 395: val_loss improved from 0.25634 to 0.25607, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2626 - mse: 0.2501 - rmse: 0.5001 - mae: 0.2626 - mape: 8.1214 - val_loss: 0.2561 - val_mse: 0.2435 - val_rmse: 0.4935 - val_mae: 0.2561 - val_mape: 7.8700 - lr: 0.0010\n",
      "Epoch 396/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2634 - mse: 0.2530 - rmse: 0.5029 - mae: 0.2634 - mape: 8.1538\n",
      "Epoch 396: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2639 - mse: 0.2530 - rmse: 0.5030 - mae: 0.2639 - mape: 8.1560 - val_loss: 0.2738 - val_mse: 0.2702 - val_rmse: 0.5198 - val_mae: 0.2738 - val_mape: 8.7726 - lr: 0.0010\n",
      "Epoch 397/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2652 - mse: 0.2570 - rmse: 0.5070 - mae: 0.2652 - mape: 8.1451\n",
      "Epoch 397: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2662 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2662 - mape: 8.1728 - val_loss: 0.2685 - val_mse: 0.2390 - val_rmse: 0.4889 - val_mae: 0.2685 - val_mape: 8.5347 - lr: 0.0010\n",
      "Epoch 398/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2657 - mse: 0.2537 - rmse: 0.5037 - mae: 0.2657 - mape: 8.2139\n",
      "Epoch 398: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2643 - mse: 0.2511 - rmse: 0.5011 - mae: 0.2643 - mape: 8.1744 - val_loss: 0.2573 - val_mse: 0.2443 - val_rmse: 0.4943 - val_mae: 0.2573 - val_mape: 7.9766 - lr: 0.0010\n",
      "Epoch 399/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2634 - mse: 0.2497 - rmse: 0.4997 - mae: 0.2634 - mape: 8.1718\n",
      "Epoch 399: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2652 - mse: 0.2542 - rmse: 0.5042 - mae: 0.2652 - mape: 8.2066 - val_loss: 0.2603 - val_mse: 0.2442 - val_rmse: 0.4942 - val_mae: 0.2603 - val_mape: 8.0027 - lr: 0.0010\n",
      "Epoch 400/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2629 - mse: 0.2505 - rmse: 0.5005 - mae: 0.2629 - mape: 8.1187\n",
      "Epoch 400: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2641 - mse: 0.2524 - rmse: 0.5024 - mae: 0.2641 - mape: 8.1533 - val_loss: 0.2620 - val_mse: 0.2508 - val_rmse: 0.5008 - val_mae: 0.2620 - val_mape: 8.2585 - lr: 0.0010\n",
      "Epoch 401/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2624 - mse: 0.2508 - rmse: 0.5008 - mae: 0.2624 - mape: 8.1526\n",
      "Epoch 401: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2619 - mse: 0.2491 - rmse: 0.4991 - mae: 0.2619 - mape: 8.1393 - val_loss: 0.2677 - val_mse: 0.2605 - val_rmse: 0.5104 - val_mae: 0.2677 - val_mape: 8.4315 - lr: 0.0010\n",
      "Epoch 402/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2646 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2646 - mape: 8.2123\n",
      "Epoch 402: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2644 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2644 - mape: 8.1873 - val_loss: 0.2655 - val_mse: 0.2527 - val_rmse: 0.5026 - val_mae: 0.2655 - val_mape: 8.3066 - lr: 0.0010\n",
      "Epoch 403/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2653 - mse: 0.2539 - rmse: 0.5038 - mae: 0.2653 - mape: 8.2074\n",
      "Epoch 403: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2652 - mse: 0.2536 - rmse: 0.5036 - mae: 0.2652 - mape: 8.2047 - val_loss: 0.2578 - val_mse: 0.2389 - val_rmse: 0.4888 - val_mae: 0.2578 - val_mape: 7.9026 - lr: 0.0010\n",
      "Epoch 404/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2645 - mse: 0.2554 - rmse: 0.5054 - mae: 0.2645 - mape: 8.1562\n",
      "Epoch 404: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2651 - mse: 0.2567 - rmse: 0.5067 - mae: 0.2651 - mape: 8.1850 - val_loss: 0.2638 - val_mse: 0.2432 - val_rmse: 0.4932 - val_mae: 0.2638 - val_mape: 8.3262 - lr: 0.0010\n",
      "Epoch 405/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2660 - mse: 0.2583 - rmse: 0.5082 - mae: 0.2660 - mape: 8.2551\n",
      "Epoch 405: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2661 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2661 - mape: 8.2529 - val_loss: 0.2621 - val_mse: 0.2408 - val_rmse: 0.4908 - val_mae: 0.2621 - val_mape: 8.0980 - lr: 0.0010\n",
      "Epoch 406/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2698 - mse: 0.2640 - rmse: 0.5138 - mae: 0.2698 - mape: 8.3453\n",
      "Epoch 406: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2695 - mse: 0.2635 - rmse: 0.5133 - mae: 0.2695 - mape: 8.3321 - val_loss: 0.2587 - val_mse: 0.2471 - val_rmse: 0.4971 - val_mae: 0.2587 - val_mape: 8.1802 - lr: 0.0010\n",
      "Epoch 407/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2650 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2650 - mape: 8.2195\n",
      "Epoch 407: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2645 - mse: 0.2545 - rmse: 0.5044 - mae: 0.2645 - mape: 8.2025 - val_loss: 0.2742 - val_mse: 0.2614 - val_rmse: 0.5112 - val_mae: 0.2742 - val_mape: 8.6825 - lr: 0.0010\n",
      "Epoch 408/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2657 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2657 - mape: 8.2296\n",
      "Epoch 408: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2644 - mse: 0.2520 - rmse: 0.5020 - mae: 0.2644 - mape: 8.1955 - val_loss: 0.2683 - val_mse: 0.2677 - val_rmse: 0.5174 - val_mae: 0.2683 - val_mape: 8.6893 - lr: 0.0010\n",
      "Epoch 409/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2659 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2659 - mape: 8.2002\n",
      "Epoch 409: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2652 - mse: 0.2529 - rmse: 0.5029 - mae: 0.2652 - mape: 8.1788 - val_loss: 0.2592 - val_mse: 0.2443 - val_rmse: 0.4943 - val_mae: 0.2592 - val_mape: 8.0171 - lr: 0.0010\n",
      "Epoch 410/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2686 - mse: 0.2548 - rmse: 0.5048 - mae: 0.2686 - mape: 8.3324\n",
      "Epoch 410: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2687 - mse: 0.2553 - rmse: 0.5053 - mae: 0.2687 - mape: 8.3346 - val_loss: 0.2593 - val_mse: 0.2350 - val_rmse: 0.4848 - val_mae: 0.2593 - val_mape: 8.0060 - lr: 0.0010\n",
      "Epoch 411/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2648 - mse: 0.2547 - rmse: 0.5046 - mae: 0.2648 - mape: 8.1804\n",
      "Epoch 411: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2646 - mse: 0.2540 - rmse: 0.5040 - mae: 0.2646 - mape: 8.1742 - val_loss: 0.2584 - val_mse: 0.2434 - val_rmse: 0.4934 - val_mae: 0.2584 - val_mape: 7.8167 - lr: 0.0010\n",
      "Epoch 412/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2659 - mse: 0.2550 - rmse: 0.5050 - mae: 0.2659 - mape: 8.1716\n",
      "Epoch 412: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2666 - mse: 0.2563 - rmse: 0.5062 - mae: 0.2666 - mape: 8.1932 - val_loss: 0.2589 - val_mse: 0.2456 - val_rmse: 0.4956 - val_mae: 0.2589 - val_mape: 7.8951 - lr: 0.0010\n",
      "Epoch 413/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2623 - mse: 0.2502 - rmse: 0.5002 - mae: 0.2623 - mape: 8.0921\n",
      "Epoch 413: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2621 - mse: 0.2501 - rmse: 0.5001 - mae: 0.2621 - mape: 8.0936 - val_loss: 0.2694 - val_mse: 0.2584 - val_rmse: 0.5084 - val_mae: 0.2694 - val_mape: 8.5026 - lr: 0.0010\n",
      "Epoch 414/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2626 - mse: 0.2509 - rmse: 0.5009 - mae: 0.2626 - mape: 8.1322\n",
      "Epoch 414: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2634 - mse: 0.2517 - rmse: 0.5017 - mae: 0.2634 - mape: 8.1489 - val_loss: 0.2573 - val_mse: 0.2363 - val_rmse: 0.4861 - val_mae: 0.2573 - val_mape: 7.9118 - lr: 0.0010\n",
      "Epoch 415/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2656 - mse: 0.2532 - rmse: 0.5032 - mae: 0.2656 - mape: 8.2116\n",
      "Epoch 415: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2646 - mse: 0.2521 - rmse: 0.5021 - mae: 0.2646 - mape: 8.2038 - val_loss: 0.2761 - val_mse: 0.2756 - val_rmse: 0.5250 - val_mae: 0.2761 - val_mape: 8.9008 - lr: 0.0010\n",
      "Epoch 416/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2626 - mse: 0.2481 - rmse: 0.4981 - mae: 0.2626 - mape: 8.1070\n",
      "Epoch 416: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2641 - mse: 0.2512 - rmse: 0.5012 - mae: 0.2641 - mape: 8.1500 - val_loss: 0.2600 - val_mse: 0.2434 - val_rmse: 0.4934 - val_mae: 0.2600 - val_mape: 7.8561 - lr: 0.0010\n",
      "Epoch 417/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2671 - mse: 0.2523 - rmse: 0.5022 - mae: 0.2671 - mape: 8.2733\n",
      "Epoch 417: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2664 - mse: 0.2516 - rmse: 0.5016 - mae: 0.2664 - mape: 8.2784 - val_loss: 0.2737 - val_mse: 0.2619 - val_rmse: 0.5118 - val_mae: 0.2737 - val_mape: 8.7251 - lr: 0.0010\n",
      "Epoch 418/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2621 - mse: 0.2485 - rmse: 0.4985 - mae: 0.2621 - mape: 8.1096\n",
      "Epoch 418: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2624 - mse: 0.2491 - rmse: 0.4991 - mae: 0.2624 - mape: 8.1154 - val_loss: 0.2606 - val_mse: 0.2491 - val_rmse: 0.4991 - val_mae: 0.2606 - val_mape: 8.2709 - lr: 0.0010\n",
      "Epoch 419/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2686 - mse: 0.2584 - rmse: 0.5084 - mae: 0.2686 - mape: 8.3050\n",
      "Epoch 419: val_loss did not improve from 0.25607\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2667 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2667 - mape: 8.2456 - val_loss: 0.2752 - val_mse: 0.2751 - val_rmse: 0.5245 - val_mae: 0.2752 - val_mape: 8.1229 - lr: 0.0010\n",
      "Epoch 420/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2643 - mse: 0.2528 - rmse: 0.5028 - mae: 0.2643 - mape: 8.2045\n",
      "Epoch 420: val_loss improved from 0.25607 to 0.25593, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2639 - mse: 0.2517 - rmse: 0.5017 - mae: 0.2639 - mape: 8.1836 - val_loss: 0.2559 - val_mse: 0.2440 - val_rmse: 0.4940 - val_mae: 0.2559 - val_mape: 7.9787 - lr: 0.0010\n",
      "Epoch 421/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2654 - mse: 0.2567 - rmse: 0.5066 - mae: 0.2654 - mape: 8.1950\n",
      "Epoch 421: val_loss did not improve from 0.25593\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2631 - mse: 0.2526 - rmse: 0.5026 - mae: 0.2631 - mape: 8.1440 - val_loss: 0.2636 - val_mse: 0.2476 - val_rmse: 0.4976 - val_mae: 0.2636 - val_mape: 8.4297 - lr: 0.0010\n",
      "Epoch 422/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2626 - mse: 0.2505 - rmse: 0.5005 - mae: 0.2626 - mape: 8.1356\n",
      "Epoch 422: val_loss did not improve from 0.25593\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2624 - mse: 0.2498 - rmse: 0.4998 - mae: 0.2624 - mape: 8.1274 - val_loss: 0.2769 - val_mse: 0.2791 - val_rmse: 0.5283 - val_mae: 0.2769 - val_mape: 8.0205 - lr: 0.0010\n",
      "Epoch 423/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2637 - mse: 0.2491 - rmse: 0.4991 - mae: 0.2637 - mape: 8.1690\n",
      "Epoch 423: val_loss did not improve from 0.25593\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2630 - mse: 0.2489 - rmse: 0.4989 - mae: 0.2630 - mape: 8.1374 - val_loss: 0.2726 - val_mse: 0.2738 - val_rmse: 0.5233 - val_mae: 0.2726 - val_mape: 7.9783 - lr: 0.0010\n",
      "Epoch 424/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2650 - mse: 0.2550 - rmse: 0.5050 - mae: 0.2650 - mape: 8.2046\n",
      "Epoch 424: val_loss did not improve from 0.25593\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2653 - mse: 0.2556 - rmse: 0.5056 - mae: 0.2653 - mape: 8.2170 - val_loss: 0.2597 - val_mse: 0.2380 - val_rmse: 0.4878 - val_mae: 0.2597 - val_mape: 7.9799 - lr: 0.0010\n",
      "Epoch 425/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2631 - mse: 0.2525 - rmse: 0.5024 - mae: 0.2631 - mape: 8.1362\n",
      "Epoch 425: val_loss did not improve from 0.25593\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2631 - mse: 0.2525 - rmse: 0.5024 - mae: 0.2631 - mape: 8.1362 - val_loss: 0.2627 - val_mse: 0.2510 - val_rmse: 0.5010 - val_mae: 0.2627 - val_mape: 8.3490 - lr: 0.0010\n",
      "Epoch 426/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2653 - mse: 0.2537 - rmse: 0.5037 - mae: 0.2653 - mape: 8.2399\n",
      "Epoch 426: val_loss improved from 0.25593 to 0.25568, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2637 - mse: 0.2509 - rmse: 0.5009 - mae: 0.2637 - mape: 8.1917 - val_loss: 0.2557 - val_mse: 0.2447 - val_rmse: 0.4946 - val_mae: 0.2557 - val_mape: 8.0284 - lr: 0.0010\n",
      "Epoch 427/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2640 - mse: 0.2527 - rmse: 0.5027 - mae: 0.2640 - mape: 8.1470\n",
      "Epoch 427: val_loss improved from 0.25568 to 0.25565, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2634 - mse: 0.2521 - rmse: 0.5021 - mae: 0.2634 - mape: 8.1340 - val_loss: 0.2557 - val_mse: 0.2411 - val_rmse: 0.4910 - val_mae: 0.2557 - val_mape: 7.9228 - lr: 0.0010\n",
      "Epoch 428/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2618 - mse: 0.2486 - rmse: 0.4986 - mae: 0.2618 - mape: 8.1388\n",
      "Epoch 428: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2616 - mse: 0.2482 - rmse: 0.4982 - mae: 0.2616 - mape: 8.1203 - val_loss: 0.2607 - val_mse: 0.2457 - val_rmse: 0.4957 - val_mae: 0.2607 - val_mape: 7.8737 - lr: 0.0010\n",
      "Epoch 429/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2637 - mse: 0.2504 - rmse: 0.5004 - mae: 0.2637 - mape: 8.1853\n",
      "Epoch 429: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2630 - mse: 0.2491 - rmse: 0.4991 - mae: 0.2630 - mape: 8.1716 - val_loss: 0.2641 - val_mse: 0.2455 - val_rmse: 0.4955 - val_mae: 0.2641 - val_mape: 8.0495 - lr: 0.0010\n",
      "Epoch 430/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2660 - mse: 0.2555 - rmse: 0.5054 - mae: 0.2660 - mape: 8.2232\n",
      "Epoch 430: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2660 - mse: 0.2554 - rmse: 0.5053 - mae: 0.2660 - mape: 8.2234 - val_loss: 0.2643 - val_mse: 0.2467 - val_rmse: 0.4967 - val_mae: 0.2643 - val_mape: 8.3576 - lr: 0.0010\n",
      "Epoch 431/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2618 - mse: 0.2506 - rmse: 0.5006 - mae: 0.2618 - mape: 8.1133\n",
      "Epoch 431: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2613 - mse: 0.2497 - rmse: 0.4997 - mae: 0.2613 - mape: 8.0959 - val_loss: 0.2563 - val_mse: 0.2414 - val_rmse: 0.4913 - val_mae: 0.2563 - val_mape: 7.9346 - lr: 0.0010\n",
      "Epoch 432/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2648 - mse: 0.2531 - rmse: 0.5031 - mae: 0.2648 - mape: 8.2011\n",
      "Epoch 432: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2648 - mse: 0.2530 - rmse: 0.5030 - mae: 0.2648 - mape: 8.2081 - val_loss: 0.2592 - val_mse: 0.2410 - val_rmse: 0.4910 - val_mae: 0.2592 - val_mape: 7.9529 - lr: 0.0010\n",
      "Epoch 433/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2631 - mse: 0.2500 - rmse: 0.5000 - mae: 0.2631 - mape: 8.1236\n",
      "Epoch 433: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2631 - mse: 0.2498 - rmse: 0.4998 - mae: 0.2631 - mape: 8.1219 - val_loss: 0.2663 - val_mse: 0.2538 - val_rmse: 0.5038 - val_mae: 0.2663 - val_mape: 8.5763 - lr: 0.0010\n",
      "Epoch 434/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2633 - mse: 0.2538 - rmse: 0.5038 - mae: 0.2633 - mape: 8.1743\n",
      "Epoch 434: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2636 - mse: 0.2532 - rmse: 0.5032 - mae: 0.2636 - mape: 8.1823 - val_loss: 0.2587 - val_mse: 0.2426 - val_rmse: 0.4926 - val_mae: 0.2587 - val_mape: 8.1177 - lr: 0.0010\n",
      "Epoch 435/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2625 - mse: 0.2509 - rmse: 0.5009 - mae: 0.2625 - mape: 8.0969\n",
      "Epoch 435: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2637 - mse: 0.2523 - rmse: 0.5023 - mae: 0.2637 - mape: 8.1345 - val_loss: 0.2658 - val_mse: 0.2545 - val_rmse: 0.5045 - val_mae: 0.2658 - val_mape: 8.4656 - lr: 0.0010\n",
      "Epoch 436/2000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2638 - mse: 0.2523 - rmse: 0.5023 - mae: 0.2638 - mape: 8.1758\n",
      "Epoch 436: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2633 - mse: 0.2500 - rmse: 0.5000 - mae: 0.2633 - mape: 8.1555 - val_loss: 0.2751 - val_mse: 0.2901 - val_rmse: 0.5386 - val_mae: 0.2751 - val_mape: 8.0253 - lr: 0.0010\n",
      "Epoch 437/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2661 - mse: 0.2591 - rmse: 0.5090 - mae: 0.2661 - mape: 8.2353\n",
      "Epoch 437: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2649 - mse: 0.2574 - rmse: 0.5073 - mae: 0.2649 - mape: 8.1999 - val_loss: 0.2583 - val_mse: 0.2412 - val_rmse: 0.4912 - val_mae: 0.2583 - val_mape: 8.0977 - lr: 0.0010\n",
      "Epoch 438/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2640 - mse: 0.2513 - rmse: 0.5013 - mae: 0.2640 - mape: 8.1775\n",
      "Epoch 438: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2643 - mse: 0.2526 - rmse: 0.5026 - mae: 0.2643 - mape: 8.1781 - val_loss: 0.2886 - val_mse: 0.2609 - val_rmse: 0.5108 - val_mae: 0.2886 - val_mape: 9.5063 - lr: 0.0010\n",
      "Epoch 439/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2678 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2678 - mape: 8.2805\n",
      "Epoch 439: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2646 - mse: 0.2526 - rmse: 0.5026 - mae: 0.2646 - mape: 8.1823 - val_loss: 0.2644 - val_mse: 0.2564 - val_rmse: 0.5063 - val_mae: 0.2644 - val_mape: 8.4012 - lr: 0.0010\n",
      "Epoch 440/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2630 - mse: 0.2528 - rmse: 0.5028 - mae: 0.2630 - mape: 8.1218\n",
      "Epoch 440: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2622 - mse: 0.2511 - rmse: 0.5011 - mae: 0.2622 - mape: 8.0954 - val_loss: 0.2573 - val_mse: 0.2390 - val_rmse: 0.4889 - val_mae: 0.2573 - val_mape: 7.9349 - lr: 0.0010\n",
      "Epoch 441/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2679 - mse: 0.2582 - rmse: 0.5081 - mae: 0.2679 - mape: 8.3041\n",
      "Epoch 441: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2673 - mse: 0.2570 - rmse: 0.5069 - mae: 0.2673 - mape: 8.2695 - val_loss: 0.2579 - val_mse: 0.2400 - val_rmse: 0.4899 - val_mae: 0.2579 - val_mape: 7.8300 - lr: 0.0010\n",
      "Epoch 442/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2633 - mse: 0.2559 - rmse: 0.5058 - mae: 0.2633 - mape: 8.1203\n",
      "Epoch 442: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2628 - mse: 0.2532 - rmse: 0.5031 - mae: 0.2628 - mape: 8.1145 - val_loss: 0.2687 - val_mse: 0.2611 - val_rmse: 0.5110 - val_mae: 0.2687 - val_mape: 8.7235 - lr: 0.0010\n",
      "Epoch 443/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2647 - mse: 0.2533 - rmse: 0.5032 - mae: 0.2647 - mape: 8.2018\n",
      "Epoch 443: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2644 - mse: 0.2522 - rmse: 0.5022 - mae: 0.2644 - mape: 8.1987 - val_loss: 0.2582 - val_mse: 0.2451 - val_rmse: 0.4951 - val_mae: 0.2582 - val_mape: 7.9421 - lr: 0.0010\n",
      "Epoch 444/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2635 - mse: 0.2518 - rmse: 0.5018 - mae: 0.2635 - mape: 8.1455\n",
      "Epoch 444: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2645 - mse: 0.2536 - rmse: 0.5036 - mae: 0.2645 - mape: 8.1687 - val_loss: 0.2576 - val_mse: 0.2424 - val_rmse: 0.4924 - val_mae: 0.2576 - val_mape: 7.7859 - lr: 0.0010\n",
      "Epoch 445/2000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2598 - mse: 0.2455 - rmse: 0.4955 - mae: 0.2598 - mape: 8.0039\n",
      "Epoch 445: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2613 - mse: 0.2492 - rmse: 0.4992 - mae: 0.2613 - mape: 8.0526 - val_loss: 0.2614 - val_mse: 0.2467 - val_rmse: 0.4967 - val_mae: 0.2614 - val_mape: 8.2336 - lr: 0.0010\n",
      "Epoch 446/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2646 - mse: 0.2521 - rmse: 0.5021 - mae: 0.2646 - mape: 8.1371\n",
      "Epoch 446: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2649 - mse: 0.2525 - rmse: 0.5025 - mae: 0.2649 - mape: 8.1444 - val_loss: 0.2894 - val_mse: 0.3164 - val_rmse: 0.5625 - val_mae: 0.2894 - val_mape: 9.5316 - lr: 0.0010\n",
      "Epoch 447/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2630 - mse: 0.2504 - rmse: 0.5004 - mae: 0.2630 - mape: 8.1379\n",
      "Epoch 447: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2631 - mse: 0.2509 - rmse: 0.5009 - mae: 0.2631 - mape: 8.1446 - val_loss: 0.2636 - val_mse: 0.2551 - val_rmse: 0.5050 - val_mae: 0.2636 - val_mape: 7.9015 - lr: 0.0010\n",
      "Epoch 448/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2618 - mse: 0.2495 - rmse: 0.4995 - mae: 0.2618 - mape: 8.0999\n",
      "Epoch 448: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2617 - mse: 0.2492 - rmse: 0.4992 - mae: 0.2617 - mape: 8.0989 - val_loss: 0.2811 - val_mse: 0.2817 - val_rmse: 0.5308 - val_mae: 0.2811 - val_mape: 8.2732 - lr: 0.0010\n",
      "Epoch 449/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2683 - mse: 0.2595 - rmse: 0.5094 - mae: 0.2683 - mape: 8.3021\n",
      "Epoch 449: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2666 - mse: 0.2560 - rmse: 0.5060 - mae: 0.2666 - mape: 8.2593 - val_loss: 0.2600 - val_mse: 0.2434 - val_rmse: 0.4933 - val_mae: 0.2600 - val_mape: 8.1988 - lr: 0.0010\n",
      "Epoch 450/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2678 - mse: 0.2587 - rmse: 0.5086 - mae: 0.2678 - mape: 8.2825\n",
      "Epoch 450: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2677 - mse: 0.2584 - rmse: 0.5083 - mae: 0.2677 - mape: 8.2761 - val_loss: 0.2578 - val_mse: 0.2437 - val_rmse: 0.4937 - val_mae: 0.2578 - val_mape: 7.9111 - lr: 0.0010\n",
      "Epoch 451/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2629 - mse: 0.2504 - rmse: 0.5004 - mae: 0.2629 - mape: 8.1375\n",
      "Epoch 451: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2629 - mse: 0.2503 - rmse: 0.5003 - mae: 0.2629 - mape: 8.1355 - val_loss: 0.2730 - val_mse: 0.2574 - val_rmse: 0.5073 - val_mae: 0.2730 - val_mape: 8.7749 - lr: 0.0010\n",
      "Epoch 452/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2644 - mse: 0.2550 - rmse: 0.5050 - mae: 0.2644 - mape: 8.2133\n",
      "Epoch 452: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2643 - mse: 0.2537 - rmse: 0.5037 - mae: 0.2643 - mape: 8.2027 - val_loss: 0.2619 - val_mse: 0.2448 - val_rmse: 0.4948 - val_mae: 0.2619 - val_mape: 8.3149 - lr: 0.0010\n",
      "Epoch 453/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2653 - mse: 0.2556 - rmse: 0.5055 - mae: 0.2653 - mape: 8.2027\n",
      "Epoch 453: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2651 - mse: 0.2552 - rmse: 0.5052 - mae: 0.2651 - mape: 8.1949 - val_loss: 0.2573 - val_mse: 0.2448 - val_rmse: 0.4948 - val_mae: 0.2573 - val_mape: 7.8189 - lr: 0.0010\n",
      "Epoch 454/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2627 - mse: 0.2534 - rmse: 0.5034 - mae: 0.2627 - mape: 8.1391\n",
      "Epoch 454: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2626 - mse: 0.2519 - rmse: 0.5019 - mae: 0.2626 - mape: 8.1279 - val_loss: 0.2677 - val_mse: 0.2560 - val_rmse: 0.5060 - val_mae: 0.2677 - val_mape: 8.5758 - lr: 0.0010\n",
      "Epoch 455/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2620 - mse: 0.2503 - rmse: 0.5003 - mae: 0.2620 - mape: 8.1233\n",
      "Epoch 455: val_loss did not improve from 0.25565\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2617 - mse: 0.2494 - rmse: 0.4994 - mae: 0.2617 - mape: 8.0927 - val_loss: 0.2569 - val_mse: 0.2390 - val_rmse: 0.4888 - val_mae: 0.2569 - val_mape: 8.0083 - lr: 0.0010\n",
      "Epoch 456/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2617 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2617 - mape: 8.0818\n",
      "Epoch 456: val_loss improved from 0.25565 to 0.25482, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2613 - mse: 0.2478 - rmse: 0.4978 - mae: 0.2613 - mape: 8.0735 - val_loss: 0.2548 - val_mse: 0.2409 - val_rmse: 0.4908 - val_mae: 0.2548 - val_mape: 7.8919 - lr: 0.0010\n",
      "Epoch 457/2000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2632 - mse: 0.2520 - rmse: 0.5020 - mae: 0.2632 - mape: 8.1630\n",
      "Epoch 457: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2630 - mse: 0.2514 - rmse: 0.5014 - mae: 0.2630 - mape: 8.1420 - val_loss: 0.2569 - val_mse: 0.2376 - val_rmse: 0.4874 - val_mae: 0.2569 - val_mape: 7.9420 - lr: 0.0010\n",
      "Epoch 458/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2636 - mse: 0.2498 - rmse: 0.4998 - mae: 0.2636 - mape: 8.1353\n",
      "Epoch 458: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2643 - mse: 0.2512 - rmse: 0.5012 - mae: 0.2643 - mape: 8.1563 - val_loss: 0.2609 - val_mse: 0.2488 - val_rmse: 0.4988 - val_mae: 0.2609 - val_mape: 8.2519 - lr: 0.0010\n",
      "Epoch 459/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2637 - mse: 0.2551 - rmse: 0.5050 - mae: 0.2637 - mape: 8.1518\n",
      "Epoch 459: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2634 - mse: 0.2545 - rmse: 0.5044 - mae: 0.2634 - mape: 8.1596 - val_loss: 0.2736 - val_mse: 0.2670 - val_rmse: 0.5167 - val_mae: 0.2736 - val_mape: 8.0897 - lr: 0.0010\n",
      "Epoch 460/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2628 - mse: 0.2520 - rmse: 0.5020 - mae: 0.2628 - mape: 8.1304\n",
      "Epoch 460: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2625 - mse: 0.2534 - rmse: 0.5034 - mae: 0.2625 - mape: 8.1203 - val_loss: 0.2686 - val_mse: 0.2566 - val_rmse: 0.5065 - val_mae: 0.2686 - val_mape: 7.9485 - lr: 0.0010\n",
      "Epoch 461/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2622 - mse: 0.2506 - rmse: 0.5006 - mae: 0.2622 - mape: 8.0928\n",
      "Epoch 461: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2622 - mse: 0.2507 - rmse: 0.5007 - mae: 0.2622 - mape: 8.0916 - val_loss: 0.2658 - val_mse: 0.2549 - val_rmse: 0.5049 - val_mae: 0.2658 - val_mape: 8.0141 - lr: 0.0010\n",
      "Epoch 462/2000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.2580 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2580 - mape: 8.0087\n",
      "Epoch 462: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2613 - mse: 0.2471 - rmse: 0.4971 - mae: 0.2613 - mape: 8.1039 - val_loss: 0.2650 - val_mse: 0.2419 - val_rmse: 0.4918 - val_mae: 0.2650 - val_mape: 8.0676 - lr: 0.0010\n",
      "Epoch 463/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2617 - mse: 0.2472 - rmse: 0.4972 - mae: 0.2617 - mape: 8.0795\n",
      "Epoch 463: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2616 - mse: 0.2475 - rmse: 0.4975 - mae: 0.2616 - mape: 8.0772 - val_loss: 0.2559 - val_mse: 0.2410 - val_rmse: 0.4909 - val_mae: 0.2559 - val_mape: 7.9005 - lr: 0.0010\n",
      "Epoch 464/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2632 - mse: 0.2512 - rmse: 0.5012 - mae: 0.2632 - mape: 8.1457\n",
      "Epoch 464: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2625 - mse: 0.2494 - rmse: 0.4994 - mae: 0.2625 - mape: 8.1085 - val_loss: 0.2838 - val_mse: 0.2966 - val_rmse: 0.5446 - val_mae: 0.2838 - val_mape: 9.3348 - lr: 0.0010\n",
      "Epoch 465/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2632 - mse: 0.2537 - rmse: 0.5037 - mae: 0.2632 - mape: 8.1489\n",
      "Epoch 465: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2628 - mse: 0.2530 - rmse: 0.5030 - mae: 0.2628 - mape: 8.1388 - val_loss: 0.2579 - val_mse: 0.2452 - val_rmse: 0.4952 - val_mae: 0.2579 - val_mape: 7.8226 - lr: 0.0010\n",
      "Epoch 466/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2627 - mse: 0.2511 - rmse: 0.5011 - mae: 0.2627 - mape: 8.1587\n",
      "Epoch 466: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2625 - mse: 0.2505 - rmse: 0.5005 - mae: 0.2625 - mape: 8.1544 - val_loss: 0.2587 - val_mse: 0.2379 - val_rmse: 0.4877 - val_mae: 0.2587 - val_mape: 7.8490 - lr: 0.0010\n",
      "Epoch 467/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2614 - mse: 0.2476 - rmse: 0.4976 - mae: 0.2614 - mape: 8.0753\n",
      "Epoch 467: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2606 - mse: 0.2460 - rmse: 0.4960 - mae: 0.2606 - mape: 8.0511 - val_loss: 0.2571 - val_mse: 0.2432 - val_rmse: 0.4932 - val_mae: 0.2571 - val_mape: 8.0223 - lr: 0.0010\n",
      "Epoch 468/2000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2668 - mse: 0.2577 - rmse: 0.5077 - mae: 0.2668 - mape: 8.2678\n",
      "Epoch 468: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2669 - mse: 0.2594 - rmse: 0.5094 - mae: 0.2669 - mape: 8.2421 - val_loss: 0.2616 - val_mse: 0.2520 - val_rmse: 0.5020 - val_mae: 0.2616 - val_mape: 8.3618 - lr: 0.0010\n",
      "Epoch 469/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2634 - mse: 0.2551 - rmse: 0.5051 - mae: 0.2634 - mape: 8.1441\n",
      "Epoch 469: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2636 - mse: 0.2550 - rmse: 0.5050 - mae: 0.2636 - mape: 8.1423 - val_loss: 0.2637 - val_mse: 0.2481 - val_rmse: 0.4981 - val_mae: 0.2637 - val_mape: 7.8908 - lr: 0.0010\n",
      "Epoch 470/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2628 - mse: 0.2500 - rmse: 0.5000 - mae: 0.2628 - mape: 8.1347\n",
      "Epoch 470: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2619 - mse: 0.2488 - rmse: 0.4988 - mae: 0.2619 - mape: 8.1159 - val_loss: 0.2580 - val_mse: 0.2473 - val_rmse: 0.4973 - val_mae: 0.2580 - val_mape: 7.9772 - lr: 0.0010\n",
      "Epoch 471/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2651 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2651 - mape: 8.2404\n",
      "Epoch 471: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2651 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2651 - mape: 8.2404 - val_loss: 0.2565 - val_mse: 0.2385 - val_rmse: 0.4884 - val_mae: 0.2565 - val_mape: 7.9812 - lr: 0.0010\n",
      "Epoch 472/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2638 - mse: 0.2530 - rmse: 0.5030 - mae: 0.2638 - mape: 8.1822\n",
      "Epoch 472: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2641 - mse: 0.2535 - rmse: 0.5035 - mae: 0.2641 - mape: 8.1789 - val_loss: 0.2608 - val_mse: 0.2497 - val_rmse: 0.4997 - val_mae: 0.2608 - val_mape: 8.2531 - lr: 0.0010\n",
      "Epoch 473/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2627 - mse: 0.2517 - rmse: 0.5017 - mae: 0.2627 - mape: 8.1187\n",
      "Epoch 473: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2628 - mse: 0.2518 - rmse: 0.5018 - mae: 0.2628 - mape: 8.1226 - val_loss: 0.2552 - val_mse: 0.2376 - val_rmse: 0.4874 - val_mae: 0.2552 - val_mape: 7.8778 - lr: 0.0010\n",
      "Epoch 474/2000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2620 - mse: 0.2516 - rmse: 0.5016 - mae: 0.2620 - mape: 8.0687\n",
      "Epoch 474: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2634 - mse: 0.2542 - rmse: 0.5042 - mae: 0.2634 - mape: 8.1090 - val_loss: 0.2963 - val_mse: 0.3319 - val_rmse: 0.5761 - val_mae: 0.2963 - val_mape: 9.7564 - lr: 0.0010\n",
      "Epoch 475/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2670 - mse: 0.2558 - rmse: 0.5057 - mae: 0.2670 - mape: 8.3007\n",
      "Epoch 475: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2672 - mse: 0.2559 - rmse: 0.5059 - mae: 0.2672 - mape: 8.3234 - val_loss: 0.2658 - val_mse: 0.2514 - val_rmse: 0.5014 - val_mae: 0.2658 - val_mape: 8.0577 - lr: 0.0010\n",
      "Epoch 476/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2639 - mse: 0.2545 - rmse: 0.5045 - mae: 0.2639 - mape: 8.1879\n",
      "Epoch 476: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2632 - mse: 0.2529 - rmse: 0.5029 - mae: 0.2632 - mape: 8.1632 - val_loss: 0.2555 - val_mse: 0.2438 - val_rmse: 0.4938 - val_mae: 0.2555 - val_mape: 7.8058 - lr: 0.0010\n",
      "Epoch 477/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2603 - mse: 0.2487 - rmse: 0.4987 - mae: 0.2603 - mape: 8.0467\n",
      "Epoch 477: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2609 - mse: 0.2494 - rmse: 0.4994 - mae: 0.2609 - mape: 8.0738 - val_loss: 0.2573 - val_mse: 0.2421 - val_rmse: 0.4920 - val_mae: 0.2573 - val_mape: 7.9211 - lr: 0.0010\n",
      "Epoch 478/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2622 - mse: 0.2496 - rmse: 0.4996 - mae: 0.2622 - mape: 8.0980\n",
      "Epoch 478: val_loss did not improve from 0.25482\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2617 - mse: 0.2509 - rmse: 0.5009 - mae: 0.2617 - mape: 8.0874 - val_loss: 0.2619 - val_mse: 0.2492 - val_rmse: 0.4992 - val_mae: 0.2619 - val_mape: 8.3242 - lr: 0.0010\n",
      "Epoch 479/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2642 - mse: 0.2564 - rmse: 0.5064 - mae: 0.2642 - mape: 8.1455\n",
      "Epoch 479: val_loss improved from 0.25482 to 0.25479, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2631 - mse: 0.2542 - rmse: 0.5042 - mae: 0.2631 - mape: 8.1184 - val_loss: 0.2548 - val_mse: 0.2366 - val_rmse: 0.4864 - val_mae: 0.2548 - val_mape: 7.8380 - lr: 0.0010\n",
      "Epoch 480/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2644 - mse: 0.2552 - rmse: 0.5052 - mae: 0.2644 - mape: 8.1611\n",
      "Epoch 480: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2646 - mse: 0.2546 - rmse: 0.5045 - mae: 0.2646 - mape: 8.1692 - val_loss: 0.2580 - val_mse: 0.2481 - val_rmse: 0.4981 - val_mae: 0.2580 - val_mape: 8.2147 - lr: 0.0010\n",
      "Epoch 481/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2597 - mse: 0.2470 - rmse: 0.4970 - mae: 0.2597 - mape: 8.0375\n",
      "Epoch 481: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2619 - mse: 0.2504 - rmse: 0.5004 - mae: 0.2619 - mape: 8.1049 - val_loss: 0.2635 - val_mse: 0.2472 - val_rmse: 0.4971 - val_mae: 0.2635 - val_mape: 8.0276 - lr: 0.0010\n",
      "Epoch 482/2000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2598 - mse: 0.2448 - rmse: 0.4948 - mae: 0.2598 - mape: 8.0331\n",
      "Epoch 482: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2607 - mse: 0.2465 - rmse: 0.4965 - mae: 0.2607 - mape: 8.0705 - val_loss: 0.2626 - val_mse: 0.2534 - val_rmse: 0.5034 - val_mae: 0.2626 - val_mape: 7.9623 - lr: 0.0010\n",
      "Epoch 483/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2654 - mse: 0.2532 - rmse: 0.5032 - mae: 0.2654 - mape: 8.2464\n",
      "Epoch 483: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2655 - mse: 0.2532 - rmse: 0.5032 - mae: 0.2655 - mape: 8.2452 - val_loss: 0.2727 - val_mse: 0.2683 - val_rmse: 0.5180 - val_mae: 0.2727 - val_mape: 8.1905 - lr: 0.0010\n",
      "Epoch 484/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2639 - mse: 0.2555 - rmse: 0.5055 - mae: 0.2639 - mape: 8.1753\n",
      "Epoch 484: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2627 - mse: 0.2517 - rmse: 0.5017 - mae: 0.2627 - mape: 8.1423 - val_loss: 0.2560 - val_mse: 0.2445 - val_rmse: 0.4944 - val_mae: 0.2560 - val_mape: 7.9912 - lr: 0.0010\n",
      "Epoch 485/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2638 - mse: 0.2550 - rmse: 0.5050 - mae: 0.2638 - mape: 8.1944\n",
      "Epoch 485: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2638 - mse: 0.2550 - rmse: 0.5050 - mae: 0.2638 - mape: 8.1944 - val_loss: 0.2574 - val_mse: 0.2427 - val_rmse: 0.4926 - val_mae: 0.2574 - val_mape: 8.1179 - lr: 0.0010\n",
      "Epoch 486/2000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2630 - mse: 0.2468 - rmse: 0.4968 - mae: 0.2630 - mape: 8.1694\n",
      "Epoch 486: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2635 - mse: 0.2522 - rmse: 0.5022 - mae: 0.2635 - mape: 8.1817 - val_loss: 0.2554 - val_mse: 0.2421 - val_rmse: 0.4921 - val_mae: 0.2554 - val_mape: 7.9071 - lr: 0.0010\n",
      "Epoch 487/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2631 - mse: 0.2539 - rmse: 0.5038 - mae: 0.2631 - mape: 8.1242\n",
      "Epoch 487: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2642 - mse: 0.2573 - rmse: 0.5073 - mae: 0.2642 - mape: 8.1458 - val_loss: 0.2734 - val_mse: 0.2452 - val_rmse: 0.4952 - val_mae: 0.2734 - val_mape: 8.6803 - lr: 0.0010\n",
      "Epoch 488/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2661 - mse: 0.2539 - rmse: 0.5039 - mae: 0.2661 - mape: 8.2272\n",
      "Epoch 488: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2662 - mse: 0.2546 - rmse: 0.5046 - mae: 0.2662 - mape: 8.2294 - val_loss: 0.2663 - val_mse: 0.2544 - val_rmse: 0.5044 - val_mae: 0.2663 - val_mape: 8.4307 - lr: 0.0010\n",
      "Epoch 489/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2655 - mse: 0.2546 - rmse: 0.5046 - mae: 0.2655 - mape: 8.2065\n",
      "Epoch 489: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2652 - mse: 0.2544 - rmse: 0.5044 - mae: 0.2652 - mape: 8.2021 - val_loss: 0.2558 - val_mse: 0.2452 - val_rmse: 0.4952 - val_mae: 0.2558 - val_mape: 8.0296 - lr: 0.0010\n",
      "Epoch 490/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2635 - mse: 0.2525 - rmse: 0.5025 - mae: 0.2635 - mape: 8.1064\n",
      "Epoch 490: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2644 - mse: 0.2534 - rmse: 0.5034 - mae: 0.2644 - mape: 8.1303 - val_loss: 0.3013 - val_mse: 0.2739 - val_rmse: 0.5233 - val_mae: 0.3013 - val_mape: 9.5326 - lr: 0.0010\n",
      "Epoch 491/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2629 - mse: 0.2516 - rmse: 0.5016 - mae: 0.2629 - mape: 8.1554\n",
      "Epoch 491: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2624 - mse: 0.2507 - rmse: 0.5007 - mae: 0.2624 - mape: 8.1471 - val_loss: 0.2677 - val_mse: 0.2574 - val_rmse: 0.5073 - val_mae: 0.2677 - val_mape: 7.9940 - lr: 0.0010\n",
      "Epoch 492/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2629 - mse: 0.2499 - rmse: 0.4999 - mae: 0.2629 - mape: 8.1154\n",
      "Epoch 492: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2627 - mse: 0.2499 - rmse: 0.4999 - mae: 0.2627 - mape: 8.1094 - val_loss: 0.2613 - val_mse: 0.2554 - val_rmse: 0.5053 - val_mae: 0.2613 - val_mape: 8.3386 - lr: 0.0010\n",
      "Epoch 493/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2604 - mse: 0.2518 - rmse: 0.5018 - mae: 0.2604 - mape: 8.0636\n",
      "Epoch 493: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2605 - mse: 0.2515 - rmse: 0.5015 - mae: 0.2605 - mape: 8.0686 - val_loss: 0.2747 - val_mse: 0.2735 - val_rmse: 0.5230 - val_mae: 0.2747 - val_mape: 8.7436 - lr: 0.0010\n",
      "Epoch 494/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2628 - mse: 0.2508 - rmse: 0.5008 - mae: 0.2628 - mape: 8.1301\n",
      "Epoch 494: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2628 - mse: 0.2508 - rmse: 0.5008 - mae: 0.2628 - mape: 8.1301 - val_loss: 0.2590 - val_mse: 0.2427 - val_rmse: 0.4926 - val_mae: 0.2590 - val_mape: 7.9309 - lr: 0.0010\n",
      "Epoch 495/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2616 - mse: 0.2514 - rmse: 0.5014 - mae: 0.2616 - mape: 8.0837\n",
      "Epoch 495: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2608 - mse: 0.2508 - rmse: 0.5008 - mae: 0.2608 - mape: 8.0494 - val_loss: 0.2555 - val_mse: 0.2412 - val_rmse: 0.4911 - val_mae: 0.2555 - val_mape: 7.9642 - lr: 0.0010\n",
      "Epoch 496/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2603 - mse: 0.2505 - rmse: 0.5005 - mae: 0.2603 - mape: 8.0599\n",
      "Epoch 496: val_loss did not improve from 0.25479\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2604 - mse: 0.2500 - rmse: 0.5000 - mae: 0.2604 - mape: 8.0624 - val_loss: 0.2586 - val_mse: 0.2456 - val_rmse: 0.4956 - val_mae: 0.2586 - val_mape: 8.0743 - lr: 0.0010\n",
      "Epoch 497/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2540 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2540 - mape: 7.8502\n",
      "Epoch 497: val_loss improved from 0.25479 to 0.25338, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2540 - mse: 0.2388 - rmse: 0.4887 - mae: 0.2540 - mape: 7.8380 - val_loss: 0.2534 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2534 - val_mape: 7.8800 - lr: 1.0000e-04\n",
      "Epoch 498/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2534 - mse: 0.2389 - rmse: 0.4887 - mae: 0.2534 - mape: 7.8193\n",
      "Epoch 498: val_loss improved from 0.25338 to 0.25281, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2536 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2536 - mape: 7.8318 - val_loss: 0.2528 - val_mse: 0.2385 - val_rmse: 0.4884 - val_mae: 0.2528 - val_mape: 7.8067 - lr: 1.0000e-04\n",
      "Epoch 499/2000\n",
      "272/318 [========================>.....] - ETA: 0s - loss: 0.2538 - mse: 0.2388 - rmse: 0.4887 - mae: 0.2538 - mape: 7.8587\n",
      "Epoch 499: val_loss did not improve from 0.25281\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2539 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2539 - mape: 7.8380 - val_loss: 0.2533 - val_mse: 0.2405 - val_rmse: 0.4904 - val_mae: 0.2533 - val_mape: 7.8879 - lr: 1.0000e-04\n",
      "Epoch 500/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2539 - mse: 0.2420 - rmse: 0.4919 - mae: 0.2539 - mape: 7.8434\n",
      "Epoch 500: val_loss did not improve from 0.25281\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2534 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2534 - mape: 7.8319 - val_loss: 0.2537 - val_mse: 0.2398 - val_rmse: 0.4896 - val_mae: 0.2537 - val_mape: 7.9368 - lr: 1.0000e-04\n",
      "Epoch 501/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2529 - mse: 0.2399 - rmse: 0.4898 - mae: 0.2529 - mape: 7.8291\n",
      "Epoch 501: val_loss did not improve from 0.25281\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2533 - mse: 0.2408 - rmse: 0.4908 - mae: 0.2533 - mape: 7.8532 - val_loss: 0.2550 - val_mse: 0.2414 - val_rmse: 0.4913 - val_mae: 0.2550 - val_mape: 7.7791 - lr: 1.0000e-04\n",
      "Epoch 502/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2540 - mse: 0.2410 - rmse: 0.4909 - mae: 0.2540 - mape: 7.8506\n",
      "Epoch 502: val_loss improved from 0.25281 to 0.25279, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2537 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2537 - mape: 7.8391 - val_loss: 0.2528 - val_mse: 0.2411 - val_rmse: 0.4910 - val_mae: 0.2528 - val_mape: 7.7749 - lr: 1.0000e-04\n",
      "Epoch 503/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2537 - mse: 0.2410 - rmse: 0.4909 - mae: 0.2537 - mape: 7.8441\n",
      "Epoch 503: val_loss did not improve from 0.25279\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2535 - mse: 0.2412 - rmse: 0.4911 - mae: 0.2535 - mape: 7.8499 - val_loss: 0.2535 - val_mse: 0.2411 - val_rmse: 0.4910 - val_mae: 0.2535 - val_mape: 7.7559 - lr: 1.0000e-04\n",
      "Epoch 504/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2537 - mse: 0.2415 - rmse: 0.4914 - mae: 0.2537 - mape: 7.8306\n",
      "Epoch 504: val_loss did not improve from 0.25279\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2532 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2532 - mape: 7.8364 - val_loss: 0.2534 - val_mse: 0.2409 - val_rmse: 0.4908 - val_mae: 0.2534 - val_mape: 7.8989 - lr: 1.0000e-04\n",
      "Epoch 505/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2538 - mse: 0.2413 - rmse: 0.4913 - mae: 0.2538 - mape: 7.8521\n",
      "Epoch 505: val_loss did not improve from 0.25279\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2533 - mse: 0.2405 - rmse: 0.4905 - mae: 0.2533 - mape: 7.8457 - val_loss: 0.2529 - val_mse: 0.2403 - val_rmse: 0.4902 - val_mae: 0.2529 - val_mape: 7.8087 - lr: 1.0000e-04\n",
      "Epoch 506/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2533 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2533 - mape: 7.8402\n",
      "Epoch 506: val_loss did not improve from 0.25279\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2534 - mse: 0.2410 - rmse: 0.4910 - mae: 0.2534 - mape: 7.8403 - val_loss: 0.2532 - val_mse: 0.2406 - val_rmse: 0.4905 - val_mae: 0.2532 - val_mape: 7.9100 - lr: 1.0000e-04\n",
      "Epoch 507/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2542 - mse: 0.2420 - rmse: 0.4920 - mae: 0.2542 - mape: 7.8589\n",
      "Epoch 507: val_loss did not improve from 0.25279\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2537 - mse: 0.2414 - rmse: 0.4913 - mae: 0.2537 - mape: 7.8485 - val_loss: 0.2539 - val_mse: 0.2417 - val_rmse: 0.4916 - val_mae: 0.2539 - val_mape: 7.7658 - lr: 1.0000e-04\n",
      "Epoch 508/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2542 - mse: 0.2419 - rmse: 0.4919 - mae: 0.2542 - mape: 7.8804\n",
      "Epoch 508: val_loss improved from 0.25279 to 0.25242, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2539 - mse: 0.2418 - rmse: 0.4918 - mae: 0.2539 - mape: 7.8672 - val_loss: 0.2524 - val_mse: 0.2401 - val_rmse: 0.4900 - val_mae: 0.2524 - val_mape: 7.8085 - lr: 1.0000e-04\n",
      "Epoch 509/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2515 - mse: 0.2372 - rmse: 0.4870 - mae: 0.2515 - mape: 7.8002\n",
      "Epoch 509: val_loss did not improve from 0.25242\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2533 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2533 - mape: 7.8458 - val_loss: 0.2530 - val_mse: 0.2406 - val_rmse: 0.4905 - val_mae: 0.2530 - val_mape: 7.9002 - lr: 1.0000e-04\n",
      "Epoch 510/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2530 - mse: 0.2426 - rmse: 0.4925 - mae: 0.2530 - mape: 7.8623\n",
      "Epoch 510: val_loss did not improve from 0.25242\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2534 - mse: 0.2423 - rmse: 0.4922 - mae: 0.2534 - mape: 7.8647 - val_loss: 0.2538 - val_mse: 0.2419 - val_rmse: 0.4918 - val_mae: 0.2538 - val_mape: 7.7589 - lr: 1.0000e-04\n",
      "Epoch 511/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2535 - mse: 0.2415 - rmse: 0.4914 - mae: 0.2535 - mape: 7.8518\n",
      "Epoch 511: val_loss improved from 0.25242 to 0.25241, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2532 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2532 - mape: 7.8417 - val_loss: 0.2524 - val_mse: 0.2398 - val_rmse: 0.4897 - val_mae: 0.2524 - val_mape: 7.8135 - lr: 1.0000e-04\n",
      "Epoch 512/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2536 - mse: 0.2412 - rmse: 0.4911 - mae: 0.2536 - mape: 7.8619\n",
      "Epoch 512: val_loss did not improve from 0.25241\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2532 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2532 - mape: 7.8431 - val_loss: 0.2542 - val_mse: 0.2424 - val_rmse: 0.4924 - val_mae: 0.2542 - val_mape: 7.7532 - lr: 1.0000e-04\n",
      "Epoch 513/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2536 - mse: 0.2416 - rmse: 0.4915 - mae: 0.2536 - mape: 7.8504\n",
      "Epoch 513: val_loss did not improve from 0.25241\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2537 - mse: 0.2422 - rmse: 0.4921 - mae: 0.2537 - mape: 7.8550 - val_loss: 0.2540 - val_mse: 0.2425 - val_rmse: 0.4924 - val_mae: 0.2540 - val_mape: 7.9362 - lr: 1.0000e-04\n",
      "Epoch 514/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2529 - mse: 0.2412 - rmse: 0.4912 - mae: 0.2529 - mape: 7.8376\n",
      "Epoch 514: val_loss did not improve from 0.25241\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2532 - mse: 0.2410 - rmse: 0.4909 - mae: 0.2532 - mape: 7.8439 - val_loss: 0.2530 - val_mse: 0.2408 - val_rmse: 0.4907 - val_mae: 0.2530 - val_mape: 7.7644 - lr: 1.0000e-04\n",
      "Epoch 515/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2521 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2521 - mape: 7.8120\n",
      "Epoch 515: val_loss did not improve from 0.25241\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2531 - mse: 0.2417 - rmse: 0.4916 - mae: 0.2531 - mape: 7.8469 - val_loss: 0.2527 - val_mse: 0.2404 - val_rmse: 0.4903 - val_mae: 0.2527 - val_mape: 7.7621 - lr: 1.0000e-04\n",
      "Epoch 516/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2526 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2526 - mape: 7.8262\n",
      "Epoch 516: val_loss improved from 0.25241 to 0.25220, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2533 - mse: 0.2410 - rmse: 0.4909 - mae: 0.2533 - mape: 7.8370 - val_loss: 0.2522 - val_mse: 0.2398 - val_rmse: 0.4897 - val_mae: 0.2522 - val_mape: 7.8299 - lr: 1.0000e-04\n",
      "Epoch 517/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2543 - mse: 0.2427 - rmse: 0.4926 - mae: 0.2543 - mape: 7.8598\n",
      "Epoch 517: val_loss did not improve from 0.25220\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2529 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2529 - mape: 7.8304 - val_loss: 0.2532 - val_mse: 0.2412 - val_rmse: 0.4912 - val_mae: 0.2532 - val_mape: 7.7639 - lr: 1.0000e-04\n",
      "Epoch 518/2000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2540 - mse: 0.2445 - rmse: 0.4945 - mae: 0.2540 - mape: 7.8670\n",
      "Epoch 518: val_loss did not improve from 0.25220\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2532 - mse: 0.2413 - rmse: 0.4912 - mae: 0.2532 - mape: 7.8318 - val_loss: 0.2535 - val_mse: 0.2412 - val_rmse: 0.4911 - val_mae: 0.2535 - val_mape: 7.9453 - lr: 1.0000e-04\n",
      "Epoch 519/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2533 - mse: 0.2405 - rmse: 0.4904 - mae: 0.2533 - mape: 7.8300\n",
      "Epoch 519: val_loss did not improve from 0.25220\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2533 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2533 - mape: 7.8333 - val_loss: 0.2537 - val_mse: 0.2425 - val_rmse: 0.4925 - val_mae: 0.2537 - val_mape: 7.9539 - lr: 1.0000e-04\n",
      "Epoch 520/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2538 - mse: 0.2423 - rmse: 0.4922 - mae: 0.2538 - mape: 7.8846\n",
      "Epoch 520: val_loss did not improve from 0.25220\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2529 - mse: 0.2410 - rmse: 0.4909 - mae: 0.2529 - mape: 7.8403 - val_loss: 0.2529 - val_mse: 0.2399 - val_rmse: 0.4898 - val_mae: 0.2529 - val_mape: 7.9035 - lr: 1.0000e-04\n",
      "Epoch 521/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2531 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2531 - mape: 7.8538\n",
      "Epoch 521: val_loss did not improve from 0.25220\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2534 - mse: 0.2419 - rmse: 0.4919 - mae: 0.2534 - mape: 7.8498 - val_loss: 0.2536 - val_mse: 0.2417 - val_rmse: 0.4916 - val_mae: 0.2536 - val_mape: 7.9474 - lr: 1.0000e-04\n",
      "Epoch 522/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2538 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2538 - mape: 7.8684\n",
      "Epoch 522: val_loss did not improve from 0.25220\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2532 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2532 - mape: 7.8419 - val_loss: 0.2522 - val_mse: 0.2402 - val_rmse: 0.4901 - val_mae: 0.2522 - val_mape: 7.8122 - lr: 1.0000e-04\n",
      "Epoch 523/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2507 - mse: 0.2381 - rmse: 0.4879 - mae: 0.2507 - mape: 7.7895\n",
      "Epoch 523: val_loss did not improve from 0.25220\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2529 - mse: 0.2414 - rmse: 0.4913 - mae: 0.2529 - mape: 7.8407 - val_loss: 0.2535 - val_mse: 0.2411 - val_rmse: 0.4910 - val_mae: 0.2535 - val_mape: 7.7496 - lr: 1.0000e-04\n",
      "Epoch 524/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2506 - mse: 0.2383 - rmse: 0.4881 - mae: 0.2506 - mape: 7.7837\n",
      "Epoch 524: val_loss did not improve from 0.25220\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2532 - mse: 0.2411 - rmse: 0.4910 - mae: 0.2532 - mape: 7.8475 - val_loss: 0.2525 - val_mse: 0.2400 - val_rmse: 0.4899 - val_mae: 0.2525 - val_mape: 7.8742 - lr: 1.0000e-04\n",
      "Epoch 525/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2529 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2529 - mape: 7.8317\n",
      "Epoch 525: val_loss improved from 0.25220 to 0.25214, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2528 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2528 - mape: 7.8286 - val_loss: 0.2521 - val_mse: 0.2397 - val_rmse: 0.4896 - val_mae: 0.2521 - val_mape: 7.8173 - lr: 1.0000e-04\n",
      "Epoch 526/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2530 - mse: 0.2417 - rmse: 0.4916 - mae: 0.2530 - mape: 7.8385\n",
      "Epoch 526: val_loss did not improve from 0.25214\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2528 - mse: 0.2417 - rmse: 0.4916 - mae: 0.2528 - mape: 7.8428 - val_loss: 0.2527 - val_mse: 0.2404 - val_rmse: 0.4903 - val_mae: 0.2527 - val_mape: 7.7634 - lr: 1.0000e-04\n",
      "Epoch 527/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2544 - mse: 0.2448 - rmse: 0.4948 - mae: 0.2544 - mape: 7.8826\n",
      "Epoch 527: val_loss did not improve from 0.25214\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2531 - mse: 0.2414 - rmse: 0.4914 - mae: 0.2531 - mape: 7.8412 - val_loss: 0.2522 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2522 - val_mape: 7.8268 - lr: 1.0000e-04\n",
      "Epoch 528/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2528 - mse: 0.2382 - rmse: 0.4881 - mae: 0.2528 - mape: 7.8428\n",
      "Epoch 528: val_loss did not improve from 0.25214\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2530 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2530 - mape: 7.8450 - val_loss: 0.2524 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2524 - val_mape: 7.7797 - lr: 1.0000e-04\n",
      "Epoch 529/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2540 - mse: 0.2417 - rmse: 0.4917 - mae: 0.2540 - mape: 7.8507\n",
      "Epoch 529: val_loss improved from 0.25214 to 0.25205, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2535 - mse: 0.2417 - rmse: 0.4916 - mae: 0.2535 - mape: 7.8461 - val_loss: 0.2521 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2521 - val_mape: 7.7876 - lr: 1.0000e-04\n",
      "Epoch 530/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2530 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2530 - mape: 7.8196\n",
      "Epoch 530: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2530 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2530 - mape: 7.8285 - val_loss: 0.2527 - val_mse: 0.2396 - val_rmse: 0.4895 - val_mae: 0.2527 - val_mape: 7.7660 - lr: 1.0000e-04\n",
      "Epoch 531/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2526 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2526 - mape: 7.8174\n",
      "Epoch 531: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2532 - mse: 0.2411 - rmse: 0.4911 - mae: 0.2532 - mape: 7.8426 - val_loss: 0.2522 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2522 - val_mape: 7.8260 - lr: 1.0000e-04\n",
      "Epoch 532/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2519 - mse: 0.2381 - rmse: 0.4879 - mae: 0.2519 - mape: 7.8088\n",
      "Epoch 532: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2528 - mse: 0.2403 - rmse: 0.4902 - mae: 0.2528 - mape: 7.8157 - val_loss: 0.2522 - val_mse: 0.2396 - val_rmse: 0.4895 - val_mae: 0.2522 - val_mape: 7.8299 - lr: 1.0000e-04\n",
      "Epoch 533/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2536 - mse: 0.2420 - rmse: 0.4920 - mae: 0.2536 - mape: 7.8569\n",
      "Epoch 533: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2533 - mse: 0.2415 - rmse: 0.4914 - mae: 0.2533 - mape: 7.8465 - val_loss: 0.2525 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2525 - val_mape: 7.8049 - lr: 1.0000e-04\n",
      "Epoch 534/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2534 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2534 - mape: 7.8587\n",
      "Epoch 534: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2533 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2533 - mape: 7.8547 - val_loss: 0.2525 - val_mse: 0.2393 - val_rmse: 0.4891 - val_mae: 0.2525 - val_mape: 7.7958 - lr: 1.0000e-04\n",
      "Epoch 535/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2537 - mse: 0.2420 - rmse: 0.4919 - mae: 0.2537 - mape: 7.8585\n",
      "Epoch 535: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2532 - mse: 0.2411 - rmse: 0.4911 - mae: 0.2532 - mape: 7.8385 - val_loss: 0.2530 - val_mse: 0.2401 - val_rmse: 0.4900 - val_mae: 0.2530 - val_mape: 7.7385 - lr: 1.0000e-04\n",
      "Epoch 536/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2523 - mse: 0.2375 - rmse: 0.4873 - mae: 0.2523 - mape: 7.8336\n",
      "Epoch 536: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2532 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2532 - mape: 7.8460 - val_loss: 0.2575 - val_mse: 0.2468 - val_rmse: 0.4968 - val_mae: 0.2575 - val_mape: 7.7766 - lr: 1.0000e-04\n",
      "Epoch 537/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2523 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2523 - mape: 7.8102\n",
      "Epoch 537: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2528 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2528 - mape: 7.8211 - val_loss: 0.2521 - val_mse: 0.2385 - val_rmse: 0.4883 - val_mae: 0.2521 - val_mape: 7.7955 - lr: 1.0000e-04\n",
      "Epoch 538/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2536 - mse: 0.2420 - rmse: 0.4920 - mae: 0.2536 - mape: 7.8523\n",
      "Epoch 538: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2530 - mse: 0.2406 - rmse: 0.4906 - mae: 0.2530 - mape: 7.8399 - val_loss: 0.2521 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2521 - val_mape: 7.8022 - lr: 1.0000e-04\n",
      "Epoch 539/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2522 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2522 - mape: 7.8211\n",
      "Epoch 539: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2529 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2529 - mape: 7.8233 - val_loss: 0.2528 - val_mse: 0.2401 - val_rmse: 0.4900 - val_mae: 0.2528 - val_mape: 7.9097 - lr: 1.0000e-04\n",
      "Epoch 540/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2518 - mse: 0.2380 - rmse: 0.4878 - mae: 0.2518 - mape: 7.8176\n",
      "Epoch 540: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2526 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2526 - mape: 7.8385 - val_loss: 0.2535 - val_mse: 0.2411 - val_rmse: 0.4910 - val_mae: 0.2535 - val_mape: 7.7350 - lr: 1.0000e-04\n",
      "Epoch 541/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2540 - mse: 0.2429 - rmse: 0.4928 - mae: 0.2540 - mape: 7.8629\n",
      "Epoch 541: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2531 - mse: 0.2410 - rmse: 0.4909 - mae: 0.2531 - mape: 7.8362 - val_loss: 0.2531 - val_mse: 0.2407 - val_rmse: 0.4907 - val_mae: 0.2531 - val_mape: 7.7313 - lr: 1.0000e-04\n",
      "Epoch 542/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2525 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2525 - mape: 7.8026\n",
      "Epoch 542: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2531 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2531 - mape: 7.8242 - val_loss: 0.2521 - val_mse: 0.2398 - val_rmse: 0.4897 - val_mae: 0.2521 - val_mape: 7.7650 - lr: 1.0000e-04\n",
      "Epoch 543/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2518 - mse: 0.2388 - rmse: 0.4886 - mae: 0.2518 - mape: 7.7957\n",
      "Epoch 543: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2529 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2529 - mape: 7.8210 - val_loss: 0.2527 - val_mse: 0.2408 - val_rmse: 0.4907 - val_mae: 0.2527 - val_mape: 7.7400 - lr: 1.0000e-04\n",
      "Epoch 544/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2523 - mse: 0.2387 - rmse: 0.4885 - mae: 0.2523 - mape: 7.8178\n",
      "Epoch 544: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2532 - mse: 0.2412 - rmse: 0.4911 - mae: 0.2532 - mape: 7.8363 - val_loss: 0.2528 - val_mse: 0.2402 - val_rmse: 0.4901 - val_mae: 0.2528 - val_mape: 7.8329 - lr: 1.0000e-04\n",
      "Epoch 545/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2531 - mse: 0.2425 - rmse: 0.4924 - mae: 0.2531 - mape: 7.8307\n",
      "Epoch 545: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2528 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2528 - mape: 7.8344 - val_loss: 0.2524 - val_mse: 0.2389 - val_rmse: 0.4888 - val_mae: 0.2524 - val_mape: 7.8220 - lr: 1.0000e-04\n",
      "Epoch 546/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2525 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2525 - mape: 7.8196\n",
      "Epoch 546: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2531 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2531 - mape: 7.8280 - val_loss: 0.2526 - val_mse: 0.2398 - val_rmse: 0.4897 - val_mae: 0.2526 - val_mape: 7.7999 - lr: 1.0000e-04\n",
      "Epoch 547/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2528 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2528 - mape: 7.8395\n",
      "Epoch 547: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2527 - mse: 0.2407 - rmse: 0.4907 - mae: 0.2527 - mape: 7.8398 - val_loss: 0.2524 - val_mse: 0.2396 - val_rmse: 0.4895 - val_mae: 0.2524 - val_mape: 7.7656 - lr: 1.0000e-04\n",
      "Epoch 548/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2529 - mse: 0.2411 - rmse: 0.4911 - mae: 0.2529 - mape: 7.8223\n",
      "Epoch 548: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2528 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2528 - mape: 7.8169 - val_loss: 0.2541 - val_mse: 0.2415 - val_rmse: 0.4914 - val_mae: 0.2541 - val_mape: 7.7618 - lr: 1.0000e-04\n",
      "Epoch 549/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2525 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2525 - mape: 7.8106\n",
      "Epoch 549: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2528 - mse: 0.2396 - rmse: 0.4894 - mae: 0.2528 - mape: 7.8256 - val_loss: 0.2523 - val_mse: 0.2403 - val_rmse: 0.4902 - val_mae: 0.2523 - val_mape: 7.7748 - lr: 1.0000e-04\n",
      "Epoch 550/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2525 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2525 - mape: 7.8240\n",
      "Epoch 550: val_loss did not improve from 0.25205\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2528 - mse: 0.2406 - rmse: 0.4906 - mae: 0.2528 - mape: 7.8309 - val_loss: 0.2521 - val_mse: 0.2385 - val_rmse: 0.4883 - val_mae: 0.2521 - val_mape: 7.8245 - lr: 1.0000e-04\n",
      "Epoch 551/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2533 - mse: 0.2414 - rmse: 0.4913 - mae: 0.2533 - mape: 7.8432\n",
      "Epoch 551: val_loss improved from 0.25205 to 0.25190, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2529 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2529 - mape: 7.8308 - val_loss: 0.2519 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2519 - val_mape: 7.7978 - lr: 1.0000e-04\n",
      "Epoch 552/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2537 - mse: 0.2410 - rmse: 0.4909 - mae: 0.2537 - mape: 7.8610\n",
      "Epoch 552: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2531 - mse: 0.2409 - rmse: 0.4908 - mae: 0.2531 - mape: 7.8536 - val_loss: 0.2532 - val_mse: 0.2411 - val_rmse: 0.4910 - val_mae: 0.2532 - val_mape: 7.7473 - lr: 1.0000e-04\n",
      "Epoch 553/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2535 - mse: 0.2417 - rmse: 0.4916 - mae: 0.2535 - mape: 7.8446\n",
      "Epoch 553: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2532 - mse: 0.2408 - rmse: 0.4908 - mae: 0.2532 - mape: 7.8331 - val_loss: 0.2527 - val_mse: 0.2384 - val_rmse: 0.4882 - val_mae: 0.2527 - val_mape: 7.8188 - lr: 1.0000e-04\n",
      "Epoch 554/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2535 - mse: 0.2410 - rmse: 0.4909 - mae: 0.2535 - mape: 7.8372\n",
      "Epoch 554: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2533 - mse: 0.2411 - rmse: 0.4911 - mae: 0.2533 - mape: 7.8356 - val_loss: 0.2534 - val_mse: 0.2414 - val_rmse: 0.4913 - val_mae: 0.2534 - val_mape: 7.9597 - lr: 1.0000e-04\n",
      "Epoch 555/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2520 - mse: 0.2384 - rmse: 0.4883 - mae: 0.2520 - mape: 7.7983\n",
      "Epoch 555: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2525 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2525 - mape: 7.8232 - val_loss: 0.2527 - val_mse: 0.2409 - val_rmse: 0.4909 - val_mae: 0.2527 - val_mape: 7.7627 - lr: 1.0000e-04\n",
      "Epoch 556/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2533 - mse: 0.2419 - rmse: 0.4919 - mae: 0.2533 - mape: 7.8483\n",
      "Epoch 556: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2537 - mse: 0.2418 - rmse: 0.4918 - mae: 0.2537 - mape: 7.8551 - val_loss: 0.2545 - val_mse: 0.2440 - val_rmse: 0.4940 - val_mae: 0.2545 - val_mape: 8.0042 - lr: 1.0000e-04\n",
      "Epoch 557/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2538 - mse: 0.2417 - rmse: 0.4916 - mae: 0.2538 - mape: 7.8477\n",
      "Epoch 557: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2532 - mse: 0.2411 - rmse: 0.4910 - mae: 0.2532 - mape: 7.8398 - val_loss: 0.2543 - val_mse: 0.2438 - val_rmse: 0.4937 - val_mae: 0.2543 - val_mape: 7.9721 - lr: 1.0000e-04\n",
      "Epoch 558/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2536 - mse: 0.2419 - rmse: 0.4918 - mae: 0.2536 - mape: 7.8687\n",
      "Epoch 558: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2527 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2527 - mape: 7.8241 - val_loss: 0.2519 - val_mse: 0.2397 - val_rmse: 0.4896 - val_mae: 0.2519 - val_mape: 7.8174 - lr: 1.0000e-04\n",
      "Epoch 559/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2518 - mse: 0.2370 - rmse: 0.4868 - mae: 0.2518 - mape: 7.8125\n",
      "Epoch 559: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2535 - mse: 0.2416 - rmse: 0.4915 - mae: 0.2535 - mape: 7.8582 - val_loss: 0.2539 - val_mse: 0.2404 - val_rmse: 0.4903 - val_mae: 0.2539 - val_mape: 7.7622 - lr: 1.0000e-04\n",
      "Epoch 560/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2521 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2521 - mape: 7.7914\n",
      "Epoch 560: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2525 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2525 - mape: 7.8174 - val_loss: 0.2525 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2525 - val_mape: 7.7662 - lr: 1.0000e-04\n",
      "Epoch 561/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2512 - mse: 0.2375 - rmse: 0.4873 - mae: 0.2512 - mape: 7.7813\n",
      "Epoch 561: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2527 - mse: 0.2403 - rmse: 0.4902 - mae: 0.2527 - mape: 7.8099 - val_loss: 0.2520 - val_mse: 0.2388 - val_rmse: 0.4887 - val_mae: 0.2520 - val_mape: 7.8304 - lr: 1.0000e-04\n",
      "Epoch 562/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2527 - mse: 0.2379 - rmse: 0.4878 - mae: 0.2527 - mape: 7.8310\n",
      "Epoch 562: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2528 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2528 - mape: 7.8371 - val_loss: 0.2524 - val_mse: 0.2407 - val_rmse: 0.4906 - val_mae: 0.2524 - val_mape: 7.7657 - lr: 1.0000e-04\n",
      "Epoch 563/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2548 - mse: 0.2443 - rmse: 0.4943 - mae: 0.2548 - mape: 7.8718\n",
      "Epoch 563: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2528 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2528 - mape: 7.8242 - val_loss: 0.2524 - val_mse: 0.2387 - val_rmse: 0.4885 - val_mae: 0.2524 - val_mape: 7.8536 - lr: 1.0000e-04\n",
      "Epoch 564/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2543 - mse: 0.2434 - rmse: 0.4933 - mae: 0.2543 - mape: 7.8756\n",
      "Epoch 564: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2527 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2527 - mape: 7.8260 - val_loss: 0.2520 - val_mse: 0.2392 - val_rmse: 0.4890 - val_mae: 0.2520 - val_mape: 7.8274 - lr: 1.0000e-04\n",
      "Epoch 565/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2548 - mse: 0.2446 - rmse: 0.4946 - mae: 0.2548 - mape: 7.8763\n",
      "Epoch 565: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2528 - mse: 0.2410 - rmse: 0.4909 - mae: 0.2528 - mape: 7.8331 - val_loss: 0.2521 - val_mse: 0.2389 - val_rmse: 0.4888 - val_mae: 0.2521 - val_mape: 7.8405 - lr: 1.0000e-04\n",
      "Epoch 566/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2520 - mse: 0.2388 - rmse: 0.4886 - mae: 0.2520 - mape: 7.8204\n",
      "Epoch 566: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2526 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2526 - mape: 7.8291 - val_loss: 0.2531 - val_mse: 0.2414 - val_rmse: 0.4913 - val_mae: 0.2531 - val_mape: 7.9305 - lr: 1.0000e-04\n",
      "Epoch 567/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2531 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2531 - mape: 7.8491\n",
      "Epoch 567: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2531 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2531 - mape: 7.8466 - val_loss: 0.2522 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2522 - val_mape: 7.8366 - lr: 1.0000e-04\n",
      "Epoch 568/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2529 - mse: 0.2414 - rmse: 0.4913 - mae: 0.2529 - mape: 7.8481\n",
      "Epoch 568: val_loss did not improve from 0.25190\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2527 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2527 - mape: 7.8392 - val_loss: 0.2520 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2520 - val_mape: 7.7709 - lr: 1.0000e-04\n",
      "Epoch 569/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2530 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2530 - mape: 7.8426\n",
      "Epoch 569: val_loss improved from 0.25190 to 0.25183, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2529 - mse: 0.2403 - rmse: 0.4902 - mae: 0.2529 - mape: 7.8368 - val_loss: 0.2518 - val_mse: 0.2394 - val_rmse: 0.4892 - val_mae: 0.2518 - val_mape: 7.8007 - lr: 1.0000e-04\n",
      "Epoch 570/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2525 - mse: 0.2380 - rmse: 0.4878 - mae: 0.2525 - mape: 7.8312\n",
      "Epoch 570: val_loss did not improve from 0.25183\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2531 - mse: 0.2412 - rmse: 0.4911 - mae: 0.2531 - mape: 7.8412 - val_loss: 0.2522 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2522 - val_mape: 7.8038 - lr: 1.0000e-04\n",
      "Epoch 571/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2526 - mse: 0.2388 - rmse: 0.4887 - mae: 0.2526 - mape: 7.8299\n",
      "Epoch 571: val_loss did not improve from 0.25183\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2530 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2530 - mape: 7.8417 - val_loss: 0.2522 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2522 - val_mape: 7.7687 - lr: 1.0000e-04\n",
      "Epoch 572/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2528 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2528 - mape: 7.8077\n",
      "Epoch 572: val_loss improved from 0.25183 to 0.25168, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2528 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2528 - mape: 7.8130 - val_loss: 0.2517 - val_mse: 0.2398 - val_rmse: 0.4897 - val_mae: 0.2517 - val_mape: 7.8186 - lr: 1.0000e-04\n",
      "Epoch 573/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2521 - mse: 0.2410 - rmse: 0.4909 - mae: 0.2521 - mape: 7.8153\n",
      "Epoch 573: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2526 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2526 - mape: 7.8255 - val_loss: 0.2541 - val_mse: 0.2405 - val_rmse: 0.4904 - val_mae: 0.2541 - val_mape: 7.7866 - lr: 1.0000e-04\n",
      "Epoch 574/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2522 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2522 - mape: 7.8141\n",
      "Epoch 574: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2526 - mse: 0.2399 - rmse: 0.4898 - mae: 0.2526 - mape: 7.8277 - val_loss: 0.2525 - val_mse: 0.2402 - val_rmse: 0.4901 - val_mae: 0.2525 - val_mape: 7.9091 - lr: 1.0000e-04\n",
      "Epoch 575/2000\n",
      "266/318 [========================>.....] - ETA: 0s - loss: 0.2551 - mse: 0.2423 - rmse: 0.4923 - mae: 0.2551 - mape: 7.8824\n",
      "Epoch 575: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2526 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2526 - mape: 7.8336 - val_loss: 0.2525 - val_mse: 0.2408 - val_rmse: 0.4908 - val_mae: 0.2525 - val_mape: 7.7469 - lr: 1.0000e-04\n",
      "Epoch 576/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2520 - mse: 0.2386 - rmse: 0.4884 - mae: 0.2520 - mape: 7.8027\n",
      "Epoch 576: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2525 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2525 - mape: 7.8238 - val_loss: 0.2583 - val_mse: 0.2492 - val_rmse: 0.4992 - val_mae: 0.2583 - val_mape: 7.7831 - lr: 1.0000e-04\n",
      "Epoch 577/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2534 - mse: 0.2419 - rmse: 0.4919 - mae: 0.2534 - mape: 7.8181\n",
      "Epoch 577: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2528 - mse: 0.2403 - rmse: 0.4902 - mae: 0.2528 - mape: 7.8172 - val_loss: 0.2527 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2527 - val_mape: 7.7827 - lr: 1.0000e-04\n",
      "Epoch 578/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2526 - mse: 0.2409 - rmse: 0.4908 - mae: 0.2526 - mape: 7.8316\n",
      "Epoch 578: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2529 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2529 - mape: 7.8415 - val_loss: 0.2521 - val_mse: 0.2396 - val_rmse: 0.4895 - val_mae: 0.2521 - val_mape: 7.8161 - lr: 1.0000e-04\n",
      "Epoch 579/2000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2530 - mse: 0.2421 - rmse: 0.4921 - mae: 0.2530 - mape: 7.8574\n",
      "Epoch 579: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2525 - mse: 0.2409 - rmse: 0.4908 - mae: 0.2525 - mape: 7.8201 - val_loss: 0.2520 - val_mse: 0.2397 - val_rmse: 0.4896 - val_mae: 0.2520 - val_mape: 7.8589 - lr: 1.0000e-04\n",
      "Epoch 580/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2526 - mse: 0.2419 - rmse: 0.4918 - mae: 0.2526 - mape: 7.8401\n",
      "Epoch 580: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2528 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2528 - mape: 7.8528 - val_loss: 0.2521 - val_mse: 0.2403 - val_rmse: 0.4902 - val_mae: 0.2521 - val_mape: 7.7523 - lr: 1.0000e-04\n",
      "Epoch 581/2000\n",
      "264/318 [=======================>......] - ETA: 0s - loss: 0.2517 - mse: 0.2366 - rmse: 0.4864 - mae: 0.2517 - mape: 7.8115\n",
      "Epoch 581: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2527 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2527 - mape: 7.8402 - val_loss: 0.2521 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2521 - val_mape: 7.7505 - lr: 1.0000e-04\n",
      "Epoch 582/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2539 - mse: 0.2427 - rmse: 0.4927 - mae: 0.2539 - mape: 7.8695\n",
      "Epoch 582: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2525 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2525 - mape: 7.8229 - val_loss: 0.2519 - val_mse: 0.2390 - val_rmse: 0.4889 - val_mae: 0.2519 - val_mape: 7.8451 - lr: 1.0000e-04\n",
      "Epoch 583/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2511 - mse: 0.2382 - rmse: 0.4880 - mae: 0.2511 - mape: 7.7984\n",
      "Epoch 583: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2525 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2525 - mape: 7.8220 - val_loss: 0.2519 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2519 - val_mape: 7.7887 - lr: 1.0000e-04\n",
      "Epoch 584/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2528 - mse: 0.2399 - rmse: 0.4898 - mae: 0.2528 - mape: 7.8380\n",
      "Epoch 584: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2523 - mse: 0.2403 - rmse: 0.4902 - mae: 0.2523 - mape: 7.8143 - val_loss: 0.2530 - val_mse: 0.2406 - val_rmse: 0.4905 - val_mae: 0.2530 - val_mape: 7.7273 - lr: 1.0000e-04\n",
      "Epoch 585/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2547 - mse: 0.2435 - rmse: 0.4935 - mae: 0.2547 - mape: 7.8862\n",
      "Epoch 585: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2527 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2527 - mape: 7.8300 - val_loss: 0.2517 - val_mse: 0.2398 - val_rmse: 0.4897 - val_mae: 0.2517 - val_mape: 7.7972 - lr: 1.0000e-04\n",
      "Epoch 586/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2525 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2525 - mape: 7.8175\n",
      "Epoch 586: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2525 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2525 - mape: 7.8111 - val_loss: 0.2530 - val_mse: 0.2413 - val_rmse: 0.4912 - val_mae: 0.2530 - val_mape: 7.9492 - lr: 1.0000e-04\n",
      "Epoch 587/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2523 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2523 - mape: 7.8057\n",
      "Epoch 587: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2527 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2527 - mape: 7.8266 - val_loss: 0.2523 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2523 - val_mape: 7.8598 - lr: 1.0000e-04\n",
      "Epoch 588/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2528 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2528 - mape: 7.8345\n",
      "Epoch 588: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2527 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2527 - mape: 7.8318 - val_loss: 0.2518 - val_mse: 0.2403 - val_rmse: 0.4902 - val_mae: 0.2518 - val_mape: 7.7677 - lr: 1.0000e-04\n",
      "Epoch 589/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2528 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2528 - mape: 7.8357\n",
      "Epoch 589: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2524 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2524 - mape: 7.8270 - val_loss: 0.2553 - val_mse: 0.2415 - val_rmse: 0.4914 - val_mae: 0.2553 - val_mape: 7.7245 - lr: 1.0000e-04\n",
      "Epoch 590/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2522 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2522 - mape: 7.8109\n",
      "Epoch 590: val_loss did not improve from 0.25168\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2528 - mse: 0.2403 - rmse: 0.4902 - mae: 0.2528 - mape: 7.8332 - val_loss: 0.2546 - val_mse: 0.2433 - val_rmse: 0.4933 - val_mae: 0.2546 - val_mape: 7.7427 - lr: 1.0000e-04\n",
      "Epoch 591/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2525 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2525 - mape: 7.8087\n",
      "Epoch 591: val_loss improved from 0.25168 to 0.25156, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2528 - mse: 0.2408 - rmse: 0.4908 - mae: 0.2528 - mape: 7.8142 - val_loss: 0.2516 - val_mse: 0.2388 - val_rmse: 0.4887 - val_mae: 0.2516 - val_mape: 7.8070 - lr: 1.0000e-04\n",
      "Epoch 592/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2517 - mse: 0.2378 - rmse: 0.4876 - mae: 0.2517 - mape: 7.8174\n",
      "Epoch 592: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2526 - mse: 0.2403 - rmse: 0.4902 - mae: 0.2526 - mape: 7.8343 - val_loss: 0.2523 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2523 - val_mape: 7.8644 - lr: 1.0000e-04\n",
      "Epoch 593/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2530 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2530 - mape: 7.8223\n",
      "Epoch 593: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2526 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2526 - mape: 7.8197 - val_loss: 0.2516 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2516 - val_mape: 7.7694 - lr: 1.0000e-04\n",
      "Epoch 594/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2528 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2528 - mape: 7.8136\n",
      "Epoch 594: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2523 - mse: 0.2395 - rmse: 0.4893 - mae: 0.2523 - mape: 7.8043 - val_loss: 0.2533 - val_mse: 0.2419 - val_rmse: 0.4918 - val_mae: 0.2533 - val_mape: 7.7263 - lr: 1.0000e-04\n",
      "Epoch 595/2000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2528 - mse: 0.2421 - rmse: 0.4920 - mae: 0.2528 - mape: 7.7974\n",
      "Epoch 595: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2528 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2528 - mape: 7.8130 - val_loss: 0.2526 - val_mse: 0.2385 - val_rmse: 0.4884 - val_mae: 0.2526 - val_mape: 7.8584 - lr: 1.0000e-04\n",
      "Epoch 596/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2526 - mse: 0.2389 - rmse: 0.4888 - mae: 0.2526 - mape: 7.8129\n",
      "Epoch 596: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2524 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2524 - mape: 7.8089 - val_loss: 0.2516 - val_mse: 0.2386 - val_rmse: 0.4885 - val_mae: 0.2516 - val_mape: 7.7770 - lr: 1.0000e-04\n",
      "Epoch 597/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2543 - mse: 0.2437 - rmse: 0.4936 - mae: 0.2543 - mape: 7.8734\n",
      "Epoch 597: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2528 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2528 - mape: 7.8221 - val_loss: 0.2522 - val_mse: 0.2387 - val_rmse: 0.4885 - val_mae: 0.2522 - val_mape: 7.8352 - lr: 1.0000e-04\n",
      "Epoch 598/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2507 - mse: 0.2373 - rmse: 0.4872 - mae: 0.2507 - mape: 7.7664\n",
      "Epoch 598: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2522 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2522 - mape: 7.8064 - val_loss: 0.2538 - val_mse: 0.2383 - val_rmse: 0.4882 - val_mae: 0.2538 - val_mape: 7.8442 - lr: 1.0000e-04\n",
      "Epoch 599/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2548 - mse: 0.2455 - rmse: 0.4955 - mae: 0.2548 - mape: 7.8962\n",
      "Epoch 599: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2529 - mse: 0.2409 - rmse: 0.4908 - mae: 0.2529 - mape: 7.8306 - val_loss: 0.2516 - val_mse: 0.2388 - val_rmse: 0.4887 - val_mae: 0.2516 - val_mape: 7.8052 - lr: 1.0000e-04\n",
      "Epoch 600/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2525 - mse: 0.2387 - rmse: 0.4886 - mae: 0.2525 - mape: 7.8105\n",
      "Epoch 600: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2524 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2524 - mape: 7.8177 - val_loss: 0.2517 - val_mse: 0.2387 - val_rmse: 0.4886 - val_mae: 0.2517 - val_mape: 7.7547 - lr: 1.0000e-04\n",
      "Epoch 601/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2522 - mse: 0.2377 - rmse: 0.4875 - mae: 0.2522 - mape: 7.8142\n",
      "Epoch 601: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2523 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2523 - mape: 7.8209 - val_loss: 0.2518 - val_mse: 0.2383 - val_rmse: 0.4881 - val_mae: 0.2518 - val_mape: 7.8239 - lr: 1.0000e-04\n",
      "Epoch 602/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2535 - mse: 0.2418 - rmse: 0.4917 - mae: 0.2535 - mape: 7.8540\n",
      "Epoch 602: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2526 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2526 - mape: 7.8167 - val_loss: 0.2529 - val_mse: 0.2415 - val_rmse: 0.4914 - val_mae: 0.2529 - val_mape: 7.9061 - lr: 1.0000e-04\n",
      "Epoch 603/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2517 - mse: 0.2382 - rmse: 0.4880 - mae: 0.2517 - mape: 7.8122\n",
      "Epoch 603: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2526 - mse: 0.2405 - rmse: 0.4904 - mae: 0.2526 - mape: 7.8366 - val_loss: 0.2520 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2520 - val_mape: 7.8043 - lr: 1.0000e-04\n",
      "Epoch 604/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2518 - mse: 0.2387 - rmse: 0.4886 - mae: 0.2518 - mape: 7.8093\n",
      "Epoch 604: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2525 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2525 - mape: 7.8171 - val_loss: 0.2516 - val_mse: 0.2397 - val_rmse: 0.4895 - val_mae: 0.2516 - val_mape: 7.8157 - lr: 1.0000e-04\n",
      "Epoch 605/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2516 - mse: 0.2372 - rmse: 0.4870 - mae: 0.2516 - mape: 7.7986\n",
      "Epoch 605: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2528 - mse: 0.2411 - rmse: 0.4910 - mae: 0.2528 - mape: 7.8424 - val_loss: 0.2547 - val_mse: 0.2409 - val_rmse: 0.4908 - val_mae: 0.2547 - val_mape: 7.6973 - lr: 1.0000e-04\n",
      "Epoch 606/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2520 - mse: 0.2380 - rmse: 0.4879 - mae: 0.2520 - mape: 7.8087\n",
      "Epoch 606: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2525 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2525 - mape: 7.8198 - val_loss: 0.2522 - val_mse: 0.2385 - val_rmse: 0.4883 - val_mae: 0.2522 - val_mape: 7.8614 - lr: 1.0000e-04\n",
      "Epoch 607/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2520 - mse: 0.2383 - rmse: 0.4882 - mae: 0.2520 - mape: 7.8082\n",
      "Epoch 607: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2526 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2526 - mape: 7.8210 - val_loss: 0.2527 - val_mse: 0.2404 - val_rmse: 0.4903 - val_mae: 0.2527 - val_mape: 7.7692 - lr: 1.0000e-04\n",
      "Epoch 608/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2525 - mse: 0.2403 - rmse: 0.4902 - mae: 0.2525 - mape: 7.8332\n",
      "Epoch 608: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2522 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2522 - mape: 7.8231 - val_loss: 0.2537 - val_mse: 0.2369 - val_rmse: 0.4868 - val_mae: 0.2537 - val_mape: 7.7944 - lr: 1.0000e-04\n",
      "Epoch 609/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2532 - mse: 0.2412 - rmse: 0.4911 - mae: 0.2532 - mape: 7.8268\n",
      "Epoch 609: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2528 - mse: 0.2402 - rmse: 0.4902 - mae: 0.2528 - mape: 7.8179 - val_loss: 0.2520 - val_mse: 0.2385 - val_rmse: 0.4884 - val_mae: 0.2520 - val_mape: 7.8531 - lr: 1.0000e-04\n",
      "Epoch 610/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2522 - mse: 0.2391 - rmse: 0.4890 - mae: 0.2522 - mape: 7.7870\n",
      "Epoch 610: val_loss did not improve from 0.25156\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2526 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2526 - mape: 7.8112 - val_loss: 0.2556 - val_mse: 0.2428 - val_rmse: 0.4927 - val_mae: 0.2556 - val_mape: 7.7146 - lr: 1.0000e-04\n",
      "Epoch 611/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2524 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2524 - mape: 7.8290\n",
      "Epoch 611: val_loss improved from 0.25156 to 0.25150, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2523 - mse: 0.2405 - rmse: 0.4904 - mae: 0.2523 - mape: 7.8168 - val_loss: 0.2515 - val_mse: 0.2387 - val_rmse: 0.4886 - val_mae: 0.2515 - val_mape: 7.7684 - lr: 1.0000e-04\n",
      "Epoch 612/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2539 - mse: 0.2434 - rmse: 0.4934 - mae: 0.2539 - mape: 7.8703\n",
      "Epoch 612: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2525 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2525 - mape: 7.8278 - val_loss: 0.2518 - val_mse: 0.2386 - val_rmse: 0.4884 - val_mae: 0.2518 - val_mape: 7.7406 - lr: 1.0000e-04\n",
      "Epoch 613/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2518 - mse: 0.2376 - rmse: 0.4874 - mae: 0.2518 - mape: 7.7880\n",
      "Epoch 613: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2524 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2524 - mape: 7.8127 - val_loss: 0.2559 - val_mse: 0.2441 - val_rmse: 0.4941 - val_mae: 0.2559 - val_mape: 7.7307 - lr: 1.0000e-04\n",
      "Epoch 614/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2528 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2528 - mape: 7.8210\n",
      "Epoch 614: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2529 - mse: 0.2412 - rmse: 0.4912 - mae: 0.2529 - mape: 7.8329 - val_loss: 0.2528 - val_mse: 0.2412 - val_rmse: 0.4911 - val_mae: 0.2528 - val_mape: 7.7396 - lr: 1.0000e-04\n",
      "Epoch 615/2000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2529 - mse: 0.2420 - rmse: 0.4919 - mae: 0.2529 - mape: 7.8374\n",
      "Epoch 615: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2524 - mse: 0.2399 - rmse: 0.4898 - mae: 0.2524 - mape: 7.8178 - val_loss: 0.2522 - val_mse: 0.2382 - val_rmse: 0.4880 - val_mae: 0.2522 - val_mape: 7.7997 - lr: 1.0000e-04\n",
      "Epoch 616/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2543 - mse: 0.2444 - rmse: 0.4943 - mae: 0.2543 - mape: 7.8814\n",
      "Epoch 616: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2524 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2524 - mape: 7.8153 - val_loss: 0.2516 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2516 - val_mape: 7.8181 - lr: 1.0000e-04\n",
      "Epoch 617/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2533 - mse: 0.2419 - rmse: 0.4919 - mae: 0.2533 - mape: 7.8384\n",
      "Epoch 617: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2521 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2521 - mape: 7.8102 - val_loss: 0.2525 - val_mse: 0.2392 - val_rmse: 0.4890 - val_mae: 0.2525 - val_mape: 7.8501 - lr: 1.0000e-04\n",
      "Epoch 618/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2532 - mse: 0.2385 - rmse: 0.4883 - mae: 0.2532 - mape: 7.8501\n",
      "Epoch 618: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2525 - mse: 0.2399 - rmse: 0.4898 - mae: 0.2525 - mape: 7.8232 - val_loss: 0.2518 - val_mse: 0.2397 - val_rmse: 0.4895 - val_mae: 0.2518 - val_mape: 7.8580 - lr: 1.0000e-04\n",
      "Epoch 619/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2517 - mse: 0.2385 - rmse: 0.4883 - mae: 0.2517 - mape: 7.8016\n",
      "Epoch 619: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2529 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2529 - mape: 7.8320 - val_loss: 0.2519 - val_mse: 0.2399 - val_rmse: 0.4898 - val_mae: 0.2519 - val_mape: 7.8691 - lr: 1.0000e-04\n",
      "Epoch 620/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2505 - mse: 0.2363 - rmse: 0.4862 - mae: 0.2505 - mape: 7.7703\n",
      "Epoch 620: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2523 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2523 - mape: 7.8121 - val_loss: 0.2522 - val_mse: 0.2405 - val_rmse: 0.4904 - val_mae: 0.2522 - val_mape: 7.9085 - lr: 1.0000e-04\n",
      "Epoch 621/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2513 - mse: 0.2381 - rmse: 0.4879 - mae: 0.2513 - mape: 7.8250\n",
      "Epoch 621: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2525 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2525 - mape: 7.8382 - val_loss: 0.2522 - val_mse: 0.2407 - val_rmse: 0.4906 - val_mae: 0.2522 - val_mape: 7.8575 - lr: 1.0000e-04\n",
      "Epoch 622/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2532 - mse: 0.2430 - rmse: 0.4930 - mae: 0.2532 - mape: 7.8374\n",
      "Epoch 622: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2525 - mse: 0.2412 - rmse: 0.4911 - mae: 0.2525 - mape: 7.8267 - val_loss: 0.2517 - val_mse: 0.2390 - val_rmse: 0.4889 - val_mae: 0.2517 - val_mape: 7.7801 - lr: 1.0000e-04\n",
      "Epoch 623/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2520 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2520 - mape: 7.8125\n",
      "Epoch 623: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2523 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2523 - mape: 7.8266 - val_loss: 0.2526 - val_mse: 0.2397 - val_rmse: 0.4896 - val_mae: 0.2526 - val_mape: 7.7230 - lr: 1.0000e-04\n",
      "Epoch 624/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2520 - mse: 0.2390 - rmse: 0.4888 - mae: 0.2520 - mape: 7.8129\n",
      "Epoch 624: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2521 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2521 - mape: 7.8165 - val_loss: 0.2515 - val_mse: 0.2386 - val_rmse: 0.4885 - val_mae: 0.2515 - val_mape: 7.8084 - lr: 1.0000e-04\n",
      "Epoch 625/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2531 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2531 - mape: 7.8265\n",
      "Epoch 625: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2526 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2526 - mape: 7.8213 - val_loss: 0.2518 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2518 - val_mape: 7.7847 - lr: 1.0000e-04\n",
      "Epoch 626/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2525 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2525 - mape: 7.8306\n",
      "Epoch 626: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2522 - mse: 0.2393 - rmse: 0.4891 - mae: 0.2522 - mape: 7.8158 - val_loss: 0.2535 - val_mse: 0.2420 - val_rmse: 0.4919 - val_mae: 0.2535 - val_mape: 7.7413 - lr: 1.0000e-04\n",
      "Epoch 627/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2520 - mse: 0.2403 - rmse: 0.4902 - mae: 0.2520 - mape: 7.8012\n",
      "Epoch 627: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2525 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2525 - mape: 7.8201 - val_loss: 0.2516 - val_mse: 0.2397 - val_rmse: 0.4896 - val_mae: 0.2516 - val_mape: 7.8247 - lr: 1.0000e-04\n",
      "Epoch 628/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2522 - mse: 0.2405 - rmse: 0.4905 - mae: 0.2522 - mape: 7.8053\n",
      "Epoch 628: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2523 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2523 - mape: 7.8076 - val_loss: 0.2520 - val_mse: 0.2396 - val_rmse: 0.4895 - val_mae: 0.2520 - val_mape: 7.8646 - lr: 1.0000e-04\n",
      "Epoch 629/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2532 - mse: 0.2412 - rmse: 0.4911 - mae: 0.2532 - mape: 7.8515\n",
      "Epoch 629: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2521 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2521 - mape: 7.8339 - val_loss: 0.2517 - val_mse: 0.2407 - val_rmse: 0.4906 - val_mae: 0.2517 - val_mape: 7.8528 - lr: 1.0000e-04\n",
      "Epoch 630/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2522 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2522 - mape: 7.8035\n",
      "Epoch 630: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2521 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2521 - mape: 7.8016 - val_loss: 0.2525 - val_mse: 0.2406 - val_rmse: 0.4905 - val_mae: 0.2525 - val_mape: 7.9255 - lr: 1.0000e-04\n",
      "Epoch 631/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2523 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2523 - mape: 7.8194\n",
      "Epoch 631: val_loss did not improve from 0.25150\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2525 - mse: 0.2403 - rmse: 0.4902 - mae: 0.2525 - mape: 7.8223 - val_loss: 0.2516 - val_mse: 0.2396 - val_rmse: 0.4895 - val_mae: 0.2516 - val_mape: 7.8054 - lr: 1.0000e-04\n",
      "Epoch 632/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2514 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2514 - mape: 7.7984\n",
      "Epoch 632: val_loss improved from 0.25150 to 0.25115, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2514 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2514 - mape: 7.7903 - val_loss: 0.2512 - val_mse: 0.2392 - val_rmse: 0.4890 - val_mae: 0.2512 - val_mape: 7.7761 - lr: 1.0000e-05\n",
      "Epoch 633/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2514 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2514 - mape: 7.7888\n",
      "Epoch 633: val_loss improved from 0.25115 to 0.25112, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2513 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2513 - mape: 7.7882 - val_loss: 0.2511 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2511 - val_mape: 7.7821 - lr: 1.0000e-05\n",
      "Epoch 634/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2513 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2513 - mape: 7.7787\n",
      "Epoch 634: val_loss did not improve from 0.25112\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2513 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2513 - mape: 7.7827 - val_loss: 0.2512 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2512 - val_mape: 7.8021 - lr: 1.0000e-05\n",
      "Epoch 635/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2520 - mse: 0.2391 - rmse: 0.4890 - mae: 0.2520 - mape: 7.7929\n",
      "Epoch 635: val_loss did not improve from 0.25112\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2391 - rmse: 0.4890 - mae: 0.2512 - mape: 7.7796 - val_loss: 0.2512 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2512 - val_mape: 7.8083 - lr: 1.0000e-05\n",
      "Epoch 636/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2504 - mse: 0.2377 - rmse: 0.4875 - mae: 0.2504 - mape: 7.7593\n",
      "Epoch 636: val_loss did not improve from 0.25112\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2512 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2512 - mape: 7.7882 - val_loss: 0.2511 - val_mse: 0.2390 - val_rmse: 0.4889 - val_mae: 0.2511 - val_mape: 7.7843 - lr: 1.0000e-05\n",
      "Epoch 637/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2522 - mse: 0.2411 - rmse: 0.4910 - mae: 0.2522 - mape: 7.8185\n",
      "Epoch 637: val_loss improved from 0.25112 to 0.25111, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2512 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2512 - mape: 7.7894 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7669 - lr: 1.0000e-05\n",
      "Epoch 638/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2510 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2510 - mape: 7.7754\n",
      "Epoch 638: val_loss did not improve from 0.25111\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2512 - mape: 7.7780 - val_loss: 0.2512 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2512 - val_mape: 7.8129 - lr: 1.0000e-05\n",
      "Epoch 639/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2511 - mse: 0.2381 - rmse: 0.4879 - mae: 0.2511 - mape: 7.7918\n",
      "Epoch 639: val_loss improved from 0.25111 to 0.25111, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2513 - mse: 0.2394 - rmse: 0.4892 - mae: 0.2513 - mape: 7.7929 - val_loss: 0.2511 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2511 - val_mape: 7.7848 - lr: 1.0000e-05\n",
      "Epoch 640/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2514 - mse: 0.2405 - rmse: 0.4904 - mae: 0.2514 - mape: 7.8059\n",
      "Epoch 640: val_loss improved from 0.25111 to 0.25110, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2513 - mse: 0.2394 - rmse: 0.4892 - mae: 0.2513 - mape: 7.7792 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7812 - lr: 1.0000e-05\n",
      "Epoch 641/2000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.2521 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2521 - mape: 7.7988\n",
      "Epoch 641: val_loss improved from 0.25110 to 0.25110, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2512 - mse: 0.2391 - rmse: 0.4890 - mae: 0.2512 - mape: 7.7916 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7806 - lr: 1.0000e-05\n",
      "Epoch 642/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2514 - mse: 0.2391 - rmse: 0.4889 - mae: 0.2514 - mape: 7.7889\n",
      "Epoch 642: val_loss improved from 0.25110 to 0.25109, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2513 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2513 - mape: 7.7789 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7781 - lr: 1.0000e-05\n",
      "Epoch 643/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2505 - mse: 0.2379 - rmse: 0.4877 - mae: 0.2505 - mape: 7.7751\n",
      "Epoch 643: val_loss did not improve from 0.25109\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2512 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2512 - mape: 7.7938 - val_loss: 0.2511 - val_mse: 0.2393 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7879 - lr: 1.0000e-05\n",
      "Epoch 644/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2522 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2522 - mape: 7.8213\n",
      "Epoch 644: val_loss did not improve from 0.25109\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2513 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2513 - mape: 7.7889 - val_loss: 0.2511 - val_mse: 0.2393 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7809 - lr: 1.0000e-05\n",
      "Epoch 645/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2505 - mse: 0.2379 - rmse: 0.4877 - mae: 0.2505 - mape: 7.7626\n",
      "Epoch 645: val_loss did not improve from 0.25109\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2511 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2511 - mape: 7.7757 - val_loss: 0.2513 - val_mse: 0.2398 - val_rmse: 0.4897 - val_mae: 0.2513 - val_mape: 7.8270 - lr: 1.0000e-05\n",
      "Epoch 646/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2511 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2511 - mape: 7.7871\n",
      "Epoch 646: val_loss did not improve from 0.25109\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2513 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2513 - mape: 7.7972 - val_loss: 0.2511 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2511 - val_mape: 7.7830 - lr: 1.0000e-05\n",
      "Epoch 647/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2514 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2514 - mape: 7.7904\n",
      "Epoch 647: val_loss improved from 0.25109 to 0.25108, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2512 - mape: 7.7861 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7790 - lr: 1.0000e-05\n",
      "Epoch 648/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2529 - mse: 0.2432 - rmse: 0.4932 - mae: 0.2529 - mape: 7.8406\n",
      "Epoch 648: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2512 - mape: 7.7864 - val_loss: 0.2512 - val_mse: 0.2389 - val_rmse: 0.4888 - val_mae: 0.2512 - val_mape: 7.7840 - lr: 1.0000e-05\n",
      "Epoch 649/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2512 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2512 - mape: 7.7910\n",
      "Epoch 649: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2512 - mape: 7.7818 - val_loss: 0.2511 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2511 - val_mape: 7.7938 - lr: 1.0000e-05\n",
      "Epoch 650/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2514 - mse: 0.2386 - rmse: 0.4885 - mae: 0.2514 - mape: 7.7914\n",
      "Epoch 650: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2512 - mape: 7.7914 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7741 - lr: 1.0000e-05\n",
      "Epoch 651/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2504 - mse: 0.2376 - rmse: 0.4874 - mae: 0.2504 - mape: 7.7584\n",
      "Epoch 651: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2512 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2512 - mape: 7.7835 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7763 - lr: 1.0000e-05\n",
      "Epoch 652/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2506 - mse: 0.2382 - rmse: 0.4881 - mae: 0.2506 - mape: 7.7701\n",
      "Epoch 652: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2512 - mape: 7.7890 - val_loss: 0.2511 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2511 - val_mape: 7.8056 - lr: 1.0000e-05\n",
      "Epoch 653/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2508 - mse: 0.2379 - rmse: 0.4878 - mae: 0.2508 - mape: 7.7773\n",
      "Epoch 653: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2512 - mape: 7.7886 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4890 - val_mae: 0.2511 - val_mape: 7.7720 - lr: 1.0000e-05\n",
      "Epoch 654/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2514 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2514 - mape: 7.7940\n",
      "Epoch 654: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2513 - mape: 7.7879 - val_loss: 0.2511 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2511 - val_mape: 7.7672 - lr: 1.0000e-05\n",
      "Epoch 655/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2505 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2505 - mape: 7.7599\n",
      "Epoch 655: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2513 - mape: 7.7842 - val_loss: 0.2511 - val_mse: 0.2391 - val_rmse: 0.4889 - val_mae: 0.2511 - val_mape: 7.7831 - lr: 1.0000e-05\n",
      "Epoch 656/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2497 - mse: 0.2381 - rmse: 0.4880 - mae: 0.2497 - mape: 7.7538\n",
      "Epoch 656: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2512 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2512 - mape: 7.7927 - val_loss: 0.2511 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2511 - val_mape: 7.7640 - lr: 1.0000e-05\n",
      "Epoch 657/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2524 - mse: 0.2411 - rmse: 0.4910 - mae: 0.2524 - mape: 7.8200\n",
      "Epoch 657: val_loss improved from 0.25108 to 0.25108, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2392 - rmse: 0.4890 - mae: 0.2512 - mape: 7.7765 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4890 - val_mae: 0.2511 - val_mape: 7.7925 - lr: 1.0000e-05\n",
      "Epoch 658/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2511 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2511 - mape: 7.7772\n",
      "Epoch 658: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2512 - mape: 7.7839 - val_loss: 0.2511 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2511 - val_mape: 7.8002 - lr: 1.0000e-05\n",
      "Epoch 659/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2511 - mse: 0.2371 - rmse: 0.4870 - mae: 0.2511 - mape: 7.7696\n",
      "Epoch 659: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2513 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2513 - mape: 7.7918 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7864 - lr: 1.0000e-05\n",
      "Epoch 660/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2504 - mse: 0.2376 - rmse: 0.4874 - mae: 0.2504 - mape: 7.7645\n",
      "Epoch 660: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2512 - mape: 7.7886 - val_loss: 0.2511 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2511 - val_mape: 7.7667 - lr: 1.0000e-05\n",
      "Epoch 661/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2505 - mse: 0.2382 - rmse: 0.4880 - mae: 0.2505 - mape: 7.7699\n",
      "Epoch 661: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2511 - mse: 0.2391 - rmse: 0.4890 - mae: 0.2511 - mape: 7.7818 - val_loss: 0.2512 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2512 - val_mape: 7.8145 - lr: 1.0000e-05\n",
      "Epoch 662/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2531 - mse: 0.2424 - rmse: 0.4923 - mae: 0.2531 - mape: 7.8520\n",
      "Epoch 662: val_loss did not improve from 0.25108\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2512 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2512 - mape: 7.7915 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7852 - lr: 1.0000e-05\n",
      "Epoch 663/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2510 - mse: 0.2389 - rmse: 0.4888 - mae: 0.2510 - mape: 7.7897\n",
      "Epoch 663: val_loss improved from 0.25108 to 0.25106, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2512 - mape: 7.7891 - val_loss: 0.2511 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2511 - val_mape: 7.7755 - lr: 1.0000e-05\n",
      "Epoch 664/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2513 - mse: 0.2385 - rmse: 0.4884 - mae: 0.2513 - mape: 7.8114\n",
      "Epoch 664: val_loss did not improve from 0.25106\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2512 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2512 - mape: 7.7900 - val_loss: 0.2511 - val_mse: 0.2391 - val_rmse: 0.4889 - val_mae: 0.2511 - val_mape: 7.7824 - lr: 1.0000e-05\n",
      "Epoch 665/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2521 - mse: 0.2418 - rmse: 0.4917 - mae: 0.2521 - mape: 7.8092\n",
      "Epoch 665: val_loss did not improve from 0.25106\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2512 - mape: 7.7852 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4890 - val_mae: 0.2511 - val_mape: 7.7698 - lr: 1.0000e-05\n",
      "Epoch 666/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2506 - mse: 0.2384 - rmse: 0.4883 - mae: 0.2506 - mape: 7.7733\n",
      "Epoch 666: val_loss improved from 0.25106 to 0.25106, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2512 - mape: 7.7909 - val_loss: 0.2511 - val_mse: 0.2391 - val_rmse: 0.4890 - val_mae: 0.2511 - val_mape: 7.7757 - lr: 1.0000e-05\n",
      "Epoch 667/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2511 - mse: 0.2384 - rmse: 0.4883 - mae: 0.2511 - mape: 7.7738\n",
      "Epoch 667: val_loss did not improve from 0.25106\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2512 - mape: 7.7845 - val_loss: 0.2511 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2511 - val_mape: 7.8014 - lr: 1.0000e-05\n",
      "Epoch 668/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2518 - mse: 0.2405 - rmse: 0.4904 - mae: 0.2518 - mape: 7.8148\n",
      "Epoch 668: val_loss did not improve from 0.25106\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2512 - mape: 7.7966 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7660 - lr: 1.0000e-05\n",
      "Epoch 669/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2510 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2510 - mape: 7.7998\n",
      "Epoch 669: val_loss improved from 0.25106 to 0.25104, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2512 - mape: 7.7865 - val_loss: 0.2510 - val_mse: 0.2392 - val_rmse: 0.4890 - val_mae: 0.2510 - val_mape: 7.7732 - lr: 1.0000e-05\n",
      "Epoch 670/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2514 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2514 - mape: 7.7991\n",
      "Epoch 670: val_loss did not improve from 0.25104\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2513 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2513 - mape: 7.7880 - val_loss: 0.2510 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2510 - val_mape: 7.7842 - lr: 1.0000e-05\n",
      "Epoch 671/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2512 - mse: 0.2385 - rmse: 0.4883 - mae: 0.2512 - mape: 7.7811\n",
      "Epoch 671: val_loss did not improve from 0.25104\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2512 - mape: 7.7818 - val_loss: 0.2511 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2511 - val_mape: 7.8029 - lr: 1.0000e-05\n",
      "Epoch 672/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2513 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2513 - mape: 7.7970\n",
      "Epoch 672: val_loss did not improve from 0.25104\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2512 - mape: 7.7933 - val_loss: 0.2511 - val_mse: 0.2390 - val_rmse: 0.4889 - val_mae: 0.2511 - val_mape: 7.7679 - lr: 1.0000e-05\n",
      "Epoch 673/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2515 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2515 - mape: 7.7749\n",
      "Epoch 673: val_loss did not improve from 0.25104\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2511 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2511 - mape: 7.7798 - val_loss: 0.2511 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2511 - val_mape: 7.8049 - lr: 1.0000e-05\n",
      "Epoch 674/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2513 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2513 - mape: 7.8025\n",
      "Epoch 674: val_loss did not improve from 0.25104\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2512 - mape: 7.7847 - val_loss: 0.2512 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2512 - val_mape: 7.8035 - lr: 1.0000e-05\n",
      "Epoch 675/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2515 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2515 - mape: 7.8116\n",
      "Epoch 675: val_loss did not improve from 0.25104\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2513 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2513 - mape: 7.7975 - val_loss: 0.2511 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2511 - val_mape: 7.7591 - lr: 1.0000e-05\n",
      "Epoch 676/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2519 - mse: 0.2422 - rmse: 0.4922 - mae: 0.2519 - mape: 7.8034\n",
      "Epoch 676: val_loss did not improve from 0.25104\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2512 - mse: 0.2395 - rmse: 0.4893 - mae: 0.2512 - mape: 7.7836 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7623 - lr: 1.0000e-05\n",
      "Epoch 677/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2500 - mse: 0.2373 - rmse: 0.4871 - mae: 0.2500 - mape: 7.7483\n",
      "Epoch 677: val_loss improved from 0.25104 to 0.25103, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2512 - mape: 7.7837 - val_loss: 0.2510 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2510 - val_mape: 7.7669 - lr: 1.0000e-05\n",
      "Epoch 678/2000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2531 - mse: 0.2432 - rmse: 0.4932 - mae: 0.2531 - mape: 7.8467\n",
      "Epoch 678: val_loss improved from 0.25103 to 0.25101, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2512 - mape: 7.7805 - val_loss: 0.2510 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2510 - val_mape: 7.7827 - lr: 1.0000e-05\n",
      "Epoch 679/2000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2495 - mse: 0.2362 - rmse: 0.4860 - mae: 0.2495 - mape: 7.7470\n",
      "Epoch 679: val_loss did not improve from 0.25101\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2512 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2512 - mape: 7.7912 - val_loss: 0.2511 - val_mse: 0.2393 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7551 - lr: 1.0000e-05\n",
      "Epoch 680/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2524 - mse: 0.2422 - rmse: 0.4922 - mae: 0.2524 - mape: 7.8178\n",
      "Epoch 680: val_loss did not improve from 0.25101\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2511 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2511 - mape: 7.7803 - val_loss: 0.2510 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2510 - val_mape: 7.7732 - lr: 1.0000e-05\n",
      "Epoch 681/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2505 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2505 - mape: 7.7517\n",
      "Epoch 681: val_loss improved from 0.25101 to 0.25101, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2512 - mse: 0.2396 - rmse: 0.4894 - mae: 0.2512 - mape: 7.7803 - val_loss: 0.2510 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2510 - val_mape: 7.7781 - lr: 1.0000e-05\n",
      "Epoch 682/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2502 - mse: 0.2384 - rmse: 0.4883 - mae: 0.2502 - mape: 7.7628\n",
      "Epoch 682: val_loss improved from 0.25101 to 0.25100, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2396 - rmse: 0.4894 - mae: 0.2512 - mape: 7.7843 - val_loss: 0.2510 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2510 - val_mape: 7.7771 - lr: 1.0000e-05\n",
      "Epoch 683/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2510 - mse: 0.2383 - rmse: 0.4882 - mae: 0.2510 - mape: 7.7689\n",
      "Epoch 683: val_loss did not improve from 0.25100\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2511 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2511 - mape: 7.7814 - val_loss: 0.2511 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2511 - val_mape: 7.8105 - lr: 1.0000e-05\n",
      "Epoch 684/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2509 - mse: 0.2403 - rmse: 0.4902 - mae: 0.2509 - mape: 7.7806\n",
      "Epoch 684: val_loss did not improve from 0.25100\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2512 - mape: 7.7932 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4890 - val_mae: 0.2511 - val_mape: 7.7558 - lr: 1.0000e-05\n",
      "Epoch 685/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2514 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2514 - mape: 7.7880\n",
      "Epoch 685: val_loss improved from 0.25100 to 0.25100, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2511 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2511 - mape: 7.7814 - val_loss: 0.2510 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2510 - val_mape: 7.7850 - lr: 1.0000e-05\n",
      "Epoch 686/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2537 - mse: 0.2439 - rmse: 0.4939 - mae: 0.2537 - mape: 7.8661\n",
      "Epoch 686: val_loss did not improve from 0.25100\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2511 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2511 - mape: 7.7775 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2510 - val_mape: 7.7877 - lr: 1.0000e-05\n",
      "Epoch 687/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2514 - mse: 0.2397 - rmse: 0.4895 - mae: 0.2514 - mape: 7.7927\n",
      "Epoch 687: val_loss did not improve from 0.25100\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2511 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2511 - mape: 7.7874 - val_loss: 0.2511 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2511 - val_mape: 7.7948 - lr: 1.0000e-05\n",
      "Epoch 688/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2510 - mse: 0.2389 - rmse: 0.4887 - mae: 0.2510 - mape: 7.7910\n",
      "Epoch 688: val_loss improved from 0.25100 to 0.25099, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2512 - mape: 7.7872 - val_loss: 0.2510 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2510 - val_mape: 7.7710 - lr: 1.0000e-05\n",
      "Epoch 689/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2511 - mse: 0.2388 - rmse: 0.4886 - mae: 0.2511 - mape: 7.7916\n",
      "Epoch 689: val_loss did not improve from 0.25099\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2512 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2512 - mape: 7.7851 - val_loss: 0.2510 - val_mse: 0.2392 - val_rmse: 0.4891 - val_mae: 0.2510 - val_mape: 7.7779 - lr: 1.0000e-05\n",
      "Epoch 690/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2513 - mse: 0.2399 - rmse: 0.4898 - mae: 0.2513 - mape: 7.7672\n",
      "Epoch 690: val_loss did not improve from 0.25099\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2511 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2511 - mape: 7.7730 - val_loss: 0.2511 - val_mse: 0.2398 - val_rmse: 0.4896 - val_mae: 0.2511 - val_mape: 7.8189 - lr: 1.0000e-05\n",
      "Epoch 691/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2509 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2509 - mape: 7.7732\n",
      "Epoch 691: val_loss did not improve from 0.25099\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2511 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2511 - mape: 7.7863 - val_loss: 0.2510 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2510 - val_mape: 7.7984 - lr: 1.0000e-05\n",
      "Epoch 692/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2498 - mse: 0.2376 - rmse: 0.4874 - mae: 0.2498 - mape: 7.7406\n",
      "Epoch 692: val_loss improved from 0.25099 to 0.25099, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2511 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2511 - mape: 7.7854 - val_loss: 0.2510 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2510 - val_mape: 7.7845 - lr: 1.0000e-05\n",
      "Epoch 693/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2523 - mse: 0.2415 - rmse: 0.4914 - mae: 0.2523 - mape: 7.8258\n",
      "Epoch 693: val_loss improved from 0.25099 to 0.25099, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2511 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2511 - mape: 7.7821 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7817 - lr: 1.0000e-05\n",
      "Epoch 694/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2524 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2524 - mape: 7.8081\n",
      "Epoch 694: val_loss did not improve from 0.25099\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2511 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2511 - mape: 7.7858 - val_loss: 0.2511 - val_mse: 0.2396 - val_rmse: 0.4895 - val_mae: 0.2511 - val_mape: 7.8020 - lr: 1.0000e-05\n",
      "Epoch 695/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2504 - mse: 0.2377 - rmse: 0.4876 - mae: 0.2504 - mape: 7.7633\n",
      "Epoch 695: val_loss did not improve from 0.25099\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2512 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2512 - mape: 7.7849 - val_loss: 0.2510 - val_mse: 0.2396 - val_rmse: 0.4895 - val_mae: 0.2510 - val_mape: 7.7852 - lr: 1.0000e-05\n",
      "Epoch 696/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2513 - mse: 0.2405 - rmse: 0.4904 - mae: 0.2513 - mape: 7.8043\n",
      "Epoch 696: val_loss did not improve from 0.25099\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2512 - mape: 7.7863 - val_loss: 0.2510 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2510 - val_mape: 7.7768 - lr: 1.0000e-05\n",
      "Epoch 697/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2512 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2512 - mape: 7.7846\n",
      "Epoch 697: val_loss improved from 0.25099 to 0.25098, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2511 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2511 - mape: 7.7832 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7805 - lr: 1.0000e-05\n",
      "Epoch 698/2000\n",
      "273/318 [========================>.....] - ETA: 0s - loss: 0.2518 - mse: 0.2403 - rmse: 0.4902 - mae: 0.2518 - mape: 7.8001\n",
      "Epoch 698: val_loss did not improve from 0.25098\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2511 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2511 - mape: 7.7782 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2510 - val_mape: 7.7915 - lr: 1.0000e-05\n",
      "Epoch 699/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2516 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2516 - mape: 7.7793\n",
      "Epoch 699: val_loss did not improve from 0.25098\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2511 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2511 - mape: 7.7813 - val_loss: 0.2510 - val_mse: 0.2396 - val_rmse: 0.4895 - val_mae: 0.2510 - val_mape: 7.7982 - lr: 1.0000e-05\n",
      "Epoch 700/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2519 - mse: 0.2415 - rmse: 0.4914 - mae: 0.2519 - mape: 7.8017\n",
      "Epoch 700: val_loss did not improve from 0.25098\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2511 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2511 - mape: 7.7862 - val_loss: 0.2512 - val_mse: 0.2397 - val_rmse: 0.4896 - val_mae: 0.2512 - val_mape: 7.7550 - lr: 1.0000e-05\n",
      "Epoch 701/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2510 - mse: 0.2380 - rmse: 0.4878 - mae: 0.2510 - mape: 7.7822\n",
      "Epoch 701: val_loss did not improve from 0.25098\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2511 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2511 - mape: 7.7786 - val_loss: 0.2510 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2510 - val_mape: 7.7726 - lr: 1.0000e-05\n",
      "Epoch 702/2000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2537 - mse: 0.2449 - rmse: 0.4949 - mae: 0.2537 - mape: 7.8400\n",
      "Epoch 702: val_loss improved from 0.25098 to 0.25097, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2511 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2511 - mape: 7.7844 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2510 - val_mape: 7.7840 - lr: 1.0000e-05\n",
      "Epoch 703/2000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2491 - mse: 0.2368 - rmse: 0.4866 - mae: 0.2491 - mape: 7.7454\n",
      "Epoch 703: val_loss did not improve from 0.25097\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2511 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2511 - mape: 7.7839 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7781 - lr: 1.0000e-05\n",
      "Epoch 704/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2511 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2511 - mape: 7.7959\n",
      "Epoch 704: val_loss did not improve from 0.25097\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7907 - val_loss: 0.2513 - val_mse: 0.2398 - val_rmse: 0.4897 - val_mae: 0.2513 - val_mape: 7.7387 - lr: 1.0000e-05\n",
      "Epoch 705/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2507 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2507 - mape: 7.7668\n",
      "Epoch 705: val_loss improved from 0.25097 to 0.25097, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2512 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2512 - mape: 7.7786 - val_loss: 0.2510 - val_mse: 0.2393 - val_rmse: 0.4892 - val_mae: 0.2510 - val_mape: 7.7784 - lr: 1.0000e-05\n",
      "Epoch 706/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2503 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2503 - mape: 7.7495\n",
      "Epoch 706: val_loss did not improve from 0.25097\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2510 - mape: 7.7770 - val_loss: 0.2512 - val_mse: 0.2400 - val_rmse: 0.4899 - val_mae: 0.2512 - val_mape: 7.8286 - lr: 1.0000e-05\n",
      "Epoch 707/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2513 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2513 - mape: 7.8023\n",
      "Epoch 707: val_loss did not improve from 0.25097\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2511 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2511 - mape: 7.7961 - val_loss: 0.2511 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2511 - val_mape: 7.7604 - lr: 1.0000e-05\n",
      "Epoch 708/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2517 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2517 - mape: 7.7994\n",
      "Epoch 708: val_loss did not improve from 0.25097\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2510 - mape: 7.7780 - val_loss: 0.2510 - val_mse: 0.2396 - val_rmse: 0.4895 - val_mae: 0.2510 - val_mape: 7.8064 - lr: 1.0000e-05\n",
      "Epoch 709/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2509 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2509 - mape: 7.7883\n",
      "Epoch 709: val_loss did not improve from 0.25097\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2510 - mape: 7.7882 - val_loss: 0.2512 - val_mse: 0.2400 - val_rmse: 0.4899 - val_mae: 0.2512 - val_mape: 7.8278 - lr: 1.0000e-05\n",
      "Epoch 710/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2508 - mse: 0.2391 - rmse: 0.4890 - mae: 0.2508 - mape: 7.8081\n",
      "Epoch 710: val_loss did not improve from 0.25097\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2511 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2511 - mape: 7.8162 - val_loss: 0.2511 - val_mse: 0.2397 - val_rmse: 0.4895 - val_mae: 0.2511 - val_mape: 7.8087 - lr: 1.0000e-06\n",
      "Epoch 711/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7974\n",
      "Epoch 711: val_loss did not improve from 0.25097\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2397 - rmse: 0.4895 - mae: 0.2510 - mape: 7.8067 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2510 - val_mape: 7.7969 - lr: 1.0000e-06\n",
      "Epoch 712/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2510 - mse: 0.2399 - rmse: 0.4898 - mae: 0.2510 - mape: 7.7880\n",
      "Epoch 712: val_loss improved from 0.25097 to 0.25097, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7911 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7866 - lr: 1.0000e-06\n",
      "Epoch 713/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2507 - mse: 0.2385 - rmse: 0.4884 - mae: 0.2507 - mape: 7.7789\n",
      "Epoch 713: val_loss improved from 0.25097 to 0.25096, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4893 - mae: 0.2510 - mape: 7.7855 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7836 - lr: 1.0000e-06\n",
      "Epoch 714/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2488 - mse: 0.2351 - rmse: 0.4849 - mae: 0.2488 - mape: 7.7363\n",
      "Epoch 714: val_loss improved from 0.25096 to 0.25096, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7811 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7830 - lr: 1.0000e-06\n",
      "Epoch 715/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2500 - mse: 0.2377 - rmse: 0.4876 - mae: 0.2500 - mape: 7.7586\n",
      "Epoch 715: val_loss improved from 0.25096 to 0.25096, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2510 - mape: 7.7820 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7808 - lr: 1.0000e-06\n",
      "Epoch 716/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7887\n",
      "Epoch 716: val_loss improved from 0.25096 to 0.25096, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2510 - mape: 7.7801 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7787 - lr: 1.0000e-06\n",
      "Epoch 717/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2512 - mse: 0.2410 - rmse: 0.4910 - mae: 0.2512 - mape: 7.7906\n",
      "Epoch 717: val_loss improved from 0.25096 to 0.25096, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2510 - mape: 7.7790 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7807 - lr: 1.0000e-06\n",
      "Epoch 718/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2510 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2510 - mape: 7.7793\n",
      "Epoch 718: val_loss did not improve from 0.25096\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2510 - mape: 7.7802 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7827 - lr: 1.0000e-06\n",
      "Epoch 719/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2505 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2505 - mape: 7.7732\n",
      "Epoch 719: val_loss improved from 0.25096 to 0.25096, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4893 - mae: 0.2510 - mape: 7.7825 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7809 - lr: 1.0000e-06\n",
      "Epoch 720/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2518 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2518 - mape: 7.7830\n",
      "Epoch 720: val_loss improved from 0.25096 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7824 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7805 - lr: 1.0000e-06\n",
      "Epoch 721/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2501 - mse: 0.2388 - rmse: 0.4887 - mae: 0.2501 - mape: 7.7543\n",
      "Epoch 721: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7797 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7790 - lr: 1.0000e-06\n",
      "Epoch 722/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2511 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2511 - mape: 7.7788\n",
      "Epoch 722: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4893 - mae: 0.2510 - mape: 7.7767 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7805 - lr: 1.0000e-06\n",
      "Epoch 723/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2512 - mse: 0.2405 - rmse: 0.4904 - mae: 0.2512 - mape: 7.7895\n",
      "Epoch 723: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4893 - mae: 0.2510 - mape: 7.7798 - val_loss: 0.2510 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7790 - lr: 1.0000e-06\n",
      "Epoch 724/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2501 - mse: 0.2376 - rmse: 0.4875 - mae: 0.2501 - mape: 7.7413\n",
      "Epoch 724: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4893 - mae: 0.2510 - mape: 7.7783 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7804 - lr: 1.0000e-06\n",
      "Epoch 725/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2495 - mse: 0.2364 - rmse: 0.4862 - mae: 0.2495 - mape: 7.7304\n",
      "Epoch 725: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7795 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7781 - lr: 1.0000e-06\n",
      "Epoch 726/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2515 - mse: 0.2405 - rmse: 0.4904 - mae: 0.2515 - mape: 7.7956\n",
      "Epoch 726: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2510 - mape: 7.7798 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2510 - val_mape: 7.7794 - lr: 1.0000e-06\n",
      "Epoch 727/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2505 - mse: 0.2391 - rmse: 0.4890 - mae: 0.2505 - mape: 7.7655\n",
      "Epoch 727: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7811 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7785 - lr: 1.0000e-06\n",
      "Epoch 728/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2509 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2509 - mape: 7.7568\n",
      "Epoch 728: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7762 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7789 - lr: 1.0000e-06\n",
      "Epoch 729/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2517 - mse: 0.2396 - rmse: 0.4894 - mae: 0.2517 - mape: 7.8049\n",
      "Epoch 729: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7765 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2510 - val_mape: 7.7789 - lr: 1.0000e-06\n",
      "Epoch 730/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2507 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2507 - mape: 7.7743\n",
      "Epoch 730: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7811 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2510 - val_mape: 7.7789 - lr: 1.0000e-06\n",
      "Epoch 731/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2508 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2508 - mape: 7.7683\n",
      "Epoch 731: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7790 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2510 - val_mape: 7.7784 - lr: 1.0000e-06\n",
      "Epoch 732/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2506 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2506 - mape: 7.7755\n",
      "Epoch 732: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7796 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4893 - val_mae: 0.2510 - val_mape: 7.7779 - lr: 1.0000e-06\n",
      "Epoch 733/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2497 - mse: 0.2366 - rmse: 0.4865 - mae: 0.2497 - mape: 7.7394\n",
      "Epoch 733: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7800 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2510 - val_mape: 7.7786 - lr: 1.0000e-06\n",
      "Epoch 734/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2511 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2511 - mape: 7.7976\n",
      "Epoch 734: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7804 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2510 - val_mape: 7.7779 - lr: 1.0000e-06\n",
      "Epoch 735/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2521 - mse: 0.2415 - rmse: 0.4914 - mae: 0.2521 - mape: 7.7944\n",
      "Epoch 735: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7795 - val_loss: 0.2510 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2510 - val_mape: 7.7790 - lr: 1.0000e-06\n",
      "Epoch 736/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2511 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2511 - mape: 7.7850\n",
      "Epoch 736: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7779 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7789 - lr: 1.0000e-06\n",
      "Epoch 737/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2524 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2524 - mape: 7.7982\n",
      "Epoch 737: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7790 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7797 - lr: 1.0000e-06\n",
      "Epoch 738/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2504 - mse: 0.2385 - rmse: 0.4884 - mae: 0.2504 - mape: 7.7711\n",
      "Epoch 738: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7803 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7786 - lr: 1.0000e-06\n",
      "Epoch 739/2000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2508 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2508 - mape: 7.7901\n",
      "Epoch 739: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7781 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7777 - lr: 1.0000e-06\n",
      "Epoch 740/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2524 - mse: 0.2434 - rmse: 0.4934 - mae: 0.2524 - mape: 7.8206\n",
      "Epoch 740: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7789 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7765 - lr: 1.0000e-06\n",
      "Epoch 741/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2515 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2515 - mape: 7.7951\n",
      "Epoch 741: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7756 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7799 - lr: 1.0000e-06\n",
      "Epoch 742/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2522 - mse: 0.2412 - rmse: 0.4911 - mae: 0.2522 - mape: 7.8161\n",
      "Epoch 742: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7814 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7806 - lr: 1.0000e-06\n",
      "Epoch 743/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2507 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2507 - mape: 7.7665\n",
      "Epoch 743: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7801 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7784 - lr: 1.0000e-06\n",
      "Epoch 744/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2515 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2515 - mape: 7.7875\n",
      "Epoch 744: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7777 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7786 - lr: 1.0000e-06\n",
      "Epoch 745/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2510 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2510 - mape: 7.7808\n",
      "Epoch 745: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7789 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7800 - lr: 1.0000e-06\n",
      "Epoch 746/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2513 - mse: 0.2411 - rmse: 0.4910 - mae: 0.2513 - mape: 7.7875\n",
      "Epoch 746: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7786 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7803 - lr: 1.0000e-06\n",
      "Epoch 747/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2511 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2511 - mape: 7.7855\n",
      "Epoch 747: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7827 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7780 - lr: 1.0000e-06\n",
      "Epoch 748/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2521 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2521 - mape: 7.8214\n",
      "Epoch 748: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7772 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7783 - lr: 1.0000e-06\n",
      "Epoch 749/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2508 - mse: 0.2381 - rmse: 0.4879 - mae: 0.2508 - mape: 7.7929\n",
      "Epoch 749: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7758 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7778 - lr: 1.0000e-06\n",
      "Epoch 750/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2520 - mse: 0.2405 - rmse: 0.4904 - mae: 0.2520 - mape: 7.8091\n",
      "Epoch 750: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7781 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7781 - lr: 1.0000e-07\n",
      "Epoch 751/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2520 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2520 - mape: 7.8068\n",
      "Epoch 751: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7778 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7781 - lr: 1.0000e-07\n",
      "Epoch 752/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2499 - mse: 0.2374 - rmse: 0.4873 - mae: 0.2499 - mape: 7.7545\n",
      "Epoch 752: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7781 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7782 - lr: 1.0000e-07\n",
      "Epoch 753/2000\n",
      "271/318 [========================>.....] - ETA: 0s - loss: 0.2507 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2507 - mape: 7.7605\n",
      "Epoch 753: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7784 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7782 - lr: 1.0000e-07\n",
      "Epoch 754/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2506 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2506 - mape: 7.7872\n",
      "Epoch 754: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7780 - lr: 1.0000e-07\n",
      "Epoch 755/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2508 - mse: 0.2393 - rmse: 0.4891 - mae: 0.2508 - mape: 7.7666\n",
      "Epoch 755: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7784 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7781 - lr: 1.0000e-07\n",
      "Epoch 756/2000\n",
      "270/318 [========================>.....] - ETA: 0s - loss: 0.2524 - mse: 0.2411 - rmse: 0.4910 - mae: 0.2524 - mape: 7.8253\n",
      "Epoch 756: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2510 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2510 - mape: 7.7783 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7781 - lr: 1.0000e-07\n",
      "Epoch 757/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2517 - mse: 0.2418 - rmse: 0.4917 - mae: 0.2517 - mape: 7.8028\n",
      "Epoch 757: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7782 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7783 - lr: 1.0000e-07\n",
      "Epoch 758/2000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2530 - mse: 0.2451 - rmse: 0.4951 - mae: 0.2530 - mape: 7.8333\n",
      "Epoch 758: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7783 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7783 - lr: 1.0000e-07\n",
      "Epoch 759/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2493 - mse: 0.2358 - rmse: 0.4856 - mae: 0.2493 - mape: 7.7357\n",
      "Epoch 759: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7783 - lr: 1.0000e-07\n",
      "Epoch 760/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2508 - mse: 0.2391 - rmse: 0.4890 - mae: 0.2508 - mape: 7.7685\n",
      "Epoch 760: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-07\n",
      "Epoch 761/2000\n",
      "261/318 [=======================>......] - ETA: 0s - loss: 0.2526 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2526 - mape: 7.8203\n",
      "Epoch 761: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7787 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7787 - lr: 1.0000e-07\n",
      "Epoch 762/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2512 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2512 - mape: 7.7902\n",
      "Epoch 762: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7787 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7787 - lr: 1.0000e-07\n",
      "Epoch 763/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2505 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2505 - mape: 7.7658\n",
      "Epoch 763: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7789 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7786 - lr: 1.0000e-07\n",
      "Epoch 764/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2510 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2510 - mape: 7.7820\n",
      "Epoch 764: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7784 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7784 - lr: 1.0000e-07\n",
      "Epoch 765/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2509 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2509 - mape: 7.7764\n",
      "Epoch 765: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7788 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-07\n",
      "Epoch 766/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2512 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2512 - mape: 7.7875\n",
      "Epoch 766: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7791 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7787 - lr: 1.0000e-07\n",
      "Epoch 767/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2520 - mse: 0.2415 - rmse: 0.4915 - mae: 0.2520 - mape: 7.8085\n",
      "Epoch 767: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7787 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-07\n",
      "Epoch 768/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2511 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2511 - mape: 7.7829\n",
      "Epoch 768: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7788 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7786 - lr: 1.0000e-07\n",
      "Epoch 769/2000\n",
      "262/318 [=======================>......] - ETA: 0s - loss: 0.2530 - mse: 0.2411 - rmse: 0.4910 - mae: 0.2530 - mape: 7.8299\n",
      "Epoch 769: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7787 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7787 - lr: 1.0000e-07\n",
      "Epoch 770/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2508 - mse: 0.2378 - rmse: 0.4876 - mae: 0.2508 - mape: 7.7768\n",
      "Epoch 770: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7788 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7786 - lr: 1.0000e-07\n",
      "Epoch 771/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2504 - mse: 0.2372 - rmse: 0.4870 - mae: 0.2504 - mape: 7.7672\n",
      "Epoch 771: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7786 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7788 - lr: 1.0000e-07\n",
      "Epoch 772/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2516 - mse: 0.2413 - rmse: 0.4912 - mae: 0.2516 - mape: 7.7865\n",
      "Epoch 772: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7787 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7788 - lr: 1.0000e-07\n",
      "Epoch 773/2000\n",
      "268/318 [========================>.....] - ETA: 0s - loss: 0.2503 - mse: 0.2382 - rmse: 0.4881 - mae: 0.2503 - mape: 7.7608\n",
      "Epoch 773: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7787 - lr: 1.0000e-07\n",
      "Epoch 774/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2507 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2507 - mape: 7.7417\n",
      "Epoch 774: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7788 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7789 - lr: 1.0000e-07\n",
      "Epoch 775/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2509 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2509 - mape: 7.7663\n",
      "Epoch 775: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7791 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7789 - lr: 1.0000e-07\n",
      "Epoch 776/2000\n",
      "265/318 [========================>.....] - ETA: 0s - loss: 0.2502 - mse: 0.2399 - rmse: 0.4898 - mae: 0.2502 - mape: 7.7551\n",
      "Epoch 776: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7789 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7788 - lr: 1.0000e-07\n",
      "Epoch 777/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2527 - mse: 0.2424 - rmse: 0.4923 - mae: 0.2527 - mape: 7.8251\n",
      "Epoch 777: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7788 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7788 - lr: 1.0000e-07\n",
      "Epoch 778/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2500 - mse: 0.2382 - rmse: 0.4881 - mae: 0.2500 - mape: 7.7585\n",
      "Epoch 778: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7789 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7786 - lr: 1.0000e-07\n",
      "Epoch 779/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2521 - mse: 0.2428 - rmse: 0.4927 - mae: 0.2521 - mape: 7.8331\n",
      "Epoch 779: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7786 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-07\n",
      "Epoch 780/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2520 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2520 - mape: 7.8272\n",
      "Epoch 780: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7784 - lr: 1.0000e-07\n",
      "Epoch 781/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2505 - mse: 0.2388 - rmse: 0.4886 - mae: 0.2505 - mape: 7.7666\n",
      "Epoch 781: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7787 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7784 - lr: 1.0000e-07\n",
      "Epoch 782/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2514 - mse: 0.2399 - rmse: 0.4898 - mae: 0.2514 - mape: 7.7841\n",
      "Epoch 782: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7787 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7786 - lr: 1.0000e-07\n",
      "Epoch 783/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2512 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2512 - mape: 7.7915\n",
      "Epoch 783: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7784 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7787 - lr: 1.0000e-07\n",
      "Epoch 784/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2515 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2515 - mape: 7.7883\n",
      "Epoch 784: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7784 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7787 - lr: 1.0000e-07\n",
      "Epoch 785/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2508 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2508 - mape: 7.7730\n",
      "Epoch 785: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7787 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7787 - lr: 1.0000e-07\n",
      "Epoch 786/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2507 - mse: 0.2385 - rmse: 0.4883 - mae: 0.2507 - mape: 7.7840\n",
      "Epoch 786: val_loss improved from 0.25095 to 0.25095, saving model to model_weights/20221123-225504_mlp_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7786 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7786 - lr: 1.0000e-07\n",
      "Epoch 787/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2519 - mse: 0.2415 - rmse: 0.4915 - mae: 0.2519 - mape: 7.8037\n",
      "Epoch 787: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7786 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7787 - lr: 1.0000e-07\n",
      "Epoch 788/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2509 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2509 - mape: 7.7766\n",
      "Epoch 788: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7789 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7787 - lr: 1.0000e-07\n",
      "Epoch 789/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2507 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2507 - mape: 7.7746\n",
      "Epoch 789: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7784 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-07\n",
      "Epoch 790/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2517 - mse: 0.2412 - rmse: 0.4911 - mae: 0.2517 - mape: 7.7804\n",
      "Epoch 790: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 791/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2517 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2517 - mape: 7.8021\n",
      "Epoch 791: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 792/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2515 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2515 - mape: 7.7936\n",
      "Epoch 792: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 793/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2507 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2507 - mape: 7.7696\n",
      "Epoch 793: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 794/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2489 - mse: 0.2354 - rmse: 0.4852 - mae: 0.2489 - mape: 7.6993\n",
      "Epoch 794: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 795/2000\n",
      "279/318 [=========================>....] - ETA: 0s - loss: 0.2522 - mse: 0.2416 - rmse: 0.4915 - mae: 0.2522 - mape: 7.8026\n",
      "Epoch 795: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 796/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2511 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2511 - mape: 7.7763\n",
      "Epoch 796: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 797/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2495 - mse: 0.2371 - rmse: 0.4869 - mae: 0.2495 - mape: 7.7447\n",
      "Epoch 797: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 798/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2525 - mse: 0.2409 - rmse: 0.4908 - mae: 0.2525 - mape: 7.8073\n",
      "Epoch 798: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 799/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2513 - mse: 0.2407 - rmse: 0.4906 - mae: 0.2513 - mape: 7.7969\n",
      "Epoch 799: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 800/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2523 - mse: 0.2425 - rmse: 0.4925 - mae: 0.2523 - mape: 7.7959\n",
      "Epoch 800: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 801/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2496 - mse: 0.2364 - rmse: 0.4862 - mae: 0.2496 - mape: 7.7638\n",
      "Epoch 801: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 802/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2505 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2505 - mape: 7.7693\n",
      "Epoch 802: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 803/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2510 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2510 - mape: 7.7729\n",
      "Epoch 803: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 804/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2513 - mse: 0.2403 - rmse: 0.4902 - mae: 0.2513 - mape: 7.7907\n",
      "Epoch 804: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 805/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2503 - mse: 0.2374 - rmse: 0.4873 - mae: 0.2503 - mape: 7.7648\n",
      "Epoch 805: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 806/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2510 - mse: 0.2399 - rmse: 0.4898 - mae: 0.2510 - mape: 7.7794\n",
      "Epoch 806: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 807/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2511 - mse: 0.2402 - rmse: 0.4901 - mae: 0.2511 - mape: 7.7822\n",
      "Epoch 807: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 808/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2517 - mse: 0.2408 - rmse: 0.4907 - mae: 0.2517 - mape: 7.7990\n",
      "Epoch 808: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 809/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785\n",
      "Epoch 809: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 810/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2517 - mse: 0.2414 - rmse: 0.4913 - mae: 0.2517 - mape: 7.8055\n",
      "Epoch 810: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 811/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2507 - mse: 0.2378 - rmse: 0.4876 - mae: 0.2507 - mape: 7.7713\n",
      "Epoch 811: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 812/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2505 - mse: 0.2388 - rmse: 0.4887 - mae: 0.2505 - mape: 7.7491\n",
      "Epoch 812: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 813/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2518 - mse: 0.2416 - rmse: 0.4915 - mae: 0.2518 - mape: 7.8072\n",
      "Epoch 813: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 814/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2506 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2506 - mape: 7.7689\n",
      "Epoch 814: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 815/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2500 - mse: 0.2388 - rmse: 0.4887 - mae: 0.2500 - mape: 7.7401\n",
      "Epoch 815: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 816/2000\n",
      "263/318 [=======================>......] - ETA: 0s - loss: 0.2506 - mse: 0.2382 - rmse: 0.4880 - mae: 0.2506 - mape: 7.7554\n",
      "Epoch 816: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 817/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2514 - mse: 0.2401 - rmse: 0.4900 - mae: 0.2514 - mape: 7.7860\n",
      "Epoch 817: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 818/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2508 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2508 - mape: 7.7741\n",
      "Epoch 818: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 819/2000\n",
      "282/318 [=========================>....] - ETA: 0s - loss: 0.2494 - mse: 0.2351 - rmse: 0.4849 - mae: 0.2494 - mape: 7.7127\n",
      "Epoch 819: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 820/2000\n",
      "262/318 [=======================>......] - ETA: 0s - loss: 0.2515 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2515 - mape: 7.7622\n",
      "Epoch 820: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 821/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7739\n",
      "Epoch 821: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 822/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2511 - mse: 0.2399 - rmse: 0.4898 - mae: 0.2511 - mape: 7.7850\n",
      "Epoch 822: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 823/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2508 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2508 - mape: 7.7721\n",
      "Epoch 823: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 824/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2507 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2507 - mape: 7.7743\n",
      "Epoch 824: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 825/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2507 - mse: 0.2387 - rmse: 0.4886 - mae: 0.2507 - mape: 7.7739\n",
      "Epoch 825: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 826/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2502 - mse: 0.2383 - rmse: 0.4881 - mae: 0.2502 - mape: 7.7647\n",
      "Epoch 826: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 827/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2511 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2511 - mape: 7.7736\n",
      "Epoch 827: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 828/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2515 - mse: 0.2405 - rmse: 0.4904 - mae: 0.2515 - mape: 7.7940\n",
      "Epoch 828: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 829/2000\n",
      "267/318 [========================>.....] - ETA: 0s - loss: 0.2485 - mse: 0.2378 - rmse: 0.4877 - mae: 0.2485 - mape: 7.6827\n",
      "Epoch 829: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-08\n",
      "Epoch 830/2000\n",
      "277/318 [=========================>....] - ETA: 0s - loss: 0.2516 - mse: 0.2421 - rmse: 0.4920 - mae: 0.2516 - mape: 7.8056\n",
      "Epoch 830: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 831/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2484 - mse: 0.2354 - rmse: 0.4851 - mae: 0.2484 - mape: 7.7020\n",
      "Epoch 831: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 832/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2505 - mse: 0.2373 - rmse: 0.4871 - mae: 0.2505 - mape: 7.7724\n",
      "Epoch 832: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 833/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2515 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2515 - mape: 7.7945\n",
      "Epoch 833: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 834/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2542 - mse: 0.2443 - rmse: 0.4943 - mae: 0.2542 - mape: 7.8606\n",
      "Epoch 834: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 835/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2498 - mse: 0.2383 - rmse: 0.4882 - mae: 0.2498 - mape: 7.7717\n",
      "Epoch 835: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 836/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2515 - mse: 0.2405 - rmse: 0.4904 - mae: 0.2515 - mape: 7.7901\n",
      "Epoch 836: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 837/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2512 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2512 - mape: 7.7953\n",
      "Epoch 837: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 838/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2510 - mse: 0.2397 - rmse: 0.4896 - mae: 0.2510 - mape: 7.7841\n",
      "Epoch 838: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 839/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2514 - mse: 0.2396 - rmse: 0.4895 - mae: 0.2514 - mape: 7.7974\n",
      "Epoch 839: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 840/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2519 - mse: 0.2403 - rmse: 0.4903 - mae: 0.2519 - mape: 7.8207\n",
      "Epoch 840: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 841/2000\n",
      "283/318 [=========================>....] - ETA: 0s - loss: 0.2506 - mse: 0.2379 - rmse: 0.4877 - mae: 0.2506 - mape: 7.7647\n",
      "Epoch 841: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 842/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2511 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2511 - mape: 7.7817\n",
      "Epoch 842: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 843/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2503 - mse: 0.2387 - rmse: 0.4885 - mae: 0.2503 - mape: 7.7617\n",
      "Epoch 843: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 844/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2511 - mse: 0.2400 - rmse: 0.4899 - mae: 0.2511 - mape: 7.7839\n",
      "Epoch 844: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 845/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2500 - mse: 0.2384 - rmse: 0.4882 - mae: 0.2500 - mape: 7.7516\n",
      "Epoch 845: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 846/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2501 - mse: 0.2385 - rmse: 0.4884 - mae: 0.2501 - mape: 7.7577\n",
      "Epoch 846: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 847/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2508 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2508 - mape: 7.7849\n",
      "Epoch 847: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 848/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2507 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2507 - mape: 7.7692\n",
      "Epoch 848: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 849/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2511 - mse: 0.2399 - rmse: 0.4898 - mae: 0.2511 - mape: 7.7984\n",
      "Epoch 849: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 850/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2498 - mse: 0.2365 - rmse: 0.4863 - mae: 0.2498 - mape: 7.7426\n",
      "Epoch 850: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 851/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2515 - mse: 0.2405 - rmse: 0.4904 - mae: 0.2515 - mape: 7.7901\n",
      "Epoch 851: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 852/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2509 - mse: 0.2392 - rmse: 0.4891 - mae: 0.2509 - mape: 7.7765\n",
      "Epoch 852: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 853/2000\n",
      "278/318 [=========================>....] - ETA: 0s - loss: 0.2519 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2519 - mape: 7.8084\n",
      "Epoch 853: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 854/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2508 - mse: 0.2379 - rmse: 0.4878 - mae: 0.2508 - mape: 7.7881\n",
      "Epoch 854: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 855/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2508 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2508 - mape: 7.7754\n",
      "Epoch 855: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 856/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2503 - mse: 0.2385 - rmse: 0.4883 - mae: 0.2503 - mape: 7.7773\n",
      "Epoch 856: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 857/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785\n",
      "Epoch 857: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 858/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785\n",
      "Epoch 858: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 859/2000\n",
      "269/318 [========================>.....] - ETA: 0s - loss: 0.2496 - mse: 0.2378 - rmse: 0.4877 - mae: 0.2496 - mape: 7.7283\n",
      "Epoch 859: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 860/2000\n",
      "262/318 [=======================>......] - ETA: 0s - loss: 0.2509 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2509 - mape: 7.7794\n",
      "Epoch 860: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 861/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2505 - mse: 0.2391 - rmse: 0.4890 - mae: 0.2505 - mape: 7.7704\n",
      "Epoch 861: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 862/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2508 - mse: 0.2390 - rmse: 0.4889 - mae: 0.2508 - mape: 7.7735\n",
      "Epoch 862: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 863/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2539 - mse: 0.2457 - rmse: 0.4957 - mae: 0.2539 - mape: 7.8619\n",
      "Epoch 863: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 864/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785\n",
      "Epoch 864: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 865/2000\n",
      "280/318 [=========================>....] - ETA: 0s - loss: 0.2490 - mse: 0.2353 - rmse: 0.4850 - mae: 0.2490 - mape: 7.7220\n",
      "Epoch 865: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 866/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2501 - mse: 0.2366 - rmse: 0.4864 - mae: 0.2501 - mape: 7.7464\n",
      "Epoch 866: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 867/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2493 - mse: 0.2355 - rmse: 0.4853 - mae: 0.2493 - mape: 7.7309\n",
      "Epoch 867: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 868/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2522 - mse: 0.2420 - rmse: 0.4919 - mae: 0.2522 - mape: 7.8260\n",
      "Epoch 868: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 869/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2512 - mse: 0.2399 - rmse: 0.4898 - mae: 0.2512 - mape: 7.7863\n",
      "Epoch 869: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-09\n",
      "Epoch 870/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2513 - mse: 0.2398 - rmse: 0.4897 - mae: 0.2513 - mape: 7.7842\n",
      "Epoch 870: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 871/2000\n",
      "276/318 [=========================>....] - ETA: 0s - loss: 0.2506 - mse: 0.2385 - rmse: 0.4884 - mae: 0.2506 - mape: 7.7719\n",
      "Epoch 871: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 872/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2506 - mse: 0.2379 - rmse: 0.4877 - mae: 0.2506 - mape: 7.7686\n",
      "Epoch 872: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 873/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.2517 - mse: 0.2409 - rmse: 0.4909 - mae: 0.2517 - mape: 7.7975\n",
      "Epoch 873: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 874/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785\n",
      "Epoch 874: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 875/2000\n",
      "281/318 [=========================>....] - ETA: 0s - loss: 0.2500 - mse: 0.2387 - rmse: 0.4885 - mae: 0.2500 - mape: 7.7554\n",
      "Epoch 875: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 876/2000\n",
      "284/318 [=========================>....] - ETA: 0s - loss: 0.2526 - mse: 0.2422 - rmse: 0.4921 - mae: 0.2526 - mape: 7.8308\n",
      "Epoch 876: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 877/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2519 - mse: 0.2410 - rmse: 0.4909 - mae: 0.2519 - mape: 7.8213\n",
      "Epoch 877: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 878/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2521 - mse: 0.2416 - rmse: 0.4915 - mae: 0.2521 - mape: 7.8053\n",
      "Epoch 878: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 879/2000\n",
      "285/318 [=========================>....] - ETA: 0s - loss: 0.2495 - mse: 0.2364 - rmse: 0.4862 - mae: 0.2495 - mape: 7.7379\n",
      "Epoch 879: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 880/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2508 - mse: 0.2395 - rmse: 0.4893 - mae: 0.2508 - mape: 7.7778\n",
      "Epoch 880: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 881/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7774\n",
      "Epoch 881: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 882/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2511 - mse: 0.2391 - rmse: 0.4890 - mae: 0.2511 - mape: 7.7838\n",
      "Epoch 882: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 883/2000\n",
      "275/318 [========================>.....] - ETA: 0s - loss: 0.2495 - mse: 0.2382 - rmse: 0.4880 - mae: 0.2495 - mape: 7.7588\n",
      "Epoch 883: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 884/2000\n",
      "274/318 [========================>.....] - ETA: 0s - loss: 0.2535 - mse: 0.2423 - rmse: 0.4922 - mae: 0.2535 - mape: 7.8399\n",
      "Epoch 884: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 1s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 885/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2514 - mse: 0.2405 - rmse: 0.4904 - mae: 0.2514 - mape: 7.7867\n",
      "Epoch 885: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 886/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2518 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2518 - mape: 7.7899\n",
      "Epoch 886: val_loss did not improve from 0.25095\n",
      "318/318 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.2395 - rmse: 0.4894 - mae: 0.2509 - mape: 7.7785 - val_loss: 0.2509 - val_mse: 0.2395 - val_rmse: 0.4894 - val_mae: 0.2509 - val_mape: 7.7785 - lr: 1.0000e-10\n",
      "Epoch 886: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdad8120100>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 2000, batch_size = 64, validation_data = (X_val, y_val), callbacks=[checkpoint_callback, es_callback, tf.keras.callbacks.ReduceLROnPlateau(patience=40)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20221123-225504\n"
     ]
    }
   ],
   "source": [
    "print(date_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models.load_model(f'model_weights/{date_actual}_mlp_seg_best_weights.hdf5')\n",
    "#best_model = models.load_model(f'model_weights/MLPRegressor_Seg_V3_1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 521us/step - loss: 0.2611 - mse: 0.1861 - rmse: 0.4314 - mae: 0.2611 - mape: 8.8584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26108360290527344,\n",
       " 0.18613235652446747,\n",
       " 0.4314305782318115,\n",
       " 0.26108360290527344,\n",
       " 8.858388900756836]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 533us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.5236201277295904\n",
      "mse:  0.18613228803747162\n",
      "rmse:  0.43143051356791123\n",
      "mae:  0.2610836343129526\n",
      "mape:  0.08858387429836301\n",
      "Error estandar:  0.4313682287921825\n"
     ]
    }
   ],
   "source": [
    "print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "print(\"mse: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"rmse: \", mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"mae: \", mean_absolute_error(y_test, y_pred))\n",
    "print(\"mape: \", mean_absolute_percentage_error(y_test, y_pred))\n",
    "print(\"Error estandar: \", stde(y_test.squeeze(),\n",
    "      y_pred.squeeze(), ddof=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x640 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residuals = y_test - y_pred\n",
    "residuals_std = residuals/residuals.std()\n",
    "\n",
    "y_real_stage = np.array([i[0] for i in y_test])\n",
    "residual_stage = np.array([i[0] for i in residuals])\n",
    "\n",
    "#y_real_discharge = np.array([i[-1] for i in y_test])\n",
    "#residual_discharge = np.array([i[-1] for i in residuals])\n",
    "\n",
    "\n",
    "figure, ax = plt.subplots(ncols=2, figsize=(20, 8), dpi=80)\n",
    "\n",
    "ax[0].scatter(y_real_stage, residual_stage / residual_stage.std(), label=\"stage residuals\")\n",
    "#ax[1].scatter(y_real_discharge, residual_discharge / residual_discharge.std(), label=\"discharge residuals\")\n",
    "ax[0].axhline(y=0.0, color='r', linestyle='-')\n",
    "ax[1].axhline(y=0.0, color='r', linestyle='-')\n",
    "\n",
    "ax[0].set_title(\"Stage residuals\")\n",
    "ax[1].set_title(\"Discharge residuals\")\n",
    "\n",
    "ax[1].set_xlabel(\"Fitted values\")\n",
    "ax[0].set_xlabel(\"Fitted values\")\n",
    "ax[1].set_ylabel(\"Standarized residuals\")\n",
    "ax[0].set_ylabel(\"Standarized residuals\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import normal_ad\n",
    "\n",
    "#figure = sm.qqplot(residual_stage / residual_stage.std(), line ='45', label='stage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = sm.qqplot(residual_stage / residual_stage.std(), line='45', label='discharge')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(residual_stage / residual_stage.std(), density=True, bins = 60)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Residual stage');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtmUlEQVR4nO3dfVxVVb7H8e8B9YApaCKoDImW+ZCJCkqoTXlDKY07TlPj7UlyjEYzM0lTKjVrEitFbophTmo109WZHpxS00FMe5BGQ2nSlLIiHRPUNA5igcK+f3Q7c0+i8nBgw+Lzfr3O6+VZe+29f0uS823tdfZ2WJZlCQAAwBA+dhcAAADgTYQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjNLO7gPpWUVGhb775Rq1bt5bD4bC7HAAAUAWWZam4uFidOnWSj8/552aaXLj55ptvFBYWZncZAACgBg4ePKhf/OIX5+3T5MJN69atJf34lxMQEGBzNQAAoCpcLpfCwsLcn+Pn0+TCzU+XogICAgg3AAA0MlVZUsKCYgAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRmtldAADg/MJnrDvntvx5I+uxEqBxYOYGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGsTXcvPvuu4qPj1enTp3kcDi0Zs2aC+6zZcsW9e/fX06nU5dddplWrlxZ53UCAIDGw9ZwU1JSooiICKWnp1ep/1dffaWRI0dq6NChys3N1QMPPKC7775bGzdurONKAQBAY9HMzpPfcMMNuuGGG6rcPyMjQ126dNGCBQskST179tT777+vhQsXKi4urq7KBAAAjUijWnOTnZ2t2NhYj7a4uDhlZ2efc5/S0lK5XC6PFwAAMFejCjcFBQUKCQnxaAsJCZHL5dL3339f6T4pKSkKDAx0v8LCwuqjVAAAYJNGFW5qIjk5WUVFRe7XwYMH7S4JAADUIVvX3FRXhw4dVFhY6NFWWFiogIAA+fv7V7qP0+mU0+msj/IAAEAD0KhmbmJiYpSVleXRlpmZqZiYGJsqAgAADY2t4ebkyZPKzc1Vbm6upB+/6p2bm6sDBw5I+vGS0pgxY9z9x48fry+//FIPPfSQ9u3bpyVLlugvf/mLpkyZYkf5AACgAbI13Hz00Ufq16+f+vXrJ0lKSkpSv379NGvWLEnS4cOH3UFHkrp06aJ169YpMzNTERERWrBggf74xz/yNXAAAODmsCzLsruI+uRyuRQYGKiioiIFBATYXQ4AXFD4jHXn3JY/b2Q9VgLYpzqf341qzQ0AAMCFEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADCK7eEmPT1d4eHh8vPzU3R0tLZv337e/mlpaerevbv8/f0VFhamKVOm6IcffqinagEAQENna7hZvXq1kpKSNHv2bO3cuVMRERGKi4vTkSNHKu3/yiuvaMaMGZo9e7b27t2rF154QatXr9bDDz9cz5UDAICGytZwk5qaqsTERI0dO1a9evVSRkaGWrZsqeXLl1faf9u2bRo8eLBuu+02hYeHa/jw4br11lsvONsDAACaDtvCTVlZmXJychQbG/vvYnx8FBsbq+zs7Er3GTRokHJyctxh5ssvv9T69es1YsSIc56ntLRULpfL4wUAAMzVzK4THzt2TOXl5QoJCfFoDwkJ0b59+yrd57bbbtOxY8c0ZMgQWZalM2fOaPz48ee9LJWSkqI5c+Z4tXYAANBw2b6guDq2bNmiuXPnasmSJdq5c6def/11rVu3Tk888cQ590lOTlZRUZH7dfDgwXqsGAAA1DfbZm6CgoLk6+urwsJCj/bCwkJ16NCh0n1mzpypO++8U3fffbck6corr1RJSYnuuecePfLII/LxOTurOZ1OOZ1O7w8AAAA0SLbN3LRo0UKRkZHKyspyt1VUVCgrK0sxMTGV7nPq1KmzAoyvr68kybKsuisWAAA0GrbN3EhSUlKSEhISFBUVpYEDByotLU0lJSUaO3asJGnMmDEKDQ1VSkqKJCk+Pl6pqanq16+foqOjtX//fs2cOVPx8fHukAMAAJo2W8PN6NGjdfToUc2aNUsFBQXq27evNmzY4F5kfODAAY+ZmkcffVQOh0OPPvqoDh06pPbt2ys+Pl5PPvmkXUMAAAANjMNqYtdzXC6XAgMDVVRUpICAALvLAYALCp+x7pzb8ueNrMdKAPtU5/O7UX1bCgAA4EIINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxSo3DTtWtXffvtt2e1f/fdd+ratWutiwIAAKipGoWb/Px8lZeXn9VeWlqqQ4cO1booAACAmmpWnc5vvvmm+88bN25UYGCg+315ebmysrIUHh7uteIAAACqq1rhZtSoUZIkh8OhhIQEj23NmzdXeHi4FixY4LXiAAAAqqta4aaiokKS1KVLF+3YsUNBQUF1UhQAAEBNVSvc/OSrr77ydh0AAABeUaNwI0lZWVnKysrSkSNH3DM6P1m+fHmtCwMAXFj4jHXn3Z4/b2Q9VQI0HDX6ttScOXM0fPhwZWVl6dixYzpx4oTHqzrS09MVHh4uPz8/RUdHa/v27eft/91332nixInq2LGjnE6nLr/8cq1fv74mwwAAAAaq0cxNRkaGVq5cqTvvvLNWJ1+9erWSkpKUkZGh6OhopaWlKS4uTnl5eQoODj6rf1lZmYYNG6bg4GC9+uqrCg0N1ddff602bdrUqg4AAGCOGoWbsrIyDRo0qNYnT01NVWJiosaOHSvpx9C0bt06LV++XDNmzDir//Lly3X8+HFt27ZNzZs3lyS+eg4AADzU6LLU3XffrVdeeaVWJy4rK1NOTo5iY2P/XYyPj2JjY5WdnV3pPm+++aZiYmI0ceJEhYSEqHfv3po7d26lNxT8SWlpqVwul8cLAACYq0YzNz/88IOef/55bdq0SX369HHPovwkNTX1gsc4duyYysvLFRIS4tEeEhKiffv2VbrPl19+qc2bN+v222/X+vXrtX//ft177706ffq0Zs+eXek+KSkpmjNnThVHBgAAGrsahZt//vOf6tu3ryRp9+7dHtscDketizqXiooKBQcH6/nnn5evr68iIyN16NAhPfPMM+cMN8nJyUpKSnK/d7lcCgsLq7MaAQCAvWoUbt55551anzgoKEi+vr4qLCz0aC8sLFSHDh0q3adjx45q3ry5fH193W09e/ZUQUGBysrK1KJFi7P2cTqdcjqdta4XAAA0DjVac+MNLVq0UGRkpLKystxtFRUVysrKUkxMTKX7DB48WPv37/e4r85nn32mjh07VhpsAABA01OjmZuhQ4ee9/LT5s2bq3ScpKQkJSQkKCoqSgMHDlRaWppKSkrc354aM2aMQkNDlZKSIkmaMGGCFi9erMmTJ2vSpEn6/PPPNXfuXN1///01GQYAADBQjcLNT+ttfnL69Gnl5uZq9+7dZz1Q83xGjx6to0ePatasWSooKFDfvn21YcMG9yLjAwcOyMfn35NLYWFh2rhxo6ZMmaI+ffooNDRUkydP1vTp02syDAAAYCCHZVmWtw722GOP6eTJk5o/f763Dul1LpdLgYGBKioqUkBAgN3lAMAFXegRC+fD4xdgiup8fnt1zc0dd9zBc6UAAICtvBpusrOz5efn581DAgAAVEuN1tzcdNNNHu8ty9Lhw4f10UcfaebMmV4pDAAAoCZqFG4CAwM93vv4+Kh79+56/PHHNXz4cK8UBgAAUBM1CjcrVqzwdh0AAABeUaNw85OcnBzt3btXknTFFVeoX79+XikKAACgpmoUbo4cOaL/+q//0pYtW9SmTRtJ0nfffaehQ4dq1apVat++vTdrBAAAqLIafVtq0qRJKi4u1p49e3T8+HEdP35cu3fvlsvl4m7BAADAVjWaudmwYYM2bdqknj17utt69eql9PR0FhQDAABb1WjmpqKiQs2bNz+rvXnz5h4PtQQAAKhvNQo3//Ef/6HJkyfrm2++cbcdOnRIU6ZM0XXXXee14gAAAKqrRuFm8eLFcrlcCg8P16WXXqpLL71UXbp0kcvl0qJFi7xdIwAAQJXVaM1NWFiYdu7cqU2bNmnfvn2SpJ49eyo2NtarxQEAAFRXtWZuNm/erF69esnlcsnhcGjYsGGaNGmSJk2apAEDBuiKK67Qe++9V1e1AgAAXFC1wk1aWpoSExMrfdR4YGCgfv/73ys1NdVrxQEAAFRXtcLNxx9/rOuvv/6c24cPH66cnJxaFwUAAFBT1Qo3hYWFlX4F/CfNmjXT0aNHa10UAABATVUr3ISGhmr37t3n3P7Pf/5THTt2rHVRAAAANVWtcDNixAjNnDlTP/zww1nbvv/+e82ePVs33nij14oDAACormp9FfzRRx/V66+/rssvv1z33XefunfvLknat2+f0tPTVV5erkceeaROCgUAAKiKaoWbkJAQbdu2TRMmTFBycrIsy5IkORwOxcXFKT09XSEhIXVSKAAAQFVU+yZ+nTt31vr163XixAnt379flmWpW7duatu2bV3UBwAAUC01ukOxJLVt21YDBgzwZi0AAAC1VqNnSwEAADRUhBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjFLjp4IDALwjfMY6u0sAjMLMDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGCUBhFu0tPTFR4eLj8/P0VHR2v79u1V2m/VqlVyOBwaNWpU3RYIAAAaDdvDzerVq5WUlKTZs2dr586dioiIUFxcnI4cOXLe/fLz8zV16lRdffXV9VQpAABoDGwPN6mpqUpMTNTYsWPVq1cvZWRkqGXLllq+fPk59ykvL9ftt9+uOXPmqGvXrvVYLQAAaOhsDTdlZWXKyclRbGysu83Hx0exsbHKzs4+536PP/64goODNW7cuAueo7S0VC6Xy+MFAADMZWu4OXbsmMrLyxUSEuLRHhISooKCgkr3ef/99/XCCy9o2bJlVTpHSkqKAgMD3a+wsLBa1w0AABquZnYXUB3FxcW68847tWzZMgUFBVVpn+TkZCUlJbnfu1wuAg6AJiN8xrpzbsufN7IeKwHqj63hJigoSL6+viosLPRoLywsVIcOHc7q/8UXXyg/P1/x8fHutoqKCklSs2bNlJeXp0svvdRjH6fTKafTWQfVAwCAhsjWy1ItWrRQZGSksrKy3G0VFRXKyspSTEzMWf179OihTz75RLm5ue7Xf/7nf2ro0KHKzc1lRgYAANh/WSopKUkJCQmKiorSwIEDlZaWppKSEo0dO1aSNGbMGIWGhiolJUV+fn7q3bu3x/5t2rSRpLPaAQBA02R7uBk9erSOHj2qWbNmqaCgQH379tWGDRvci4wPHDggHx/bv7EOAAAaCYdlWZbdRdQnl8ulwMBAFRUVKSAgwO5yAOC8i37rEguK0ZhU5/ObKREAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEZpEOEmPT1d4eHh8vPzU3R0tLZv337OvsuWLdPVV1+ttm3bqm3btoqNjT1vfwAA0LQ0s7uA1atXKykpSRkZGYqOjlZaWpri4uKUl5en4ODgs/pv2bJFt956qwYNGiQ/Pz899dRTGj58uPbs2aPQ0FAbRgAAjVP4jHXn3Z4/b2Q9VQJ4l8OyLMvOAqKjozVgwAAtXrxYklRRUaGwsDBNmjRJM2bMuOD+5eXlatu2rRYvXqwxY8ZcsL/L5VJgYKCKiooUEBBQ6/oBoLYuFDLsQrhBQ1Kdz29bL0uVlZUpJydHsbGx7jYfHx/FxsYqOzu7Ssc4deqUTp8+rYsvvrjS7aWlpXK5XB4vAABgLlvDzbFjx1ReXq6QkBCP9pCQEBUUFFTpGNOnT1enTp08AtL/l5KSosDAQPcrLCys1nUDAICGq0EsKK6pefPmadWqVXrjjTfk5+dXaZ/k5GQVFRW5XwcPHqznKgEAQH2ydUFxUFCQfH19VVhY6NFeWFioDh06nHff+fPna968edq0aZP69Olzzn5Op1NOp9Mr9QIAgIbP1pmbFi1aKDIyUllZWe62iooKZWVlKSYm5pz7Pf3003riiSe0YcMGRUVF1UepAACgkbD9q+BJSUlKSEhQVFSUBg4cqLS0NJWUlGjs2LGSpDFjxig0NFQpKSmSpKeeekqzZs3SK6+8ovDwcPfanFatWqlVq1a2jQMAADQMtoeb0aNH6+jRo5o1a5YKCgrUt29fbdiwwb3I+MCBA/Lx+fcE03PPPaeysjLdfPPNHseZPXu2HnvssfosHQAANEC23+emvnGfGwANDfe5AS6s0dznBgAAwNtsvywFAE1BQ52dAUzEzA0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCjc5wYAvID72AANBzM3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMwrelAACVOt83wPLnjazHSoDqYeYGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYJRmdhcAAEB1hM9Yd85t+fNG1mMlaKgINwDwf873oSnxwQk0FoQbAEC1EQTRkLHmBgAAGIVwAwAAjEK4AQAARmHNDQBU0YXWmQBoGJi5AQAARmHmBgDgddyLBnZi5gYAABiFcAMAAIzCZSkATQqLggHzEW4ANDqs52jcCJioa4QbAEbhgxMAa24AAIBRCDcAAMAoXJYCYAsuHwGoK8zcAAAAoxBuAACAUQg3AADAKKy5AQAY40JrubgPUtNAuAFQJ1gwjIaIG0A2DQ0i3KSnp+uZZ55RQUGBIiIitGjRIg0cOPCc/f/6179q5syZys/PV7du3fTUU09pxIgR9VgxAIkAA6Bhsj3crF69WklJScrIyFB0dLTS0tIUFxenvLw8BQcHn9V/27ZtuvXWW5WSkqIbb7xRr7zyikaNGqWdO3eqd+/eNowAMBfhBUBj5LAsy7KzgOjoaA0YMECLFy+WJFVUVCgsLEyTJk3SjBkzzuo/evRolZSUaO3ate62q666Sn379lVGRsYFz+dyuRQYGKiioiIFBAR4byBAA0VAAaqGy1INW3U+v22duSkrK1NOTo6Sk5PdbT4+PoqNjVV2dnal+2RnZyspKcmjLS4uTmvWrKm0f2lpqUpLS93vi4qKJP34l1QXes/eeM5tu+fE1clxa3vs2py3Ns5Xc23HW5ufQ13tC6Bhu9DnQm3+fdfV7+im5KefT1XmZGwNN8eOHVN5eblCQkI82kNCQrRv375K9ykoKKi0f0FBQaX9U1JSNGfOnLPaw8LCalh1zQWmNc5j15Xa1NwY9wXQsPE7unEoLi5WYGDgefvYvuamriUnJ3vM9FRUVOj48eNq166dHA5Hpfu4XC6FhYXp4MGDxl+6akpjlZrWeJvSWKWmNd6mNFapaY23KY1Vqt54LctScXGxOnXqdMHj2hpugoKC5Ovrq8LCQo/2wsJCdejQodJ9OnToUK3+TqdTTqfTo61NmzZVqi8gIKBJ/MclNa2xSk1rvE1prFLTGm9TGqvUtMbblMYqVX28F5qx+Ymtdyhu0aKFIiMjlZWV5W6rqKhQVlaWYmJiKt0nJibGo78kZWZmnrM/AABoWmy/LJWUlKSEhARFRUVp4MCBSktLU0lJicaOHStJGjNmjEJDQ5WSkiJJmjx5sq655hotWLBAI0eO1KpVq/TRRx/p+eeft3MYAACggbA93IwePVpHjx7VrFmzVFBQoL59+2rDhg3uRcMHDhyQj8+/J5gGDRqkV155RY8++qgefvhhdevWTWvWrPHqPW6cTqdmz5591uUsEzWlsUpNa7xNaaxS0xpvUxqr1LTG25TGKtXdeG2/zw0AAIA38VRwAABgFMINAAAwCuEGAAAYhXADAACMQrj5mSeffFKDBg1Sy5Ytz3mzvwMHDmjkyJFq2bKlgoODNW3aNJ05c6Z+C60jn332mX71q18pKChIAQEBGjJkiN555x27y6oz69atU3R0tPz9/dW2bVuNGjXK7pLqXGlpqfr27SuHw6Hc3Fy7y/G6/Px8jRs3Tl26dJG/v78uvfRSzZ49W2VlZXaX5jXp6ekKDw+Xn5+foqOjtX37drtL8rqUlBQNGDBArVu3VnBwsEaNGqW8vDy7y6oX8+bNk8Ph0AMPPGB3KXXm0KFDuuOOO9SuXTv5+/vryiuv1EcffeS14xNufqasrEy33HKLJkyYUOn28vJyjRw5UmVlZdq2bZtefPFFrVy5UrNmzarnSuvGjTfeqDNnzmjz5s3KyclRRESEbrzxxnM+u6sxe+2113TnnXdq7Nix+vjjj/XBBx/otttus7usOvfQQw9V6fbljdW+fftUUVGhpUuXas+ePVq4cKEyMjL08MMP212aV6xevVpJSUmaPXu2du7cqYiICMXFxenIkSN2l+ZVW7du1cSJE/Xhhx8qMzNTp0+f1vDhw1VSUmJ3aXVqx44dWrp0qfr06WN3KXXmxIkTGjx4sJo3b663335bn376qRYsWKC2bdt67yQWKrVixQorMDDwrPb169dbPj4+VkFBgbvtueeeswICAqzS0tJ6rND7jh49akmy3n33XXeby+WyJFmZmZk2VuZ9p0+ftkJDQ60//vGPdpdSr9avX2/16NHD2rNnjyXJ2rVrl90l1Yunn37a6tKli91leMXAgQOtiRMnut+Xl5dbnTp1slJSUmysqu4dOXLEkmRt3brV7lLqTHFxsdWtWzcrMzPTuuaaa6zJkyfbXVKdmD59ujVkyJA6PQczN9WUnZ2tK6+80uPJ5HFxcXK5XNqzZ4+NldVeu3bt1L17d7300ksqKSnRmTNntHTpUgUHBysyMtLu8rxq586dOnTokHx8fNSvXz917NhRN9xwg3bv3m13aXWmsLBQiYmJevnll9WyZUu7y6lXRUVFuvjii+0uo9bKysqUk5Oj2NhYd5uPj49iY2OVnZ1tY2V1r6ioSJKM+Dmey8SJEzVy5EiPn6+J3nzzTUVFRemWW25RcHCw+vXrp2XLlnn1HISbaiooKPAINpLc7xv7pRuHw6FNmzZp165dat26tfz8/JSamqoNGzZ4d7qwAfjyyy8lSY899pgeffRRrV27Vm3bttW1116r48eP21yd91mWpbvuukvjx49XVFSU3eXUq/3792vRokX6/e9/b3cptXbs2DGVl5dX+juosf/+OZ+Kigo98MADGjx4sFfvRt+QrFq1Sjt37nQ/ashkX375pZ577jl169ZNGzdu1IQJE3T//ffrxRdf9No5mkS4mTFjhhwOx3lf+/bts7vMOlPV8VuWpYkTJyo4OFjvvfeetm/frlGjRik+Pl6HDx+2exhVUtWxVlRUSJIeeeQR/eY3v1FkZKRWrFghh8Ohv/71rzaPouqqOt5FixapuLhYycnJdpdcYzX5d3zo0CFdf/31uuWWW5SYmGhT5aitiRMnavfu3Vq1apXdpdSJgwcPavLkyfrzn/8sPz8/u8upcxUVFerfv7/mzp2rfv366Z577lFiYqIyMjK8dg7bny1VHx588EHddddd5+3TtWvXKh2rQ4cOZ30zobCw0L2tIarq+Ddv3qy1a9fqxIkT7kfPL1myRJmZmXrxxRc1Y8aMeqi2dqo61p/CWq9evdztTqdTXbt21YEDB+qyRK+qzs82Ozv7rOe3REVF6fbbb/fq/zHVler+O/7mm280dOhQDRo0yJgH6wYFBcnX19f9O+cnhYWFDfb3T23dd999Wrt2rd5991394he/sLucOpGTk6MjR46of//+7rby8nK9++67Wrx4sUpLS+Xr62tjhd7VsWNHj9+9ktSzZ0+99tprXjtHkwg37du3V/v27b1yrJiYGD355JM6cuSIgoODJUmZmZkKCAg464fVUFR1/KdOnZIkjweV/vT+p5mOhq6qY42MjJTT6VReXp6GDBkiSTp9+rTy8/PVuXPnui7Ta6o63meffVZ/+MMf3O+/+eYbxcXFafXq1YqOjq7LEr2mOv+ODx06pKFDh7pn5H7+33Rj1aJFC0VGRiorK8t924KKigplZWXpvvvus7c4L7MsS5MmTdIbb7yhLVu2qEuXLnaXVGeuu+46ffLJJx5tY8eOVY8ePTR9+nSjgo0kDR48+Kyv9X/22Wfe/d1bp8uVG6Gvv/7a2rVrlzVnzhyrVatW1q5du6xdu3ZZxcXFlmVZ1pkzZ6zevXtbw4cPt3Jzc60NGzZY7du3t5KTk22uvPaOHj1qtWvXzrrpppus3NxcKy8vz5o6darVvHlzKzc31+7yvG7y5MlWaGiotXHjRmvfvn3WuHHjrODgYOv48eN2l1bnvvrqK2O/LfWvf/3Luuyyy6zrrrvO+te//mUdPnzY/TLBqlWrLKfTaa1cudL69NNPrXvuucdq06aNxzc4TTBhwgQrMDDQ2rJli8fP8NSpU3aXVi9M/rbU9u3brWbNmllPPvmk9fnnn1t//vOfrZYtW1p/+tOfvHYOws3PJCQkWJLOer3zzjvuPvn5+dYNN9xg+fv7W0FBQdaDDz5onT592r6ivWjHjh3W8OHDrYsvvthq3bq1ddVVV1nr16+3u6w6UVZWZj344INWcHCw1bp1ays2NtbavXu33WXVC5PDzYoVKyr9N2zS/8stWrTIuuSSS6wWLVpYAwcOtD788EO7S/K6c/0MV6xYYXdp9cLkcGNZlvXWW29ZvXv3tpxOp9WjRw/r+eef9+rxHZZlWd6bBwIAALCXGReiAQAA/g/hBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINgGrJz8+Xw+FQbm7uOfts2bJFDodD3333nVfP7XA4tGbNmmrtc+211+qBBx5wvw8PD1daWlqta6mrMQKoPcINYJi77rpLDodDDodDzZs3V5cuXfTQQw/phx9+8Mrxw8LCdPjwYfXu3dsrx6tvO3bs0D333GN3GQDqUJN4KjjQ1Fx//fVasWKFTp8+rZycHCUkJMjhcOipp56q9bF9fX3VoUMHL1Rpj6o+WdwuZWVlatGihd1lAI0aMzeAgZxOpzp06KCwsDCNGjVKsbGxyszMdG+vqKhQSkqKunTpIn9/f0VEROjVV191bz9x4oRuv/12tW/fXv7+/urWrZtWrFghqfLLUuvXr9fll18uf39/DR06VPn5+R71PPbYY+rbt69HW1pamsLDw93vd+zYoWHDhikoKEiBgYG65pprtHPnzmqNu6SkRGPGjFGrVq3UsWNHLViw4Kw+//+ylGVZeuyxx3TJJZfI6XSqU6dOuv/++919S0tLNX36dIWFhcnpdOqyyy7TCy+84HG8nJwcRUVFqWXLlho0aJDy8vLc27744gv96le/UkhIiFq1aqUBAwZo06ZNZ9XzxBNPaMyYMQoICHDPKi1btkxhYWFq2bKlfv3rXys1NVVt2rTx2Pdvf/ub+vfvLz8/P3Xt2lVz5szRmTNnqvV3BpiIcAMYbvfu3dq2bZvHbEBKSopeeuklZWRkaM+ePZoyZYruuOMObd26VZI0c+ZMffrpp3r77be1d+9ePffccwoKCqr0+AcPHtRNN92k+Ph45ebm6u6779aMGTOqXWdxcbESEhL0/vvv68MPP1S3bt00YsQIFRcXV/kY06ZN09atW/W3v/1Nf//737Vly5bzBqTXXntNCxcu1NKlS/X5559rzZo1uvLKK93bx4wZo//5n//Rs88+q71792rp0qVq1aqVxzEeeeQRLViwQB999JGaNWum3/3ud+5tJ0+e1IgRI5SVlaVdu3bp+uuvV3x8vA4cOOBxjPnz5ysiIkK7du3SzJkz9cEHH2j8+PGaPHmycnNzNWzYMD355JMe+7z33nsaM2aMJk+erE8//VRLly7VypUrz+oHNElefcY4ANslJCRYvr6+1kUXXWQ5nU5LkuXj42O9+uqrlmVZ1g8//GC1bNnS2rZtm8d+48aNs2699VbLsiwrPj7eGjt2bKXH/+qrryxJ1q5duyzLsqzk5GSrV69eHn2mT59uSbJOnDhhWZZlzZ4924qIiPDos3DhQqtz587nHEd5ebnVunVr66233nK3SbLeeOONSvsXFxdbLVq0sP7yl7+427799lvL39/fmjx5srutc+fO1sKFCy3LsqwFCxZYl19+uVVWVnbW8fLy8ixJVmZmZqXne+eddyxJ1qZNm9xt69atsyRZ33///TnHdcUVV1iLFi3yqGfUqFEefUaPHm2NHDnSo+3222+3AgMD3e+vu+46a+7cuR59Xn75Zatjx47nPDfQVDBzAxho6NChys3N1T/+8Q8lJCRo7Nix+s1vfiNJ2r9/v06dOqVhw4apVatW7tdLL72kL774QpI0YcIErVq1Sn379tVDDz2kbdu2nfNce/fuVXR0tEdbTExMtWsuLCxUYmKiunXrpsDAQAUEBOjkyZNnzXKcyxdffKGysjKPWi6++GJ17979nPvccsst+v7779W1a1clJibqjTfecF/Wyc3Nla+vr6655prznrdPnz7uP3fs2FGSdOTIEUk/ztxMnTpVPXv2VJs2bdSqVSvt3bv3rDFFRUV5vM/Ly9PAgQM92n7+/uOPP9bjjz/u8TNMTEzU4cOHderUqfPWDJiOBcWAgS666CJddtllkqTly5crIiJCL7zwgsaNG6eTJ09KktatW6fQ0FCP/ZxOpyTphhtu0Ndff63169crMzNT1113nSZOnKj58+fXqB4fHx9ZluXRdvr0aY/3CQkJ+vbbb/Xf//3f6ty5s5xOp2JiYlRWVlajc1ZFWFiY8vLytGnTJmVmZuree+/VM888o61bt8rf379Kx2jevLn7zw6HQ9KPa5okaerUqcrMzNT8+fN12WWXyd/fXzfffPNZY7rooouqXfvJkyc1Z84c3XTTTWdt8/Pzq/bxAJMQbgDD+fj46OGHH1ZSUpJuu+029erVS06nUwcOHDjvrET79u2VkJCghIQEXX311Zo2bVql4aZnz5568803Pdo+/PDDs45VUFAgy7LcAeDn98n54IMPtGTJEo0YMULSj2t5jh07VuVxXnrppWrevLn+8Y9/6JJLLpH048Lozz777Lzj9Pf3V3x8vOLj4zVx4kT16NFDn3zyia688kpVVFRo69atio2NrXIdPx/TXXfdpV//+teSfgwkP19sXZnu3btrx44dHm0/f9+/f3/l5eW5QyyAfyPcAE3ALbfcomnTpik9PV1Tp07V1KlTNWXKFFVUVGjIkCEqKirSBx98oICAACUkJGjWrFmKjIzUFVdcodLSUq1du1Y9e/as9Njjx4/XggULNG3aNN19993KycnRypUrPfpce+21Onr0qJ5++mndfPPN2rBhg95++20FBAS4+3Tr1k0vv/yyoqKi5HK5NG3atCrPnkhSq1atNG7cOE2bNk3t2rVTcHCwHnnkEfn4nPvq+8qVK1VeXq7o6Gi1bNlSf/rTn+Tv76/OnTurXbt2SkhI0O9+9zs9++yzioiI0Ndff60jR47ot7/9bZVq6tatm15//XXFx8fL4XBo5syZ7lmd85k0aZJ++ctfKjU1VfHx8dq8ebPefvttdzCUpFmzZunGG2/UJZdcoptvvlk+Pj76+OOPtXv3bv3hD3+oUn2AqVhzAzQBzZo103333aenn35aJSUleuKJJzRz5kylpKSoZ8+euv7667Vu3Tp16dJFktSiRQslJyerT58++uUvfylfX1+tWrWq0mNfcskleu2117RmzRpFREQoIyNDc+fO9ejTs2dPLVmyROnp6YqIiND27ds1depUjz4vvPCCTpw4of79++vOO+/U/fffr+Dg4GqN85lnntHVV1+t+Ph4xcbGasiQIYqMjDxn/zZt2mjZsmUaPHiw+vTpo02bNumtt95Su3btJEnPPfecbr75Zt17773q0aOHEhMTVVJSUuV6UlNT1bZtWw0aNEjx8fGKi4tT//79L7jf4MGDlZGRodTUVEVERGjDhg2aMmWKx+WmuLg4rV27Vn//+981YMAAXXXVVVq4cKE6d+5c5foAUzmsn18IBwA0OImJidq3b5/ee+89u0sBGjwuSwFAAzR//nwNGzZMF110kd5++229+OKLWrJkid1lAY0CMzcA0AD99re/1ZYtW1RcXKyuXbtq0qRJGj9+vN1lAY0C4QYAABiFBcUAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFH+F1jtlhitPYsHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"plt.hist(residual_discharge / residual_discharge.std(), density=True, bins = 60)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Residual discharge');\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.0\n",
      "Hay evidencia de que los residuos no provienen de una distribución normal.\n"
     ]
    }
   ],
   "source": [
    "stat, pval = normal_ad(residual_stage / residual_stage.std())\n",
    "print(\"p-value:\", pval)\n",
    "\n",
    "if pval < 0.05:\n",
    "    print(\"Hay evidencia de que los residuos no provienen de una distribución normal.\")\n",
    "else:\n",
    "    print(\"No hay evidencia para rechazar la hipótesis de que los residuos vienen de una distribución normal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x640 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure, ax = plt.subplots(ncols=2, figsize=(20, 8), dpi=80)\n",
    "\n",
    "ax[0].plot(np.arange(len(y_test)), y_test, label=\"Stage real\")\n",
    "ax[0].plot(np.arange(len(y_test)), y_pred, label=\"Stage pred\")\n",
    "\n",
    "ax[0].set_title(\"Stage predictions\")\n",
    "ax[1].set_title(\"Discharge predictions\")\n",
    "\n",
    "ax[1].set_ylabel(\"Values\")\n",
    "ax[0].set_ylabel(\"Values\")\n",
    "ax[1].set_xlabel(\"Time\")\n",
    "ax[0].set_xlabel(\"Time\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79f576286c1276b480d6696ed40f6607e18214e4a2875a618cb5be817ff26007"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
