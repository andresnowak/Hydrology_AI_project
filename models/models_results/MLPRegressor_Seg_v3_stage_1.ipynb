{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-25 12:40:19.437220: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-25 12:40:19.676917: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-25 12:40:20.172658: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nkspartan/miniconda3/envs/tf-gpu/lib/\n",
      "2022-11-25 12:40:20.172768: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nkspartan/miniconda3/envs/tf-gpu/lib/\n",
      "2022-11-25 12:40:20.172776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/tmp/ipykernel_40052/891238804.py:15: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tools.eval_measures import stde\n",
    "\n",
    "import kerastuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-25 12:40:20.725222: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-25 12:40:20.734914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:20.739384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:20.739582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:21.188541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:21.188746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:21.188903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:21.189033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 110 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the etl info results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>remove_time_features</th>\n",
       "      <th>generic_features</th>\n",
       "      <th>remove_atypical_values</th>\n",
       "      <th>feature_combination</th>\n",
       "      <th>remove_feature_selection</th>\n",
       "      <th>remove_invalid_correlated_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   remove_time_features  generic_features  remove_atypical_values  \\\n",
       "0                 False              True                   False   \n",
       "\n",
       "   feature_combination remove_feature_selection  \\\n",
       "0                False                    Lasso   \n",
       "\n",
       "   remove_invalid_correlated_features  \n",
       "0                               False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info = pd.read_csv('../dataset_clean/options_csv_v1_etl.csv')\n",
    "df_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SensorTime</th>\n",
       "      <th>CaptureTime</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Agency</th>\n",
       "      <th>SiteNumber</th>\n",
       "      <th>TimeZone</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Discharge</th>\n",
       "      <th>...</th>\n",
       "      <th>WwRawLineMax</th>\n",
       "      <th>WwRawLineMean</th>\n",
       "      <th>WwRawLineSigma</th>\n",
       "      <th>WwCurveLineMin</th>\n",
       "      <th>WwCurveLineMax</th>\n",
       "      <th>WwCurveLineMean</th>\n",
       "      <th>WwCurveLineSigma</th>\n",
       "      <th>RiverArea</th>\n",
       "      <th>RiverWidth</th>\n",
       "      <th>FilenameLower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2012-06-09T13:09:07</td>\n",
       "      <td>StateLineWeir_20120609_Farrell_001.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48600.0</td>\n",
       "      <td>204.684959</td>\n",
       "      <td>statelineweir_20120609_farrell_001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2012-06-09T13:10:29</td>\n",
       "      <td>StateLineWeir_20120609_Farrell_002.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48672.0</td>\n",
       "      <td>205.898452</td>\n",
       "      <td>statelineweir_20120609_farrell_002.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-06-09 13:45:00</td>\n",
       "      <td>2012-06-09T13:44:01</td>\n",
       "      <td>StateLineWeir_20120609_Farrell_003.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.96</td>\n",
       "      <td>873.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48789.0</td>\n",
       "      <td>206.525571</td>\n",
       "      <td>statelineweir_20120609_farrell_003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2012-06-09 14:45:00</td>\n",
       "      <td>2012-06-09T14:44:30</td>\n",
       "      <td>StateLineWeir_20120609_Farrell_004.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49458.0</td>\n",
       "      <td>209.652156</td>\n",
       "      <td>statelineweir_20120609_farrell_004.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-06-09 15:45:00</td>\n",
       "      <td>2012-06-09T15:44:59</td>\n",
       "      <td>StateLineWeir_20120609_Farrell_005.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49719.0</td>\n",
       "      <td>210.088266</td>\n",
       "      <td>statelineweir_20120609_farrell_005.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42054</th>\n",
       "      <td>42054</td>\n",
       "      <td>42054</td>\n",
       "      <td>2019-10-11 09:00:00</td>\n",
       "      <td>2019-10-11T08:59:53</td>\n",
       "      <td>StateLineWeir_20191011_Farrell_409.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>77521.0</td>\n",
       "      <td>38385.370066</td>\n",
       "      <td>15952.029728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70085.0</td>\n",
       "      <td>37550.894823</td>\n",
       "      <td>16444.401209</td>\n",
       "      <td>41501.0</td>\n",
       "      <td>166.685071</td>\n",
       "      <td>statelineweir_20191011_farrell_409.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42055</th>\n",
       "      <td>42055</td>\n",
       "      <td>42055</td>\n",
       "      <td>2019-10-11 10:00:00</td>\n",
       "      <td>2019-10-11T09:59:52</td>\n",
       "      <td>StateLineWeir_20191011_Farrell_410.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>74614.0</td>\n",
       "      <td>40162.989292</td>\n",
       "      <td>15467.708856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70061.0</td>\n",
       "      <td>39397.339095</td>\n",
       "      <td>16009.008049</td>\n",
       "      <td>41591.0</td>\n",
       "      <td>171.758237</td>\n",
       "      <td>statelineweir_20191011_farrell_410.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42056</th>\n",
       "      <td>42056</td>\n",
       "      <td>42056</td>\n",
       "      <td>2019-10-11 11:00:00</td>\n",
       "      <td>2019-10-11T10:59:52</td>\n",
       "      <td>StateLineWeir_20191011_Farrell_411.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83260.0</td>\n",
       "      <td>42095.946590</td>\n",
       "      <td>16770.357949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76335.0</td>\n",
       "      <td>41350.006568</td>\n",
       "      <td>17489.374617</td>\n",
       "      <td>41949.0</td>\n",
       "      <td>176.424371</td>\n",
       "      <td>statelineweir_20191011_farrell_411.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42057</th>\n",
       "      <td>42057</td>\n",
       "      <td>42057</td>\n",
       "      <td>2019-10-11 12:00:00</td>\n",
       "      <td>2019-10-11T11:59:53</td>\n",
       "      <td>StateLineWeir_20191011_Farrell_412.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83045.0</td>\n",
       "      <td>45345.490954</td>\n",
       "      <td>17498.432849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78882.0</td>\n",
       "      <td>44553.920296</td>\n",
       "      <td>18268.294896</td>\n",
       "      <td>42822.0</td>\n",
       "      <td>172.268391</td>\n",
       "      <td>statelineweir_20191011_farrell_412.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42058</th>\n",
       "      <td>42058</td>\n",
       "      <td>42058</td>\n",
       "      <td>2019-10-11 12:45:00</td>\n",
       "      <td>2019-10-11T12:59:52</td>\n",
       "      <td>StateLineWeir_20191011_Farrell_413.jpg</td>\n",
       "      <td>USGS</td>\n",
       "      <td>6674500</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>...</td>\n",
       "      <td>89813.0</td>\n",
       "      <td>47877.870782</td>\n",
       "      <td>19963.166359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82630.0</td>\n",
       "      <td>47280.270559</td>\n",
       "      <td>20559.358767</td>\n",
       "      <td>43076.0</td>\n",
       "      <td>175.271876</td>\n",
       "      <td>statelineweir_20191011_farrell_413.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42059 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0           SensorTime          CaptureTime  \\\n",
       "0                 0           0  2012-06-09 13:15:00  2012-06-09T13:09:07   \n",
       "1                 1           1  2012-06-09 13:15:00  2012-06-09T13:10:29   \n",
       "2                 2           2  2012-06-09 13:45:00  2012-06-09T13:44:01   \n",
       "3                 3           3  2012-06-09 14:45:00  2012-06-09T14:44:30   \n",
       "4                 4           4  2012-06-09 15:45:00  2012-06-09T15:44:59   \n",
       "...             ...         ...                  ...                  ...   \n",
       "42054         42054       42054  2019-10-11 09:00:00  2019-10-11T08:59:53   \n",
       "42055         42055       42055  2019-10-11 10:00:00  2019-10-11T09:59:52   \n",
       "42056         42056       42056  2019-10-11 11:00:00  2019-10-11T10:59:52   \n",
       "42057         42057       42057  2019-10-11 12:00:00  2019-10-11T11:59:53   \n",
       "42058         42058       42058  2019-10-11 12:45:00  2019-10-11T12:59:52   \n",
       "\n",
       "                                     Filename Agency  SiteNumber TimeZone  \\\n",
       "0      StateLineWeir_20120609_Farrell_001.jpg   USGS     6674500      MDT   \n",
       "1      StateLineWeir_20120609_Farrell_002.jpg   USGS     6674500      MDT   \n",
       "2      StateLineWeir_20120609_Farrell_003.jpg   USGS     6674500      MDT   \n",
       "3      StateLineWeir_20120609_Farrell_004.jpg   USGS     6674500      MDT   \n",
       "4      StateLineWeir_20120609_Farrell_005.jpg   USGS     6674500      MDT   \n",
       "...                                       ...    ...         ...      ...   \n",
       "42054  StateLineWeir_20191011_Farrell_409.jpg   USGS     6674500      MDT   \n",
       "42055  StateLineWeir_20191011_Farrell_410.jpg   USGS     6674500      MDT   \n",
       "42056  StateLineWeir_20191011_Farrell_411.jpg   USGS     6674500      MDT   \n",
       "42057  StateLineWeir_20191011_Farrell_412.jpg   USGS     6674500      MDT   \n",
       "42058  StateLineWeir_20191011_Farrell_413.jpg   USGS     6674500      MDT   \n",
       "\n",
       "       Stage  Discharge  ... WwRawLineMax  WwRawLineMean  WwRawLineSigma  \\\n",
       "0       2.99      916.0  ...          0.0       0.000000        0.000000   \n",
       "1       2.99      916.0  ...          0.0       0.000000        0.000000   \n",
       "2       2.96      873.0  ...          0.0       0.000000        0.000000   \n",
       "3       2.94      846.0  ...          0.0       0.000000        0.000000   \n",
       "4       2.94      846.0  ...          0.0       0.000000        0.000000   \n",
       "...      ...        ...  ...          ...            ...             ...   \n",
       "42054   2.54      434.0  ...      77521.0   38385.370066    15952.029728   \n",
       "42055   2.54      434.0  ...      74614.0   40162.989292    15467.708856   \n",
       "42056   2.54      434.0  ...      83260.0   42095.946590    16770.357949   \n",
       "42057   2.54      434.0  ...      83045.0   45345.490954    17498.432849   \n",
       "42058   2.54      434.0  ...      89813.0   47877.870782    19963.166359   \n",
       "\n",
       "       WwCurveLineMin  WwCurveLineMax  WwCurveLineMean  WwCurveLineSigma  \\\n",
       "0                 0.0             0.0         0.000000          0.000000   \n",
       "1                 0.0             0.0         0.000000          0.000000   \n",
       "2                 0.0             0.0         0.000000          0.000000   \n",
       "3                 0.0             0.0         0.000000          0.000000   \n",
       "4                 0.0             0.0         0.000000          0.000000   \n",
       "...               ...             ...              ...               ...   \n",
       "42054             0.0         70085.0     37550.894823      16444.401209   \n",
       "42055             0.0         70061.0     39397.339095      16009.008049   \n",
       "42056             0.0         76335.0     41350.006568      17489.374617   \n",
       "42057             0.0         78882.0     44553.920296      18268.294896   \n",
       "42058             0.0         82630.0     47280.270559      20559.358767   \n",
       "\n",
       "       RiverArea  RiverWidth                           FilenameLower  \n",
       "0        48600.0  204.684959  statelineweir_20120609_farrell_001.jpg  \n",
       "1        48672.0  205.898452  statelineweir_20120609_farrell_002.jpg  \n",
       "2        48789.0  206.525571  statelineweir_20120609_farrell_003.jpg  \n",
       "3        49458.0  209.652156  statelineweir_20120609_farrell_004.jpg  \n",
       "4        49719.0  210.088266  statelineweir_20120609_farrell_005.jpg  \n",
       "...          ...         ...                                     ...  \n",
       "42054    41501.0  166.685071  statelineweir_20191011_farrell_409.jpg  \n",
       "42055    41591.0  171.758237  statelineweir_20191011_farrell_410.jpg  \n",
       "42056    41949.0  176.424371  statelineweir_20191011_farrell_411.jpg  \n",
       "42057    42822.0  172.268391  statelineweir_20191011_farrell_412.jpg  \n",
       "42058    43076.0  175.271876  statelineweir_20191011_farrell_413.jpg  \n",
       "\n",
       "[42059 rows x 64 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset/Seresnet_101_V1_PlatteRiverWeir_features_merged_all.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SensorTime'] = pd.to_datetime(df['SensorTime'])\n",
    "df['Year'] = df['SensorTime'].dt.year\n",
    "df['Month'] = df['SensorTime'].dt.month\n",
    "df['date_offset'] = (df.SensorTime.dt.month * 100 + df.SensorTime.dt.day - 320)%1300\n",
    "\n",
    "df['Season'] = pd.cut(df['date_offset'], [0, 300, 602, 900, 1300], \n",
    "                      labels=['spring', 'summer', 'autumn', 'winter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CaptureTime</th>\n",
       "      <th>SensorTime</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Discharge</th>\n",
       "      <th>RiverArea</th>\n",
       "      <th>RiverWidth</th>\n",
       "      <th>Month</th>\n",
       "      <th>Season</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-06-09T13:09:07</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>48600.0</td>\n",
       "      <td>204.684959</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-06-09T13:10:29</td>\n",
       "      <td>2012-06-09 13:15:00</td>\n",
       "      <td>2.99</td>\n",
       "      <td>916.0</td>\n",
       "      <td>48672.0</td>\n",
       "      <td>205.898452</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-06-09T13:44:01</td>\n",
       "      <td>2012-06-09 13:45:00</td>\n",
       "      <td>2.96</td>\n",
       "      <td>873.0</td>\n",
       "      <td>48789.0</td>\n",
       "      <td>206.525571</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-06-09T14:44:30</td>\n",
       "      <td>2012-06-09 14:45:00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>49458.0</td>\n",
       "      <td>209.652156</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-06-09T15:44:59</td>\n",
       "      <td>2012-06-09 15:45:00</td>\n",
       "      <td>2.94</td>\n",
       "      <td>846.0</td>\n",
       "      <td>49719.0</td>\n",
       "      <td>210.088266</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42054</th>\n",
       "      <td>2019-10-11T08:59:53</td>\n",
       "      <td>2019-10-11 09:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>41501.0</td>\n",
       "      <td>166.685071</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42055</th>\n",
       "      <td>2019-10-11T09:59:52</td>\n",
       "      <td>2019-10-11 10:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>41591.0</td>\n",
       "      <td>171.758237</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42056</th>\n",
       "      <td>2019-10-11T10:59:52</td>\n",
       "      <td>2019-10-11 11:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>41949.0</td>\n",
       "      <td>176.424371</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42057</th>\n",
       "      <td>2019-10-11T11:59:53</td>\n",
       "      <td>2019-10-11 12:00:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>42822.0</td>\n",
       "      <td>172.268391</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42058</th>\n",
       "      <td>2019-10-11T12:59:52</td>\n",
       "      <td>2019-10-11 12:45:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>434.0</td>\n",
       "      <td>43076.0</td>\n",
       "      <td>175.271876</td>\n",
       "      <td>10</td>\n",
       "      <td>autumn</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42059 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               CaptureTime          SensorTime  Stage  Discharge  RiverArea  \\\n",
       "0      2012-06-09T13:09:07 2012-06-09 13:15:00   2.99      916.0    48600.0   \n",
       "1      2012-06-09T13:10:29 2012-06-09 13:15:00   2.99      916.0    48672.0   \n",
       "2      2012-06-09T13:44:01 2012-06-09 13:45:00   2.96      873.0    48789.0   \n",
       "3      2012-06-09T14:44:30 2012-06-09 14:45:00   2.94      846.0    49458.0   \n",
       "4      2012-06-09T15:44:59 2012-06-09 15:45:00   2.94      846.0    49719.0   \n",
       "...                    ...                 ...    ...        ...        ...   \n",
       "42054  2019-10-11T08:59:53 2019-10-11 09:00:00   2.54      434.0    41501.0   \n",
       "42055  2019-10-11T09:59:52 2019-10-11 10:00:00   2.54      434.0    41591.0   \n",
       "42056  2019-10-11T10:59:52 2019-10-11 11:00:00   2.54      434.0    41949.0   \n",
       "42057  2019-10-11T11:59:53 2019-10-11 12:00:00   2.54      434.0    42822.0   \n",
       "42058  2019-10-11T12:59:52 2019-10-11 12:45:00   2.54      434.0    43076.0   \n",
       "\n",
       "       RiverWidth  Month  Season  Year  \n",
       "0      204.684959      6  spring  2012  \n",
       "1      205.898452      6  spring  2012  \n",
       "2      206.525571      6  spring  2012  \n",
       "3      209.652156      6  spring  2012  \n",
       "4      210.088266      6  spring  2012  \n",
       "...           ...    ...     ...   ...  \n",
       "42054  166.685071     10  autumn  2019  \n",
       "42055  171.758237     10  autumn  2019  \n",
       "42056  176.424371     10  autumn  2019  \n",
       "42057  172.268391     10  autumn  2019  \n",
       "42058  175.271876     10  autumn  2019  \n",
       "\n",
       "[42059 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"CaptureTime\", \"SensorTime\", \"Stage\", \"Discharge\", \"RiverArea\", \"RiverWidth\", \"Month\", \"Season\", \"Year\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaptureTime            object\n",
       "SensorTime     datetime64[ns]\n",
       "Stage                 float64\n",
       "Discharge             float64\n",
       "RiverArea             float64\n",
       "RiverWidth            float64\n",
       "Month                   int64\n",
       "Season               category\n",
       "Year                    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40148, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.Stage > 0]\n",
    "df = df[df.Discharge > 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40142, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.RiverWidth > 0]\n",
    "#df = df[df.Discharge > 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.Season != \"winter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaptureTime      0\n",
       "SensorTime       0\n",
       "Stage            0\n",
       "Discharge        0\n",
       "RiverArea        0\n",
       "RiverWidth       0\n",
       "Month            0\n",
       "Season         126\n",
       "Year             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide dataset to X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "df_train = df[(df.Year >= 2012) & (df.Year <= 2016)]\n",
    "df_train = df_train.iloc[np.random.permutation(len(df_train))]\n",
    "\n",
    "df_val = df[(df.Year >= 2017) & (df.Year <= 2017)]\n",
    "df_val = df_val.iloc[np.random.permutation(len(df_val))]\n",
    "\n",
    "df_test = df[(df.Year >= 2018) & (df.Year <= 2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])\n",
    "df_val = df_val.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])\n",
    "df_test = df_test.drop(columns=[\"Year\", \"SensorTime\", \"CaptureTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = df_train[[\"Stage\"]].values\n",
    "X_train = df_train[[\"RiverWidth\", \"Month\"]].values\n",
    "\n",
    "y_val = df_train[[\"Stage\"]].values\n",
    "X_val = df_train[[\"RiverWidth\", \"Month\"]].values\n",
    "\n",
    "y_test = df_test[[\"Stage\"]].values\n",
    "X_test = df_test[[\"RiverWidth\", \"Month\"]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20304, 2)\n",
      "(20304, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "print(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(lr):\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.Input(shape=input_shape))\n",
    "  \n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "  model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "  \"\"\"model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(512, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(512, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(256, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(128, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(64, activation=\"tanh\"))\n",
    "  model.add(tf.keras.layers.Dense(32, activation=\"tanh\"))\"\"\"\n",
    "\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(output_shape, activation = 'linear'))\n",
    "\n",
    "  \n",
    "  model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr), loss = 'mse', metrics = ['mse', tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mae', 'mape'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-25 12:40:22.166786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:22.167093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:22.167296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:22.167977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:22.168178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:22.168366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:22.168632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:22.168851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-25 12:40:22.169015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 110 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = model_builder(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date_actual = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/fit/\" + date_actual\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"model_weights/{date_actual}_mlp_seg_best_weights.hdf5\",\n",
    "                               monitor='val_loss',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 1.5942 - mse: 1.5942 - rmse: 1.2626 - mae: 0.8093 - mape: 28.2478\n",
      "Epoch 1: val_loss improved from inf to 0.69714, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 2s 4ms/step - loss: 1.5885 - mse: 1.5885 - rmse: 1.2604 - mae: 0.8088 - mape: 28.2368 - val_loss: 0.6971 - val_mse: 0.6971 - val_rmse: 0.8349 - val_mae: 0.5825 - val_mape: 18.0457 - lr: 0.0010\n",
      "Epoch 2/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.7022 - mse: 0.7022 - rmse: 0.8380 - mae: 0.6340 - mape: 21.7619\n",
      "Epoch 2: val_loss improved from 0.69714 to 0.62361, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.6995 - mse: 0.6995 - rmse: 0.8364 - mae: 0.6320 - mape: 21.6950 - val_loss: 0.6236 - val_mse: 0.6236 - val_rmse: 0.7897 - val_mae: 0.6082 - val_mape: 21.6198 - lr: 0.0010\n",
      "Epoch 3/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.7326 - mse: 0.7326 - rmse: 0.8559 - mae: 0.6451 - mape: 22.0975\n",
      "Epoch 3: val_loss did not improve from 0.62361\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.7316 - mse: 0.7316 - rmse: 0.8553 - mae: 0.6448 - mape: 22.0846 - val_loss: 0.6708 - val_mse: 0.6708 - val_rmse: 0.8190 - val_mae: 0.5539 - val_mape: 16.8147 - lr: 0.0010\n",
      "Epoch 4/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.7336 - mse: 0.7336 - rmse: 0.8565 - mae: 0.6417 - mape: 21.9240\n",
      "Epoch 4: val_loss did not improve from 0.62361\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.7239 - mse: 0.7239 - rmse: 0.8508 - mae: 0.6360 - mape: 21.7323 - val_loss: 0.6370 - val_mse: 0.6370 - val_rmse: 0.7981 - val_mae: 0.6583 - val_mape: 24.5437 - lr: 0.0010\n",
      "Epoch 5/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.6416 - mse: 0.6416 - rmse: 0.8010 - mae: 0.5960 - mape: 20.3342\n",
      "Epoch 5: val_loss did not improve from 0.62361\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.6403 - mse: 0.6403 - rmse: 0.8002 - mae: 0.5949 - mape: 20.2834 - val_loss: 1.4377 - val_mse: 1.4377 - val_rmse: 1.1990 - val_mae: 1.1014 - val_mape: 43.5422 - lr: 0.0010\n",
      "Epoch 6/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.5863 - mse: 0.5863 - rmse: 0.7657 - mae: 0.5635 - mape: 19.2127\n",
      "Epoch 6: val_loss improved from 0.62361 to 0.48546, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.5863 - mse: 0.5863 - rmse: 0.7657 - mae: 0.5635 - mape: 19.2127 - val_loss: 0.4855 - val_mse: 0.4855 - val_rmse: 0.6967 - val_mae: 0.4768 - val_mape: 15.5111 - lr: 0.0010\n",
      "Epoch 7/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.5688 - mse: 0.5688 - rmse: 0.7542 - mae: 0.5520 - mape: 18.8414\n",
      "Epoch 7: val_loss did not improve from 0.48546\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.5658 - mse: 0.5658 - rmse: 0.7522 - mae: 0.5506 - mape: 18.7814 - val_loss: 0.5091 - val_mse: 0.5091 - val_rmse: 0.7135 - val_mae: 0.5632 - val_mape: 20.4574 - lr: 0.0010\n",
      "Epoch 8/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.5211 - mse: 0.5211 - rmse: 0.7218 - mae: 0.5250 - mape: 17.8376\n",
      "Epoch 8: val_loss improved from 0.48546 to 0.45261, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.5212 - mse: 0.5212 - rmse: 0.7219 - mae: 0.5248 - mape: 17.8293 - val_loss: 0.4526 - val_mse: 0.4526 - val_rmse: 0.6728 - val_mae: 0.4822 - val_mape: 16.3474 - lr: 0.0010\n",
      "Epoch 9/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.5418 - mse: 0.5418 - rmse: 0.7361 - mae: 0.5392 - mape: 18.4131\n",
      "Epoch 9: val_loss did not improve from 0.45261\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.5505 - mse: 0.5505 - rmse: 0.7420 - mae: 0.5438 - mape: 18.5695 - val_loss: 0.5222 - val_mse: 0.5222 - val_rmse: 0.7226 - val_mae: 0.5803 - val_mape: 21.2856 - lr: 0.0010\n",
      "Epoch 10/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.4980 - mse: 0.4980 - rmse: 0.7057 - mae: 0.5126 - mape: 17.3625\n",
      "Epoch 10: val_loss did not improve from 0.45261\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4983 - mse: 0.4983 - rmse: 0.7059 - mae: 0.5133 - mape: 17.4190 - val_loss: 0.8169 - val_mse: 0.8169 - val_rmse: 0.9038 - val_mae: 0.6685 - val_mape: 20.9104 - lr: 0.0010\n",
      "Epoch 11/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.4963 - mse: 0.4963 - rmse: 0.7045 - mae: 0.5127 - mape: 17.3421\n",
      "Epoch 11: val_loss did not improve from 0.45261\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4930 - mse: 0.4930 - rmse: 0.7021 - mae: 0.5103 - mape: 17.2966 - val_loss: 0.4851 - val_mse: 0.4851 - val_rmse: 0.6965 - val_mae: 0.5433 - val_mape: 19.0993 - lr: 0.0010\n",
      "Epoch 12/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.4709 - mse: 0.4709 - rmse: 0.6862 - mae: 0.4966 - mape: 16.8022\n",
      "Epoch 12: val_loss improved from 0.45261 to 0.43310, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4714 - mse: 0.4714 - rmse: 0.6866 - mae: 0.4970 - mape: 16.8074 - val_loss: 0.4331 - val_mse: 0.4331 - val_rmse: 0.6581 - val_mae: 0.4661 - val_mape: 15.5829 - lr: 0.0010\n",
      "Epoch 13/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.4870 - mse: 0.4870 - rmse: 0.6979 - mae: 0.5064 - mape: 17.1631\n",
      "Epoch 13: val_loss did not improve from 0.43310\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4867 - mse: 0.4867 - rmse: 0.6976 - mae: 0.5064 - mape: 17.1680 - val_loss: 0.4364 - val_mse: 0.4364 - val_rmse: 0.6606 - val_mae: 0.4577 - val_mape: 15.0534 - lr: 0.0010\n",
      "Epoch 14/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.4913 - mse: 0.4913 - rmse: 0.7009 - mae: 0.5098 - mape: 17.2668\n",
      "Epoch 14: val_loss did not improve from 0.43310\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4907 - mse: 0.4907 - rmse: 0.7005 - mae: 0.5095 - mape: 17.2642 - val_loss: 0.4939 - val_mse: 0.4939 - val_rmse: 0.7028 - val_mae: 0.4511 - val_mape: 13.8378 - lr: 0.0010\n",
      "Epoch 15/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.4565 - mse: 0.4565 - rmse: 0.6757 - mae: 0.4856 - mape: 16.3339\n",
      "Epoch 15: val_loss did not improve from 0.43310\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4626 - mse: 0.4626 - rmse: 0.6801 - mae: 0.4893 - mape: 16.4710 - val_loss: 0.4340 - val_mse: 0.4340 - val_rmse: 0.6588 - val_mae: 0.4568 - val_mape: 15.0991 - lr: 0.0010\n",
      "Epoch 16/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.4674 - mse: 0.4674 - rmse: 0.6836 - mae: 0.4920 - mape: 16.6039\n",
      "Epoch 16: val_loss did not improve from 0.43310\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4672 - mse: 0.4672 - rmse: 0.6835 - mae: 0.4919 - mape: 16.6027 - val_loss: 0.4360 - val_mse: 0.4360 - val_rmse: 0.6603 - val_mae: 0.4534 - val_mape: 14.8001 - lr: 0.0010\n",
      "Epoch 17/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.4447 - mse: 0.4447 - rmse: 0.6669 - mae: 0.4755 - mape: 15.9864\n",
      "Epoch 17: val_loss improved from 0.43310 to 0.42041, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4453 - mse: 0.4453 - rmse: 0.6673 - mae: 0.4761 - mape: 15.9838 - val_loss: 0.4204 - val_mse: 0.4204 - val_rmse: 0.6484 - val_mae: 0.4355 - val_mape: 14.0447 - lr: 0.0010\n",
      "Epoch 18/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.4393 - mse: 0.4393 - rmse: 0.6628 - mae: 0.4723 - mape: 15.8564\n",
      "Epoch 18: val_loss did not improve from 0.42041\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4393 - mse: 0.4393 - rmse: 0.6628 - mae: 0.4723 - mape: 15.8564 - val_loss: 0.4653 - val_mse: 0.4653 - val_rmse: 0.6822 - val_mae: 0.5347 - val_mape: 19.2559 - lr: 0.0010\n",
      "Epoch 19/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.4487 - mse: 0.4487 - rmse: 0.6698 - mae: 0.4809 - mape: 16.2133\n",
      "Epoch 19: val_loss improved from 0.42041 to 0.41862, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4446 - mse: 0.4446 - rmse: 0.6668 - mae: 0.4779 - mape: 16.1021 - val_loss: 0.4186 - val_mse: 0.4186 - val_rmse: 0.6470 - val_mae: 0.4697 - val_mape: 15.8722 - lr: 0.0010\n",
      "Epoch 20/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.4404 - mse: 0.4404 - rmse: 0.6636 - mae: 0.4719 - mape: 15.8210\n",
      "Epoch 20: val_loss did not improve from 0.41862\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4426 - mse: 0.4426 - rmse: 0.6652 - mae: 0.4737 - mape: 15.8933 - val_loss: 0.4211 - val_mse: 0.4211 - val_rmse: 0.6489 - val_mae: 0.4322 - val_mape: 13.9343 - lr: 0.0010\n",
      "Epoch 21/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.4303 - mse: 0.4303 - rmse: 0.6560 - mae: 0.4667 - mape: 15.6432\n",
      "Epoch 21: val_loss did not improve from 0.41862\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4291 - mse: 0.4291 - rmse: 0.6551 - mae: 0.4660 - mape: 15.6211 - val_loss: 0.4415 - val_mse: 0.4415 - val_rmse: 0.6645 - val_mae: 0.4737 - val_mape: 15.5545 - lr: 0.0010\n",
      "Epoch 22/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.4235 - mse: 0.4235 - rmse: 0.6508 - mae: 0.4601 - mape: 15.3654\n",
      "Epoch 22: val_loss improved from 0.41862 to 0.39305, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.4222 - mse: 0.4222 - rmse: 0.6498 - mae: 0.4592 - mape: 15.3176 - val_loss: 0.3931 - val_mse: 0.3931 - val_rmse: 0.6269 - val_mae: 0.4646 - val_mape: 15.9913 - lr: 0.0010\n",
      "Epoch 23/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.3984 - mse: 0.3984 - rmse: 0.6312 - mae: 0.4435 - mape: 14.7497\n",
      "Epoch 23: val_loss improved from 0.39305 to 0.37579, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3987 - mse: 0.3987 - rmse: 0.6314 - mae: 0.4438 - mape: 14.7635 - val_loss: 0.3758 - val_mse: 0.3758 - val_rmse: 0.6130 - val_mae: 0.4122 - val_mape: 13.2974 - lr: 0.0010\n",
      "Epoch 24/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.3946 - mse: 0.3946 - rmse: 0.6281 - mae: 0.4407 - mape: 14.6362\n",
      "Epoch 24: val_loss improved from 0.37579 to 0.35664, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3966 - mse: 0.3966 - rmse: 0.6298 - mae: 0.4419 - mape: 14.6751 - val_loss: 0.3566 - val_mse: 0.3566 - val_rmse: 0.5972 - val_mae: 0.4079 - val_mape: 13.1407 - lr: 0.0010\n",
      "Epoch 25/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.3965 - mse: 0.3965 - rmse: 0.6297 - mae: 0.4388 - mape: 14.4982\n",
      "Epoch 25: val_loss did not improve from 0.35664\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3954 - mse: 0.3954 - rmse: 0.6288 - mae: 0.4371 - mape: 14.4319 - val_loss: 0.3647 - val_mse: 0.3647 - val_rmse: 0.6039 - val_mae: 0.4368 - val_mape: 15.0881 - lr: 0.0010\n",
      "Epoch 26/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.3601 - mse: 0.3601 - rmse: 0.6001 - mae: 0.4099 - mape: 13.4085\n",
      "Epoch 26: val_loss did not improve from 0.35664\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3602 - mse: 0.3602 - rmse: 0.6001 - mae: 0.4099 - mape: 13.4102 - val_loss: 0.4079 - val_mse: 0.4079 - val_rmse: 0.6387 - val_mae: 0.4779 - val_mape: 16.9809 - lr: 0.0010\n",
      "Epoch 27/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.3582 - mse: 0.3582 - rmse: 0.5985 - mae: 0.4093 - mape: 13.3674\n",
      "Epoch 27: val_loss did not improve from 0.35664\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3593 - mse: 0.3593 - rmse: 0.5994 - mae: 0.4105 - mape: 13.4158 - val_loss: 0.4177 - val_mse: 0.4177 - val_rmse: 0.6463 - val_mae: 0.4800 - val_mape: 16.9831 - lr: 0.0010\n",
      "Epoch 28/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.3669 - mse: 0.3669 - rmse: 0.6057 - mae: 0.4158 - mape: 13.6117\n",
      "Epoch 28: val_loss improved from 0.35664 to 0.32974, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3666 - mse: 0.3666 - rmse: 0.6055 - mae: 0.4155 - mape: 13.6003 - val_loss: 0.3297 - val_mse: 0.3297 - val_rmse: 0.5742 - val_mae: 0.3987 - val_mape: 13.3569 - lr: 0.0010\n",
      "Epoch 29/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.3416 - mse: 0.3416 - rmse: 0.5845 - mae: 0.3963 - mape: 12.8262\n",
      "Epoch 29: val_loss did not improve from 0.32974\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3409 - mse: 0.3409 - rmse: 0.5839 - mae: 0.3948 - mape: 12.7804 - val_loss: 0.3332 - val_mse: 0.3332 - val_rmse: 0.5772 - val_mae: 0.3568 - val_mape: 10.7645 - lr: 0.0010\n",
      "Epoch 30/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.3418 - mse: 0.3418 - rmse: 0.5847 - mae: 0.3909 - mape: 12.6123\n",
      "Epoch 30: val_loss did not improve from 0.32974\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3436 - mse: 0.3436 - rmse: 0.5862 - mae: 0.3914 - mape: 12.6238 - val_loss: 0.4997 - val_mse: 0.4997 - val_rmse: 0.7069 - val_mae: 0.5452 - val_mape: 20.0564 - lr: 0.0010\n",
      "Epoch 31/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.3373 - mse: 0.3373 - rmse: 0.5808 - mae: 0.3858 - mape: 12.4570\n",
      "Epoch 31: val_loss did not improve from 0.32974\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3371 - mse: 0.3371 - rmse: 0.5806 - mae: 0.3852 - mape: 12.4236 - val_loss: 0.4119 - val_mse: 0.4119 - val_rmse: 0.6418 - val_mae: 0.4518 - val_mape: 16.0517 - lr: 0.0010\n",
      "Epoch 32/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.3208 - mse: 0.3208 - rmse: 0.5664 - mae: 0.3733 - mape: 12.0096\n",
      "Epoch 32: val_loss did not improve from 0.32974\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3259 - mse: 0.3259 - rmse: 0.5709 - mae: 0.3763 - mape: 12.1135 - val_loss: 0.5235 - val_mse: 0.5235 - val_rmse: 0.7235 - val_mae: 0.5179 - val_mape: 19.2291 - lr: 0.0010\n",
      "Epoch 33/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.3299 - mse: 0.3299 - rmse: 0.5743 - mae: 0.3762 - mape: 12.1259\n",
      "Epoch 33: val_loss improved from 0.32974 to 0.30600, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3294 - mse: 0.3294 - rmse: 0.5739 - mae: 0.3759 - mape: 12.1107 - val_loss: 0.3060 - val_mse: 0.3060 - val_rmse: 0.5532 - val_mae: 0.3706 - val_mape: 12.1684 - lr: 0.0010\n",
      "Epoch 34/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.3108 - mse: 0.3108 - rmse: 0.5575 - mae: 0.3645 - mape: 11.6999\n",
      "Epoch 34: val_loss did not improve from 0.30600\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3106 - mse: 0.3106 - rmse: 0.5573 - mae: 0.3638 - mape: 11.6580 - val_loss: 0.3339 - val_mse: 0.3339 - val_rmse: 0.5779 - val_mae: 0.3493 - val_mape: 10.2367 - lr: 0.0010\n",
      "Epoch 35/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.3068 - mse: 0.3068 - rmse: 0.5539 - mae: 0.3584 - mape: 11.4785\n",
      "Epoch 35: val_loss did not improve from 0.30600\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3147 - mse: 0.3147 - rmse: 0.5610 - mae: 0.3613 - mape: 11.5632 - val_loss: 0.5891 - val_mse: 0.5891 - val_rmse: 0.7675 - val_mae: 0.5755 - val_mape: 19.9539 - lr: 0.0010\n",
      "Epoch 36/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.3398 - mse: 0.3398 - rmse: 0.5829 - mae: 0.3927 - mape: 12.8145\n",
      "Epoch 36: val_loss did not improve from 0.30600\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3371 - mse: 0.3371 - rmse: 0.5806 - mae: 0.3910 - mape: 12.7562 - val_loss: 0.3706 - val_mse: 0.3706 - val_rmse: 0.6088 - val_mae: 0.4018 - val_mape: 12.6282 - lr: 0.0010\n",
      "Epoch 37/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.3329 - mse: 0.3329 - rmse: 0.5770 - mae: 0.3740 - mape: 12.0965\n",
      "Epoch 37: val_loss improved from 0.30600 to 0.29921, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3322 - mse: 0.3322 - rmse: 0.5764 - mae: 0.3742 - mape: 12.1246 - val_loss: 0.2992 - val_mse: 0.2992 - val_rmse: 0.5470 - val_mae: 0.3230 - val_mape: 9.5858 - lr: 0.0010\n",
      "Epoch 38/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2943 - mse: 0.2943 - rmse: 0.5425 - mae: 0.3425 - mape: 10.9076\n",
      "Epoch 38: val_loss did not improve from 0.29921\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3016 - mse: 0.3016 - rmse: 0.5492 - mae: 0.3471 - mape: 11.0858 - val_loss: 0.5120 - val_mse: 0.5120 - val_rmse: 0.7155 - val_mae: 0.5149 - val_mape: 16.7127 - lr: 0.0010\n",
      "Epoch 39/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.3099 - mse: 0.3099 - rmse: 0.5567 - mae: 0.3555 - mape: 11.4113\n",
      "Epoch 39: val_loss improved from 0.29921 to 0.27169, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3094 - mse: 0.3094 - rmse: 0.5563 - mae: 0.3556 - mape: 11.4048 - val_loss: 0.2717 - val_mse: 0.2717 - val_rmse: 0.5212 - val_mae: 0.3394 - val_mape: 10.8840 - lr: 0.0010\n",
      "Epoch 40/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2828 - mse: 0.2828 - rmse: 0.5318 - mae: 0.3333 - mape: 10.5663\n",
      "Epoch 40: val_loss did not improve from 0.27169\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2826 - mse: 0.2826 - rmse: 0.5316 - mae: 0.3337 - mape: 10.5846 - val_loss: 0.3522 - val_mse: 0.3522 - val_rmse: 0.5935 - val_mae: 0.4067 - val_mape: 13.5761 - lr: 0.0010\n",
      "Epoch 41/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.3062 - mse: 0.3062 - rmse: 0.5533 - mae: 0.3484 - mape: 11.1573\n",
      "Epoch 41: val_loss did not improve from 0.27169\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3058 - mse: 0.3058 - rmse: 0.5530 - mae: 0.3481 - mape: 11.1474 - val_loss: 0.2740 - val_mse: 0.2740 - val_rmse: 0.5235 - val_mae: 0.3063 - val_mape: 9.0954 - lr: 0.0010\n",
      "Epoch 42/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2872 - mse: 0.2872 - rmse: 0.5359 - mae: 0.3364 - mape: 10.6790\n",
      "Epoch 42: val_loss improved from 0.27169 to 0.26767, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2880 - mse: 0.2880 - rmse: 0.5366 - mae: 0.3374 - mape: 10.7255 - val_loss: 0.2677 - val_mse: 0.2677 - val_rmse: 0.5174 - val_mae: 0.3165 - val_mape: 9.6303 - lr: 0.0010\n",
      "Epoch 43/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2933 - mse: 0.2933 - rmse: 0.5416 - mae: 0.3373 - mape: 10.7367\n",
      "Epoch 43: val_loss did not improve from 0.26767\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2945 - mse: 0.2945 - rmse: 0.5427 - mae: 0.3375 - mape: 10.7519 - val_loss: 0.2717 - val_mse: 0.2717 - val_rmse: 0.5212 - val_mae: 0.3282 - val_mape: 10.3028 - lr: 0.0010\n",
      "Epoch 44/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.3063 - mse: 0.3063 - rmse: 0.5534 - mae: 0.3420 - mape: 10.9284\n",
      "Epoch 44: val_loss improved from 0.26767 to 0.25408, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3054 - mse: 0.3054 - rmse: 0.5526 - mae: 0.3415 - mape: 10.9053 - val_loss: 0.2541 - val_mse: 0.2541 - val_rmse: 0.5041 - val_mae: 0.3168 - val_mape: 10.3179 - lr: 0.0010\n",
      "Epoch 45/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2731 - mse: 0.2731 - rmse: 0.5226 - mae: 0.3255 - mape: 10.3505\n",
      "Epoch 45: val_loss did not improve from 0.25408\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2731 - mse: 0.2731 - rmse: 0.5226 - mae: 0.3255 - mape: 10.3505 - val_loss: 0.2831 - val_mse: 0.2831 - val_rmse: 0.5321 - val_mae: 0.3129 - val_mape: 9.1939 - lr: 0.0010\n",
      "Epoch 46/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.3141 - mse: 0.3141 - rmse: 0.5604 - mae: 0.3516 - mape: 11.3241\n",
      "Epoch 46: val_loss did not improve from 0.25408\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.3132 - mse: 0.3132 - rmse: 0.5597 - mae: 0.3510 - mape: 11.2991 - val_loss: 0.2549 - val_mse: 0.2549 - val_rmse: 0.5049 - val_mae: 0.3095 - val_mape: 9.4486 - lr: 0.0010\n",
      "Epoch 47/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2775 - mse: 0.2775 - rmse: 0.5268 - mae: 0.3280 - mape: 10.3913\n",
      "Epoch 47: val_loss improved from 0.25408 to 0.22755, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2761 - mse: 0.2761 - rmse: 0.5255 - mae: 0.3271 - mape: 10.3688 - val_loss: 0.2276 - val_mse: 0.2276 - val_rmse: 0.4770 - val_mae: 0.2854 - val_mape: 8.7867 - lr: 0.0010\n",
      "Epoch 48/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2833 - mse: 0.2833 - rmse: 0.5323 - mae: 0.3301 - mape: 10.4707\n",
      "Epoch 48: val_loss did not improve from 0.22755\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2813 - mse: 0.2813 - rmse: 0.5304 - mae: 0.3285 - mape: 10.4206 - val_loss: 0.3653 - val_mse: 0.3653 - val_rmse: 0.6044 - val_mae: 0.3310 - val_mape: 9.3714 - lr: 0.0010\n",
      "Epoch 49/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2600 - mse: 0.2600 - rmse: 0.5099 - mae: 0.3136 - mape: 9.9321\n",
      "Epoch 49: val_loss did not improve from 0.22755\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2597 - mse: 0.2597 - rmse: 0.5096 - mae: 0.3134 - mape: 9.9240 - val_loss: 0.2460 - val_mse: 0.2460 - val_rmse: 0.4960 - val_mae: 0.3038 - val_mape: 9.4105 - lr: 0.0010\n",
      "Epoch 50/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2730 - mse: 0.2730 - rmse: 0.5225 - mae: 0.3225 - mape: 10.2380\n",
      "Epoch 50: val_loss did not improve from 0.22755\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2737 - mse: 0.2737 - rmse: 0.5232 - mae: 0.3230 - mape: 10.2723 - val_loss: 0.2707 - val_mse: 0.2707 - val_rmse: 0.5203 - val_mae: 0.3348 - val_mape: 10.5362 - lr: 0.0010\n",
      "Epoch 51/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2648 - mse: 0.2648 - rmse: 0.5146 - mae: 0.3175 - mape: 10.0140\n",
      "Epoch 51: val_loss did not improve from 0.22755\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2610 - mse: 0.2610 - rmse: 0.5108 - mae: 0.3155 - mape: 9.9816 - val_loss: 0.2692 - val_mse: 0.2692 - val_rmse: 0.5188 - val_mae: 0.3649 - val_mape: 12.1459 - lr: 0.0010\n",
      "Epoch 52/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2523 - mse: 0.2523 - rmse: 0.5023 - mae: 0.3104 - mape: 9.8299\n",
      "Epoch 52: val_loss did not improve from 0.22755\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2525 - mse: 0.2525 - rmse: 0.5025 - mae: 0.3104 - mape: 9.8272 - val_loss: 0.2394 - val_mse: 0.2394 - val_rmse: 0.4893 - val_mae: 0.2826 - val_mape: 8.4206 - lr: 0.0010\n",
      "Epoch 53/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2496 - mse: 0.2496 - rmse: 0.4996 - mae: 0.3036 - mape: 9.6012\n",
      "Epoch 53: val_loss did not improve from 0.22755\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2506 - mse: 0.2506 - rmse: 0.5006 - mae: 0.3041 - mape: 9.6087 - val_loss: 0.2390 - val_mse: 0.2390 - val_rmse: 0.4889 - val_mae: 0.2995 - val_mape: 9.6063 - lr: 0.0010\n",
      "Epoch 54/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2983 - mse: 0.2983 - rmse: 0.5462 - mae: 0.3395 - mape: 10.9222\n",
      "Epoch 54: val_loss did not improve from 0.22755\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2995 - mse: 0.2995 - rmse: 0.5473 - mae: 0.3407 - mape: 10.9428 - val_loss: 0.2484 - val_mse: 0.2484 - val_rmse: 0.4984 - val_mae: 0.3148 - val_mape: 10.0927 - lr: 0.0010\n",
      "Epoch 55/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2558 - mse: 0.2558 - rmse: 0.5058 - mae: 0.3084 - mape: 9.7422\n",
      "Epoch 55: val_loss did not improve from 0.22755\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2564 - mse: 0.2564 - rmse: 0.5064 - mae: 0.3092 - mape: 9.7782 - val_loss: 0.3374 - val_mse: 0.3374 - val_rmse: 0.5808 - val_mae: 0.3484 - val_mape: 10.3511 - lr: 0.0010\n",
      "Epoch 56/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2627 - mse: 0.2627 - rmse: 0.5125 - mae: 0.3182 - mape: 10.1268\n",
      "Epoch 56: val_loss did not improve from 0.22755\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2639 - mse: 0.2639 - rmse: 0.5137 - mae: 0.3182 - mape: 10.1181 - val_loss: 0.2812 - val_mse: 0.2812 - val_rmse: 0.5302 - val_mae: 0.3524 - val_mape: 12.0181 - lr: 0.0010\n",
      "Epoch 57/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2695 - mse: 0.2695 - rmse: 0.5191 - mae: 0.3191 - mape: 10.1858\n",
      "Epoch 57: val_loss did not improve from 0.22755\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2722 - mse: 0.2722 - rmse: 0.5218 - mae: 0.3206 - mape: 10.2142 - val_loss: 0.2728 - val_mse: 0.2728 - val_rmse: 0.5223 - val_mae: 0.3376 - val_mape: 10.9254 - lr: 0.0010\n",
      "Epoch 58/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.2686 - mse: 0.2686 - rmse: 0.5182 - mae: 0.3208 - mape: 10.1811\n",
      "Epoch 58: val_loss improved from 0.22755 to 0.22728, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2670 - mse: 0.2670 - rmse: 0.5167 - mae: 0.3192 - mape: 10.1210 - val_loss: 0.2273 - val_mse: 0.2273 - val_rmse: 0.4767 - val_mae: 0.2812 - val_mape: 8.5166 - lr: 0.0010\n",
      "Epoch 59/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2532 - mse: 0.2532 - rmse: 0.5032 - mae: 0.3078 - mape: 9.7003\n",
      "Epoch 59: val_loss did not improve from 0.22728\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2547 - mse: 0.2547 - rmse: 0.5047 - mae: 0.3094 - mape: 9.7787 - val_loss: 0.2813 - val_mse: 0.2813 - val_rmse: 0.5304 - val_mae: 0.3174 - val_mape: 9.4251 - lr: 0.0010\n",
      "Epoch 60/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2588 - mse: 0.2588 - rmse: 0.5088 - mae: 0.3130 - mape: 9.9102\n",
      "Epoch 60: val_loss did not improve from 0.22728\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2561 - mse: 0.2561 - rmse: 0.5061 - mae: 0.3111 - mape: 9.8480 - val_loss: 0.3249 - val_mse: 0.3249 - val_rmse: 0.5700 - val_mae: 0.3257 - val_mape: 9.4314 - lr: 0.0010\n",
      "Epoch 61/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2692 - mse: 0.2692 - rmse: 0.5189 - mae: 0.3170 - mape: 10.0828\n",
      "Epoch 61: val_loss did not improve from 0.22728\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2681 - mse: 0.2681 - rmse: 0.5178 - mae: 0.3172 - mape: 10.0945 - val_loss: 0.2560 - val_mse: 0.2560 - val_rmse: 0.5060 - val_mae: 0.3490 - val_mape: 11.7168 - lr: 0.0010\n",
      "Epoch 62/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2497 - mse: 0.2497 - rmse: 0.4997 - mae: 0.3071 - mape: 9.7355\n",
      "Epoch 62: val_loss did not improve from 0.22728\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2504 - mse: 0.2504 - rmse: 0.5004 - mae: 0.3078 - mape: 9.7541 - val_loss: 0.2456 - val_mse: 0.2456 - val_rmse: 0.4956 - val_mae: 0.3436 - val_mape: 11.5003 - lr: 0.0010\n",
      "Epoch 63/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2624 - mse: 0.2624 - rmse: 0.5122 - mae: 0.3170 - mape: 10.1164\n",
      "Epoch 63: val_loss did not improve from 0.22728\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2627 - mse: 0.2627 - rmse: 0.5125 - mae: 0.3153 - mape: 10.0399 - val_loss: 0.2278 - val_mse: 0.2278 - val_rmse: 0.4773 - val_mae: 0.2788 - val_mape: 8.4763 - lr: 0.0010\n",
      "Epoch 64/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2502 - mse: 0.2502 - rmse: 0.5002 - mae: 0.3096 - mape: 9.8649\n",
      "Epoch 64: val_loss did not improve from 0.22728\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2492 - mse: 0.2492 - rmse: 0.4992 - mae: 0.3083 - mape: 9.8286 - val_loss: 0.2674 - val_mse: 0.2674 - val_rmse: 0.5171 - val_mae: 0.3061 - val_mape: 9.2701 - lr: 0.0010\n",
      "Epoch 65/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2720 - mse: 0.2720 - rmse: 0.5216 - mae: 0.3258 - mape: 10.4131\n",
      "Epoch 65: val_loss did not improve from 0.22728\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2689 - mse: 0.2689 - rmse: 0.5186 - mae: 0.3237 - mape: 10.3586 - val_loss: 0.2452 - val_mse: 0.2452 - val_rmse: 0.4952 - val_mae: 0.2848 - val_mape: 8.5856 - lr: 0.0010\n",
      "Epoch 66/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2499 - mse: 0.2499 - rmse: 0.4999 - mae: 0.3040 - mape: 9.5775\n",
      "Epoch 66: val_loss did not improve from 0.22728\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2478 - mse: 0.2478 - rmse: 0.4978 - mae: 0.3028 - mape: 9.5299 - val_loss: 0.2339 - val_mse: 0.2339 - val_rmse: 0.4836 - val_mae: 0.3039 - val_mape: 9.7481 - lr: 0.0010\n",
      "Epoch 67/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2518 - mse: 0.2518 - rmse: 0.5018 - mae: 0.3095 - mape: 9.8101\n",
      "Epoch 67: val_loss improved from 0.22728 to 0.22447, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2494 - mse: 0.2494 - rmse: 0.4994 - mae: 0.3081 - mape: 9.7681 - val_loss: 0.2245 - val_mse: 0.2245 - val_rmse: 0.4738 - val_mae: 0.2849 - val_mape: 8.8774 - lr: 0.0010\n",
      "Epoch 68/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2504 - mse: 0.2504 - rmse: 0.5004 - mae: 0.3061 - mape: 9.7311\n",
      "Epoch 68: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2484 - mse: 0.2484 - rmse: 0.4984 - mae: 0.3056 - mape: 9.7113 - val_loss: 0.2976 - val_mse: 0.2976 - val_rmse: 0.5455 - val_mae: 0.3492 - val_mape: 10.6259 - lr: 0.0010\n",
      "Epoch 69/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2496 - mse: 0.2496 - rmse: 0.4996 - mae: 0.3068 - mape: 9.7409\n",
      "Epoch 69: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2565 - mse: 0.2565 - rmse: 0.5064 - mae: 0.3109 - mape: 9.8862 - val_loss: 0.3650 - val_mse: 0.3650 - val_rmse: 0.6041 - val_mae: 0.3702 - val_mape: 11.2626 - lr: 0.0010\n",
      "Epoch 70/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2472 - mse: 0.2472 - rmse: 0.4972 - mae: 0.3031 - mape: 9.5845\n",
      "Epoch 70: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2471 - mse: 0.2471 - rmse: 0.4971 - mae: 0.3028 - mape: 9.5611 - val_loss: 0.2328 - val_mse: 0.2328 - val_rmse: 0.4825 - val_mae: 0.3075 - val_mape: 10.1326 - lr: 0.0010\n",
      "Epoch 71/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2533 - mse: 0.2533 - rmse: 0.5033 - mae: 0.3092 - mape: 9.8346\n",
      "Epoch 71: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2516 - mse: 0.2516 - rmse: 0.5016 - mae: 0.3085 - mape: 9.8124 - val_loss: 0.2555 - val_mse: 0.2555 - val_rmse: 0.5054 - val_mae: 0.2966 - val_mape: 8.9664 - lr: 0.0010\n",
      "Epoch 72/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2490 - mse: 0.2490 - rmse: 0.4990 - mae: 0.3036 - mape: 9.5774\n",
      "Epoch 72: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2485 - mse: 0.2485 - rmse: 0.4985 - mae: 0.3045 - mape: 9.6199 - val_loss: 0.4254 - val_mse: 0.4254 - val_rmse: 0.6522 - val_mae: 0.4090 - val_mape: 14.2943 - lr: 0.0010\n",
      "Epoch 73/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2571 - mse: 0.2571 - rmse: 0.5070 - mae: 0.3154 - mape: 10.0278\n",
      "Epoch 73: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2555 - rmse: 0.5055 - mae: 0.3135 - mape: 9.9637 - val_loss: 0.5291 - val_mse: 0.5291 - val_rmse: 0.7274 - val_mae: 0.4107 - val_mape: 11.6730 - lr: 0.0010\n",
      "Epoch 74/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2637 - mse: 0.2637 - rmse: 0.5135 - mae: 0.3149 - mape: 10.0159\n",
      "Epoch 74: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2635 - mse: 0.2635 - rmse: 0.5133 - mae: 0.3150 - mape: 10.0221 - val_loss: 0.2332 - val_mse: 0.2332 - val_rmse: 0.4829 - val_mae: 0.2995 - val_mape: 9.6170 - lr: 0.0010\n",
      "Epoch 75/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2428 - mse: 0.2428 - rmse: 0.4928 - mae: 0.3008 - mape: 9.4905\n",
      "Epoch 75: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2437 - mse: 0.2437 - rmse: 0.4937 - mae: 0.3008 - mape: 9.4926 - val_loss: 0.2407 - val_mse: 0.2407 - val_rmse: 0.4906 - val_mae: 0.2880 - val_mape: 8.6920 - lr: 0.0010\n",
      "Epoch 76/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2667 - mse: 0.2667 - rmse: 0.5164 - mae: 0.3203 - mape: 10.2333\n",
      "Epoch 76: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2666 - mse: 0.2666 - rmse: 0.5164 - mae: 0.3210 - mape: 10.2439 - val_loss: 0.2630 - val_mse: 0.2630 - val_rmse: 0.5129 - val_mae: 0.3108 - val_mape: 9.5691 - lr: 0.0010\n",
      "Epoch 77/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2789 - mse: 0.2789 - rmse: 0.5281 - mae: 0.3303 - mape: 10.4975\n",
      "Epoch 77: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2778 - mse: 0.2778 - rmse: 0.5271 - mae: 0.3297 - mape: 10.4605 - val_loss: 0.3308 - val_mse: 0.3308 - val_rmse: 0.5752 - val_mae: 0.3584 - val_mape: 12.0830 - lr: 0.0010\n",
      "Epoch 78/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2555 - mse: 0.2555 - rmse: 0.5055 - mae: 0.3132 - mape: 9.9278\n",
      "Epoch 78: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2572 - mse: 0.2572 - rmse: 0.5072 - mae: 0.3138 - mape: 9.9412 - val_loss: 0.3253 - val_mse: 0.3253 - val_rmse: 0.5703 - val_mae: 0.3525 - val_mape: 11.8714 - lr: 0.0010\n",
      "Epoch 79/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2662 - mse: 0.2662 - rmse: 0.5159 - mae: 0.3204 - mape: 10.1704\n",
      "Epoch 79: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2666 - mse: 0.2666 - rmse: 0.5164 - mae: 0.3206 - mape: 10.1878 - val_loss: 0.2485 - val_mse: 0.2485 - val_rmse: 0.4985 - val_mae: 0.3040 - val_mape: 9.5452 - lr: 0.0010\n",
      "Epoch 80/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2646 - mse: 0.2646 - rmse: 0.5144 - mae: 0.3197 - mape: 10.1205\n",
      "Epoch 80: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2640 - mse: 0.2640 - rmse: 0.5138 - mae: 0.3198 - mape: 10.1179 - val_loss: 0.2391 - val_mse: 0.2391 - val_rmse: 0.4889 - val_mae: 0.3075 - val_mape: 9.9393 - lr: 0.0010\n",
      "Epoch 81/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2722 - mse: 0.2722 - rmse: 0.5217 - mae: 0.3248 - mape: 10.2629\n",
      "Epoch 81: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2718 - mse: 0.2718 - rmse: 0.5214 - mae: 0.3247 - mape: 10.2646 - val_loss: 0.2259 - val_mse: 0.2259 - val_rmse: 0.4752 - val_mae: 0.2826 - val_mape: 8.6343 - lr: 0.0010\n",
      "Epoch 82/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2494 - mse: 0.2494 - rmse: 0.4994 - mae: 0.3088 - mape: 9.7374\n",
      "Epoch 82: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2511 - mse: 0.2511 - rmse: 0.5011 - mae: 0.3098 - mape: 9.7634 - val_loss: 0.2728 - val_mse: 0.2728 - val_rmse: 0.5223 - val_mae: 0.3557 - val_mape: 11.4043 - lr: 0.0010\n",
      "Epoch 83/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2448 - mse: 0.2448 - rmse: 0.4948 - mae: 0.3054 - mape: 9.6503\n",
      "Epoch 83: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2435 - mse: 0.2435 - rmse: 0.4935 - mae: 0.3043 - mape: 9.6072 - val_loss: 0.2900 - val_mse: 0.2900 - val_rmse: 0.5385 - val_mae: 0.3286 - val_mape: 10.0155 - lr: 0.0010\n",
      "Epoch 84/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2541 - mse: 0.2541 - rmse: 0.5041 - mae: 0.3110 - mape: 9.8311\n",
      "Epoch 84: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2532 - mse: 0.2532 - rmse: 0.5032 - mae: 0.3104 - mape: 9.8347 - val_loss: 0.2299 - val_mse: 0.2299 - val_rmse: 0.4794 - val_mae: 0.2762 - val_mape: 8.3866 - lr: 0.0010\n",
      "Epoch 85/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2554 - mse: 0.2554 - rmse: 0.5054 - mae: 0.3177 - mape: 10.1818\n",
      "Epoch 85: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2555 - mse: 0.2555 - rmse: 0.5055 - mae: 0.3177 - mape: 10.1745 - val_loss: 0.2245 - val_mse: 0.2245 - val_rmse: 0.4738 - val_mae: 0.2864 - val_mape: 8.9310 - lr: 0.0010\n",
      "Epoch 86/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2419 - mse: 0.2419 - rmse: 0.4918 - mae: 0.3012 - mape: 9.5237\n",
      "Epoch 86: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2439 - mse: 0.2439 - rmse: 0.4939 - mae: 0.3029 - mape: 9.5945 - val_loss: 0.2346 - val_mse: 0.2346 - val_rmse: 0.4844 - val_mae: 0.3178 - val_mape: 10.1711 - lr: 0.0010\n",
      "Epoch 87/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2471 - mse: 0.2471 - rmse: 0.4971 - mae: 0.3064 - mape: 9.7115\n",
      "Epoch 87: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2475 - mse: 0.2475 - rmse: 0.4975 - mae: 0.3066 - mape: 9.7221 - val_loss: 0.2479 - val_mse: 0.2479 - val_rmse: 0.4979 - val_mae: 0.3435 - val_mape: 11.6029 - lr: 0.0010\n",
      "Epoch 88/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2569 - mse: 0.2569 - rmse: 0.5068 - mae: 0.3128 - mape: 9.9734\n",
      "Epoch 88: val_loss did not improve from 0.22447\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2553 - mse: 0.2553 - rmse: 0.5053 - mae: 0.3117 - mape: 9.9344 - val_loss: 0.2755 - val_mse: 0.2755 - val_rmse: 0.5249 - val_mae: 0.3074 - val_mape: 9.2149 - lr: 0.0010\n",
      "Epoch 89/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2413 - mse: 0.2413 - rmse: 0.4912 - mae: 0.3030 - mape: 9.6471\n",
      "Epoch 89: val_loss improved from 0.22447 to 0.22038, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2436 - mse: 0.2436 - rmse: 0.4935 - mae: 0.3032 - mape: 9.6493 - val_loss: 0.2204 - val_mse: 0.2204 - val_rmse: 0.4694 - val_mae: 0.2753 - val_mape: 8.6162 - lr: 0.0010\n",
      "Epoch 90/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2363 - mse: 0.2363 - rmse: 0.4861 - mae: 0.2942 - mape: 9.2601\n",
      "Epoch 90: val_loss did not improve from 0.22038\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2344 - mse: 0.2344 - rmse: 0.4842 - mae: 0.2920 - mape: 9.1919 - val_loss: 0.2275 - val_mse: 0.2275 - val_rmse: 0.4770 - val_mae: 0.2851 - val_mape: 8.9622 - lr: 0.0010\n",
      "Epoch 91/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2664 - mse: 0.2664 - rmse: 0.5162 - mae: 0.3245 - mape: 10.3511\n",
      "Epoch 91: val_loss did not improve from 0.22038\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2695 - mse: 0.2695 - rmse: 0.5191 - mae: 0.3250 - mape: 10.3543 - val_loss: 0.3770 - val_mse: 0.3770 - val_rmse: 0.6140 - val_mae: 0.4014 - val_mape: 13.6006 - lr: 0.0010\n",
      "Epoch 92/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2515 - mse: 0.2515 - rmse: 0.5015 - mae: 0.3133 - mape: 9.9766 \n",
      "Epoch 92: val_loss improved from 0.22038 to 0.21614, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2512 - mse: 0.2512 - rmse: 0.5012 - mae: 0.3120 - mape: 9.9120 - val_loss: 0.2161 - val_mse: 0.2161 - val_rmse: 0.4649 - val_mae: 0.2742 - val_mape: 8.5421 - lr: 0.0010\n",
      "Epoch 93/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2407 - mse: 0.2407 - rmse: 0.4907 - mae: 0.2966 - mape: 9.3405\n",
      "Epoch 93: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2404 - mse: 0.2404 - rmse: 0.4903 - mae: 0.2965 - mape: 9.3329 - val_loss: 0.2435 - val_mse: 0.2435 - val_rmse: 0.4935 - val_mae: 0.2983 - val_mape: 9.2151 - lr: 0.0010\n",
      "Epoch 94/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2428 - mse: 0.2428 - rmse: 0.4928 - mae: 0.3013 - mape: 9.5008\n",
      "Epoch 94: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2416 - mse: 0.2416 - rmse: 0.4916 - mae: 0.3009 - mape: 9.5035 - val_loss: 0.2477 - val_mse: 0.2477 - val_rmse: 0.4977 - val_mae: 0.3090 - val_mape: 10.1396 - lr: 0.0010\n",
      "Epoch 95/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2356 - mse: 0.2356 - rmse: 0.4854 - mae: 0.2972 - mape: 9.4138\n",
      "Epoch 95: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2351 - mse: 0.2351 - rmse: 0.4848 - mae: 0.2970 - mape: 9.4040 - val_loss: 0.2500 - val_mse: 0.2500 - val_rmse: 0.5000 - val_mae: 0.2860 - val_mape: 8.5282 - lr: 0.0010\n",
      "Epoch 96/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2381 - mse: 0.2381 - rmse: 0.4879 - mae: 0.2975 - mape: 9.4278\n",
      "Epoch 96: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2394 - mse: 0.2394 - rmse: 0.4893 - mae: 0.2992 - mape: 9.4893 - val_loss: 0.2634 - val_mse: 0.2634 - val_rmse: 0.5132 - val_mae: 0.3356 - val_mape: 11.0944 - lr: 0.0010\n",
      "Epoch 97/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2375 - mse: 0.2375 - rmse: 0.4873 - mae: 0.2988 - mape: 9.4758\n",
      "Epoch 97: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2370 - mse: 0.2370 - rmse: 0.4868 - mae: 0.2982 - mape: 9.4410 - val_loss: 0.2169 - val_mse: 0.2169 - val_rmse: 0.4657 - val_mae: 0.2776 - val_mape: 8.7655 - lr: 0.0010\n",
      "Epoch 98/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2316 - mse: 0.2316 - rmse: 0.4812 - mae: 0.2948 - mape: 9.3401\n",
      "Epoch 98: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2328 - mse: 0.2328 - rmse: 0.4825 - mae: 0.2944 - mape: 9.3167 - val_loss: 0.2273 - val_mse: 0.2273 - val_rmse: 0.4768 - val_mae: 0.2918 - val_mape: 9.4105 - lr: 0.0010\n",
      "Epoch 99/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2372 - mse: 0.2372 - rmse: 0.4870 - mae: 0.2983 - mape: 9.4849\n",
      "Epoch 99: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2418 - mse: 0.2418 - rmse: 0.4917 - mae: 0.3020 - mape: 9.6011 - val_loss: 0.2319 - val_mse: 0.2319 - val_rmse: 0.4816 - val_mae: 0.2915 - val_mape: 9.3138 - lr: 0.0010\n",
      "Epoch 100/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2340 - mse: 0.2340 - rmse: 0.4838 - mae: 0.2938 - mape: 9.2652\n",
      "Epoch 100: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2328 - mse: 0.2328 - rmse: 0.4825 - mae: 0.2931 - mape: 9.2529 - val_loss: 0.2449 - val_mse: 0.2449 - val_rmse: 0.4949 - val_mae: 0.2786 - val_mape: 8.3089 - lr: 0.0010\n",
      "Epoch 101/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2337 - mse: 0.2337 - rmse: 0.4834 - mae: 0.2940 - mape: 9.2765\n",
      "Epoch 101: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2322 - mse: 0.2322 - rmse: 0.4819 - mae: 0.2929 - mape: 9.2384 - val_loss: 0.2335 - val_mse: 0.2335 - val_rmse: 0.4832 - val_mae: 0.2886 - val_mape: 9.3154 - lr: 0.0010\n",
      "Epoch 102/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2299 - mse: 0.2299 - rmse: 0.4794 - mae: 0.2882 - mape: 9.0784\n",
      "Epoch 102: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2311 - mse: 0.2311 - rmse: 0.4807 - mae: 0.2890 - mape: 9.0989 - val_loss: 0.4554 - val_mse: 0.4554 - val_rmse: 0.6748 - val_mae: 0.4213 - val_mape: 12.6359 - lr: 0.0010\n",
      "Epoch 103/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.2510 - mse: 0.2510 - rmse: 0.5010 - mae: 0.3101 - mape: 9.9223\n",
      "Epoch 103: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2508 - mse: 0.2508 - rmse: 0.5008 - mae: 0.3100 - mape: 9.9186 - val_loss: 0.2194 - val_mse: 0.2194 - val_rmse: 0.4684 - val_mae: 0.2752 - val_mape: 8.4674 - lr: 0.0010\n",
      "Epoch 104/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2298 - mse: 0.2298 - rmse: 0.4794 - mae: 0.2881 - mape: 9.0552\n",
      "Epoch 104: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2285 - mse: 0.2285 - rmse: 0.4781 - mae: 0.2875 - mape: 9.0421 - val_loss: 0.2212 - val_mse: 0.2212 - val_rmse: 0.4703 - val_mae: 0.2747 - val_mape: 8.5138 - lr: 0.0010\n",
      "Epoch 105/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2375 - mse: 0.2375 - rmse: 0.4873 - mae: 0.2964 - mape: 9.3773\n",
      "Epoch 105: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2371 - mse: 0.2371 - rmse: 0.4869 - mae: 0.2953 - mape: 9.3397 - val_loss: 0.3539 - val_mse: 0.3539 - val_rmse: 0.5949 - val_mae: 0.3597 - val_mape: 10.4942 - lr: 0.0010\n",
      "Epoch 106/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2281 - mse: 0.2281 - rmse: 0.4776 - mae: 0.2881 - mape: 9.1181\n",
      "Epoch 106: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2283 - mse: 0.2283 - rmse: 0.4778 - mae: 0.2879 - mape: 9.0866 - val_loss: 0.2191 - val_mse: 0.2191 - val_rmse: 0.4680 - val_mae: 0.2782 - val_mape: 8.6669 - lr: 0.0010\n",
      "Epoch 107/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2423 - mse: 0.2423 - rmse: 0.4922 - mae: 0.2967 - mape: 9.3711\n",
      "Epoch 107: val_loss did not improve from 0.21614\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2430 - mse: 0.2430 - rmse: 0.4930 - mae: 0.2971 - mape: 9.3738 - val_loss: 0.2180 - val_mse: 0.2180 - val_rmse: 0.4669 - val_mae: 0.2784 - val_mape: 8.8231 - lr: 0.0010\n",
      "Epoch 108/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2363 - mse: 0.2363 - rmse: 0.4861 - mae: 0.2920 - mape: 9.2047\n",
      "Epoch 108: val_loss improved from 0.21614 to 0.21273, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2334 - mse: 0.2334 - rmse: 0.4831 - mae: 0.2903 - mape: 9.1457 - val_loss: 0.2127 - val_mse: 0.2127 - val_rmse: 0.4612 - val_mae: 0.2710 - val_mape: 8.4848 - lr: 0.0010\n",
      "Epoch 109/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2335 - mse: 0.2335 - rmse: 0.4832 - mae: 0.2921 - mape: 9.2723\n",
      "Epoch 109: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2329 - mse: 0.2329 - rmse: 0.4826 - mae: 0.2918 - mape: 9.2558 - val_loss: 0.2242 - val_mse: 0.2242 - val_rmse: 0.4735 - val_mae: 0.2881 - val_mape: 9.2939 - lr: 0.0010\n",
      "Epoch 110/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2588 - mse: 0.2588 - rmse: 0.5087 - mae: 0.3135 - mape: 10.0590\n",
      "Epoch 110: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2576 - mse: 0.2576 - rmse: 0.5075 - mae: 0.3128 - mape: 10.0383 - val_loss: 0.2320 - val_mse: 0.2320 - val_rmse: 0.4816 - val_mae: 0.2874 - val_mape: 8.9218 - lr: 0.0010\n",
      "Epoch 111/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2297 - mse: 0.2297 - rmse: 0.4793 - mae: 0.2880 - mape: 9.0385\n",
      "Epoch 111: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2285 - mse: 0.2285 - rmse: 0.4780 - mae: 0.2875 - mape: 9.0187 - val_loss: 0.2230 - val_mse: 0.2230 - val_rmse: 0.4722 - val_mae: 0.3047 - val_mape: 9.9495 - lr: 0.0010\n",
      "Epoch 112/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2283 - mse: 0.2283 - rmse: 0.4778 - mae: 0.2855 - mape: 8.9612\n",
      "Epoch 112: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2292 - mse: 0.2292 - rmse: 0.4788 - mae: 0.2861 - mape: 8.9846 - val_loss: 0.2292 - val_mse: 0.2292 - val_rmse: 0.4787 - val_mae: 0.2893 - val_mape: 9.2123 - lr: 0.0010\n",
      "Epoch 113/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2361 - mse: 0.2361 - rmse: 0.4859 - mae: 0.2963 - mape: 9.3684\n",
      "Epoch 113: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2348 - mse: 0.2348 - rmse: 0.4845 - mae: 0.2953 - mape: 9.3375 - val_loss: 0.2190 - val_mse: 0.2190 - val_rmse: 0.4680 - val_mae: 0.2673 - val_mape: 8.2636 - lr: 0.0010\n",
      "Epoch 114/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2357 - mse: 0.2357 - rmse: 0.4855 - mae: 0.2961 - mape: 9.4493\n",
      "Epoch 114: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2365 - mse: 0.2365 - rmse: 0.4863 - mae: 0.2974 - mape: 9.4774 - val_loss: 0.2253 - val_mse: 0.2253 - val_rmse: 0.4746 - val_mae: 0.2783 - val_mape: 8.5811 - lr: 0.0010\n",
      "Epoch 115/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2296 - mse: 0.2296 - rmse: 0.4791 - mae: 0.2869 - mape: 9.0579\n",
      "Epoch 115: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2293 - mse: 0.2293 - rmse: 0.4788 - mae: 0.2869 - mape: 9.0596 - val_loss: 0.2272 - val_mse: 0.2272 - val_rmse: 0.4766 - val_mae: 0.2819 - val_mape: 8.5975 - lr: 0.0010\n",
      "Epoch 116/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2273 - mse: 0.2273 - rmse: 0.4767 - mae: 0.2844 - mape: 8.9405\n",
      "Epoch 116: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2288 - mse: 0.2288 - rmse: 0.4783 - mae: 0.2854 - mape: 8.9749 - val_loss: 0.2241 - val_mse: 0.2241 - val_rmse: 0.4734 - val_mae: 0.2844 - val_mape: 9.2512 - lr: 0.0010\n",
      "Epoch 117/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2367 - mse: 0.2367 - rmse: 0.4865 - mae: 0.2913 - mape: 9.2161\n",
      "Epoch 117: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2357 - mse: 0.2357 - rmse: 0.4855 - mae: 0.2905 - mape: 9.1860 - val_loss: 0.2226 - val_mse: 0.2226 - val_rmse: 0.4718 - val_mae: 0.2779 - val_mape: 8.6337 - lr: 0.0010\n",
      "Epoch 118/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2393 - mse: 0.2393 - rmse: 0.4892 - mae: 0.2938 - mape: 9.2526\n",
      "Epoch 118: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2377 - mse: 0.2377 - rmse: 0.4875 - mae: 0.2937 - mape: 9.2865 - val_loss: 0.2259 - val_mse: 0.2259 - val_rmse: 0.4753 - val_mae: 0.2910 - val_mape: 9.2729 - lr: 0.0010\n",
      "Epoch 119/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2354 - mse: 0.2354 - rmse: 0.4852 - mae: 0.2911 - mape: 9.1678\n",
      "Epoch 119: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2333 - mse: 0.2333 - rmse: 0.4830 - mae: 0.2900 - mape: 9.1444 - val_loss: 0.2173 - val_mse: 0.2173 - val_rmse: 0.4662 - val_mae: 0.2756 - val_mape: 8.6356 - lr: 0.0010\n",
      "Epoch 120/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2312 - mse: 0.2312 - rmse: 0.4808 - mae: 0.2852 - mape: 8.9695\n",
      "Epoch 120: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2314 - mse: 0.2314 - rmse: 0.4810 - mae: 0.2852 - mape: 8.9662 - val_loss: 0.2272 - val_mse: 0.2272 - val_rmse: 0.4767 - val_mae: 0.2793 - val_mape: 9.0185 - lr: 0.0010\n",
      "Epoch 121/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2277 - mse: 0.2277 - rmse: 0.4772 - mae: 0.2846 - mape: 8.9559\n",
      "Epoch 121: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2272 - mse: 0.2272 - rmse: 0.4766 - mae: 0.2839 - mape: 8.9282 - val_loss: 0.2243 - val_mse: 0.2243 - val_rmse: 0.4736 - val_mae: 0.2749 - val_mape: 8.7161 - lr: 0.0010\n",
      "Epoch 122/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2263 - mse: 0.2263 - rmse: 0.4757 - mae: 0.2827 - mape: 8.8921\n",
      "Epoch 122: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2268 - mse: 0.2268 - rmse: 0.4763 - mae: 0.2831 - mape: 8.9068 - val_loss: 0.2138 - val_mse: 0.2138 - val_rmse: 0.4624 - val_mae: 0.2675 - val_mape: 8.3111 - lr: 0.0010\n",
      "Epoch 123/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2267 - mse: 0.2267 - rmse: 0.4761 - mae: 0.2827 - mape: 8.8814\n",
      "Epoch 123: val_loss did not improve from 0.21273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2267 - mse: 0.2267 - rmse: 0.4761 - mae: 0.2827 - mape: 8.8814 - val_loss: 0.2254 - val_mse: 0.2254 - val_rmse: 0.4747 - val_mae: 0.2820 - val_mape: 8.7955 - lr: 0.0010\n",
      "Epoch 124/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2344 - mse: 0.2344 - rmse: 0.4841 - mae: 0.2893 - mape: 9.1321\n",
      "Epoch 124: val_loss improved from 0.21273 to 0.21155, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2342 - mse: 0.2342 - rmse: 0.4840 - mae: 0.2889 - mape: 9.1029 - val_loss: 0.2115 - val_mse: 0.2115 - val_rmse: 0.4599 - val_mae: 0.2719 - val_mape: 8.6545 - lr: 0.0010\n",
      "Epoch 125/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2221 - mse: 0.2221 - rmse: 0.4712 - mae: 0.2784 - mape: 8.7835\n",
      "Epoch 125: val_loss did not improve from 0.21155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2234 - mse: 0.2234 - rmse: 0.4726 - mae: 0.2783 - mape: 8.7736 - val_loss: 0.2320 - val_mse: 0.2320 - val_rmse: 0.4816 - val_mae: 0.3071 - val_mape: 9.8313 - lr: 0.0010\n",
      "Epoch 126/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2225 - mse: 0.2225 - rmse: 0.4716 - mae: 0.2782 - mape: 8.7390\n",
      "Epoch 126: val_loss did not improve from 0.21155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2236 - mse: 0.2236 - rmse: 0.4729 - mae: 0.2800 - mape: 8.7967 - val_loss: 0.2274 - val_mse: 0.2274 - val_rmse: 0.4768 - val_mae: 0.2918 - val_mape: 9.2530 - lr: 0.0010\n",
      "Epoch 127/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2312 - mse: 0.2312 - rmse: 0.4809 - mae: 0.2853 - mape: 8.9527\n",
      "Epoch 127: val_loss did not improve from 0.21155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2312 - mse: 0.2312 - rmse: 0.4808 - mae: 0.2852 - mape: 8.9567 - val_loss: 0.2274 - val_mse: 0.2274 - val_rmse: 0.4769 - val_mae: 0.2779 - val_mape: 8.7827 - lr: 0.0010\n",
      "Epoch 128/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2282 - mse: 0.2282 - rmse: 0.4777 - mae: 0.2822 - mape: 8.8425\n",
      "Epoch 128: val_loss did not improve from 0.21155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2282 - mse: 0.2282 - rmse: 0.4777 - mae: 0.2822 - mape: 8.8434 - val_loss: 0.2245 - val_mse: 0.2245 - val_rmse: 0.4738 - val_mae: 0.2745 - val_mape: 8.4213 - lr: 0.0010\n",
      "Epoch 129/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2256 - mse: 0.2256 - rmse: 0.4750 - mae: 0.2826 - mape: 8.9264\n",
      "Epoch 129: val_loss did not improve from 0.21155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2264 - mse: 0.2264 - rmse: 0.4759 - mae: 0.2826 - mape: 8.9095 - val_loss: 0.2218 - val_mse: 0.2218 - val_rmse: 0.4709 - val_mae: 0.2837 - val_mape: 9.0990 - lr: 0.0010\n",
      "Epoch 130/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2242 - mse: 0.2242 - rmse: 0.4735 - mae: 0.2785 - mape: 8.7413\n",
      "Epoch 130: val_loss did not improve from 0.21155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2245 - mse: 0.2245 - rmse: 0.4738 - mae: 0.2782 - mape: 8.7320 - val_loss: 0.2129 - val_mse: 0.2129 - val_rmse: 0.4614 - val_mae: 0.2689 - val_mape: 8.3436 - lr: 0.0010\n",
      "Epoch 131/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2255 - mse: 0.2255 - rmse: 0.4749 - mae: 0.2818 - mape: 8.8535\n",
      "Epoch 131: val_loss improved from 0.21155 to 0.21067, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2258 - mse: 0.2258 - rmse: 0.4752 - mae: 0.2813 - mape: 8.8252 - val_loss: 0.2107 - val_mse: 0.2107 - val_rmse: 0.4590 - val_mae: 0.2633 - val_mape: 8.3088 - lr: 0.0010\n",
      "Epoch 132/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2249 - mse: 0.2249 - rmse: 0.4742 - mae: 0.2787 - mape: 8.7569\n",
      "Epoch 132: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2257 - mse: 0.2257 - rmse: 0.4751 - mae: 0.2790 - mape: 8.7678 - val_loss: 0.2245 - val_mse: 0.2245 - val_rmse: 0.4738 - val_mae: 0.2688 - val_mape: 8.1425 - lr: 0.0010\n",
      "Epoch 133/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2326 - mse: 0.2326 - rmse: 0.4823 - mae: 0.2868 - mape: 9.0360\n",
      "Epoch 133: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2327 - mse: 0.2327 - rmse: 0.4824 - mae: 0.2876 - mape: 9.0616 - val_loss: 0.2440 - val_mse: 0.2440 - val_rmse: 0.4939 - val_mae: 0.3136 - val_mape: 9.9187 - lr: 0.0010\n",
      "Epoch 134/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2298 - mse: 0.2298 - rmse: 0.4794 - mae: 0.2844 - mape: 8.9400\n",
      "Epoch 134: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2295 - mse: 0.2295 - rmse: 0.4791 - mae: 0.2842 - mape: 8.9319 - val_loss: 0.2197 - val_mse: 0.2197 - val_rmse: 0.4687 - val_mae: 0.2826 - val_mape: 9.1296 - lr: 0.0010\n",
      "Epoch 135/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2294 - mse: 0.2294 - rmse: 0.4790 - mae: 0.2846 - mape: 8.9343\n",
      "Epoch 135: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2292 - mse: 0.2292 - rmse: 0.4787 - mae: 0.2846 - mape: 8.9425 - val_loss: 0.2223 - val_mse: 0.2223 - val_rmse: 0.4715 - val_mae: 0.2831 - val_mape: 8.8962 - lr: 0.0010\n",
      "Epoch 136/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2406 - mse: 0.2406 - rmse: 0.4905 - mae: 0.2936 - mape: 9.2809\n",
      "Epoch 136: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2383 - mse: 0.2383 - rmse: 0.4882 - mae: 0.2925 - mape: 9.2510 - val_loss: 0.2319 - val_mse: 0.2319 - val_rmse: 0.4815 - val_mae: 0.2951 - val_mape: 9.1981 - lr: 0.0010\n",
      "Epoch 137/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2295 - mse: 0.2295 - rmse: 0.4791 - mae: 0.2844 - mape: 8.9673\n",
      "Epoch 137: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2283 - mse: 0.2283 - rmse: 0.4778 - mae: 0.2834 - mape: 8.9461 - val_loss: 0.2761 - val_mse: 0.2761 - val_rmse: 0.5255 - val_mae: 0.3052 - val_mape: 9.1907 - lr: 0.0010\n",
      "Epoch 138/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2252 - mse: 0.2252 - rmse: 0.4745 - mae: 0.2817 - mape: 8.8595\n",
      "Epoch 138: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2253 - mse: 0.2253 - rmse: 0.4747 - mae: 0.2819 - mape: 8.8633 - val_loss: 0.2209 - val_mse: 0.2209 - val_rmse: 0.4700 - val_mae: 0.2945 - val_mape: 9.5453 - lr: 0.0010\n",
      "Epoch 139/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2290 - mse: 0.2290 - rmse: 0.4785 - mae: 0.2851 - mape: 8.9574\n",
      "Epoch 139: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2282 - mse: 0.2282 - rmse: 0.4778 - mae: 0.2850 - mape: 8.9575 - val_loss: 0.2161 - val_mse: 0.2161 - val_rmse: 0.4649 - val_mae: 0.2858 - val_mape: 9.1162 - lr: 0.0010\n",
      "Epoch 140/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2343 - mse: 0.2343 - rmse: 0.4840 - mae: 0.2896 - mape: 9.1351\n",
      "Epoch 140: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2323 - mse: 0.2323 - rmse: 0.4820 - mae: 0.2882 - mape: 9.0913 - val_loss: 0.2256 - val_mse: 0.2256 - val_rmse: 0.4749 - val_mae: 0.3082 - val_mape: 10.1088 - lr: 0.0010\n",
      "Epoch 141/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2310 - mse: 0.2310 - rmse: 0.4807 - mae: 0.2868 - mape: 9.0414\n",
      "Epoch 141: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2305 - mse: 0.2305 - rmse: 0.4801 - mae: 0.2866 - mape: 9.0449 - val_loss: 0.2249 - val_mse: 0.2249 - val_rmse: 0.4743 - val_mae: 0.2813 - val_mape: 8.7729 - lr: 0.0010\n",
      "Epoch 142/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2301 - mse: 0.2301 - rmse: 0.4797 - mae: 0.2836 - mape: 8.9386\n",
      "Epoch 142: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2290 - mse: 0.2290 - rmse: 0.4785 - mae: 0.2830 - mape: 8.9185 - val_loss: 0.2251 - val_mse: 0.2251 - val_rmse: 0.4745 - val_mae: 0.2701 - val_mape: 8.1777 - lr: 0.0010\n",
      "Epoch 143/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2317 - mse: 0.2317 - rmse: 0.4814 - mae: 0.2854 - mape: 8.9893\n",
      "Epoch 143: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2313 - mse: 0.2313 - rmse: 0.4809 - mae: 0.2850 - mape: 8.9703 - val_loss: 0.2418 - val_mse: 0.2418 - val_rmse: 0.4918 - val_mae: 0.2909 - val_mape: 9.3714 - lr: 0.0010\n",
      "Epoch 144/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2271 - mse: 0.2271 - rmse: 0.4765 - mae: 0.2813 - mape: 8.8564\n",
      "Epoch 144: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2271 - mse: 0.2271 - rmse: 0.4765 - mae: 0.2812 - mape: 8.8571 - val_loss: 0.2163 - val_mse: 0.2163 - val_rmse: 0.4651 - val_mae: 0.2720 - val_mape: 8.3631 - lr: 0.0010\n",
      "Epoch 145/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2228 - mse: 0.2228 - rmse: 0.4720 - mae: 0.2756 - mape: 8.6404\n",
      "Epoch 145: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2228 - mse: 0.2228 - rmse: 0.4720 - mae: 0.2756 - mape: 8.6404 - val_loss: 0.2133 - val_mse: 0.2133 - val_rmse: 0.4619 - val_mae: 0.2711 - val_mape: 8.7738 - lr: 0.0010\n",
      "Epoch 146/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2269 - mse: 0.2269 - rmse: 0.4763 - mae: 0.2803 - mape: 8.8238\n",
      "Epoch 146: val_loss did not improve from 0.21067\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2267 - mse: 0.2267 - rmse: 0.4762 - mae: 0.2804 - mape: 8.8255 - val_loss: 0.2121 - val_mse: 0.2121 - val_rmse: 0.4605 - val_mae: 0.2657 - val_mape: 8.2050 - lr: 0.0010\n",
      "Epoch 147/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2206 - mse: 0.2206 - rmse: 0.4697 - mae: 0.2767 - mape: 8.6917\n",
      "Epoch 147: val_loss improved from 0.21067 to 0.20884, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2219 - mse: 0.2219 - rmse: 0.4711 - mae: 0.2772 - mape: 8.7035 - val_loss: 0.2088 - val_mse: 0.2088 - val_rmse: 0.4570 - val_mae: 0.2664 - val_mape: 8.3153 - lr: 0.0010\n",
      "Epoch 148/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2288 - mse: 0.2288 - rmse: 0.4783 - mae: 0.2827 - mape: 8.8863\n",
      "Epoch 148: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2280 - mse: 0.2280 - rmse: 0.4775 - mae: 0.2819 - mape: 8.8579 - val_loss: 0.2163 - val_mse: 0.2163 - val_rmse: 0.4651 - val_mae: 0.2804 - val_mape: 8.9659 - lr: 0.0010\n",
      "Epoch 149/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2197 - mse: 0.2197 - rmse: 0.4687 - mae: 0.2733 - mape: 8.5855\n",
      "Epoch 149: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2207 - mse: 0.2207 - rmse: 0.4698 - mae: 0.2742 - mape: 8.6047 - val_loss: 0.2094 - val_mse: 0.2094 - val_rmse: 0.4576 - val_mae: 0.2646 - val_mape: 8.1863 - lr: 0.0010\n",
      "Epoch 150/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2212 - mse: 0.2212 - rmse: 0.4703 - mae: 0.2758 - mape: 8.6648\n",
      "Epoch 150: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2233 - mse: 0.2233 - rmse: 0.4725 - mae: 0.2775 - mape: 8.7119 - val_loss: 0.2134 - val_mse: 0.2134 - val_rmse: 0.4620 - val_mae: 0.2704 - val_mape: 8.5276 - lr: 0.0010\n",
      "Epoch 151/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2232 - mse: 0.2232 - rmse: 0.4724 - mae: 0.2811 - mape: 8.8571\n",
      "Epoch 151: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2221 - mse: 0.2221 - rmse: 0.4713 - mae: 0.2805 - mape: 8.8333 - val_loss: 0.2162 - val_mse: 0.2162 - val_rmse: 0.4650 - val_mae: 0.2730 - val_mape: 8.4813 - lr: 0.0010\n",
      "Epoch 152/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2255 - mse: 0.2255 - rmse: 0.4749 - mae: 0.2803 - mape: 8.8380\n",
      "Epoch 152: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2240 - mse: 0.2240 - rmse: 0.4732 - mae: 0.2790 - mape: 8.7963 - val_loss: 0.2150 - val_mse: 0.2150 - val_rmse: 0.4637 - val_mae: 0.2630 - val_mape: 8.2063 - lr: 0.0010\n",
      "Epoch 153/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2229 - mse: 0.2229 - rmse: 0.4721 - mae: 0.2790 - mape: 8.7676\n",
      "Epoch 153: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2228 - mse: 0.2228 - rmse: 0.4720 - mae: 0.2790 - mape: 8.7660 - val_loss: 0.2231 - val_mse: 0.2231 - val_rmse: 0.4723 - val_mae: 0.2732 - val_mape: 8.4259 - lr: 0.0010\n",
      "Epoch 154/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2239 - mse: 0.2239 - rmse: 0.4732 - mae: 0.2782 - mape: 8.7296\n",
      "Epoch 154: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2239 - mse: 0.2239 - rmse: 0.4732 - mae: 0.2782 - mape: 8.7296 - val_loss: 0.2315 - val_mse: 0.2315 - val_rmse: 0.4811 - val_mae: 0.2791 - val_mape: 8.6254 - lr: 0.0010\n",
      "Epoch 155/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2260 - mse: 0.2260 - rmse: 0.4754 - mae: 0.2795 - mape: 8.7893\n",
      "Epoch 155: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2256 - mse: 0.2256 - rmse: 0.4749 - mae: 0.2794 - mape: 8.7730 - val_loss: 0.2320 - val_mse: 0.2320 - val_rmse: 0.4816 - val_mae: 0.2849 - val_mape: 9.2274 - lr: 0.0010\n",
      "Epoch 156/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2174 - mse: 0.2174 - rmse: 0.4663 - mae: 0.2744 - mape: 8.6166\n",
      "Epoch 156: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2184 - mse: 0.2184 - rmse: 0.4673 - mae: 0.2747 - mape: 8.6351 - val_loss: 0.2141 - val_mse: 0.2141 - val_rmse: 0.4627 - val_mae: 0.2650 - val_mape: 8.1094 - lr: 0.0010\n",
      "Epoch 157/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2226 - mse: 0.2226 - rmse: 0.4718 - mae: 0.2782 - mape: 8.7293\n",
      "Epoch 157: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2231 - mse: 0.2231 - rmse: 0.4723 - mae: 0.2782 - mape: 8.7287 - val_loss: 0.2176 - val_mse: 0.2176 - val_rmse: 0.4665 - val_mae: 0.2710 - val_mape: 8.2729 - lr: 0.0010\n",
      "Epoch 158/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2154 - mse: 0.2154 - rmse: 0.4641 - mae: 0.2701 - mape: 8.4492\n",
      "Epoch 158: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2187 - mse: 0.2187 - rmse: 0.4676 - mae: 0.2718 - mape: 8.4987 - val_loss: 0.2093 - val_mse: 0.2093 - val_rmse: 0.4575 - val_mae: 0.2641 - val_mape: 8.2887 - lr: 0.0010\n",
      "Epoch 159/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2184 - mse: 0.2184 - rmse: 0.4673 - mae: 0.2753 - mape: 8.6447\n",
      "Epoch 159: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2197 - mse: 0.2197 - rmse: 0.4687 - mae: 0.2755 - mape: 8.6399 - val_loss: 0.2225 - val_mse: 0.2225 - val_rmse: 0.4717 - val_mae: 0.2946 - val_mape: 9.6049 - lr: 0.0010\n",
      "Epoch 160/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2275 - mse: 0.2275 - rmse: 0.4769 - mae: 0.2810 - mape: 8.8613\n",
      "Epoch 160: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2275 - mse: 0.2275 - rmse: 0.4769 - mae: 0.2817 - mape: 8.8967 - val_loss: 0.2270 - val_mse: 0.2270 - val_rmse: 0.4764 - val_mae: 0.2791 - val_mape: 8.8653 - lr: 0.0010\n",
      "Epoch 161/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2170 - mse: 0.2170 - rmse: 0.4658 - mae: 0.2737 - mape: 8.6020\n",
      "Epoch 161: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2190 - mse: 0.2190 - rmse: 0.4680 - mae: 0.2750 - mape: 8.6535 - val_loss: 0.2311 - val_mse: 0.2311 - val_rmse: 0.4807 - val_mae: 0.2808 - val_mape: 9.1182 - lr: 0.0010\n",
      "Epoch 162/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2196 - mse: 0.2196 - rmse: 0.4687 - mae: 0.2741 - mape: 8.5926\n",
      "Epoch 162: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2206 - mse: 0.2206 - rmse: 0.4697 - mae: 0.2748 - mape: 8.6138 - val_loss: 0.2111 - val_mse: 0.2111 - val_rmse: 0.4595 - val_mae: 0.2682 - val_mape: 8.5348 - lr: 0.0010\n",
      "Epoch 163/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2177 - mse: 0.2177 - rmse: 0.4666 - mae: 0.2738 - mape: 8.5832\n",
      "Epoch 163: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2182 - mse: 0.2182 - rmse: 0.4671 - mae: 0.2742 - mape: 8.5939 - val_loss: 0.2383 - val_mse: 0.2383 - val_rmse: 0.4882 - val_mae: 0.2801 - val_mape: 8.3567 - lr: 0.0010\n",
      "Epoch 164/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2194 - mse: 0.2194 - rmse: 0.4684 - mae: 0.2748 - mape: 8.6272\n",
      "Epoch 164: val_loss did not improve from 0.20884\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2200 - mse: 0.2200 - rmse: 0.4690 - mae: 0.2748 - mape: 8.6326 - val_loss: 0.2426 - val_mse: 0.2426 - val_rmse: 0.4925 - val_mae: 0.3001 - val_mape: 9.1200 - lr: 0.0010\n",
      "Epoch 165/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2241 - mse: 0.2241 - rmse: 0.4734 - mae: 0.2788 - mape: 8.7455\n",
      "Epoch 165: val_loss improved from 0.20884 to 0.20589, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2227 - mse: 0.2227 - rmse: 0.4719 - mae: 0.2781 - mape: 8.7212 - val_loss: 0.2059 - val_mse: 0.2059 - val_rmse: 0.4538 - val_mae: 0.2622 - val_mape: 8.2643 - lr: 0.0010\n",
      "Epoch 166/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2222 - mse: 0.2222 - rmse: 0.4714 - mae: 0.2768 - mape: 8.6970\n",
      "Epoch 166: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2224 - mse: 0.2224 - rmse: 0.4716 - mae: 0.2769 - mape: 8.7019 - val_loss: 0.2179 - val_mse: 0.2179 - val_rmse: 0.4668 - val_mae: 0.2690 - val_mape: 8.5394 - lr: 0.0010\n",
      "Epoch 167/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2163 - mse: 0.2163 - rmse: 0.4651 - mae: 0.2697 - mape: 8.4430\n",
      "Epoch 167: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2162 - mse: 0.2162 - rmse: 0.4649 - mae: 0.2696 - mape: 8.4402 - val_loss: 0.2087 - val_mse: 0.2087 - val_rmse: 0.4568 - val_mae: 0.2642 - val_mape: 8.1988 - lr: 0.0010\n",
      "Epoch 168/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2226 - mse: 0.2226 - rmse: 0.4718 - mae: 0.2735 - mape: 8.5830\n",
      "Epoch 168: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2216 - mse: 0.2216 - rmse: 0.4707 - mae: 0.2734 - mape: 8.6023 - val_loss: 0.2163 - val_mse: 0.2163 - val_rmse: 0.4651 - val_mae: 0.2612 - val_mape: 7.8446 - lr: 0.0010\n",
      "Epoch 169/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2275 - mse: 0.2275 - rmse: 0.4770 - mae: 0.2814 - mape: 8.8410\n",
      "Epoch 169: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2274 - mse: 0.2274 - rmse: 0.4768 - mae: 0.2814 - mape: 8.8414 - val_loss: 0.2522 - val_mse: 0.2522 - val_rmse: 0.5022 - val_mae: 0.3003 - val_mape: 9.1542 - lr: 0.0010\n",
      "Epoch 170/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2216 - mse: 0.2216 - rmse: 0.4708 - mae: 0.2762 - mape: 8.6728\n",
      "Epoch 170: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2216 - mse: 0.2216 - rmse: 0.4707 - mae: 0.2758 - mape: 8.6544 - val_loss: 0.2139 - val_mse: 0.2139 - val_rmse: 0.4625 - val_mae: 0.2758 - val_mape: 8.9660 - lr: 0.0010\n",
      "Epoch 171/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2214 - mse: 0.2214 - rmse: 0.4705 - mae: 0.2766 - mape: 8.6908\n",
      "Epoch 171: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2207 - mse: 0.2207 - rmse: 0.4698 - mae: 0.2762 - mape: 8.6742 - val_loss: 0.2094 - val_mse: 0.2094 - val_rmse: 0.4576 - val_mae: 0.2664 - val_mape: 8.4726 - lr: 0.0010\n",
      "Epoch 172/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2195 - mse: 0.2195 - rmse: 0.4685 - mae: 0.2735 - mape: 8.5782\n",
      "Epoch 172: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2188 - mse: 0.2188 - rmse: 0.4678 - mae: 0.2731 - mape: 8.5685 - val_loss: 0.2200 - val_mse: 0.2200 - val_rmse: 0.4691 - val_mae: 0.2817 - val_mape: 8.8955 - lr: 0.0010\n",
      "Epoch 173/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2218 - mse: 0.2218 - rmse: 0.4710 - mae: 0.2766 - mape: 8.7221\n",
      "Epoch 173: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2221 - mse: 0.2221 - rmse: 0.4712 - mae: 0.2767 - mape: 8.7207 - val_loss: 0.2270 - val_mse: 0.2270 - val_rmse: 0.4764 - val_mae: 0.2772 - val_mape: 9.0170 - lr: 0.0010\n",
      "Epoch 174/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2166 - mse: 0.2166 - rmse: 0.4654 - mae: 0.2731 - mape: 8.5958\n",
      "Epoch 174: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2174 - mse: 0.2174 - rmse: 0.4662 - mae: 0.2734 - mape: 8.6015 - val_loss: 0.2080 - val_mse: 0.2080 - val_rmse: 0.4561 - val_mae: 0.2590 - val_mape: 8.1165 - lr: 0.0010\n",
      "Epoch 175/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2170 - mse: 0.2170 - rmse: 0.4659 - mae: 0.2746 - mape: 8.6156\n",
      "Epoch 175: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2184 - mse: 0.2184 - rmse: 0.4673 - mae: 0.2747 - mape: 8.6238 - val_loss: 0.2251 - val_mse: 0.2251 - val_rmse: 0.4744 - val_mae: 0.2722 - val_mape: 8.5827 - lr: 0.0010\n",
      "Epoch 176/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2237 - mse: 0.2237 - rmse: 0.4729 - mae: 0.2786 - mape: 8.7514\n",
      "Epoch 176: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2218 - mse: 0.2218 - rmse: 0.4710 - mae: 0.2774 - mape: 8.7084 - val_loss: 0.2081 - val_mse: 0.2081 - val_rmse: 0.4561 - val_mae: 0.2554 - val_mape: 7.9616 - lr: 0.0010\n",
      "Epoch 177/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2167 - mse: 0.2167 - rmse: 0.4655 - mae: 0.2701 - mape: 8.4623\n",
      "Epoch 177: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2169 - mse: 0.2169 - rmse: 0.4657 - mae: 0.2705 - mape: 8.4722 - val_loss: 0.2100 - val_mse: 0.2100 - val_rmse: 0.4583 - val_mae: 0.2716 - val_mape: 8.7412 - lr: 0.0010\n",
      "Epoch 178/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2145 - mse: 0.2145 - rmse: 0.4631 - mae: 0.2707 - mape: 8.4711\n",
      "Epoch 178: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2150 - mse: 0.2150 - rmse: 0.4637 - mae: 0.2709 - mape: 8.4835 - val_loss: 0.2214 - val_mse: 0.2214 - val_rmse: 0.4705 - val_mae: 0.2723 - val_mape: 8.2868 - lr: 0.0010\n",
      "Epoch 179/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2208 - mse: 0.2208 - rmse: 0.4698 - mae: 0.2759 - mape: 8.6437\n",
      "Epoch 179: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2202 - mse: 0.2202 - rmse: 0.4693 - mae: 0.2757 - mape: 8.6388 - val_loss: 0.2256 - val_mse: 0.2256 - val_rmse: 0.4749 - val_mae: 0.2791 - val_mape: 8.9791 - lr: 0.0010\n",
      "Epoch 180/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2200 - mse: 0.2200 - rmse: 0.4690 - mae: 0.2736 - mape: 8.5911\n",
      "Epoch 180: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2199 - mse: 0.2199 - rmse: 0.4690 - mae: 0.2736 - mape: 8.5928 - val_loss: 0.2087 - val_mse: 0.2087 - val_rmse: 0.4569 - val_mae: 0.2654 - val_mape: 8.2095 - lr: 0.0010\n",
      "Epoch 181/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2132 - mse: 0.2132 - rmse: 0.4617 - mae: 0.2691 - mape: 8.4288\n",
      "Epoch 181: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2171 - mse: 0.2171 - rmse: 0.4660 - mae: 0.2720 - mape: 8.5264 - val_loss: 0.2061 - val_mse: 0.2061 - val_rmse: 0.4540 - val_mae: 0.2659 - val_mape: 8.4206 - lr: 0.0010\n",
      "Epoch 182/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2142 - mse: 0.2142 - rmse: 0.4629 - mae: 0.2675 - mape: 8.3787\n",
      "Epoch 182: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2127 - mse: 0.2127 - rmse: 0.4612 - mae: 0.2670 - mape: 8.3578 - val_loss: 0.2100 - val_mse: 0.2100 - val_rmse: 0.4583 - val_mae: 0.2723 - val_mape: 8.7499 - lr: 0.0010\n",
      "Epoch 183/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2194 - mse: 0.2194 - rmse: 0.4684 - mae: 0.2732 - mape: 8.5444\n",
      "Epoch 183: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2181 - mse: 0.2181 - rmse: 0.4670 - mae: 0.2724 - mape: 8.5312 - val_loss: 0.2079 - val_mse: 0.2079 - val_rmse: 0.4560 - val_mae: 0.2732 - val_mape: 8.8467 - lr: 0.0010\n",
      "Epoch 184/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2195 - mse: 0.2195 - rmse: 0.4685 - mae: 0.2738 - mape: 8.5963\n",
      "Epoch 184: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2186 - mse: 0.2186 - rmse: 0.4676 - mae: 0.2733 - mape: 8.5783 - val_loss: 0.2102 - val_mse: 0.2102 - val_rmse: 0.4585 - val_mae: 0.2742 - val_mape: 8.7656 - lr: 0.0010\n",
      "Epoch 185/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2174 - mse: 0.2174 - rmse: 0.4662 - mae: 0.2722 - mape: 8.5407\n",
      "Epoch 185: val_loss did not improve from 0.20589\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2176 - mse: 0.2176 - rmse: 0.4665 - mae: 0.2723 - mape: 8.5494 - val_loss: 0.2068 - val_mse: 0.2068 - val_rmse: 0.4547 - val_mae: 0.2598 - val_mape: 8.1887 - lr: 0.0010\n",
      "Epoch 186/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2172 - mse: 0.2172 - rmse: 0.4661 - mae: 0.2710 - mape: 8.4855\n",
      "Epoch 186: val_loss improved from 0.20589 to 0.20497, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2172 - mse: 0.2172 - rmse: 0.4660 - mae: 0.2709 - mape: 8.4865 - val_loss: 0.2050 - val_mse: 0.2050 - val_rmse: 0.4527 - val_mae: 0.2561 - val_mape: 7.9651 - lr: 0.0010\n",
      "Epoch 187/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2217 - mse: 0.2217 - rmse: 0.4709 - mae: 0.2736 - mape: 8.5703\n",
      "Epoch 187: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2199 - mse: 0.2199 - rmse: 0.4689 - mae: 0.2727 - mape: 8.5510 - val_loss: 0.2088 - val_mse: 0.2088 - val_rmse: 0.4570 - val_mae: 0.2564 - val_mape: 7.8966 - lr: 0.0010\n",
      "Epoch 188/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2147 - mse: 0.2147 - rmse: 0.4634 - mae: 0.2682 - mape: 8.4080\n",
      "Epoch 188: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2137 - mse: 0.2137 - rmse: 0.4622 - mae: 0.2681 - mape: 8.4072 - val_loss: 0.2151 - val_mse: 0.2151 - val_rmse: 0.4638 - val_mae: 0.2644 - val_mape: 8.1291 - lr: 0.0010\n",
      "Epoch 189/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2173 - mse: 0.2173 - rmse: 0.4662 - mae: 0.2691 - mape: 8.4240\n",
      "Epoch 189: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2174 - mse: 0.2174 - rmse: 0.4662 - mae: 0.2692 - mape: 8.4271 - val_loss: 0.2059 - val_mse: 0.2059 - val_rmse: 0.4537 - val_mae: 0.2633 - val_mape: 8.2807 - lr: 0.0010\n",
      "Epoch 190/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2129 - mse: 0.2129 - rmse: 0.4614 - mae: 0.2665 - mape: 8.3623\n",
      "Epoch 190: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2131 - mse: 0.2131 - rmse: 0.4617 - mae: 0.2670 - mape: 8.3639 - val_loss: 0.2112 - val_mse: 0.2112 - val_rmse: 0.4596 - val_mae: 0.2618 - val_mape: 8.1222 - lr: 0.0010\n",
      "Epoch 191/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.2172 - mse: 0.2172 - rmse: 0.4661 - mae: 0.2699 - mape: 8.4811\n",
      "Epoch 191: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2173 - mse: 0.2173 - rmse: 0.4661 - mae: 0.2701 - mape: 8.4800 - val_loss: 0.2412 - val_mse: 0.2412 - val_rmse: 0.4912 - val_mae: 0.2930 - val_mape: 8.9662 - lr: 0.0010\n",
      "Epoch 192/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2189 - mse: 0.2189 - rmse: 0.4678 - mae: 0.2726 - mape: 8.5493\n",
      "Epoch 192: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2190 - mse: 0.2190 - rmse: 0.4680 - mae: 0.2727 - mape: 8.5595 - val_loss: 0.2080 - val_mse: 0.2080 - val_rmse: 0.4561 - val_mae: 0.2604 - val_mape: 8.0228 - lr: 0.0010\n",
      "Epoch 193/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2114 - mse: 0.2114 - rmse: 0.4597 - mae: 0.2663 - mape: 8.3633\n",
      "Epoch 193: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2137 - mse: 0.2137 - rmse: 0.4622 - mae: 0.2672 - mape: 8.3887 - val_loss: 0.2060 - val_mse: 0.2060 - val_rmse: 0.4539 - val_mae: 0.2683 - val_mape: 8.5474 - lr: 0.0010\n",
      "Epoch 194/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2162 - mse: 0.2162 - rmse: 0.4650 - mae: 0.2716 - mape: 8.5175\n",
      "Epoch 194: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2161 - mse: 0.2161 - rmse: 0.4649 - mae: 0.2716 - mape: 8.5152 - val_loss: 0.2056 - val_mse: 0.2056 - val_rmse: 0.4534 - val_mae: 0.2591 - val_mape: 8.0743 - lr: 0.0010\n",
      "Epoch 195/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2157 - mse: 0.2157 - rmse: 0.4644 - mae: 0.2714 - mape: 8.5064\n",
      "Epoch 195: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2155 - mse: 0.2155 - rmse: 0.4642 - mae: 0.2711 - mape: 8.4904 - val_loss: 0.2500 - val_mse: 0.2500 - val_rmse: 0.5000 - val_mae: 0.3039 - val_mape: 9.8342 - lr: 0.0010\n",
      "Epoch 196/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2257 - mse: 0.2257 - rmse: 0.4750 - mae: 0.2776 - mape: 8.7287\n",
      "Epoch 196: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2239 - mse: 0.2239 - rmse: 0.4732 - mae: 0.2761 - mape: 8.6747 - val_loss: 0.2175 - val_mse: 0.2175 - val_rmse: 0.4664 - val_mae: 0.2705 - val_mape: 8.7162 - lr: 0.0010\n",
      "Epoch 197/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2190 - mse: 0.2190 - rmse: 0.4679 - mae: 0.2722 - mape: 8.5328\n",
      "Epoch 197: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2191 - mse: 0.2191 - rmse: 0.4680 - mae: 0.2721 - mape: 8.5286 - val_loss: 0.2065 - val_mse: 0.2065 - val_rmse: 0.4544 - val_mae: 0.2584 - val_mape: 8.0675 - lr: 0.0010\n",
      "Epoch 198/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2177 - mse: 0.2177 - rmse: 0.4666 - mae: 0.2700 - mape: 8.4677\n",
      "Epoch 198: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2181 - mse: 0.2181 - rmse: 0.4670 - mae: 0.2702 - mape: 8.4706 - val_loss: 0.2195 - val_mse: 0.2195 - val_rmse: 0.4686 - val_mae: 0.2677 - val_mape: 8.1823 - lr: 0.0010\n",
      "Epoch 199/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2134 - mse: 0.2134 - rmse: 0.4620 - mae: 0.2683 - mape: 8.4049\n",
      "Epoch 199: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2153 - mse: 0.2153 - rmse: 0.4640 - mae: 0.2698 - mape: 8.4377 - val_loss: 0.2147 - val_mse: 0.2147 - val_rmse: 0.4633 - val_mae: 0.2897 - val_mape: 9.3410 - lr: 0.0010\n",
      "Epoch 200/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2146 - mse: 0.2146 - rmse: 0.4633 - mae: 0.2682 - mape: 8.4014\n",
      "Epoch 200: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2141 - mse: 0.2141 - rmse: 0.4627 - mae: 0.2676 - mape: 8.3826 - val_loss: 0.2116 - val_mse: 0.2116 - val_rmse: 0.4600 - val_mae: 0.2708 - val_mape: 8.6891 - lr: 0.0010\n",
      "Epoch 201/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2121 - mse: 0.2121 - rmse: 0.4605 - mae: 0.2668 - mape: 8.3595\n",
      "Epoch 201: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2144 - mse: 0.2144 - rmse: 0.4631 - mae: 0.2680 - mape: 8.3985 - val_loss: 0.2594 - val_mse: 0.2594 - val_rmse: 0.5093 - val_mae: 0.3120 - val_mape: 9.5622 - lr: 0.0010\n",
      "Epoch 202/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2271 - mse: 0.2271 - rmse: 0.4766 - mae: 0.2816 - mape: 8.8777\n",
      "Epoch 202: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2269 - mse: 0.2269 - rmse: 0.4763 - mae: 0.2812 - mape: 8.8600 - val_loss: 0.2114 - val_mse: 0.2114 - val_rmse: 0.4598 - val_mae: 0.2601 - val_mape: 8.0346 - lr: 0.0010\n",
      "Epoch 203/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2106 - mse: 0.2106 - rmse: 0.4589 - mae: 0.2645 - mape: 8.2781\n",
      "Epoch 203: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2103 - mse: 0.2103 - rmse: 0.4586 - mae: 0.2646 - mape: 8.2830 - val_loss: 0.2096 - val_mse: 0.2096 - val_rmse: 0.4579 - val_mae: 0.2686 - val_mape: 8.3685 - lr: 0.0010\n",
      "Epoch 204/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2204 - mse: 0.2204 - rmse: 0.4695 - mae: 0.2751 - mape: 8.6401\n",
      "Epoch 204: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2186 - mse: 0.2186 - rmse: 0.4675 - mae: 0.2743 - mape: 8.6209 - val_loss: 0.2053 - val_mse: 0.2053 - val_rmse: 0.4532 - val_mae: 0.2557 - val_mape: 7.8801 - lr: 0.0010\n",
      "Epoch 205/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2148 - mse: 0.2148 - rmse: 0.4635 - mae: 0.2687 - mape: 8.4464\n",
      "Epoch 205: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2148 - mse: 0.2148 - rmse: 0.4634 - mae: 0.2685 - mape: 8.4368 - val_loss: 0.2066 - val_mse: 0.2066 - val_rmse: 0.4546 - val_mae: 0.2599 - val_mape: 8.1170 - lr: 0.0010\n",
      "Epoch 206/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2147 - mse: 0.2147 - rmse: 0.4633 - mae: 0.2690 - mape: 8.4352\n",
      "Epoch 206: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2153 - mse: 0.2153 - rmse: 0.4641 - mae: 0.2697 - mape: 8.4524 - val_loss: 0.2275 - val_mse: 0.2275 - val_rmse: 0.4770 - val_mae: 0.3037 - val_mape: 9.9880 - lr: 0.0010\n",
      "Epoch 207/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2150 - mse: 0.2150 - rmse: 0.4637 - mae: 0.2700 - mape: 8.4722\n",
      "Epoch 207: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2165 - mse: 0.2165 - rmse: 0.4653 - mae: 0.2701 - mape: 8.4812 - val_loss: 0.2261 - val_mse: 0.2261 - val_rmse: 0.4755 - val_mae: 0.2863 - val_mape: 9.1373 - lr: 0.0010\n",
      "Epoch 208/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.2138 - mse: 0.2138 - rmse: 0.4624 - mae: 0.2679 - mape: 8.4215\n",
      "Epoch 208: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2139 - mse: 0.2139 - rmse: 0.4624 - mae: 0.2681 - mape: 8.4261 - val_loss: 0.2082 - val_mse: 0.2082 - val_rmse: 0.4563 - val_mae: 0.2745 - val_mape: 8.6552 - lr: 0.0010\n",
      "Epoch 209/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.2161 - mse: 0.2161 - rmse: 0.4649 - mae: 0.2718 - mape: 8.5126\n",
      "Epoch 209: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2181 - mse: 0.2181 - rmse: 0.4670 - mae: 0.2728 - mape: 8.5357 - val_loss: 0.2115 - val_mse: 0.2115 - val_rmse: 0.4599 - val_mae: 0.2699 - val_mape: 8.6170 - lr: 0.0010\n",
      "Epoch 210/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2115 - mse: 0.2115 - rmse: 0.4599 - mae: 0.2654 - mape: 8.3235\n",
      "Epoch 210: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2133 - mse: 0.2133 - rmse: 0.4618 - mae: 0.2670 - mape: 8.3644 - val_loss: 0.2250 - val_mse: 0.2250 - val_rmse: 0.4744 - val_mae: 0.2966 - val_mape: 9.8419 - lr: 0.0010\n",
      "Epoch 211/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2189 - mse: 0.2189 - rmse: 0.4679 - mae: 0.2710 - mape: 8.4956\n",
      "Epoch 211: val_loss did not improve from 0.20497\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2187 - mse: 0.2187 - rmse: 0.4677 - mae: 0.2707 - mape: 8.4874 - val_loss: 0.2151 - val_mse: 0.2151 - val_rmse: 0.4637 - val_mae: 0.2707 - val_mape: 8.5333 - lr: 0.0010\n",
      "Epoch 212/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2141 - mse: 0.2141 - rmse: 0.4627 - mae: 0.2687 - mape: 8.4196\n",
      "Epoch 212: val_loss improved from 0.20497 to 0.20371, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2147 - mse: 0.2147 - rmse: 0.4634 - mae: 0.2689 - mape: 8.4306 - val_loss: 0.2037 - val_mse: 0.2037 - val_rmse: 0.4513 - val_mae: 0.2570 - val_mape: 7.9679 - lr: 0.0010\n",
      "Epoch 213/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2121 - mse: 0.2121 - rmse: 0.4605 - mae: 0.2663 - mape: 8.3418\n",
      "Epoch 213: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2120 - mse: 0.2120 - rmse: 0.4605 - mae: 0.2662 - mape: 8.3363 - val_loss: 0.2056 - val_mse: 0.2056 - val_rmse: 0.4535 - val_mae: 0.2624 - val_mape: 8.2800 - lr: 0.0010\n",
      "Epoch 214/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2137 - mse: 0.2137 - rmse: 0.4623 - mae: 0.2672 - mape: 8.3737\n",
      "Epoch 214: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2137 - mse: 0.2137 - rmse: 0.4623 - mae: 0.2672 - mape: 8.3737 - val_loss: 0.2115 - val_mse: 0.2115 - val_rmse: 0.4599 - val_mae: 0.2654 - val_mape: 8.2071 - lr: 0.0010\n",
      "Epoch 215/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2193 - mse: 0.2193 - rmse: 0.4683 - mae: 0.2743 - mape: 8.6568\n",
      "Epoch 215: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2187 - mse: 0.2187 - rmse: 0.4677 - mae: 0.2739 - mape: 8.6326 - val_loss: 0.2101 - val_mse: 0.2101 - val_rmse: 0.4584 - val_mae: 0.2599 - val_mape: 8.0830 - lr: 0.0010\n",
      "Epoch 216/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2112 - mse: 0.2112 - rmse: 0.4596 - mae: 0.2646 - mape: 8.2965\n",
      "Epoch 216: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2115 - mse: 0.2115 - rmse: 0.4599 - mae: 0.2647 - mape: 8.2929 - val_loss: 0.2102 - val_mse: 0.2102 - val_rmse: 0.4585 - val_mae: 0.2650 - val_mape: 8.4861 - lr: 0.0010\n",
      "Epoch 217/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2159 - mse: 0.2159 - rmse: 0.4646 - mae: 0.2692 - mape: 8.4257\n",
      "Epoch 217: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2163 - mse: 0.2163 - rmse: 0.4650 - mae: 0.2693 - mape: 8.4219 - val_loss: 0.2173 - val_mse: 0.2173 - val_rmse: 0.4661 - val_mae: 0.2940 - val_mape: 9.7663 - lr: 0.0010\n",
      "Epoch 218/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.2124 - mse: 0.2124 - rmse: 0.4609 - mae: 0.2671 - mape: 8.4020\n",
      "Epoch 218: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2115 - mse: 0.2115 - rmse: 0.4599 - mae: 0.2664 - mape: 8.3796 - val_loss: 0.2067 - val_mse: 0.2067 - val_rmse: 0.4547 - val_mae: 0.2574 - val_mape: 7.8731 - lr: 0.0010\n",
      "Epoch 219/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2133 - mse: 0.2133 - rmse: 0.4618 - mae: 0.2663 - mape: 8.3241\n",
      "Epoch 219: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2129 - mse: 0.2129 - rmse: 0.4614 - mae: 0.2663 - mape: 8.3324 - val_loss: 0.2358 - val_mse: 0.2358 - val_rmse: 0.4856 - val_mae: 0.2923 - val_mape: 9.5757 - lr: 0.0010\n",
      "Epoch 220/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2099 - mse: 0.2099 - rmse: 0.4582 - mae: 0.2651 - mape: 8.3362\n",
      "Epoch 220: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2126 - mse: 0.2126 - rmse: 0.4611 - mae: 0.2675 - mape: 8.3890 - val_loss: 0.2116 - val_mse: 0.2116 - val_rmse: 0.4600 - val_mae: 0.2742 - val_mape: 8.6972 - lr: 0.0010\n",
      "Epoch 221/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2152 - mse: 0.2152 - rmse: 0.4639 - mae: 0.2713 - mape: 8.5079\n",
      "Epoch 221: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2164 - mse: 0.2164 - rmse: 0.4652 - mae: 0.2712 - mape: 8.5018 - val_loss: 0.2133 - val_mse: 0.2133 - val_rmse: 0.4618 - val_mae: 0.2789 - val_mape: 8.8248 - lr: 0.0010\n",
      "Epoch 222/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2139 - mse: 0.2139 - rmse: 0.4625 - mae: 0.2688 - mape: 8.4254\n",
      "Epoch 222: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2160 - mse: 0.2160 - rmse: 0.4647 - mae: 0.2707 - mape: 8.5059 - val_loss: 0.2227 - val_mse: 0.2227 - val_rmse: 0.4720 - val_mae: 0.2884 - val_mape: 9.5231 - lr: 0.0010\n",
      "Epoch 223/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2130 - mse: 0.2130 - rmse: 0.4616 - mae: 0.2645 - mape: 8.2932\n",
      "Epoch 223: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2122 - mse: 0.2122 - rmse: 0.4606 - mae: 0.2645 - mape: 8.2888 - val_loss: 0.2066 - val_mse: 0.2066 - val_rmse: 0.4546 - val_mae: 0.2637 - val_mape: 8.3848 - lr: 0.0010\n",
      "Epoch 224/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2131 - mse: 0.2131 - rmse: 0.4616 - mae: 0.2669 - mape: 8.3600\n",
      "Epoch 224: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2120 - mse: 0.2120 - rmse: 0.4605 - mae: 0.2664 - mape: 8.3345 - val_loss: 0.2099 - val_mse: 0.2099 - val_rmse: 0.4581 - val_mae: 0.2624 - val_mape: 8.2282 - lr: 0.0010\n",
      "Epoch 225/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2092 - mse: 0.2092 - rmse: 0.4574 - mae: 0.2641 - mape: 8.2617\n",
      "Epoch 225: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2102 - mse: 0.2102 - rmse: 0.4585 - mae: 0.2644 - mape: 8.2726 - val_loss: 0.2086 - val_mse: 0.2086 - val_rmse: 0.4567 - val_mae: 0.2566 - val_mape: 7.8575 - lr: 0.0010\n",
      "Epoch 226/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2146 - mse: 0.2146 - rmse: 0.4632 - mae: 0.2689 - mape: 8.3931\n",
      "Epoch 226: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2154 - mse: 0.2154 - rmse: 0.4641 - mae: 0.2692 - mape: 8.4180 - val_loss: 0.2109 - val_mse: 0.2109 - val_rmse: 0.4593 - val_mae: 0.2732 - val_mape: 8.6303 - lr: 0.0010\n",
      "Epoch 227/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2134 - mse: 0.2134 - rmse: 0.4620 - mae: 0.2685 - mape: 8.4223\n",
      "Epoch 227: val_loss did not improve from 0.20371\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2137 - mse: 0.2137 - rmse: 0.4623 - mae: 0.2684 - mape: 8.4267 - val_loss: 0.2098 - val_mse: 0.2098 - val_rmse: 0.4580 - val_mae: 0.2668 - val_mape: 8.3022 - lr: 0.0010\n",
      "Epoch 228/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2166 - mse: 0.2166 - rmse: 0.4654 - mae: 0.2679 - mape: 8.3918\n",
      "Epoch 228: val_loss improved from 0.20371 to 0.20288, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2150 - mse: 0.2150 - rmse: 0.4637 - mae: 0.2674 - mape: 8.3753 - val_loss: 0.2029 - val_mse: 0.2029 - val_rmse: 0.4504 - val_mae: 0.2577 - val_mape: 8.0557 - lr: 0.0010\n",
      "Epoch 229/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.2117 - mse: 0.2117 - rmse: 0.4601 - mae: 0.2653 - mape: 8.3118\n",
      "Epoch 229: val_loss did not improve from 0.20288\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2115 - mse: 0.2115 - rmse: 0.4599 - mae: 0.2653 - mape: 8.3116 - val_loss: 0.2102 - val_mse: 0.2102 - val_rmse: 0.4585 - val_mae: 0.2712 - val_mape: 8.5250 - lr: 0.0010\n",
      "Epoch 230/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2120 - mse: 0.2120 - rmse: 0.4604 - mae: 0.2656 - mape: 8.3254\n",
      "Epoch 230: val_loss did not improve from 0.20288\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2123 - mse: 0.2123 - rmse: 0.4608 - mae: 0.2657 - mape: 8.3274 - val_loss: 0.2056 - val_mse: 0.2056 - val_rmse: 0.4535 - val_mae: 0.2605 - val_mape: 8.1156 - lr: 0.0010\n",
      "Epoch 231/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2104 - mse: 0.2104 - rmse: 0.4587 - mae: 0.2668 - mape: 8.3622\n",
      "Epoch 231: val_loss did not improve from 0.20288\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2127 - mse: 0.2127 - rmse: 0.4612 - mae: 0.2671 - mape: 8.3599 - val_loss: 0.2036 - val_mse: 0.2036 - val_rmse: 0.4512 - val_mae: 0.2526 - val_mape: 7.8772 - lr: 0.0010\n",
      "Epoch 232/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2113 - mse: 0.2113 - rmse: 0.4597 - mae: 0.2656 - mape: 8.3375\n",
      "Epoch 232: val_loss did not improve from 0.20288\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2132 - mse: 0.2132 - rmse: 0.4617 - mae: 0.2667 - mape: 8.3510 - val_loss: 0.2057 - val_mse: 0.2057 - val_rmse: 0.4536 - val_mae: 0.2609 - val_mape: 8.0202 - lr: 0.0010\n",
      "Epoch 233/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2144 - mse: 0.2144 - rmse: 0.4630 - mae: 0.2706 - mape: 8.5109\n",
      "Epoch 233: val_loss did not improve from 0.20288\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2146 - mse: 0.2146 - rmse: 0.4632 - mae: 0.2708 - mape: 8.5177 - val_loss: 0.2078 - val_mse: 0.2078 - val_rmse: 0.4559 - val_mae: 0.2621 - val_mape: 8.1333 - lr: 0.0010\n",
      "Epoch 234/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.2144 - mse: 0.2144 - rmse: 0.4630 - mae: 0.2669 - mape: 8.3631\n",
      "Epoch 234: val_loss did not improve from 0.20288\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2146 - mse: 0.2146 - rmse: 0.4632 - mae: 0.2665 - mape: 8.3482 - val_loss: 0.2053 - val_mse: 0.2053 - val_rmse: 0.4531 - val_mae: 0.2636 - val_mape: 8.2527 - lr: 0.0010\n",
      "Epoch 235/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2119 - mse: 0.2119 - rmse: 0.4603 - mae: 0.2655 - mape: 8.3209\n",
      "Epoch 235: val_loss did not improve from 0.20288\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2118 - mse: 0.2118 - rmse: 0.4602 - mae: 0.2649 - mape: 8.2929 - val_loss: 0.2054 - val_mse: 0.2054 - val_rmse: 0.4532 - val_mae: 0.2583 - val_mape: 8.1815 - lr: 0.0010\n",
      "Epoch 236/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2117 - mse: 0.2117 - rmse: 0.4601 - mae: 0.2668 - mape: 8.3805\n",
      "Epoch 236: val_loss did not improve from 0.20288\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2121 - mse: 0.2121 - rmse: 0.4605 - mae: 0.2668 - mape: 8.3841 - val_loss: 0.2169 - val_mse: 0.2169 - val_rmse: 0.4657 - val_mae: 0.2719 - val_mape: 8.7714 - lr: 0.0010\n",
      "Epoch 237/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2119 - mse: 0.2119 - rmse: 0.4603 - mae: 0.2654 - mape: 8.3299\n",
      "Epoch 237: val_loss did not improve from 0.20288\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2125 - mse: 0.2125 - rmse: 0.4610 - mae: 0.2657 - mape: 8.3283 - val_loss: 0.2182 - val_mse: 0.2182 - val_rmse: 0.4671 - val_mae: 0.2702 - val_mape: 8.7543 - lr: 0.0010\n",
      "Epoch 238/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2144 - mse: 0.2144 - rmse: 0.4631 - mae: 0.2686 - mape: 8.4211\n",
      "Epoch 238: val_loss did not improve from 0.20288\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2145 - mse: 0.2145 - rmse: 0.4632 - mae: 0.2688 - mape: 8.4314 - val_loss: 0.2102 - val_mse: 0.2102 - val_rmse: 0.4585 - val_mae: 0.2619 - val_mape: 8.1586 - lr: 0.0010\n",
      "Epoch 239/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2128 - mse: 0.2128 - rmse: 0.4613 - mae: 0.2667 - mape: 8.3749\n",
      "Epoch 239: val_loss did not improve from 0.20288\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2148 - mse: 0.2148 - rmse: 0.4634 - mae: 0.2682 - mape: 8.4142 - val_loss: 0.2097 - val_mse: 0.2097 - val_rmse: 0.4580 - val_mae: 0.2720 - val_mape: 8.5999 - lr: 0.0010\n",
      "Epoch 240/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.2118 - mse: 0.2118 - rmse: 0.4602 - mae: 0.2649 - mape: 8.2824\n",
      "Epoch 240: val_loss did not improve from 0.20288\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2114 - mse: 0.2114 - rmse: 0.4598 - mae: 0.2651 - mape: 8.2908 - val_loss: 0.2359 - val_mse: 0.2359 - val_rmse: 0.4857 - val_mae: 0.2852 - val_mape: 8.5418 - lr: 0.0010\n",
      "Epoch 241/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2156 - mse: 0.2156 - rmse: 0.4643 - mae: 0.2720 - mape: 8.5650\n",
      "Epoch 241: val_loss did not improve from 0.20288\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2165 - mse: 0.2165 - rmse: 0.4653 - mae: 0.2722 - mape: 8.5574 - val_loss: 0.2054 - val_mse: 0.2054 - val_rmse: 0.4532 - val_mae: 0.2560 - val_mape: 7.9862 - lr: 0.0010\n",
      "Epoch 242/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2104 - mse: 0.2104 - rmse: 0.4587 - mae: 0.2645 - mape: 8.2823\n",
      "Epoch 242: val_loss improved from 0.20288 to 0.20210, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2116 - mse: 0.2116 - rmse: 0.4600 - mae: 0.2652 - mape: 8.3021 - val_loss: 0.2021 - val_mse: 0.2021 - val_rmse: 0.4496 - val_mae: 0.2559 - val_mape: 8.0624 - lr: 0.0010\n",
      "Epoch 243/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.2138 - mse: 0.2138 - rmse: 0.4624 - mae: 0.2677 - mape: 8.3928\n",
      "Epoch 243: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2130 - mse: 0.2130 - rmse: 0.4615 - mae: 0.2674 - mape: 8.3814 - val_loss: 0.2149 - val_mse: 0.2149 - val_rmse: 0.4635 - val_mae: 0.2676 - val_mape: 8.5589 - lr: 0.0010\n",
      "Epoch 244/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2180 - mse: 0.2180 - rmse: 0.4669 - mae: 0.2734 - mape: 8.6100\n",
      "Epoch 244: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2173 - mse: 0.2173 - rmse: 0.4661 - mae: 0.2732 - mape: 8.5906 - val_loss: 0.2033 - val_mse: 0.2033 - val_rmse: 0.4509 - val_mae: 0.2522 - val_mape: 7.7852 - lr: 0.0010\n",
      "Epoch 245/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2137 - mse: 0.2137 - rmse: 0.4623 - mae: 0.2675 - mape: 8.3971\n",
      "Epoch 245: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2136 - mse: 0.2136 - rmse: 0.4621 - mae: 0.2678 - mape: 8.4008 - val_loss: 0.2052 - val_mse: 0.2052 - val_rmse: 0.4529 - val_mae: 0.2548 - val_mape: 7.8385 - lr: 0.0010\n",
      "Epoch 246/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2102 - mse: 0.2102 - rmse: 0.4585 - mae: 0.2644 - mape: 8.2978\n",
      "Epoch 246: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2099 - mse: 0.2099 - rmse: 0.4582 - mae: 0.2640 - mape: 8.2795 - val_loss: 0.2079 - val_mse: 0.2079 - val_rmse: 0.4560 - val_mae: 0.2608 - val_mape: 8.1005 - lr: 0.0010\n",
      "Epoch 247/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.2111 - mse: 0.2111 - rmse: 0.4595 - mae: 0.2637 - mape: 8.2717\n",
      "Epoch 247: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2106 - mse: 0.2106 - rmse: 0.4589 - mae: 0.2635 - mape: 8.2573 - val_loss: 0.2292 - val_mse: 0.2292 - val_rmse: 0.4787 - val_mae: 0.2790 - val_mape: 9.0501 - lr: 0.0010\n",
      "Epoch 248/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2116 - mse: 0.2116 - rmse: 0.4600 - mae: 0.2647 - mape: 8.2767\n",
      "Epoch 248: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2112 - mse: 0.2112 - rmse: 0.4596 - mae: 0.2644 - mape: 8.2714 - val_loss: 0.2076 - val_mse: 0.2076 - val_rmse: 0.4557 - val_mae: 0.2689 - val_mape: 8.5529 - lr: 0.0010\n",
      "Epoch 249/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2215 - mse: 0.2215 - rmse: 0.4706 - mae: 0.2753 - mape: 8.6833\n",
      "Epoch 249: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2220 - mse: 0.2220 - rmse: 0.4712 - mae: 0.2756 - mape: 8.6857 - val_loss: 0.2324 - val_mse: 0.2324 - val_rmse: 0.4821 - val_mae: 0.2726 - val_mape: 8.4224 - lr: 0.0010\n",
      "Epoch 250/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.2143 - mse: 0.2143 - rmse: 0.4629 - mae: 0.2675 - mape: 8.3596\n",
      "Epoch 250: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2130 - mse: 0.2130 - rmse: 0.4615 - mae: 0.2665 - mape: 8.3422 - val_loss: 0.2066 - val_mse: 0.2066 - val_rmse: 0.4546 - val_mae: 0.2562 - val_mape: 8.0104 - lr: 0.0010\n",
      "Epoch 251/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.2096 - mse: 0.2096 - rmse: 0.4578 - mae: 0.2622 - mape: 8.2290\n",
      "Epoch 251: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2109 - mse: 0.2109 - rmse: 0.4593 - mae: 0.2634 - mape: 8.2547 - val_loss: 0.2059 - val_mse: 0.2059 - val_rmse: 0.4537 - val_mae: 0.2618 - val_mape: 8.2410 - lr: 0.0010\n",
      "Epoch 252/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.2097 - mse: 0.2097 - rmse: 0.4580 - mae: 0.2631 - mape: 8.2484\n",
      "Epoch 252: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2109 - mse: 0.2109 - rmse: 0.4593 - mae: 0.2638 - mape: 8.2697 - val_loss: 0.2078 - val_mse: 0.2078 - val_rmse: 0.4558 - val_mae: 0.2662 - val_mape: 8.5434 - lr: 0.0010\n",
      "Epoch 253/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.2120 - mse: 0.2120 - rmse: 0.4604 - mae: 0.2652 - mape: 8.3166\n",
      "Epoch 253: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2119 - mse: 0.2119 - rmse: 0.4603 - mae: 0.2655 - mape: 8.3213 - val_loss: 0.2076 - val_mse: 0.2076 - val_rmse: 0.4556 - val_mae: 0.2635 - val_mape: 8.3624 - lr: 0.0010\n",
      "Epoch 254/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2172 - mse: 0.2172 - rmse: 0.4661 - mae: 0.2704 - mape: 8.4845\n",
      "Epoch 254: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2172 - mse: 0.2172 - rmse: 0.4661 - mae: 0.2704 - mape: 8.4845 - val_loss: 0.2101 - val_mse: 0.2101 - val_rmse: 0.4584 - val_mae: 0.2565 - val_mape: 7.7673 - lr: 0.0010\n",
      "Epoch 255/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2141 - mse: 0.2141 - rmse: 0.4627 - mae: 0.2670 - mape: 8.3693\n",
      "Epoch 255: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2126 - mse: 0.2126 - rmse: 0.4611 - mae: 0.2663 - mape: 8.3472 - val_loss: 0.2085 - val_mse: 0.2085 - val_rmse: 0.4566 - val_mae: 0.2623 - val_mape: 8.0564 - lr: 0.0010\n",
      "Epoch 256/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.2161 - mse: 0.2161 - rmse: 0.4649 - mae: 0.2678 - mape: 8.3946\n",
      "Epoch 256: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2136 - mse: 0.2136 - rmse: 0.4622 - mae: 0.2664 - mape: 8.3442 - val_loss: 0.2082 - val_mse: 0.2082 - val_rmse: 0.4563 - val_mae: 0.2675 - val_mape: 8.4335 - lr: 0.0010\n",
      "Epoch 257/2000\n",
      "286/318 [=========================>....] - ETA: 0s - loss: 0.2117 - mse: 0.2117 - rmse: 0.4601 - mae: 0.2654 - mape: 8.3369\n",
      "Epoch 257: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2097 - mse: 0.2097 - rmse: 0.4579 - mae: 0.2649 - mape: 8.3127 - val_loss: 0.2064 - val_mse: 0.2064 - val_rmse: 0.4543 - val_mae: 0.2576 - val_mape: 7.9908 - lr: 0.0010\n",
      "Epoch 258/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2115 - mse: 0.2115 - rmse: 0.4599 - mae: 0.2646 - mape: 8.2764\n",
      "Epoch 258: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2114 - mse: 0.2114 - rmse: 0.4598 - mae: 0.2646 - mape: 8.2860 - val_loss: 0.2143 - val_mse: 0.2143 - val_rmse: 0.4630 - val_mae: 0.2643 - val_mape: 8.5172 - lr: 0.0010\n",
      "Epoch 259/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.2146 - mse: 0.2146 - rmse: 0.4633 - mae: 0.2677 - mape: 8.4122\n",
      "Epoch 259: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2129 - mse: 0.2129 - rmse: 0.4614 - mae: 0.2666 - mape: 8.3750 - val_loss: 0.2038 - val_mse: 0.2038 - val_rmse: 0.4514 - val_mae: 0.2556 - val_mape: 7.9891 - lr: 0.0010\n",
      "Epoch 260/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2086 - mse: 0.2086 - rmse: 0.4568 - mae: 0.2618 - mape: 8.2214\n",
      "Epoch 260: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2098 - mse: 0.2098 - rmse: 0.4581 - mae: 0.2632 - mape: 8.2546 - val_loss: 0.2097 - val_mse: 0.2097 - val_rmse: 0.4579 - val_mae: 0.2587 - val_mape: 7.8821 - lr: 0.0010\n",
      "Epoch 261/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2133 - mse: 0.2133 - rmse: 0.4619 - mae: 0.2668 - mape: 8.3517\n",
      "Epoch 261: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2144 - mse: 0.2144 - rmse: 0.4630 - mae: 0.2672 - mape: 8.3683 - val_loss: 0.2172 - val_mse: 0.2172 - val_rmse: 0.4661 - val_mae: 0.2730 - val_mape: 8.7735 - lr: 0.0010\n",
      "Epoch 262/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.2098 - mse: 0.2098 - rmse: 0.4580 - mae: 0.2640 - mape: 8.2806\n",
      "Epoch 262: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2119 - mse: 0.2119 - rmse: 0.4603 - mae: 0.2648 - mape: 8.2968 - val_loss: 0.2048 - val_mse: 0.2048 - val_rmse: 0.4525 - val_mae: 0.2630 - val_mape: 8.4876 - lr: 0.0010\n",
      "Epoch 263/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2127 - mse: 0.2127 - rmse: 0.4612 - mae: 0.2656 - mape: 8.3379\n",
      "Epoch 263: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2123 - mse: 0.2123 - rmse: 0.4607 - mae: 0.2651 - mape: 8.3205 - val_loss: 0.2281 - val_mse: 0.2281 - val_rmse: 0.4776 - val_mae: 0.2835 - val_mape: 9.2392 - lr: 0.0010\n",
      "Epoch 264/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2100 - mse: 0.2100 - rmse: 0.4582 - mae: 0.2646 - mape: 8.2935\n",
      "Epoch 264: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2106 - mse: 0.2106 - rmse: 0.4590 - mae: 0.2650 - mape: 8.3175 - val_loss: 0.2042 - val_mse: 0.2042 - val_rmse: 0.4518 - val_mae: 0.2562 - val_mape: 8.0053 - lr: 0.0010\n",
      "Epoch 265/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2107 - mse: 0.2107 - rmse: 0.4590 - mae: 0.2649 - mape: 8.3089\n",
      "Epoch 265: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2111 - mse: 0.2111 - rmse: 0.4594 - mae: 0.2652 - mape: 8.3149 - val_loss: 0.2106 - val_mse: 0.2106 - val_rmse: 0.4589 - val_mae: 0.2663 - val_mape: 8.2758 - lr: 0.0010\n",
      "Epoch 266/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2097 - mse: 0.2097 - rmse: 0.4579 - mae: 0.2634 - mape: 8.2461\n",
      "Epoch 266: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2098 - mse: 0.2098 - rmse: 0.4580 - mae: 0.2636 - mape: 8.2539 - val_loss: 0.2032 - val_mse: 0.2032 - val_rmse: 0.4508 - val_mae: 0.2569 - val_mape: 8.0031 - lr: 0.0010\n",
      "Epoch 267/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.2090 - mse: 0.2090 - rmse: 0.4571 - mae: 0.2628 - mape: 8.2324\n",
      "Epoch 267: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2097 - mse: 0.2097 - rmse: 0.4580 - mae: 0.2632 - mape: 8.2448 - val_loss: 0.2037 - val_mse: 0.2037 - val_rmse: 0.4513 - val_mae: 0.2549 - val_mape: 7.8569 - lr: 0.0010\n",
      "Epoch 268/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.2109 - mse: 0.2109 - rmse: 0.4592 - mae: 0.2664 - mape: 8.3554\n",
      "Epoch 268: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2119 - mse: 0.2119 - rmse: 0.4604 - mae: 0.2664 - mape: 8.3515 - val_loss: 0.2100 - val_mse: 0.2100 - val_rmse: 0.4582 - val_mae: 0.2584 - val_mape: 7.9608 - lr: 0.0010\n",
      "Epoch 269/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.2145 - mse: 0.2145 - rmse: 0.4632 - mae: 0.2692 - mape: 8.4482\n",
      "Epoch 269: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2157 - mse: 0.2157 - rmse: 0.4644 - mae: 0.2698 - mape: 8.4698 - val_loss: 0.2044 - val_mse: 0.2044 - val_rmse: 0.4521 - val_mae: 0.2652 - val_mape: 8.3286 - lr: 0.0010\n",
      "Epoch 270/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2109 - mse: 0.2109 - rmse: 0.4593 - mae: 0.2654 - mape: 8.3155\n",
      "Epoch 270: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2109 - mse: 0.2109 - rmse: 0.4593 - mae: 0.2654 - mape: 8.3155 - val_loss: 0.2024 - val_mse: 0.2024 - val_rmse: 0.4499 - val_mae: 0.2562 - val_mape: 8.0136 - lr: 0.0010\n",
      "Epoch 271/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.2089 - mse: 0.2089 - rmse: 0.4571 - mae: 0.2637 - mape: 8.2606\n",
      "Epoch 271: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2092 - mse: 0.2092 - rmse: 0.4574 - mae: 0.2639 - mape: 8.2643 - val_loss: 0.2073 - val_mse: 0.2073 - val_rmse: 0.4553 - val_mae: 0.2620 - val_mape: 8.3214 - lr: 0.0010\n",
      "Epoch 272/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.2149 - mse: 0.2149 - rmse: 0.4636 - mae: 0.2675 - mape: 8.4138\n",
      "Epoch 272: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2143 - mse: 0.2143 - rmse: 0.4629 - mae: 0.2675 - mape: 8.4156 - val_loss: 0.2355 - val_mse: 0.2355 - val_rmse: 0.4853 - val_mae: 0.2829 - val_mape: 8.7497 - lr: 0.0010\n",
      "Epoch 273/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2102 - mse: 0.2102 - rmse: 0.4585 - mae: 0.2647 - mape: 8.3109\n",
      "Epoch 273: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2101 - mse: 0.2101 - rmse: 0.4584 - mae: 0.2644 - mape: 8.3042 - val_loss: 0.2049 - val_mse: 0.2049 - val_rmse: 0.4527 - val_mae: 0.2579 - val_mape: 7.8663 - lr: 0.0010\n",
      "Epoch 274/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.2153 - mse: 0.2153 - rmse: 0.4640 - mae: 0.2694 - mape: 8.4199\n",
      "Epoch 274: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2151 - mse: 0.2151 - rmse: 0.4638 - mae: 0.2690 - mape: 8.4158 - val_loss: 0.2238 - val_mse: 0.2238 - val_rmse: 0.4731 - val_mae: 0.2693 - val_mape: 8.4141 - lr: 0.0010\n",
      "Epoch 275/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.2147 - mse: 0.2147 - rmse: 0.4633 - mae: 0.2678 - mape: 8.3875\n",
      "Epoch 275: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2134 - mse: 0.2134 - rmse: 0.4620 - mae: 0.2670 - mape: 8.3723 - val_loss: 0.2084 - val_mse: 0.2084 - val_rmse: 0.4565 - val_mae: 0.2583 - val_mape: 8.1395 - lr: 0.0010\n",
      "Epoch 276/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.2078 - mse: 0.2078 - rmse: 0.4558 - mae: 0.2619 - mape: 8.2038\n",
      "Epoch 276: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2079 - mse: 0.2079 - rmse: 0.4559 - mae: 0.2620 - mape: 8.2075 - val_loss: 0.2067 - val_mse: 0.2067 - val_rmse: 0.4546 - val_mae: 0.2608 - val_mape: 8.2252 - lr: 0.0010\n",
      "Epoch 277/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2055 - mse: 0.2055 - rmse: 0.4533 - mae: 0.2598 - mape: 8.1279\n",
      "Epoch 277: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2085 - mse: 0.2085 - rmse: 0.4567 - mae: 0.2619 - mape: 8.1982 - val_loss: 0.2135 - val_mse: 0.2135 - val_rmse: 0.4620 - val_mae: 0.2662 - val_mape: 8.1728 - lr: 0.0010\n",
      "Epoch 278/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.2118 - mse: 0.2118 - rmse: 0.4602 - mae: 0.2641 - mape: 8.2832\n",
      "Epoch 278: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2113 - mse: 0.2113 - rmse: 0.4597 - mae: 0.2639 - mape: 8.2627 - val_loss: 0.2127 - val_mse: 0.2127 - val_rmse: 0.4612 - val_mae: 0.2747 - val_mape: 8.9934 - lr: 0.0010\n",
      "Epoch 279/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.2154 - mse: 0.2154 - rmse: 0.4641 - mae: 0.2691 - mape: 8.4596\n",
      "Epoch 279: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2130 - mse: 0.2130 - rmse: 0.4616 - mae: 0.2675 - mape: 8.4058 - val_loss: 0.2060 - val_mse: 0.2060 - val_rmse: 0.4539 - val_mae: 0.2662 - val_mape: 8.5170 - lr: 0.0010\n",
      "Epoch 280/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.2087 - mse: 0.2087 - rmse: 0.4568 - mae: 0.2629 - mape: 8.2320\n",
      "Epoch 280: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2089 - mse: 0.2089 - rmse: 0.4571 - mae: 0.2634 - mape: 8.2466 - val_loss: 0.2077 - val_mse: 0.2077 - val_rmse: 0.4558 - val_mae: 0.2629 - val_mape: 8.0921 - lr: 0.0010\n",
      "Epoch 281/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2071 - mse: 0.2071 - rmse: 0.4551 - mae: 0.2660 - mape: 8.3488\n",
      "Epoch 281: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2099 - mse: 0.2099 - rmse: 0.4581 - mae: 0.2662 - mape: 8.3528 - val_loss: 0.2024 - val_mse: 0.2024 - val_rmse: 0.4499 - val_mae: 0.2565 - val_mape: 7.8678 - lr: 0.0010\n",
      "Epoch 282/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.2068 - mse: 0.2068 - rmse: 0.4548 - mae: 0.2622 - mape: 8.2068\n",
      "Epoch 282: val_loss did not improve from 0.20210\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2068 - mse: 0.2068 - rmse: 0.4548 - mae: 0.2622 - mape: 8.2068 - val_loss: 0.2173 - val_mse: 0.2173 - val_rmse: 0.4662 - val_mae: 0.2728 - val_mape: 8.8410 - lr: 0.0010\n",
      "Epoch 283/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1983 - mse: 0.1983 - rmse: 0.4454 - mae: 0.2530 - mape: 7.9363\n",
      "Epoch 283: val_loss improved from 0.20210 to 0.19845, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.2002 - mse: 0.2002 - rmse: 0.4474 - mae: 0.2538 - mape: 7.9386 - val_loss: 0.1984 - val_mse: 0.1984 - val_rmse: 0.4455 - val_mae: 0.2527 - val_mape: 7.8962 - lr: 1.0000e-04\n",
      "Epoch 284/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1979 - mse: 0.1979 - rmse: 0.4449 - mae: 0.2515 - mape: 7.8599\n",
      "Epoch 284: val_loss did not improve from 0.19845\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1990 - mse: 0.1990 - rmse: 0.4461 - mae: 0.2525 - mape: 7.8843 - val_loss: 0.2008 - val_mse: 0.2008 - val_rmse: 0.4481 - val_mae: 0.2582 - val_mape: 8.1072 - lr: 1.0000e-04\n",
      "Epoch 285/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.2009 - mse: 0.2009 - rmse: 0.4482 - mae: 0.2536 - mape: 7.8885\n",
      "Epoch 285: val_loss did not improve from 0.19845\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1987 - mse: 0.1987 - rmse: 0.4458 - mae: 0.2524 - mape: 7.8842 - val_loss: 0.1992 - val_mse: 0.1992 - val_rmse: 0.4463 - val_mae: 0.2518 - val_mape: 7.7665 - lr: 1.0000e-04\n",
      "Epoch 286/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1985 - mse: 0.1985 - rmse: 0.4455 - mae: 0.2521 - mape: 7.8625\n",
      "Epoch 286: val_loss improved from 0.19845 to 0.19780, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1986 - mse: 0.1986 - rmse: 0.4457 - mae: 0.2522 - mape: 7.8649 - val_loss: 0.1978 - val_mse: 0.1978 - val_rmse: 0.4447 - val_mae: 0.2505 - val_mape: 7.8380 - lr: 1.0000e-04\n",
      "Epoch 287/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1983 - mse: 0.1983 - rmse: 0.4453 - mae: 0.2513 - mape: 7.8611\n",
      "Epoch 287: val_loss improved from 0.19780 to 0.19706, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1986 - mse: 0.1986 - rmse: 0.4456 - mae: 0.2518 - mape: 7.8658 - val_loss: 0.1971 - val_mse: 0.1971 - val_rmse: 0.4439 - val_mae: 0.2508 - val_mape: 7.8327 - lr: 1.0000e-04\n",
      "Epoch 288/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1978 - mse: 0.1978 - rmse: 0.4447 - mae: 0.2519 - mape: 7.8707\n",
      "Epoch 288: val_loss improved from 0.19706 to 0.19685, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1982 - mse: 0.1982 - rmse: 0.4452 - mae: 0.2520 - mape: 7.8721 - val_loss: 0.1968 - val_mse: 0.1968 - val_rmse: 0.4437 - val_mae: 0.2511 - val_mape: 7.8566 - lr: 1.0000e-04\n",
      "Epoch 289/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.1961 - mse: 0.1961 - rmse: 0.4428 - mae: 0.2515 - mape: 7.8469\n",
      "Epoch 289: val_loss did not improve from 0.19685\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1981 - mse: 0.1981 - rmse: 0.4451 - mae: 0.2521 - mape: 7.8802 - val_loss: 0.1973 - val_mse: 0.1973 - val_rmse: 0.4442 - val_mae: 0.2498 - val_mape: 7.8014 - lr: 1.0000e-04\n",
      "Epoch 290/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1961 - mse: 0.1961 - rmse: 0.4428 - mae: 0.2503 - mape: 7.8200\n",
      "Epoch 290: val_loss improved from 0.19685 to 0.19665, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1977 - mse: 0.1977 - rmse: 0.4446 - mae: 0.2513 - mape: 7.8503 - val_loss: 0.1967 - val_mse: 0.1967 - val_rmse: 0.4435 - val_mae: 0.2508 - val_mape: 7.8432 - lr: 1.0000e-04\n",
      "Epoch 291/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1981 - mse: 0.1981 - rmse: 0.4450 - mae: 0.2519 - mape: 7.8737\n",
      "Epoch 291: val_loss improved from 0.19665 to 0.19662, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1978 - mse: 0.1978 - rmse: 0.4448 - mae: 0.2518 - mape: 7.8712 - val_loss: 0.1966 - val_mse: 0.1966 - val_rmse: 0.4434 - val_mae: 0.2494 - val_mape: 7.7400 - lr: 1.0000e-04\n",
      "Epoch 292/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1979 - mse: 0.1979 - rmse: 0.4448 - mae: 0.2513 - mape: 7.8548\n",
      "Epoch 292: val_loss did not improve from 0.19662\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1971 - mse: 0.1971 - rmse: 0.4440 - mae: 0.2512 - mape: 7.8424 - val_loss: 0.1978 - val_mse: 0.1978 - val_rmse: 0.4448 - val_mae: 0.2531 - val_mape: 7.9523 - lr: 1.0000e-04\n",
      "Epoch 293/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1983 - mse: 0.1983 - rmse: 0.4453 - mae: 0.2517 - mape: 7.8654\n",
      "Epoch 293: val_loss improved from 0.19662 to 0.19661, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1981 - mse: 0.1981 - rmse: 0.4451 - mae: 0.2515 - mape: 7.8580 - val_loss: 0.1966 - val_mse: 0.1966 - val_rmse: 0.4434 - val_mae: 0.2529 - val_mape: 7.9109 - lr: 1.0000e-04\n",
      "Epoch 294/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1967 - mse: 0.1967 - rmse: 0.4436 - mae: 0.2513 - mape: 7.8542\n",
      "Epoch 294: val_loss did not improve from 0.19661\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1972 - mse: 0.1972 - rmse: 0.4441 - mae: 0.2514 - mape: 7.8572 - val_loss: 0.1981 - val_mse: 0.1981 - val_rmse: 0.4451 - val_mae: 0.2506 - val_mape: 7.8701 - lr: 1.0000e-04\n",
      "Epoch 295/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1989 - mse: 0.1989 - rmse: 0.4460 - mae: 0.2520 - mape: 7.8775\n",
      "Epoch 295: val_loss did not improve from 0.19661\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1974 - mse: 0.1974 - rmse: 0.4443 - mae: 0.2516 - mape: 7.8758 - val_loss: 0.1976 - val_mse: 0.1976 - val_rmse: 0.4445 - val_mae: 0.2508 - val_mape: 7.8724 - lr: 1.0000e-04\n",
      "Epoch 296/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1939 - mse: 0.1939 - rmse: 0.4404 - mae: 0.2493 - mape: 7.7883\n",
      "Epoch 296: val_loss improved from 0.19661 to 0.19619, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1973 - mse: 0.1973 - rmse: 0.4442 - mae: 0.2507 - mape: 7.8257 - val_loss: 0.1962 - val_mse: 0.1962 - val_rmse: 0.4429 - val_mae: 0.2519 - val_mape: 7.9003 - lr: 1.0000e-04\n",
      "Epoch 297/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1967 - mse: 0.1967 - rmse: 0.4436 - mae: 0.2513 - mape: 7.8567\n",
      "Epoch 297: val_loss improved from 0.19619 to 0.19599, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1969 - mse: 0.1969 - rmse: 0.4437 - mae: 0.2514 - mape: 7.8594 - val_loss: 0.1960 - val_mse: 0.1960 - val_rmse: 0.4427 - val_mae: 0.2495 - val_mape: 7.7901 - lr: 1.0000e-04\n",
      "Epoch 298/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1990 - mse: 0.1990 - rmse: 0.4461 - mae: 0.2520 - mape: 7.8749\n",
      "Epoch 298: val_loss did not improve from 0.19599\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1977 - mse: 0.1977 - rmse: 0.4447 - mae: 0.2514 - mape: 7.8566 - val_loss: 0.1969 - val_mse: 0.1969 - val_rmse: 0.4437 - val_mae: 0.2505 - val_mape: 7.8654 - lr: 1.0000e-04\n",
      "Epoch 299/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1965 - mse: 0.1965 - rmse: 0.4433 - mae: 0.2507 - mape: 7.8207\n",
      "Epoch 299: val_loss did not improve from 0.19599\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1971 - mse: 0.1971 - rmse: 0.4439 - mae: 0.2510 - mape: 7.8378 - val_loss: 0.2030 - val_mse: 0.2030 - val_rmse: 0.4506 - val_mae: 0.2583 - val_mape: 8.1639 - lr: 1.0000e-04\n",
      "Epoch 300/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1978 - mse: 0.1978 - rmse: 0.4447 - mae: 0.2514 - mape: 7.8717\n",
      "Epoch 300: val_loss improved from 0.19599 to 0.19586, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1970 - mse: 0.1970 - rmse: 0.4439 - mae: 0.2511 - mape: 7.8569 - val_loss: 0.1959 - val_mse: 0.1959 - val_rmse: 0.4426 - val_mae: 0.2516 - val_mape: 7.8685 - lr: 1.0000e-04\n",
      "Epoch 301/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1971 - mse: 0.1971 - rmse: 0.4440 - mae: 0.2517 - mape: 7.8616\n",
      "Epoch 301: val_loss did not improve from 0.19586\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1971 - mse: 0.1971 - rmse: 0.4440 - mae: 0.2517 - mape: 7.8663 - val_loss: 0.1960 - val_mse: 0.1960 - val_rmse: 0.4427 - val_mae: 0.2500 - val_mape: 7.8588 - lr: 1.0000e-04\n",
      "Epoch 302/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1966 - mse: 0.1966 - rmse: 0.4434 - mae: 0.2503 - mape: 7.8318\n",
      "Epoch 302: val_loss did not improve from 0.19586\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1966 - mse: 0.1966 - rmse: 0.4434 - mae: 0.2502 - mape: 7.8298 - val_loss: 0.1961 - val_mse: 0.1961 - val_rmse: 0.4428 - val_mae: 0.2490 - val_mape: 7.7262 - lr: 1.0000e-04\n",
      "Epoch 303/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1967 - mse: 0.1967 - rmse: 0.4435 - mae: 0.2513 - mape: 7.8506\n",
      "Epoch 303: val_loss improved from 0.19586 to 0.19550, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1965 - mse: 0.1965 - rmse: 0.4433 - mae: 0.2511 - mape: 7.8495 - val_loss: 0.1955 - val_mse: 0.1955 - val_rmse: 0.4422 - val_mae: 0.2486 - val_mape: 7.7295 - lr: 1.0000e-04\n",
      "Epoch 304/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1961 - mse: 0.1961 - rmse: 0.4428 - mae: 0.2505 - mape: 7.8123\n",
      "Epoch 304: val_loss did not improve from 0.19550\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1968 - mse: 0.1968 - rmse: 0.4436 - mae: 0.2507 - mape: 7.8255 - val_loss: 0.1964 - val_mse: 0.1964 - val_rmse: 0.4431 - val_mae: 0.2515 - val_mape: 7.9487 - lr: 1.0000e-04\n",
      "Epoch 305/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1963 - mse: 0.1963 - rmse: 0.4431 - mae: 0.2499 - mape: 7.8178\n",
      "Epoch 305: val_loss did not improve from 0.19550\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1970 - mse: 0.1970 - rmse: 0.4438 - mae: 0.2506 - mape: 7.8365 - val_loss: 0.1975 - val_mse: 0.1975 - val_rmse: 0.4444 - val_mae: 0.2514 - val_mape: 7.9098 - lr: 1.0000e-04\n",
      "Epoch 306/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.1967 - mse: 0.1967 - rmse: 0.4435 - mae: 0.2505 - mape: 7.8282\n",
      "Epoch 306: val_loss did not improve from 0.19550\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1968 - mse: 0.1968 - rmse: 0.4436 - mae: 0.2509 - mape: 7.8335 - val_loss: 0.1957 - val_mse: 0.1957 - val_rmse: 0.4423 - val_mae: 0.2506 - val_mape: 7.8897 - lr: 1.0000e-04\n",
      "Epoch 307/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1963 - mse: 0.1963 - rmse: 0.4431 - mae: 0.2505 - mape: 7.8346\n",
      "Epoch 307: val_loss improved from 0.19550 to 0.19532, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1967 - mse: 0.1967 - rmse: 0.4435 - mae: 0.2503 - mape: 7.8332 - val_loss: 0.1953 - val_mse: 0.1953 - val_rmse: 0.4419 - val_mae: 0.2493 - val_mape: 7.7703 - lr: 1.0000e-04\n",
      "Epoch 308/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1979 - mse: 0.1979 - rmse: 0.4448 - mae: 0.2512 - mape: 7.8475\n",
      "Epoch 308: val_loss did not improve from 0.19532\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1967 - mse: 0.1967 - rmse: 0.4435 - mae: 0.2507 - mape: 7.8464 - val_loss: 0.1956 - val_mse: 0.1956 - val_rmse: 0.4422 - val_mae: 0.2497 - val_mape: 7.8369 - lr: 1.0000e-04\n",
      "Epoch 309/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1973 - mse: 0.1973 - rmse: 0.4442 - mae: 0.2510 - mape: 7.8492\n",
      "Epoch 309: val_loss did not improve from 0.19532\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1967 - mse: 0.1967 - rmse: 0.4435 - mae: 0.2507 - mape: 7.8395 - val_loss: 0.1955 - val_mse: 0.1955 - val_rmse: 0.4421 - val_mae: 0.2482 - val_mape: 7.7260 - lr: 1.0000e-04\n",
      "Epoch 310/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1958 - mse: 0.1958 - rmse: 0.4425 - mae: 0.2495 - mape: 7.8020\n",
      "Epoch 310: val_loss improved from 0.19532 to 0.19530, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1963 - mse: 0.1963 - rmse: 0.4431 - mae: 0.2498 - mape: 7.8119 - val_loss: 0.1953 - val_mse: 0.1953 - val_rmse: 0.4419 - val_mae: 0.2497 - val_mape: 7.8065 - lr: 1.0000e-04\n",
      "Epoch 311/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1948 - mse: 0.1948 - rmse: 0.4413 - mae: 0.2493 - mape: 7.8087\n",
      "Epoch 311: val_loss did not improve from 0.19530\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1963 - mse: 0.1963 - rmse: 0.4431 - mae: 0.2501 - mape: 7.8229 - val_loss: 0.1955 - val_mse: 0.1955 - val_rmse: 0.4421 - val_mae: 0.2498 - val_mape: 7.8565 - lr: 1.0000e-04\n",
      "Epoch 312/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1969 - mse: 0.1969 - rmse: 0.4438 - mae: 0.2503 - mape: 7.8336\n",
      "Epoch 312: val_loss improved from 0.19530 to 0.19516, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1963 - mse: 0.1963 - rmse: 0.4431 - mae: 0.2502 - mape: 7.8221 - val_loss: 0.1952 - val_mse: 0.1952 - val_rmse: 0.4418 - val_mae: 0.2514 - val_mape: 7.8719 - lr: 1.0000e-04\n",
      "Epoch 313/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1975 - mse: 0.1975 - rmse: 0.4444 - mae: 0.2508 - mape: 7.8438\n",
      "Epoch 313: val_loss improved from 0.19516 to 0.19494, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1960 - mse: 0.1960 - rmse: 0.4428 - mae: 0.2497 - mape: 7.8127 - val_loss: 0.1949 - val_mse: 0.1949 - val_rmse: 0.4415 - val_mae: 0.2520 - val_mape: 7.9436 - lr: 1.0000e-04\n",
      "Epoch 314/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1959 - mse: 0.1959 - rmse: 0.4426 - mae: 0.2503 - mape: 7.8361\n",
      "Epoch 314: val_loss did not improve from 0.19494\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1959 - mse: 0.1959 - rmse: 0.4426 - mae: 0.2503 - mape: 7.8363 - val_loss: 0.1959 - val_mse: 0.1959 - val_rmse: 0.4426 - val_mae: 0.2508 - val_mape: 7.8493 - lr: 1.0000e-04\n",
      "Epoch 315/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.1954 - mse: 0.1954 - rmse: 0.4420 - mae: 0.2498 - mape: 7.8249\n",
      "Epoch 315: val_loss did not improve from 0.19494\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1959 - mse: 0.1959 - rmse: 0.4426 - mae: 0.2505 - mape: 7.8269 - val_loss: 0.1965 - val_mse: 0.1965 - val_rmse: 0.4433 - val_mae: 0.2518 - val_mape: 7.9161 - lr: 1.0000e-04\n",
      "Epoch 316/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1954 - mse: 0.1954 - rmse: 0.4421 - mae: 0.2498 - mape: 7.8164\n",
      "Epoch 316: val_loss improved from 0.19494 to 0.19464, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1956 - mse: 0.1956 - rmse: 0.4422 - mae: 0.2500 - mape: 7.8227 - val_loss: 0.1946 - val_mse: 0.1946 - val_rmse: 0.4412 - val_mae: 0.2493 - val_mape: 7.8266 - lr: 1.0000e-04\n",
      "Epoch 317/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1965 - mse: 0.1965 - rmse: 0.4433 - mae: 0.2503 - mape: 7.8301\n",
      "Epoch 317: val_loss did not improve from 0.19464\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1962 - mse: 0.1962 - rmse: 0.4429 - mae: 0.2504 - mape: 7.8275 - val_loss: 0.1951 - val_mse: 0.1951 - val_rmse: 0.4417 - val_mae: 0.2522 - val_mape: 7.9389 - lr: 1.0000e-04\n",
      "Epoch 318/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1972 - mse: 0.1972 - rmse: 0.4441 - mae: 0.2509 - mape: 7.8466\n",
      "Epoch 318: val_loss did not improve from 0.19464\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1963 - mse: 0.1963 - rmse: 0.4431 - mae: 0.2507 - mape: 7.8457 - val_loss: 0.1970 - val_mse: 0.1970 - val_rmse: 0.4438 - val_mae: 0.2518 - val_mape: 7.7889 - lr: 1.0000e-04\n",
      "Epoch 319/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1965 - mse: 0.1965 - rmse: 0.4433 - mae: 0.2505 - mape: 7.8357\n",
      "Epoch 319: val_loss improved from 0.19464 to 0.19451, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1960 - mse: 0.1960 - rmse: 0.4427 - mae: 0.2503 - mape: 7.8266 - val_loss: 0.1945 - val_mse: 0.1945 - val_rmse: 0.4410 - val_mae: 0.2491 - val_mape: 7.8165 - lr: 1.0000e-04\n",
      "Epoch 320/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1974 - mse: 0.1974 - rmse: 0.4443 - mae: 0.2508 - mape: 7.8505\n",
      "Epoch 320: val_loss did not improve from 0.19451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1964 - mse: 0.1964 - rmse: 0.4432 - mae: 0.2503 - mape: 7.8315 - val_loss: 0.1947 - val_mse: 0.1947 - val_rmse: 0.4413 - val_mae: 0.2493 - val_mape: 7.7633 - lr: 1.0000e-04\n",
      "Epoch 321/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1978 - mse: 0.1978 - rmse: 0.4447 - mae: 0.2509 - mape: 7.8499\n",
      "Epoch 321: val_loss did not improve from 0.19451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1960 - mse: 0.1960 - rmse: 0.4428 - mae: 0.2503 - mape: 7.8326 - val_loss: 0.1948 - val_mse: 0.1948 - val_rmse: 0.4413 - val_mae: 0.2490 - val_mape: 7.7477 - lr: 1.0000e-04\n",
      "Epoch 322/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1971 - mse: 0.1971 - rmse: 0.4439 - mae: 0.2506 - mape: 7.8338\n",
      "Epoch 322: val_loss did not improve from 0.19451\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1954 - mse: 0.1954 - rmse: 0.4420 - mae: 0.2498 - mape: 7.8157 - val_loss: 0.1974 - val_mse: 0.1974 - val_rmse: 0.4443 - val_mae: 0.2482 - val_mape: 7.6835 - lr: 1.0000e-04\n",
      "Epoch 323/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1962 - mse: 0.1962 - rmse: 0.4430 - mae: 0.2503 - mape: 7.8294\n",
      "Epoch 323: val_loss improved from 0.19451 to 0.19425, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1962 - mse: 0.1962 - rmse: 0.4430 - mae: 0.2503 - mape: 7.8294 - val_loss: 0.1942 - val_mse: 0.1942 - val_rmse: 0.4407 - val_mae: 0.2485 - val_mape: 7.7744 - lr: 1.0000e-04\n",
      "Epoch 324/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1979 - mse: 0.1979 - rmse: 0.4448 - mae: 0.2507 - mape: 7.8354\n",
      "Epoch 324: val_loss did not improve from 0.19425\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1961 - mse: 0.1961 - rmse: 0.4429 - mae: 0.2502 - mape: 7.8241 - val_loss: 0.1944 - val_mse: 0.1944 - val_rmse: 0.4409 - val_mae: 0.2484 - val_mape: 7.7662 - lr: 1.0000e-04\n",
      "Epoch 325/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.1955 - mse: 0.1955 - rmse: 0.4422 - mae: 0.2495 - mape: 7.8056\n",
      "Epoch 325: val_loss did not improve from 0.19425\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1956 - mse: 0.1956 - rmse: 0.4422 - mae: 0.2500 - mape: 7.8101 - val_loss: 0.1946 - val_mse: 0.1946 - val_rmse: 0.4412 - val_mae: 0.2489 - val_mape: 7.7359 - lr: 1.0000e-04\n",
      "Epoch 326/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1962 - mse: 0.1962 - rmse: 0.4429 - mae: 0.2505 - mape: 7.8379\n",
      "Epoch 326: val_loss improved from 0.19425 to 0.19395, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1954 - mse: 0.1954 - rmse: 0.4420 - mae: 0.2500 - mape: 7.8280 - val_loss: 0.1940 - val_mse: 0.1940 - val_rmse: 0.4404 - val_mae: 0.2474 - val_mape: 7.7211 - lr: 1.0000e-04\n",
      "Epoch 327/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2475 - mape: 7.7346\n",
      "Epoch 327: val_loss did not improve from 0.19395\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1951 - mse: 0.1951 - rmse: 0.4417 - mae: 0.2491 - mape: 7.7862 - val_loss: 0.1964 - val_mse: 0.1964 - val_rmse: 0.4432 - val_mae: 0.2494 - val_mape: 7.7144 - lr: 1.0000e-04\n",
      "Epoch 328/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1967 - mse: 0.1967 - rmse: 0.4435 - mae: 0.2502 - mape: 7.8318\n",
      "Epoch 328: val_loss did not improve from 0.19395\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1948 - mse: 0.1948 - rmse: 0.4414 - mae: 0.2494 - mape: 7.7987 - val_loss: 0.1972 - val_mse: 0.1972 - val_rmse: 0.4441 - val_mae: 0.2515 - val_mape: 7.8555 - lr: 1.0000e-04\n",
      "Epoch 329/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1947 - mse: 0.1947 - rmse: 0.4413 - mae: 0.2494 - mape: 7.8166\n",
      "Epoch 329: val_loss did not improve from 0.19395\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1955 - mse: 0.1955 - rmse: 0.4422 - mae: 0.2499 - mape: 7.8213 - val_loss: 0.1953 - val_mse: 0.1953 - val_rmse: 0.4419 - val_mae: 0.2485 - val_mape: 7.7716 - lr: 1.0000e-04\n",
      "Epoch 330/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1957 - mse: 0.1957 - rmse: 0.4424 - mae: 0.2500 - mape: 7.8213\n",
      "Epoch 330: val_loss did not improve from 0.19395\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1950 - mse: 0.1950 - rmse: 0.4416 - mae: 0.2496 - mape: 7.8129 - val_loss: 0.1954 - val_mse: 0.1954 - val_rmse: 0.4420 - val_mae: 0.2471 - val_mape: 7.6841 - lr: 1.0000e-04\n",
      "Epoch 331/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1971 - mse: 0.1971 - rmse: 0.4440 - mae: 0.2500 - mape: 7.8114\n",
      "Epoch 331: val_loss did not improve from 0.19395\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1953 - mse: 0.1953 - rmse: 0.4419 - mae: 0.2492 - mape: 7.7899 - val_loss: 0.1950 - val_mse: 0.1950 - val_rmse: 0.4416 - val_mae: 0.2492 - val_mape: 7.8644 - lr: 1.0000e-04\n",
      "Epoch 332/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1961 - mse: 0.1961 - rmse: 0.4428 - mae: 0.2492 - mape: 7.7870\n",
      "Epoch 332: val_loss did not improve from 0.19395\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1952 - mse: 0.1952 - rmse: 0.4418 - mae: 0.2489 - mape: 7.7889 - val_loss: 0.1961 - val_mse: 0.1961 - val_rmse: 0.4429 - val_mae: 0.2518 - val_mape: 7.9998 - lr: 1.0000e-04\n",
      "Epoch 333/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1953 - mse: 0.1953 - rmse: 0.4419 - mae: 0.2499 - mape: 7.8130\n",
      "Epoch 333: val_loss improved from 0.19395 to 0.19372, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1954 - mse: 0.1954 - rmse: 0.4421 - mae: 0.2498 - mape: 7.8171 - val_loss: 0.1937 - val_mse: 0.1937 - val_rmse: 0.4401 - val_mae: 0.2474 - val_mape: 7.7177 - lr: 1.0000e-04\n",
      "Epoch 334/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1951 - mse: 0.1951 - rmse: 0.4417 - mae: 0.2494 - mape: 7.8049\n",
      "Epoch 334: val_loss did not improve from 0.19372\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1951 - mse: 0.1951 - rmse: 0.4418 - mae: 0.2494 - mape: 7.7949 - val_loss: 0.1938 - val_mse: 0.1938 - val_rmse: 0.4402 - val_mae: 0.2488 - val_mape: 7.8295 - lr: 1.0000e-04\n",
      "Epoch 335/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1947 - mse: 0.1947 - rmse: 0.4412 - mae: 0.2490 - mape: 7.8092\n",
      "Epoch 335: val_loss did not improve from 0.19372\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1952 - mse: 0.1952 - rmse: 0.4418 - mae: 0.2493 - mape: 7.8103 - val_loss: 0.1945 - val_mse: 0.1945 - val_rmse: 0.4411 - val_mae: 0.2480 - val_mape: 7.7951 - lr: 1.0000e-04\n",
      "Epoch 336/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1945 - mse: 0.1945 - rmse: 0.4410 - mae: 0.2493 - mape: 7.8151\n",
      "Epoch 336: val_loss did not improve from 0.19372\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1954 - mse: 0.1954 - rmse: 0.4421 - mae: 0.2496 - mape: 7.8146 - val_loss: 0.1945 - val_mse: 0.1945 - val_rmse: 0.4411 - val_mae: 0.2507 - val_mape: 7.8706 - lr: 1.0000e-04\n",
      "Epoch 337/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1945 - mse: 0.1945 - rmse: 0.4411 - mae: 0.2493 - mape: 7.8082\n",
      "Epoch 337: val_loss improved from 0.19372 to 0.19357, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1953 - mse: 0.1953 - rmse: 0.4420 - mae: 0.2495 - mape: 7.8047 - val_loss: 0.1936 - val_mse: 0.1936 - val_rmse: 0.4400 - val_mae: 0.2470 - val_mape: 7.7108 - lr: 1.0000e-04\n",
      "Epoch 338/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1953 - mse: 0.1953 - rmse: 0.4419 - mae: 0.2494 - mape: 7.7991\n",
      "Epoch 338: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1951 - mse: 0.1951 - rmse: 0.4417 - mae: 0.2494 - mape: 7.8092 - val_loss: 0.1958 - val_mse: 0.1958 - val_rmse: 0.4425 - val_mae: 0.2496 - val_mape: 7.7256 - lr: 1.0000e-04\n",
      "Epoch 339/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1959 - mse: 0.1959 - rmse: 0.4426 - mae: 0.2495 - mape: 7.7927\n",
      "Epoch 339: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1960 - mse: 0.1960 - rmse: 0.4427 - mae: 0.2500 - mape: 7.8129 - val_loss: 0.1936 - val_mse: 0.1936 - val_rmse: 0.4400 - val_mae: 0.2478 - val_mape: 7.7298 - lr: 1.0000e-04\n",
      "Epoch 340/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1951 - mse: 0.1951 - rmse: 0.4417 - mae: 0.2491 - mape: 7.8006\n",
      "Epoch 340: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1948 - mse: 0.1948 - rmse: 0.4414 - mae: 0.2491 - mape: 7.7990 - val_loss: 0.1955 - val_mse: 0.1955 - val_rmse: 0.4422 - val_mae: 0.2490 - val_mape: 7.7364 - lr: 1.0000e-04\n",
      "Epoch 341/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1951 - mse: 0.1951 - rmse: 0.4417 - mae: 0.2494 - mape: 7.8014\n",
      "Epoch 341: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1951 - mse: 0.1951 - rmse: 0.4417 - mae: 0.2494 - mape: 7.8014 - val_loss: 0.1943 - val_mse: 0.1943 - val_rmse: 0.4408 - val_mae: 0.2478 - val_mape: 7.7956 - lr: 1.0000e-04\n",
      "Epoch 342/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1950 - mse: 0.1950 - rmse: 0.4416 - mae: 0.2492 - mape: 7.7964\n",
      "Epoch 342: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1951 - mse: 0.1951 - rmse: 0.4417 - mae: 0.2492 - mape: 7.7950 - val_loss: 0.1962 - val_mse: 0.1962 - val_rmse: 0.4430 - val_mae: 0.2520 - val_mape: 7.9588 - lr: 1.0000e-04\n",
      "Epoch 343/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1956 - mse: 0.1956 - rmse: 0.4423 - mae: 0.2494 - mape: 7.8017\n",
      "Epoch 343: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1956 - mse: 0.1956 - rmse: 0.4423 - mae: 0.2495 - mape: 7.8132 - val_loss: 0.1981 - val_mse: 0.1981 - val_rmse: 0.4451 - val_mae: 0.2521 - val_mape: 7.8977 - lr: 1.0000e-04\n",
      "Epoch 344/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1953 - mse: 0.1953 - rmse: 0.4419 - mae: 0.2495 - mape: 7.8017\n",
      "Epoch 344: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1955 - mse: 0.1955 - rmse: 0.4422 - mae: 0.2494 - mape: 7.7919 - val_loss: 0.1951 - val_mse: 0.1951 - val_rmse: 0.4417 - val_mae: 0.2505 - val_mape: 7.8903 - lr: 1.0000e-04\n",
      "Epoch 345/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1950 - mse: 0.1950 - rmse: 0.4416 - mae: 0.2493 - mape: 7.7991\n",
      "Epoch 345: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1957 - mse: 0.1957 - rmse: 0.4424 - mae: 0.2502 - mape: 7.8299 - val_loss: 0.2003 - val_mse: 0.2003 - val_rmse: 0.4475 - val_mae: 0.2543 - val_mape: 7.9984 - lr: 1.0000e-04\n",
      "Epoch 346/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1954 - mse: 0.1954 - rmse: 0.4420 - mae: 0.2490 - mape: 7.7826\n",
      "Epoch 346: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1953 - mse: 0.1953 - rmse: 0.4419 - mae: 0.2490 - mape: 7.7823 - val_loss: 0.1945 - val_mse: 0.1945 - val_rmse: 0.4410 - val_mae: 0.2507 - val_mape: 7.9438 - lr: 1.0000e-04\n",
      "Epoch 347/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1961 - mse: 0.1961 - rmse: 0.4429 - mae: 0.2498 - mape: 7.8157\n",
      "Epoch 347: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1954 - mse: 0.1954 - rmse: 0.4421 - mae: 0.2498 - mape: 7.8173 - val_loss: 0.1944 - val_mse: 0.1944 - val_rmse: 0.4409 - val_mae: 0.2500 - val_mape: 7.8627 - lr: 1.0000e-04\n",
      "Epoch 348/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1944 - mse: 0.1944 - rmse: 0.4409 - mae: 0.2483 - mape: 7.7876\n",
      "Epoch 348: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1946 - mse: 0.1946 - rmse: 0.4412 - mae: 0.2490 - mape: 7.8042 - val_loss: 0.1944 - val_mse: 0.1944 - val_rmse: 0.4410 - val_mae: 0.2475 - val_mape: 7.6950 - lr: 1.0000e-04\n",
      "Epoch 349/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1945 - mse: 0.1945 - rmse: 0.4410 - mae: 0.2487 - mape: 7.7764\n",
      "Epoch 349: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1948 - mse: 0.1948 - rmse: 0.4414 - mae: 0.2487 - mape: 7.7784 - val_loss: 0.1973 - val_mse: 0.1973 - val_rmse: 0.4442 - val_mae: 0.2539 - val_mape: 8.0593 - lr: 1.0000e-04\n",
      "Epoch 350/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1919 - mse: 0.1919 - rmse: 0.4381 - mae: 0.2484 - mape: 7.7765\n",
      "Epoch 350: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1943 - mse: 0.1943 - rmse: 0.4408 - mae: 0.2495 - mape: 7.8091 - val_loss: 0.1940 - val_mse: 0.1940 - val_rmse: 0.4405 - val_mae: 0.2482 - val_mape: 7.8108 - lr: 1.0000e-04\n",
      "Epoch 351/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1963 - mse: 0.1963 - rmse: 0.4430 - mae: 0.2489 - mape: 7.7817\n",
      "Epoch 351: val_loss did not improve from 0.19357\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1953 - mse: 0.1953 - rmse: 0.4419 - mae: 0.2488 - mape: 7.7850 - val_loss: 0.1953 - val_mse: 0.1953 - val_rmse: 0.4420 - val_mae: 0.2495 - val_mape: 7.7347 - lr: 1.0000e-04\n",
      "Epoch 352/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1945 - mse: 0.1945 - rmse: 0.4410 - mae: 0.2486 - mape: 7.7850\n",
      "Epoch 352: val_loss improved from 0.19357 to 0.19303, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1945 - mse: 0.1945 - rmse: 0.4410 - mae: 0.2486 - mape: 7.7860 - val_loss: 0.1930 - val_mse: 0.1930 - val_rmse: 0.4393 - val_mae: 0.2472 - val_mape: 7.7172 - lr: 1.0000e-04\n",
      "Epoch 353/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1946 - mse: 0.1946 - rmse: 0.4412 - mae: 0.2493 - mape: 7.7977\n",
      "Epoch 353: val_loss did not improve from 0.19303\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1943 - mse: 0.1943 - rmse: 0.4408 - mae: 0.2491 - mape: 7.7963 - val_loss: 0.1949 - val_mse: 0.1949 - val_rmse: 0.4415 - val_mae: 0.2491 - val_mape: 7.7450 - lr: 1.0000e-04\n",
      "Epoch 354/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1932 - mse: 0.1932 - rmse: 0.4396 - mae: 0.2485 - mape: 7.7668\n",
      "Epoch 354: val_loss did not improve from 0.19303\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1946 - mse: 0.1946 - rmse: 0.4411 - mae: 0.2490 - mape: 7.7938 - val_loss: 0.1936 - val_mse: 0.1936 - val_rmse: 0.4400 - val_mae: 0.2473 - val_mape: 7.7478 - lr: 1.0000e-04\n",
      "Epoch 355/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1961 - mse: 0.1961 - rmse: 0.4428 - mae: 0.2504 - mape: 7.8265\n",
      "Epoch 355: val_loss improved from 0.19303 to 0.19301, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1947 - mse: 0.1947 - rmse: 0.4413 - mae: 0.2497 - mape: 7.8123 - val_loss: 0.1930 - val_mse: 0.1930 - val_rmse: 0.4393 - val_mae: 0.2469 - val_mape: 7.7296 - lr: 1.0000e-04\n",
      "Epoch 356/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1938 - mse: 0.1938 - rmse: 0.4403 - mae: 0.2481 - mape: 7.7644\n",
      "Epoch 356: val_loss did not improve from 0.19301\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1945 - mse: 0.1945 - rmse: 0.4410 - mae: 0.2484 - mape: 7.7692 - val_loss: 0.1995 - val_mse: 0.1995 - val_rmse: 0.4466 - val_mae: 0.2526 - val_mape: 7.9641 - lr: 1.0000e-04\n",
      "Epoch 357/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1947 - mse: 0.1947 - rmse: 0.4412 - mae: 0.2485 - mape: 7.7860\n",
      "Epoch 357: val_loss did not improve from 0.19301\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1945 - mse: 0.1945 - rmse: 0.4411 - mae: 0.2485 - mape: 7.7784 - val_loss: 0.1947 - val_mse: 0.1947 - val_rmse: 0.4412 - val_mae: 0.2504 - val_mape: 7.8201 - lr: 1.0000e-04\n",
      "Epoch 358/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1934 - mse: 0.1934 - rmse: 0.4397 - mae: 0.2481 - mape: 7.7690\n",
      "Epoch 358: val_loss did not improve from 0.19301\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1943 - mse: 0.1943 - rmse: 0.4408 - mae: 0.2484 - mape: 7.7757 - val_loss: 0.1934 - val_mse: 0.1934 - val_rmse: 0.4398 - val_mae: 0.2497 - val_mape: 7.7977 - lr: 1.0000e-04\n",
      "Epoch 359/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1950 - mse: 0.1950 - rmse: 0.4415 - mae: 0.2490 - mape: 7.7928\n",
      "Epoch 359: val_loss improved from 0.19301 to 0.19285, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1947 - mse: 0.1947 - rmse: 0.4413 - mae: 0.2489 - mape: 7.7831 - val_loss: 0.1929 - val_mse: 0.1929 - val_rmse: 0.4392 - val_mae: 0.2479 - val_mape: 7.7754 - lr: 1.0000e-04\n",
      "Epoch 360/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1946 - mse: 0.1946 - rmse: 0.4412 - mae: 0.2492 - mape: 7.7993\n",
      "Epoch 360: val_loss did not improve from 0.19285\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1943 - mse: 0.1943 - rmse: 0.4407 - mae: 0.2490 - mape: 7.7973 - val_loss: 0.1936 - val_mse: 0.1936 - val_rmse: 0.4400 - val_mae: 0.2482 - val_mape: 7.7480 - lr: 1.0000e-04\n",
      "Epoch 361/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1955 - mse: 0.1955 - rmse: 0.4421 - mae: 0.2492 - mape: 7.7928\n",
      "Epoch 361: val_loss did not improve from 0.19285\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1945 - mse: 0.1945 - rmse: 0.4410 - mae: 0.2489 - mape: 7.7877 - val_loss: 0.1935 - val_mse: 0.1935 - val_rmse: 0.4398 - val_mae: 0.2499 - val_mape: 7.8428 - lr: 1.0000e-04\n",
      "Epoch 362/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.1963 - mse: 0.1963 - rmse: 0.4431 - mae: 0.2499 - mape: 7.8313\n",
      "Epoch 362: val_loss did not improve from 0.19285\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1950 - mse: 0.1950 - rmse: 0.4415 - mae: 0.2493 - mape: 7.8042 - val_loss: 0.1937 - val_mse: 0.1937 - val_rmse: 0.4401 - val_mae: 0.2480 - val_mape: 7.7646 - lr: 1.0000e-04\n",
      "Epoch 363/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1923 - mse: 0.1923 - rmse: 0.4385 - mae: 0.2475 - mape: 7.7540\n",
      "Epoch 363: val_loss did not improve from 0.19285\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1940 - mse: 0.1940 - rmse: 0.4404 - mae: 0.2485 - mape: 7.7763 - val_loss: 0.1954 - val_mse: 0.1954 - val_rmse: 0.4421 - val_mae: 0.2515 - val_mape: 7.9636 - lr: 1.0000e-04\n",
      "Epoch 364/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1960 - mse: 0.1960 - rmse: 0.4427 - mae: 0.2493 - mape: 7.8083\n",
      "Epoch 364: val_loss improved from 0.19285 to 0.19282, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1946 - mse: 0.1946 - rmse: 0.4412 - mae: 0.2489 - mape: 7.8035 - val_loss: 0.1928 - val_mse: 0.1928 - val_rmse: 0.4391 - val_mae: 0.2480 - val_mape: 7.7408 - lr: 1.0000e-04\n",
      "Epoch 365/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1949 - mse: 0.1949 - rmse: 0.4415 - mae: 0.2498 - mape: 7.8047\n",
      "Epoch 365: val_loss did not improve from 0.19282\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1937 - mse: 0.1937 - rmse: 0.4401 - mae: 0.2487 - mape: 7.7820 - val_loss: 0.1932 - val_mse: 0.1932 - val_rmse: 0.4395 - val_mae: 0.2468 - val_mape: 7.7005 - lr: 1.0000e-04\n",
      "Epoch 366/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1944 - mse: 0.1944 - rmse: 0.4410 - mae: 0.2490 - mape: 7.7952\n",
      "Epoch 366: val_loss improved from 0.19282 to 0.19273, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1941 - mse: 0.1941 - rmse: 0.4406 - mae: 0.2487 - mape: 7.7876 - val_loss: 0.1927 - val_mse: 0.1927 - val_rmse: 0.4390 - val_mae: 0.2459 - val_mape: 7.6931 - lr: 1.0000e-04\n",
      "Epoch 367/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1949 - mse: 0.1949 - rmse: 0.4415 - mae: 0.2486 - mape: 7.7884\n",
      "Epoch 367: val_loss did not improve from 0.19273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1941 - mse: 0.1941 - rmse: 0.4406 - mae: 0.2482 - mape: 7.7680 - val_loss: 0.1930 - val_mse: 0.1930 - val_rmse: 0.4393 - val_mae: 0.2507 - val_mape: 7.8940 - lr: 1.0000e-04\n",
      "Epoch 368/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1931 - mse: 0.1931 - rmse: 0.4395 - mae: 0.2481 - mape: 7.7782\n",
      "Epoch 368: val_loss did not improve from 0.19273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1938 - mse: 0.1938 - rmse: 0.4402 - mae: 0.2488 - mape: 7.7935 - val_loss: 0.1940 - val_mse: 0.1940 - val_rmse: 0.4405 - val_mae: 0.2482 - val_mape: 7.7065 - lr: 1.0000e-04\n",
      "Epoch 369/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1939 - mse: 0.1939 - rmse: 0.4403 - mae: 0.2495 - mape: 7.7952\n",
      "Epoch 369: val_loss did not improve from 0.19273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1947 - mse: 0.1947 - rmse: 0.4412 - mae: 0.2493 - mape: 7.8036 - val_loss: 0.1930 - val_mse: 0.1930 - val_rmse: 0.4394 - val_mae: 0.2482 - val_mape: 7.8074 - lr: 1.0000e-04\n",
      "Epoch 370/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1929 - mse: 0.1929 - rmse: 0.4392 - mae: 0.2476 - mape: 7.7422\n",
      "Epoch 370: val_loss did not improve from 0.19273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1941 - mse: 0.1941 - rmse: 0.4406 - mae: 0.2485 - mape: 7.7654 - val_loss: 0.1965 - val_mse: 0.1965 - val_rmse: 0.4433 - val_mae: 0.2520 - val_mape: 8.0119 - lr: 1.0000e-04\n",
      "Epoch 371/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1922 - mse: 0.1922 - rmse: 0.4384 - mae: 0.2477 - mape: 7.7663\n",
      "Epoch 371: val_loss did not improve from 0.19273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1940 - mse: 0.1940 - rmse: 0.4404 - mae: 0.2487 - mape: 7.7876 - val_loss: 0.1938 - val_mse: 0.1938 - val_rmse: 0.4403 - val_mae: 0.2469 - val_mape: 7.7367 - lr: 1.0000e-04\n",
      "Epoch 372/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1943 - mse: 0.1943 - rmse: 0.4408 - mae: 0.2487 - mape: 7.7859\n",
      "Epoch 372: val_loss did not improve from 0.19273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1948 - mse: 0.1948 - rmse: 0.4413 - mae: 0.2487 - mape: 7.7783 - val_loss: 0.1947 - val_mse: 0.1947 - val_rmse: 0.4412 - val_mae: 0.2480 - val_mape: 7.7490 - lr: 1.0000e-04\n",
      "Epoch 373/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1950 - mse: 0.1950 - rmse: 0.4416 - mae: 0.2492 - mape: 7.7943\n",
      "Epoch 373: val_loss did not improve from 0.19273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1937 - mse: 0.1937 - rmse: 0.4401 - mae: 0.2484 - mape: 7.7753 - val_loss: 0.1930 - val_mse: 0.1930 - val_rmse: 0.4393 - val_mae: 0.2473 - val_mape: 7.7345 - lr: 1.0000e-04\n",
      "Epoch 374/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1936 - mse: 0.1936 - rmse: 0.4400 - mae: 0.2483 - mape: 7.7805\n",
      "Epoch 374: val_loss did not improve from 0.19273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1937 - mse: 0.1937 - rmse: 0.4401 - mae: 0.2483 - mape: 7.7804 - val_loss: 0.1932 - val_mse: 0.1932 - val_rmse: 0.4395 - val_mae: 0.2472 - val_mape: 7.6898 - lr: 1.0000e-04\n",
      "Epoch 375/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1942 - mse: 0.1942 - rmse: 0.4406 - mae: 0.2485 - mape: 7.7818\n",
      "Epoch 375: val_loss did not improve from 0.19273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1938 - mse: 0.1938 - rmse: 0.4403 - mae: 0.2481 - mape: 7.7697 - val_loss: 0.1936 - val_mse: 0.1936 - val_rmse: 0.4400 - val_mae: 0.2461 - val_mape: 7.6189 - lr: 1.0000e-04\n",
      "Epoch 376/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1942 - mse: 0.1942 - rmse: 0.4407 - mae: 0.2486 - mape: 7.7856\n",
      "Epoch 376: val_loss did not improve from 0.19273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1938 - mse: 0.1938 - rmse: 0.4402 - mae: 0.2483 - mape: 7.7752 - val_loss: 0.1945 - val_mse: 0.1945 - val_rmse: 0.4410 - val_mae: 0.2479 - val_mape: 7.7705 - lr: 1.0000e-04\n",
      "Epoch 377/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1934 - mse: 0.1934 - rmse: 0.4398 - mae: 0.2477 - mape: 7.7561\n",
      "Epoch 377: val_loss did not improve from 0.19273\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1934 - mse: 0.1934 - rmse: 0.4398 - mae: 0.2483 - mape: 7.7706 - val_loss: 0.1957 - val_mse: 0.1957 - val_rmse: 0.4423 - val_mae: 0.2520 - val_mape: 8.0234 - lr: 1.0000e-04\n",
      "Epoch 378/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1940 - mse: 0.1940 - rmse: 0.4404 - mae: 0.2484 - mape: 7.7799\n",
      "Epoch 378: val_loss improved from 0.19273 to 0.19218, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1939 - mse: 0.1939 - rmse: 0.4403 - mae: 0.2484 - mape: 7.7777 - val_loss: 0.1922 - val_mse: 0.1922 - val_rmse: 0.4384 - val_mae: 0.2469 - val_mape: 7.7275 - lr: 1.0000e-04\n",
      "Epoch 379/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1936 - mse: 0.1936 - rmse: 0.4399 - mae: 0.2477 - mape: 7.7560\n",
      "Epoch 379: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1940 - mse: 0.1940 - rmse: 0.4405 - mae: 0.2481 - mape: 7.7660 - val_loss: 0.1949 - val_mse: 0.1949 - val_rmse: 0.4415 - val_mae: 0.2498 - val_mape: 7.7876 - lr: 1.0000e-04\n",
      "Epoch 380/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1933 - mse: 0.1933 - rmse: 0.4397 - mae: 0.2484 - mape: 7.7713\n",
      "Epoch 380: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1935 - mse: 0.1935 - rmse: 0.4399 - mae: 0.2485 - mape: 7.7820 - val_loss: 0.1925 - val_mse: 0.1925 - val_rmse: 0.4387 - val_mae: 0.2481 - val_mape: 7.7479 - lr: 1.0000e-04\n",
      "Epoch 381/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1947 - mse: 0.1947 - rmse: 0.4413 - mae: 0.2487 - mape: 7.7816\n",
      "Epoch 381: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1939 - mse: 0.1939 - rmse: 0.4403 - mae: 0.2485 - mape: 7.7740 - val_loss: 0.1926 - val_mse: 0.1926 - val_rmse: 0.4388 - val_mae: 0.2479 - val_mape: 7.7628 - lr: 1.0000e-04\n",
      "Epoch 382/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1924 - mse: 0.1924 - rmse: 0.4387 - mae: 0.2466 - mape: 7.7397\n",
      "Epoch 382: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1938 - mse: 0.1938 - rmse: 0.4403 - mae: 0.2480 - mape: 7.7694 - val_loss: 0.1935 - val_mse: 0.1935 - val_rmse: 0.4399 - val_mae: 0.2484 - val_mape: 7.7267 - lr: 1.0000e-04\n",
      "Epoch 383/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1942 - mse: 0.1942 - rmse: 0.4407 - mae: 0.2484 - mape: 7.7862\n",
      "Epoch 383: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1938 - mse: 0.1938 - rmse: 0.4402 - mae: 0.2486 - mape: 7.7847 - val_loss: 0.1923 - val_mse: 0.1923 - val_rmse: 0.4385 - val_mae: 0.2452 - val_mape: 7.6146 - lr: 1.0000e-04\n",
      "Epoch 384/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1930 - mse: 0.1930 - rmse: 0.4393 - mae: 0.2478 - mape: 7.7485\n",
      "Epoch 384: val_loss improved from 0.19218 to 0.19218, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1942 - mse: 0.1942 - rmse: 0.4407 - mae: 0.2483 - mape: 7.7724 - val_loss: 0.1922 - val_mse: 0.1922 - val_rmse: 0.4384 - val_mae: 0.2463 - val_mape: 7.7218 - lr: 1.0000e-04\n",
      "Epoch 385/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2467 - mape: 7.7387\n",
      "Epoch 385: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1932 - mse: 0.1932 - rmse: 0.4395 - mae: 0.2479 - mape: 7.7568 - val_loss: 0.1932 - val_mse: 0.1932 - val_rmse: 0.4395 - val_mae: 0.2489 - val_mape: 7.8253 - lr: 1.0000e-04\n",
      "Epoch 386/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1932 - mse: 0.1932 - rmse: 0.4396 - mae: 0.2482 - mape: 7.7811\n",
      "Epoch 386: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1932 - mse: 0.1932 - rmse: 0.4396 - mae: 0.2482 - mape: 7.7811 - val_loss: 0.1935 - val_mse: 0.1935 - val_rmse: 0.4399 - val_mae: 0.2461 - val_mape: 7.6439 - lr: 1.0000e-04\n",
      "Epoch 387/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1942 - mse: 0.1942 - rmse: 0.4407 - mae: 0.2484 - mape: 7.7823\n",
      "Epoch 387: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1940 - mse: 0.1940 - rmse: 0.4404 - mae: 0.2484 - mape: 7.7822 - val_loss: 0.1931 - val_mse: 0.1931 - val_rmse: 0.4394 - val_mae: 0.2459 - val_mape: 7.6138 - lr: 1.0000e-04\n",
      "Epoch 388/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1938 - mse: 0.1938 - rmse: 0.4403 - mae: 0.2484 - mape: 7.7647\n",
      "Epoch 388: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1942 - mse: 0.1942 - rmse: 0.4407 - mae: 0.2480 - mape: 7.7636 - val_loss: 0.1928 - val_mse: 0.1928 - val_rmse: 0.4391 - val_mae: 0.2480 - val_mape: 7.7377 - lr: 1.0000e-04\n",
      "Epoch 389/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1914 - mse: 0.1914 - rmse: 0.4374 - mae: 0.2480 - mape: 7.7687\n",
      "Epoch 389: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1937 - mse: 0.1937 - rmse: 0.4401 - mae: 0.2490 - mape: 7.7957 - val_loss: 0.1934 - val_mse: 0.1934 - val_rmse: 0.4397 - val_mae: 0.2457 - val_mape: 7.6910 - lr: 1.0000e-04\n",
      "Epoch 390/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1931 - mse: 0.1931 - rmse: 0.4394 - mae: 0.2479 - mape: 7.7671\n",
      "Epoch 390: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1933 - mse: 0.1933 - rmse: 0.4397 - mae: 0.2480 - mape: 7.7678 - val_loss: 0.1924 - val_mse: 0.1924 - val_rmse: 0.4387 - val_mae: 0.2474 - val_mape: 7.7758 - lr: 1.0000e-04\n",
      "Epoch 391/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1931 - mse: 0.1931 - rmse: 0.4395 - mae: 0.2479 - mape: 7.7596\n",
      "Epoch 391: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1935 - mse: 0.1935 - rmse: 0.4399 - mae: 0.2481 - mape: 7.7667 - val_loss: 0.1948 - val_mse: 0.1948 - val_rmse: 0.4413 - val_mae: 0.2510 - val_mape: 7.9311 - lr: 1.0000e-04\n",
      "Epoch 392/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1927 - mse: 0.1927 - rmse: 0.4390 - mae: 0.2481 - mape: 7.7799\n",
      "Epoch 392: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1933 - mse: 0.1933 - rmse: 0.4396 - mae: 0.2486 - mape: 7.7918 - val_loss: 0.1944 - val_mse: 0.1944 - val_rmse: 0.4409 - val_mae: 0.2478 - val_mape: 7.7102 - lr: 1.0000e-04\n",
      "Epoch 393/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1949 - mse: 0.1949 - rmse: 0.4414 - mae: 0.2491 - mape: 7.7995\n",
      "Epoch 393: val_loss did not improve from 0.19218\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1937 - mse: 0.1937 - rmse: 0.4401 - mae: 0.2484 - mape: 7.7817 - val_loss: 0.1931 - val_mse: 0.1931 - val_rmse: 0.4394 - val_mae: 0.2462 - val_mape: 7.6322 - lr: 1.0000e-04\n",
      "Epoch 394/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1937 - mse: 0.1937 - rmse: 0.4401 - mae: 0.2483 - mape: 7.7706\n",
      "Epoch 394: val_loss improved from 0.19218 to 0.19195, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1940 - mse: 0.1940 - rmse: 0.4405 - mae: 0.2486 - mape: 7.7758 - val_loss: 0.1920 - val_mse: 0.1920 - val_rmse: 0.4381 - val_mae: 0.2450 - val_mape: 7.6609 - lr: 1.0000e-04\n",
      "Epoch 395/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1931 - mse: 0.1931 - rmse: 0.4394 - mae: 0.2476 - mape: 7.7459\n",
      "Epoch 395: val_loss improved from 0.19195 to 0.19181, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1936 - mse: 0.1936 - rmse: 0.4400 - mae: 0.2475 - mape: 7.7512 - val_loss: 0.1918 - val_mse: 0.1918 - val_rmse: 0.4380 - val_mae: 0.2470 - val_mape: 7.7488 - lr: 1.0000e-04\n",
      "Epoch 396/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1936 - mse: 0.1936 - rmse: 0.4400 - mae: 0.2487 - mape: 7.7990\n",
      "Epoch 396: val_loss did not improve from 0.19181\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1941 - mse: 0.1941 - rmse: 0.4405 - mae: 0.2487 - mape: 7.7874 - val_loss: 0.1929 - val_mse: 0.1929 - val_rmse: 0.4391 - val_mae: 0.2459 - val_mape: 7.6909 - lr: 1.0000e-04\n",
      "Epoch 397/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1938 - mse: 0.1938 - rmse: 0.4403 - mae: 0.2486 - mape: 7.7872\n",
      "Epoch 397: val_loss did not improve from 0.19181\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1936 - mse: 0.1936 - rmse: 0.4400 - mae: 0.2485 - mape: 7.7810 - val_loss: 0.1956 - val_mse: 0.1956 - val_rmse: 0.4423 - val_mae: 0.2488 - val_mape: 7.6883 - lr: 1.0000e-04\n",
      "Epoch 398/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.1935 - mse: 0.1935 - rmse: 0.4398 - mae: 0.2479 - mape: 7.7579\n",
      "Epoch 398: val_loss did not improve from 0.19181\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1935 - mse: 0.1935 - rmse: 0.4399 - mae: 0.2484 - mape: 7.7630 - val_loss: 0.1919 - val_mse: 0.1919 - val_rmse: 0.4381 - val_mae: 0.2470 - val_mape: 7.7702 - lr: 1.0000e-04\n",
      "Epoch 399/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1928 - mse: 0.1928 - rmse: 0.4391 - mae: 0.2480 - mape: 7.7609\n",
      "Epoch 399: val_loss did not improve from 0.19181\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1931 - mse: 0.1931 - rmse: 0.4395 - mae: 0.2479 - mape: 7.7681 - val_loss: 0.1931 - val_mse: 0.1931 - val_rmse: 0.4395 - val_mae: 0.2454 - val_mape: 7.6071 - lr: 1.0000e-04\n",
      "Epoch 400/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1935 - mse: 0.1935 - rmse: 0.4399 - mae: 0.2478 - mape: 7.7516\n",
      "Epoch 400: val_loss did not improve from 0.19181\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1932 - mse: 0.1932 - rmse: 0.4396 - mae: 0.2479 - mape: 7.7556 - val_loss: 0.1922 - val_mse: 0.1922 - val_rmse: 0.4384 - val_mae: 0.2483 - val_mape: 7.8308 - lr: 1.0000e-04\n",
      "Epoch 401/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1947 - mse: 0.1947 - rmse: 0.4413 - mae: 0.2483 - mape: 7.7834\n",
      "Epoch 401: val_loss improved from 0.19181 to 0.19163, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1936 - mse: 0.1936 - rmse: 0.4399 - mae: 0.2479 - mape: 7.7705 - val_loss: 0.1916 - val_mse: 0.1916 - val_rmse: 0.4378 - val_mae: 0.2469 - val_mape: 7.7524 - lr: 1.0000e-04\n",
      "Epoch 402/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1955 - mse: 0.1955 - rmse: 0.4421 - mae: 0.2487 - mape: 7.7931\n",
      "Epoch 402: val_loss did not improve from 0.19163\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1943 - mse: 0.1943 - rmse: 0.4408 - mae: 0.2486 - mape: 7.7792 - val_loss: 0.1924 - val_mse: 0.1924 - val_rmse: 0.4386 - val_mae: 0.2473 - val_mape: 7.8004 - lr: 1.0000e-04\n",
      "Epoch 403/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1939 - mse: 0.1939 - rmse: 0.4403 - mae: 0.2483 - mape: 7.7706\n",
      "Epoch 403: val_loss did not improve from 0.19163\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1939 - mse: 0.1939 - rmse: 0.4403 - mae: 0.2484 - mape: 7.7690 - val_loss: 0.1958 - val_mse: 0.1958 - val_rmse: 0.4425 - val_mae: 0.2495 - val_mape: 7.7749 - lr: 1.0000e-04\n",
      "Epoch 404/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1934 - mse: 0.1934 - rmse: 0.4398 - mae: 0.2477 - mape: 7.7644\n",
      "Epoch 404: val_loss improved from 0.19163 to 0.19155, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1934 - mse: 0.1934 - rmse: 0.4398 - mae: 0.2478 - mape: 7.7658 - val_loss: 0.1916 - val_mse: 0.1916 - val_rmse: 0.4377 - val_mae: 0.2461 - val_mape: 7.7132 - lr: 1.0000e-04\n",
      "Epoch 405/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1927 - mse: 0.1927 - rmse: 0.4390 - mae: 0.2472 - mape: 7.7379\n",
      "Epoch 405: val_loss did not improve from 0.19155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1932 - mse: 0.1932 - rmse: 0.4395 - mae: 0.2478 - mape: 7.7574 - val_loss: 0.1919 - val_mse: 0.1919 - val_rmse: 0.4380 - val_mae: 0.2482 - val_mape: 7.8058 - lr: 1.0000e-04\n",
      "Epoch 406/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1940 - mse: 0.1940 - rmse: 0.4404 - mae: 0.2483 - mape: 7.7718\n",
      "Epoch 406: val_loss did not improve from 0.19155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1942 - mse: 0.1942 - rmse: 0.4407 - mae: 0.2485 - mape: 7.7767 - val_loss: 0.1934 - val_mse: 0.1934 - val_rmse: 0.4398 - val_mae: 0.2466 - val_mape: 7.6824 - lr: 1.0000e-04\n",
      "Epoch 407/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1925 - mse: 0.1925 - rmse: 0.4388 - mae: 0.2475 - mape: 7.7570\n",
      "Epoch 407: val_loss did not improve from 0.19155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1932 - mse: 0.1932 - rmse: 0.4396 - mae: 0.2479 - mape: 7.7604 - val_loss: 0.1936 - val_mse: 0.1936 - val_rmse: 0.4400 - val_mae: 0.2475 - val_mape: 7.7257 - lr: 1.0000e-04\n",
      "Epoch 408/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1918 - mse: 0.1918 - rmse: 0.4380 - mae: 0.2473 - mape: 7.7613\n",
      "Epoch 408: val_loss did not improve from 0.19155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1929 - mse: 0.1929 - rmse: 0.4392 - mae: 0.2478 - mape: 7.7791 - val_loss: 0.1919 - val_mse: 0.1919 - val_rmse: 0.4380 - val_mae: 0.2465 - val_mape: 7.6769 - lr: 1.0000e-04\n",
      "Epoch 409/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1950 - mse: 0.1950 - rmse: 0.4416 - mae: 0.2485 - mape: 7.7750\n",
      "Epoch 409: val_loss did not improve from 0.19155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1942 - mse: 0.1942 - rmse: 0.4407 - mae: 0.2482 - mape: 7.7726 - val_loss: 0.1930 - val_mse: 0.1930 - val_rmse: 0.4393 - val_mae: 0.2464 - val_mape: 7.7420 - lr: 1.0000e-04\n",
      "Epoch 410/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1931 - mse: 0.1931 - rmse: 0.4394 - mae: 0.2485 - mape: 7.7845\n",
      "Epoch 410: val_loss did not improve from 0.19155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1930 - mse: 0.1930 - rmse: 0.4394 - mae: 0.2481 - mape: 7.7661 - val_loss: 0.1938 - val_mse: 0.1938 - val_rmse: 0.4402 - val_mae: 0.2462 - val_mape: 7.7233 - lr: 1.0000e-04\n",
      "Epoch 411/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1933 - mse: 0.1933 - rmse: 0.4396 - mae: 0.2480 - mape: 7.7584\n",
      "Epoch 411: val_loss did not improve from 0.19155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1933 - mse: 0.1933 - rmse: 0.4397 - mae: 0.2480 - mape: 7.7667 - val_loss: 0.1927 - val_mse: 0.1927 - val_rmse: 0.4390 - val_mae: 0.2467 - val_mape: 7.7004 - lr: 1.0000e-04\n",
      "Epoch 412/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1945 - mse: 0.1945 - rmse: 0.4410 - mae: 0.2486 - mape: 7.7824\n",
      "Epoch 412: val_loss did not improve from 0.19155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1932 - mse: 0.1932 - rmse: 0.4396 - mae: 0.2480 - mape: 7.7651 - val_loss: 0.1924 - val_mse: 0.1924 - val_rmse: 0.4387 - val_mae: 0.2470 - val_mape: 7.7084 - lr: 1.0000e-04\n",
      "Epoch 413/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1932 - mse: 0.1932 - rmse: 0.4396 - mae: 0.2477 - mape: 7.7614\n",
      "Epoch 413: val_loss did not improve from 0.19155\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1931 - mse: 0.1931 - rmse: 0.4395 - mae: 0.2476 - mape: 7.7579 - val_loss: 0.1931 - val_mse: 0.1931 - val_rmse: 0.4394 - val_mae: 0.2471 - val_mape: 7.7428 - lr: 1.0000e-04\n",
      "Epoch 414/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2467 - mape: 7.7273\n",
      "Epoch 414: val_loss improved from 0.19155 to 0.19154, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1929 - mse: 0.1929 - rmse: 0.4393 - mae: 0.2474 - mape: 7.7480 - val_loss: 0.1915 - val_mse: 0.1915 - val_rmse: 0.4377 - val_mae: 0.2471 - val_mape: 7.7820 - lr: 1.0000e-04\n",
      "Epoch 415/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1941 - mse: 0.1941 - rmse: 0.4406 - mae: 0.2488 - mape: 7.7871\n",
      "Epoch 415: val_loss did not improve from 0.19154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1930 - mse: 0.1930 - rmse: 0.4394 - mae: 0.2478 - mape: 7.7709 - val_loss: 0.1922 - val_mse: 0.1922 - val_rmse: 0.4384 - val_mae: 0.2451 - val_mape: 7.6370 - lr: 1.0000e-04\n",
      "Epoch 416/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1927 - mse: 0.1927 - rmse: 0.4390 - mae: 0.2475 - mape: 7.7379\n",
      "Epoch 416: val_loss did not improve from 0.19154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1930 - mse: 0.1930 - rmse: 0.4394 - mae: 0.2479 - mape: 7.7520 - val_loss: 0.1920 - val_mse: 0.1920 - val_rmse: 0.4382 - val_mae: 0.2472 - val_mape: 7.7539 - lr: 1.0000e-04\n",
      "Epoch 417/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1927 - mse: 0.1927 - rmse: 0.4389 - mae: 0.2483 - mape: 7.7775\n",
      "Epoch 417: val_loss did not improve from 0.19154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1929 - mse: 0.1929 - rmse: 0.4392 - mae: 0.2478 - mape: 7.7499 - val_loss: 0.1952 - val_mse: 0.1952 - val_rmse: 0.4418 - val_mae: 0.2521 - val_mape: 8.0202 - lr: 1.0000e-04\n",
      "Epoch 418/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1958 - mse: 0.1958 - rmse: 0.4425 - mae: 0.2487 - mape: 7.7947\n",
      "Epoch 418: val_loss did not improve from 0.19154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1931 - mse: 0.1931 - rmse: 0.4395 - mae: 0.2478 - mape: 7.7798 - val_loss: 0.1919 - val_mse: 0.1919 - val_rmse: 0.4380 - val_mae: 0.2462 - val_mape: 7.6617 - lr: 1.0000e-04\n",
      "Epoch 419/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1939 - mse: 0.1939 - rmse: 0.4403 - mae: 0.2480 - mape: 7.7605\n",
      "Epoch 419: val_loss did not improve from 0.19154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1935 - mse: 0.1935 - rmse: 0.4399 - mae: 0.2479 - mape: 7.7573 - val_loss: 0.1962 - val_mse: 0.1962 - val_rmse: 0.4429 - val_mae: 0.2499 - val_mape: 7.7728 - lr: 1.0000e-04\n",
      "Epoch 420/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1918 - mse: 0.1918 - rmse: 0.4379 - mae: 0.2467 - mape: 7.7174\n",
      "Epoch 420: val_loss did not improve from 0.19154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1934 - mse: 0.1934 - rmse: 0.4398 - mae: 0.2477 - mape: 7.7495 - val_loss: 0.1928 - val_mse: 0.1928 - val_rmse: 0.4391 - val_mae: 0.2464 - val_mape: 7.6510 - lr: 1.0000e-04\n",
      "Epoch 421/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1933 - mse: 0.1933 - rmse: 0.4397 - mae: 0.2477 - mape: 7.7640\n",
      "Epoch 421: val_loss did not improve from 0.19154\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1927 - mse: 0.1927 - rmse: 0.4390 - mae: 0.2473 - mape: 7.7557 - val_loss: 0.1917 - val_mse: 0.1917 - val_rmse: 0.4378 - val_mae: 0.2447 - val_mape: 7.6076 - lr: 1.0000e-04\n",
      "Epoch 422/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1942 - mse: 0.1942 - rmse: 0.4407 - mae: 0.2489 - mape: 7.7833\n",
      "Epoch 422: val_loss improved from 0.19154 to 0.19136, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1931 - mse: 0.1931 - rmse: 0.4395 - mae: 0.2475 - mape: 7.7394 - val_loss: 0.1914 - val_mse: 0.1914 - val_rmse: 0.4374 - val_mae: 0.2470 - val_mape: 7.7940 - lr: 1.0000e-04\n",
      "Epoch 423/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1930 - mse: 0.1930 - rmse: 0.4393 - mae: 0.2474 - mape: 7.7577\n",
      "Epoch 423: val_loss did not improve from 0.19136\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1927 - mse: 0.1927 - rmse: 0.4390 - mae: 0.2473 - mape: 7.7481 - val_loss: 0.1917 - val_mse: 0.1917 - val_rmse: 0.4379 - val_mae: 0.2460 - val_mape: 7.7035 - lr: 1.0000e-04\n",
      "Epoch 424/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1933 - mse: 0.1933 - rmse: 0.4397 - mae: 0.2471 - mape: 7.7346\n",
      "Epoch 424: val_loss did not improve from 0.19136\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1933 - mse: 0.1933 - rmse: 0.4397 - mae: 0.2471 - mape: 7.7346 - val_loss: 0.1958 - val_mse: 0.1958 - val_rmse: 0.4425 - val_mae: 0.2525 - val_mape: 7.9770 - lr: 1.0000e-04\n",
      "Epoch 425/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1934 - mse: 0.1934 - rmse: 0.4398 - mae: 0.2481 - mape: 7.7699\n",
      "Epoch 425: val_loss did not improve from 0.19136\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1930 - mse: 0.1930 - rmse: 0.4393 - mae: 0.2477 - mape: 7.7612 - val_loss: 0.1919 - val_mse: 0.1919 - val_rmse: 0.4381 - val_mae: 0.2479 - val_mape: 7.8051 - lr: 1.0000e-04\n",
      "Epoch 426/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1928 - mse: 0.1928 - rmse: 0.4391 - mae: 0.2471 - mape: 7.7426\n",
      "Epoch 426: val_loss did not improve from 0.19136\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1928 - mse: 0.1928 - rmse: 0.4391 - mae: 0.2471 - mape: 7.7426 - val_loss: 0.1920 - val_mse: 0.1920 - val_rmse: 0.4382 - val_mae: 0.2484 - val_mape: 7.8446 - lr: 1.0000e-04\n",
      "Epoch 427/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1933 - mse: 0.1933 - rmse: 0.4396 - mae: 0.2482 - mape: 7.7773\n",
      "Epoch 427: val_loss did not improve from 0.19136\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1932 - mse: 0.1932 - rmse: 0.4395 - mae: 0.2484 - mape: 7.7918 - val_loss: 0.1925 - val_mse: 0.1925 - val_rmse: 0.4388 - val_mae: 0.2452 - val_mape: 7.6227 - lr: 1.0000e-04\n",
      "Epoch 428/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1937 - mse: 0.1937 - rmse: 0.4401 - mae: 0.2476 - mape: 7.7419\n",
      "Epoch 428: val_loss did not improve from 0.19136\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1931 - mse: 0.1931 - rmse: 0.4394 - mae: 0.2474 - mape: 7.7421 - val_loss: 0.1915 - val_mse: 0.1915 - val_rmse: 0.4376 - val_mae: 0.2447 - val_mape: 7.6090 - lr: 1.0000e-04\n",
      "Epoch 429/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1927 - mse: 0.1927 - rmse: 0.4390 - mae: 0.2473 - mape: 7.7483\n",
      "Epoch 429: val_loss did not improve from 0.19136\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1925 - mse: 0.1925 - rmse: 0.4387 - mae: 0.2472 - mape: 7.7457 - val_loss: 0.1960 - val_mse: 0.1960 - val_rmse: 0.4427 - val_mae: 0.2519 - val_mape: 7.8769 - lr: 1.0000e-04\n",
      "Epoch 430/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2464 - mape: 7.7147\n",
      "Epoch 430: val_loss did not improve from 0.19136\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1926 - mse: 0.1926 - rmse: 0.4389 - mae: 0.2473 - mape: 7.7459 - val_loss: 0.1914 - val_mse: 0.1914 - val_rmse: 0.4375 - val_mae: 0.2467 - val_mape: 7.7129 - lr: 1.0000e-04\n",
      "Epoch 431/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1937 - mse: 0.1937 - rmse: 0.4401 - mae: 0.2477 - mape: 7.7539\n",
      "Epoch 431: val_loss improved from 0.19136 to 0.19124, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1932 - mse: 0.1932 - rmse: 0.4396 - mae: 0.2475 - mape: 7.7504 - val_loss: 0.1912 - val_mse: 0.1912 - val_rmse: 0.4373 - val_mae: 0.2466 - val_mape: 7.7022 - lr: 1.0000e-04\n",
      "Epoch 432/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1935 - mse: 0.1935 - rmse: 0.4399 - mae: 0.2484 - mape: 7.7723\n",
      "Epoch 432: val_loss improved from 0.19124 to 0.19124, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1930 - mse: 0.1930 - rmse: 0.4393 - mae: 0.2481 - mape: 7.7711 - val_loss: 0.1912 - val_mse: 0.1912 - val_rmse: 0.4373 - val_mae: 0.2466 - val_mape: 7.7229 - lr: 1.0000e-04\n",
      "Epoch 433/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.1931 - mse: 0.1931 - rmse: 0.4394 - mae: 0.2477 - mape: 7.7584\n",
      "Epoch 433: val_loss did not improve from 0.19124\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1925 - mse: 0.1925 - rmse: 0.4388 - mae: 0.2473 - mape: 7.7359 - val_loss: 0.1921 - val_mse: 0.1921 - val_rmse: 0.4382 - val_mae: 0.2457 - val_mape: 7.6732 - lr: 1.0000e-04\n",
      "Epoch 434/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1924 - mse: 0.1924 - rmse: 0.4386 - mae: 0.2475 - mape: 7.7390\n",
      "Epoch 434: val_loss did not improve from 0.19124\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1926 - mse: 0.1926 - rmse: 0.4389 - mae: 0.2476 - mape: 7.7554 - val_loss: 0.1923 - val_mse: 0.1923 - val_rmse: 0.4385 - val_mae: 0.2462 - val_mape: 7.7077 - lr: 1.0000e-04\n",
      "Epoch 435/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1943 - mse: 0.1943 - rmse: 0.4408 - mae: 0.2477 - mape: 7.7568\n",
      "Epoch 435: val_loss did not improve from 0.19124\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1926 - mse: 0.1926 - rmse: 0.4388 - mae: 0.2473 - mape: 7.7438 - val_loss: 0.1920 - val_mse: 0.1920 - val_rmse: 0.4382 - val_mae: 0.2486 - val_mape: 7.8138 - lr: 1.0000e-04\n",
      "Epoch 436/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2463 - mape: 7.7308\n",
      "Epoch 436: val_loss improved from 0.19124 to 0.19110, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1935 - mse: 0.1935 - rmse: 0.4398 - mae: 0.2476 - mape: 7.7576 - val_loss: 0.1911 - val_mse: 0.1911 - val_rmse: 0.4371 - val_mae: 0.2475 - val_mape: 7.7499 - lr: 1.0000e-04\n",
      "Epoch 437/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1922 - mse: 0.1922 - rmse: 0.4385 - mae: 0.2477 - mape: 7.7630\n",
      "Epoch 437: val_loss did not improve from 0.19110\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1927 - mse: 0.1927 - rmse: 0.4390 - mae: 0.2477 - mape: 7.7578 - val_loss: 0.1937 - val_mse: 0.1937 - val_rmse: 0.4402 - val_mae: 0.2473 - val_mape: 7.7349 - lr: 1.0000e-04\n",
      "Epoch 438/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2469 - mape: 7.7325\n",
      "Epoch 438: val_loss did not improve from 0.19110\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1927 - mse: 0.1927 - rmse: 0.4390 - mae: 0.2472 - mape: 7.7411 - val_loss: 0.1914 - val_mse: 0.1914 - val_rmse: 0.4375 - val_mae: 0.2465 - val_mape: 7.6988 - lr: 1.0000e-04\n",
      "Epoch 439/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1930 - mse: 0.1930 - rmse: 0.4393 - mae: 0.2476 - mape: 7.7463\n",
      "Epoch 439: val_loss improved from 0.19110 to 0.19097, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1926 - mse: 0.1926 - rmse: 0.4389 - mae: 0.2473 - mape: 7.7430 - val_loss: 0.1910 - val_mse: 0.1910 - val_rmse: 0.4370 - val_mae: 0.2459 - val_mape: 7.6959 - lr: 1.0000e-04\n",
      "Epoch 440/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1926 - mse: 0.1926 - rmse: 0.4389 - mae: 0.2470 - mape: 7.7311\n",
      "Epoch 440: val_loss did not improve from 0.19097\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1930 - mse: 0.1930 - rmse: 0.4393 - mae: 0.2475 - mape: 7.7557 - val_loss: 0.1910 - val_mse: 0.1910 - val_rmse: 0.4370 - val_mae: 0.2446 - val_mape: 7.6522 - lr: 1.0000e-04\n",
      "Epoch 441/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1926 - mse: 0.1926 - rmse: 0.4389 - mae: 0.2470 - mape: 7.7339\n",
      "Epoch 441: val_loss did not improve from 0.19097\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1927 - mse: 0.1927 - rmse: 0.4389 - mae: 0.2469 - mape: 7.7251 - val_loss: 0.1917 - val_mse: 0.1917 - val_rmse: 0.4379 - val_mae: 0.2471 - val_mape: 7.7817 - lr: 1.0000e-04\n",
      "Epoch 442/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1937 - mse: 0.1937 - rmse: 0.4402 - mae: 0.2480 - mape: 7.7694\n",
      "Epoch 442: val_loss did not improve from 0.19097\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1929 - mse: 0.1929 - rmse: 0.4393 - mae: 0.2477 - mape: 7.7658 - val_loss: 0.1936 - val_mse: 0.1936 - val_rmse: 0.4400 - val_mae: 0.2465 - val_mape: 7.6195 - lr: 1.0000e-04\n",
      "Epoch 443/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1936 - mse: 0.1936 - rmse: 0.4400 - mae: 0.2477 - mape: 7.7590\n",
      "Epoch 443: val_loss did not improve from 0.19097\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1930 - mse: 0.1930 - rmse: 0.4393 - mae: 0.2477 - mape: 7.7573 - val_loss: 0.1912 - val_mse: 0.1912 - val_rmse: 0.4372 - val_mae: 0.2467 - val_mape: 7.7680 - lr: 1.0000e-04\n",
      "Epoch 444/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1931 - mse: 0.1931 - rmse: 0.4394 - mae: 0.2474 - mape: 7.7308\n",
      "Epoch 444: val_loss did not improve from 0.19097\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1926 - mse: 0.1926 - rmse: 0.4388 - mae: 0.2472 - mape: 7.7270 - val_loss: 0.1950 - val_mse: 0.1950 - val_rmse: 0.4416 - val_mae: 0.2512 - val_mape: 7.9884 - lr: 1.0000e-04\n",
      "Epoch 445/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1927 - mse: 0.1927 - rmse: 0.4389 - mae: 0.2476 - mape: 7.7705\n",
      "Epoch 445: val_loss improved from 0.19097 to 0.19091, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1928 - mse: 0.1928 - rmse: 0.4391 - mae: 0.2475 - mape: 7.7597 - val_loss: 0.1909 - val_mse: 0.1909 - val_rmse: 0.4369 - val_mae: 0.2435 - val_mape: 7.5885 - lr: 1.0000e-04\n",
      "Epoch 446/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1923 - mse: 0.1923 - rmse: 0.4385 - mae: 0.2464 - mape: 7.7152\n",
      "Epoch 446: val_loss did not improve from 0.19091\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2464 - mape: 7.7131 - val_loss: 0.1912 - val_mse: 0.1912 - val_rmse: 0.4373 - val_mae: 0.2459 - val_mape: 7.7410 - lr: 1.0000e-04\n",
      "Epoch 447/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1933 - mse: 0.1933 - rmse: 0.4396 - mae: 0.2473 - mape: 7.7544\n",
      "Epoch 447: val_loss did not improve from 0.19091\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1923 - mse: 0.1923 - rmse: 0.4386 - mae: 0.2468 - mape: 7.7385 - val_loss: 0.1911 - val_mse: 0.1911 - val_rmse: 0.4372 - val_mae: 0.2463 - val_mape: 7.6951 - lr: 1.0000e-04\n",
      "Epoch 448/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1922 - mse: 0.1922 - rmse: 0.4384 - mae: 0.2472 - mape: 7.7690\n",
      "Epoch 448: val_loss did not improve from 0.19091\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1931 - mse: 0.1931 - rmse: 0.4395 - mae: 0.2478 - mape: 7.7574 - val_loss: 0.1967 - val_mse: 0.1967 - val_rmse: 0.4436 - val_mae: 0.2523 - val_mape: 8.0451 - lr: 1.0000e-04\n",
      "Epoch 449/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1926 - mse: 0.1926 - rmse: 0.4389 - mae: 0.2475 - mape: 7.7527\n",
      "Epoch 449: val_loss did not improve from 0.19091\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1932 - mse: 0.1932 - rmse: 0.4396 - mae: 0.2479 - mape: 7.7711 - val_loss: 0.1910 - val_mse: 0.1910 - val_rmse: 0.4371 - val_mae: 0.2438 - val_mape: 7.6016 - lr: 1.0000e-04\n",
      "Epoch 450/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1910 - mse: 0.1910 - rmse: 0.4371 - mae: 0.2460 - mape: 7.7013\n",
      "Epoch 450: val_loss did not improve from 0.19091\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1924 - mse: 0.1924 - rmse: 0.4387 - mae: 0.2467 - mape: 7.7159 - val_loss: 0.1921 - val_mse: 0.1921 - val_rmse: 0.4383 - val_mae: 0.2488 - val_mape: 7.8716 - lr: 1.0000e-04\n",
      "Epoch 451/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1938 - mse: 0.1938 - rmse: 0.4402 - mae: 0.2471 - mape: 7.7584\n",
      "Epoch 451: val_loss did not improve from 0.19091\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1926 - mse: 0.1926 - rmse: 0.4388 - mae: 0.2468 - mape: 7.7506 - val_loss: 0.1916 - val_mse: 0.1916 - val_rmse: 0.4377 - val_mae: 0.2451 - val_mape: 7.6591 - lr: 1.0000e-04\n",
      "Epoch 452/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1928 - mse: 0.1928 - rmse: 0.4391 - mae: 0.2473 - mape: 7.7357\n",
      "Epoch 452: val_loss did not improve from 0.19091\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1931 - mse: 0.1931 - rmse: 0.4395 - mae: 0.2475 - mape: 7.7380 - val_loss: 0.2000 - val_mse: 0.2000 - val_rmse: 0.4472 - val_mae: 0.2569 - val_mape: 8.1237 - lr: 1.0000e-04\n",
      "Epoch 453/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1931 - mse: 0.1931 - rmse: 0.4394 - mae: 0.2480 - mape: 7.7722\n",
      "Epoch 453: val_loss did not improve from 0.19091\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1931 - mse: 0.1931 - rmse: 0.4394 - mae: 0.2478 - mape: 7.7584 - val_loss: 0.1924 - val_mse: 0.1924 - val_rmse: 0.4387 - val_mae: 0.2497 - val_mape: 7.9148 - lr: 1.0000e-04\n",
      "Epoch 454/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1935 - mse: 0.1935 - rmse: 0.4399 - mae: 0.2478 - mape: 7.7619\n",
      "Epoch 454: val_loss did not improve from 0.19091\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1922 - mse: 0.1922 - rmse: 0.4384 - mae: 0.2471 - mape: 7.7497 - val_loss: 0.1910 - val_mse: 0.1910 - val_rmse: 0.4370 - val_mae: 0.2457 - val_mape: 7.6692 - lr: 1.0000e-04\n",
      "Epoch 455/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1928 - mse: 0.1928 - rmse: 0.4390 - mae: 0.2469 - mape: 7.7371\n",
      "Epoch 455: val_loss did not improve from 0.19091\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1924 - mse: 0.1924 - rmse: 0.4387 - mae: 0.2468 - mape: 7.7314 - val_loss: 0.1911 - val_mse: 0.1911 - val_rmse: 0.4371 - val_mae: 0.2432 - val_mape: 7.5675 - lr: 1.0000e-04\n",
      "Epoch 456/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.1899 - mse: 0.1899 - rmse: 0.4357 - mae: 0.2458 - mape: 7.7108\n",
      "Epoch 456: val_loss did not improve from 0.19091\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1922 - mse: 0.1922 - rmse: 0.4384 - mae: 0.2465 - mape: 7.7158 - val_loss: 0.1945 - val_mse: 0.1945 - val_rmse: 0.4410 - val_mae: 0.2501 - val_mape: 7.8680 - lr: 1.0000e-04\n",
      "Epoch 457/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1917 - mse: 0.1917 - rmse: 0.4379 - mae: 0.2469 - mape: 7.7238\n",
      "Epoch 457: val_loss improved from 0.19091 to 0.19076, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1925 - mse: 0.1925 - rmse: 0.4388 - mae: 0.2470 - mape: 7.7479 - val_loss: 0.1908 - val_mse: 0.1908 - val_rmse: 0.4368 - val_mae: 0.2443 - val_mape: 7.6162 - lr: 1.0000e-04\n",
      "Epoch 458/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1900 - mse: 0.1900 - rmse: 0.4359 - mae: 0.2458 - mape: 7.7057\n",
      "Epoch 458: val_loss did not improve from 0.19076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2470 - mape: 7.7291 - val_loss: 0.1955 - val_mse: 0.1955 - val_rmse: 0.4422 - val_mae: 0.2481 - val_mape: 7.8300 - lr: 1.0000e-04\n",
      "Epoch 459/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1932 - mse: 0.1932 - rmse: 0.4396 - mae: 0.2476 - mape: 7.7493\n",
      "Epoch 459: val_loss did not improve from 0.19076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1926 - mse: 0.1926 - rmse: 0.4389 - mae: 0.2475 - mape: 7.7421 - val_loss: 0.1910 - val_mse: 0.1910 - val_rmse: 0.4370 - val_mae: 0.2459 - val_mape: 7.7427 - lr: 1.0000e-04\n",
      "Epoch 460/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2463 - mape: 7.7165\n",
      "Epoch 460: val_loss did not improve from 0.19076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1924 - mse: 0.1924 - rmse: 0.4387 - mae: 0.2467 - mape: 7.7304 - val_loss: 0.1911 - val_mse: 0.1911 - val_rmse: 0.4371 - val_mae: 0.2445 - val_mape: 7.6897 - lr: 1.0000e-04\n",
      "Epoch 461/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1932 - mse: 0.1932 - rmse: 0.4396 - mae: 0.2463 - mape: 7.6969\n",
      "Epoch 461: val_loss did not improve from 0.19076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2462 - mape: 7.7222 - val_loss: 0.1932 - val_mse: 0.1932 - val_rmse: 0.4396 - val_mae: 0.2470 - val_mape: 7.6517 - lr: 1.0000e-04\n",
      "Epoch 462/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1928 - mse: 0.1928 - rmse: 0.4391 - mae: 0.2461 - mape: 7.7059\n",
      "Epoch 462: val_loss did not improve from 0.19076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1925 - mse: 0.1925 - rmse: 0.4387 - mae: 0.2467 - mape: 7.7318 - val_loss: 0.1910 - val_mse: 0.1910 - val_rmse: 0.4371 - val_mae: 0.2452 - val_mape: 7.6336 - lr: 1.0000e-04\n",
      "Epoch 463/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1927 - mse: 0.1927 - rmse: 0.4390 - mae: 0.2470 - mape: 7.7302\n",
      "Epoch 463: val_loss did not improve from 0.19076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1923 - mse: 0.1923 - rmse: 0.4386 - mae: 0.2469 - mape: 7.7282 - val_loss: 0.1926 - val_mse: 0.1926 - val_rmse: 0.4388 - val_mae: 0.2492 - val_mape: 7.9138 - lr: 1.0000e-04\n",
      "Epoch 464/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1924 - mse: 0.1924 - rmse: 0.4387 - mae: 0.2470 - mape: 7.7538\n",
      "Epoch 464: val_loss did not improve from 0.19076\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2466 - mape: 7.7419 - val_loss: 0.1909 - val_mse: 0.1909 - val_rmse: 0.4369 - val_mae: 0.2448 - val_mape: 7.6128 - lr: 1.0000e-04\n",
      "Epoch 465/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1917 - mse: 0.1917 - rmse: 0.4378 - mae: 0.2464 - mape: 7.7152\n",
      "Epoch 465: val_loss improved from 0.19076 to 0.19058, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1918 - mse: 0.1918 - rmse: 0.4380 - mae: 0.2467 - mape: 7.7190 - val_loss: 0.1906 - val_mse: 0.1906 - val_rmse: 0.4366 - val_mae: 0.2455 - val_mape: 7.7125 - lr: 1.0000e-04\n",
      "Epoch 466/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1921 - mse: 0.1921 - rmse: 0.4382 - mae: 0.2459 - mape: 7.7109\n",
      "Epoch 466: val_loss did not improve from 0.19058\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1927 - mse: 0.1927 - rmse: 0.4390 - mae: 0.2463 - mape: 7.7146 - val_loss: 0.1914 - val_mse: 0.1914 - val_rmse: 0.4375 - val_mae: 0.2492 - val_mape: 7.8861 - lr: 1.0000e-04\n",
      "Epoch 467/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1924 - mse: 0.1924 - rmse: 0.4386 - mae: 0.2465 - mape: 7.7415\n",
      "Epoch 467: val_loss did not improve from 0.19058\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1927 - mse: 0.1927 - rmse: 0.4390 - mae: 0.2471 - mape: 7.7527 - val_loss: 0.1913 - val_mse: 0.1913 - val_rmse: 0.4374 - val_mae: 0.2467 - val_mape: 7.6878 - lr: 1.0000e-04\n",
      "Epoch 468/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2462 - mape: 7.7132\n",
      "Epoch 468: val_loss did not improve from 0.19058\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2462 - mape: 7.7105 - val_loss: 0.1908 - val_mse: 0.1908 - val_rmse: 0.4369 - val_mae: 0.2471 - val_mape: 7.7964 - lr: 1.0000e-04\n",
      "Epoch 469/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2469 - mape: 7.7396\n",
      "Epoch 469: val_loss did not improve from 0.19058\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2467 - mape: 7.7263 - val_loss: 0.1932 - val_mse: 0.1932 - val_rmse: 0.4396 - val_mae: 0.2470 - val_mape: 7.7291 - lr: 1.0000e-04\n",
      "Epoch 470/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1923 - mse: 0.1923 - rmse: 0.4386 - mae: 0.2463 - mape: 7.7127\n",
      "Epoch 470: val_loss did not improve from 0.19058\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1919 - mse: 0.1919 - rmse: 0.4381 - mae: 0.2462 - mape: 7.7094 - val_loss: 0.1924 - val_mse: 0.1924 - val_rmse: 0.4386 - val_mae: 0.2504 - val_mape: 7.9934 - lr: 1.0000e-04\n",
      "Epoch 471/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1935 - mse: 0.1935 - rmse: 0.4399 - mae: 0.2465 - mape: 7.7270\n",
      "Epoch 471: val_loss did not improve from 0.19058\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1929 - mse: 0.1929 - rmse: 0.4392 - mae: 0.2465 - mape: 7.7353 - val_loss: 0.1924 - val_mse: 0.1924 - val_rmse: 0.4387 - val_mae: 0.2459 - val_mape: 7.6040 - lr: 1.0000e-04\n",
      "Epoch 472/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1937 - mse: 0.1937 - rmse: 0.4401 - mae: 0.2470 - mape: 7.7380\n",
      "Epoch 472: val_loss improved from 0.19058 to 0.19035, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1933 - mse: 0.1933 - rmse: 0.4396 - mae: 0.2476 - mape: 7.7553 - val_loss: 0.1903 - val_mse: 0.1903 - val_rmse: 0.4363 - val_mae: 0.2463 - val_mape: 7.7437 - lr: 1.0000e-04\n",
      "Epoch 473/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1925 - mse: 0.1925 - rmse: 0.4387 - mae: 0.2472 - mape: 7.7459\n",
      "Epoch 473: val_loss did not improve from 0.19035\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1924 - mse: 0.1924 - rmse: 0.4387 - mae: 0.2472 - mape: 7.7454 - val_loss: 0.1907 - val_mse: 0.1907 - val_rmse: 0.4367 - val_mae: 0.2437 - val_mape: 7.6061 - lr: 1.0000e-04\n",
      "Epoch 474/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1925 - mse: 0.1925 - rmse: 0.4387 - mae: 0.2466 - mape: 7.7143\n",
      "Epoch 474: val_loss did not improve from 0.19035\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1925 - mse: 0.1925 - rmse: 0.4388 - mae: 0.2470 - mape: 7.7294 - val_loss: 0.1911 - val_mse: 0.1911 - val_rmse: 0.4372 - val_mae: 0.2450 - val_mape: 7.6989 - lr: 1.0000e-04\n",
      "Epoch 475/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1928 - mse: 0.1928 - rmse: 0.4391 - mae: 0.2466 - mape: 7.7345\n",
      "Epoch 475: val_loss did not improve from 0.19035\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1922 - mse: 0.1922 - rmse: 0.4384 - mae: 0.2464 - mape: 7.7266 - val_loss: 0.1907 - val_mse: 0.1907 - val_rmse: 0.4366 - val_mae: 0.2445 - val_mape: 7.6137 - lr: 1.0000e-04\n",
      "Epoch 476/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1929 - mse: 0.1929 - rmse: 0.4392 - mae: 0.2467 - mape: 7.7340\n",
      "Epoch 476: val_loss improved from 0.19035 to 0.19027, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2463 - mape: 7.7148 - val_loss: 0.1903 - val_mse: 0.1903 - val_rmse: 0.4362 - val_mae: 0.2456 - val_mape: 7.7292 - lr: 1.0000e-04\n",
      "Epoch 477/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1911 - mse: 0.1911 - rmse: 0.4372 - mae: 0.2458 - mape: 7.7172\n",
      "Epoch 477: val_loss did not improve from 0.19027\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1922 - mse: 0.1922 - rmse: 0.4384 - mae: 0.2466 - mape: 7.7296 - val_loss: 0.1934 - val_mse: 0.1934 - val_rmse: 0.4398 - val_mae: 0.2488 - val_mape: 7.7793 - lr: 1.0000e-04\n",
      "Epoch 478/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2460 - mape: 7.7026\n",
      "Epoch 478: val_loss did not improve from 0.19027\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1924 - mse: 0.1924 - rmse: 0.4386 - mae: 0.2466 - mape: 7.7369 - val_loss: 0.1914 - val_mse: 0.1914 - val_rmse: 0.4375 - val_mae: 0.2456 - val_mape: 7.6517 - lr: 1.0000e-04\n",
      "Epoch 479/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1934 - mse: 0.1934 - rmse: 0.4397 - mae: 0.2467 - mape: 7.7275\n",
      "Epoch 479: val_loss did not improve from 0.19027\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1922 - mse: 0.1922 - rmse: 0.4384 - mae: 0.2463 - mape: 7.7132 - val_loss: 0.1918 - val_mse: 0.1918 - val_rmse: 0.4380 - val_mae: 0.2463 - val_mape: 7.7159 - lr: 1.0000e-04\n",
      "Epoch 480/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1929 - mse: 0.1929 - rmse: 0.4392 - mae: 0.2466 - mape: 7.7279\n",
      "Epoch 480: val_loss did not improve from 0.19027\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1925 - mse: 0.1925 - rmse: 0.4387 - mae: 0.2466 - mape: 7.7255 - val_loss: 0.1916 - val_mse: 0.1916 - val_rmse: 0.4377 - val_mae: 0.2451 - val_mape: 7.6100 - lr: 1.0000e-04\n",
      "Epoch 481/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1934 - mse: 0.1934 - rmse: 0.4398 - mae: 0.2473 - mape: 7.7365\n",
      "Epoch 481: val_loss did not improve from 0.19027\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1928 - mse: 0.1928 - rmse: 0.4391 - mae: 0.2472 - mape: 7.7407 - val_loss: 0.1908 - val_mse: 0.1908 - val_rmse: 0.4368 - val_mae: 0.2451 - val_mape: 7.6731 - lr: 1.0000e-04\n",
      "Epoch 482/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1919 - mse: 0.1919 - rmse: 0.4380 - mae: 0.2463 - mape: 7.7041\n",
      "Epoch 482: val_loss did not improve from 0.19027\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1926 - mse: 0.1926 - rmse: 0.4388 - mae: 0.2463 - mape: 7.7039 - val_loss: 0.1913 - val_mse: 0.1913 - val_rmse: 0.4374 - val_mae: 0.2480 - val_mape: 7.8410 - lr: 1.0000e-04\n",
      "Epoch 483/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1905 - mse: 0.1905 - rmse: 0.4365 - mae: 0.2459 - mape: 7.7052\n",
      "Epoch 483: val_loss did not improve from 0.19027\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4379 - mae: 0.2463 - mape: 7.7187 - val_loss: 0.1905 - val_mse: 0.1905 - val_rmse: 0.4365 - val_mae: 0.2456 - val_mape: 7.7123 - lr: 1.0000e-04\n",
      "Epoch 484/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1934 - mse: 0.1934 - rmse: 0.4398 - mae: 0.2472 - mape: 7.7439\n",
      "Epoch 484: val_loss did not improve from 0.19027\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1919 - mse: 0.1919 - rmse: 0.4380 - mae: 0.2461 - mape: 7.7084 - val_loss: 0.1912 - val_mse: 0.1912 - val_rmse: 0.4373 - val_mae: 0.2460 - val_mape: 7.7551 - lr: 1.0000e-04\n",
      "Epoch 485/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1928 - mse: 0.1928 - rmse: 0.4391 - mae: 0.2472 - mape: 7.7507\n",
      "Epoch 485: val_loss improved from 0.19027 to 0.18992, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2468 - mape: 7.7343 - val_loss: 0.1899 - val_mse: 0.1899 - val_rmse: 0.4358 - val_mae: 0.2442 - val_mape: 7.6531 - lr: 1.0000e-04\n",
      "Epoch 486/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1937 - mse: 0.1937 - rmse: 0.4401 - mae: 0.2466 - mape: 7.7259\n",
      "Epoch 486: val_loss did not improve from 0.18992\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2463 - mape: 7.7158 - val_loss: 0.1905 - val_mse: 0.1905 - val_rmse: 0.4365 - val_mae: 0.2451 - val_mape: 7.7351 - lr: 1.0000e-04\n",
      "Epoch 487/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2456 - mape: 7.6982\n",
      "Epoch 487: val_loss did not improve from 0.18992\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1918 - mse: 0.1918 - rmse: 0.4379 - mae: 0.2454 - mape: 7.6918 - val_loss: 0.1903 - val_mse: 0.1903 - val_rmse: 0.4362 - val_mae: 0.2461 - val_mape: 7.7392 - lr: 1.0000e-04\n",
      "Epoch 488/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1940 - mse: 0.1940 - rmse: 0.4405 - mae: 0.2477 - mape: 7.7649\n",
      "Epoch 488: val_loss did not improve from 0.18992\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1916 - mse: 0.1916 - rmse: 0.4378 - mae: 0.2464 - mape: 7.7213 - val_loss: 0.1901 - val_mse: 0.1901 - val_rmse: 0.4360 - val_mae: 0.2453 - val_mape: 7.6974 - lr: 1.0000e-04\n",
      "Epoch 489/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1904 - mse: 0.1904 - rmse: 0.4363 - mae: 0.2458 - mape: 7.6956\n",
      "Epoch 489: val_loss did not improve from 0.18992\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4374 - mae: 0.2460 - mape: 7.7139 - val_loss: 0.1921 - val_mse: 0.1921 - val_rmse: 0.4383 - val_mae: 0.2460 - val_mape: 7.7005 - lr: 1.0000e-04\n",
      "Epoch 490/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.1935 - mse: 0.1935 - rmse: 0.4399 - mae: 0.2467 - mape: 7.7180\n",
      "Epoch 490: val_loss did not improve from 0.18992\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4379 - mae: 0.2461 - mape: 7.7123 - val_loss: 0.1909 - val_mse: 0.1909 - val_rmse: 0.4369 - val_mae: 0.2444 - val_mape: 7.5907 - lr: 1.0000e-04\n",
      "Epoch 491/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1929 - mse: 0.1929 - rmse: 0.4392 - mae: 0.2463 - mape: 7.7031\n",
      "Epoch 491: val_loss did not improve from 0.18992\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1919 - mse: 0.1919 - rmse: 0.4381 - mae: 0.2458 - mape: 7.6881 - val_loss: 0.1922 - val_mse: 0.1922 - val_rmse: 0.4384 - val_mae: 0.2470 - val_mape: 7.8235 - lr: 1.0000e-04\n",
      "Epoch 492/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1923 - mse: 0.1923 - rmse: 0.4386 - mae: 0.2467 - mape: 7.7240\n",
      "Epoch 492: val_loss improved from 0.18992 to 0.18989, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1919 - mse: 0.1919 - rmse: 0.4380 - mae: 0.2465 - mape: 7.7266 - val_loss: 0.1899 - val_mse: 0.1899 - val_rmse: 0.4358 - val_mae: 0.2428 - val_mape: 7.5984 - lr: 1.0000e-04\n",
      "Epoch 493/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1919 - mse: 0.1919 - rmse: 0.4381 - mae: 0.2458 - mape: 7.6945\n",
      "Epoch 493: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1919 - mse: 0.1919 - rmse: 0.4381 - mae: 0.2458 - mape: 7.6955 - val_loss: 0.1908 - val_mse: 0.1908 - val_rmse: 0.4368 - val_mae: 0.2462 - val_mape: 7.7646 - lr: 1.0000e-04\n",
      "Epoch 494/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1919 - mse: 0.1919 - rmse: 0.4380 - mae: 0.2460 - mape: 7.7121\n",
      "Epoch 494: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2456 - mape: 7.7063 - val_loss: 0.1907 - val_mse: 0.1907 - val_rmse: 0.4367 - val_mae: 0.2437 - val_mape: 7.5807 - lr: 1.0000e-04\n",
      "Epoch 495/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2458 - mape: 7.6945\n",
      "Epoch 495: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4378 - mae: 0.2463 - mape: 7.7115 - val_loss: 0.1926 - val_mse: 0.1926 - val_rmse: 0.4389 - val_mae: 0.2467 - val_mape: 7.8316 - lr: 1.0000e-04\n",
      "Epoch 496/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1919 - mse: 0.1919 - rmse: 0.4380 - mae: 0.2463 - mape: 7.7177\n",
      "Epoch 496: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1918 - mse: 0.1918 - rmse: 0.4380 - mae: 0.2463 - mape: 7.7175 - val_loss: 0.1901 - val_mse: 0.1901 - val_rmse: 0.4360 - val_mae: 0.2432 - val_mape: 7.5886 - lr: 1.0000e-04\n",
      "Epoch 497/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1930 - mse: 0.1930 - rmse: 0.4394 - mae: 0.2467 - mape: 7.7243\n",
      "Epoch 497: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2462 - mape: 7.7089 - val_loss: 0.1928 - val_mse: 0.1928 - val_rmse: 0.4391 - val_mae: 0.2458 - val_mape: 7.7796 - lr: 1.0000e-04\n",
      "Epoch 498/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2463 - mape: 7.7219\n",
      "Epoch 498: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2463 - mape: 7.7219 - val_loss: 0.1906 - val_mse: 0.1906 - val_rmse: 0.4366 - val_mae: 0.2457 - val_mape: 7.7405 - lr: 1.0000e-04\n",
      "Epoch 499/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1925 - mse: 0.1925 - rmse: 0.4388 - mae: 0.2465 - mape: 7.7288\n",
      "Epoch 499: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1919 - mse: 0.1919 - rmse: 0.4380 - mae: 0.2461 - mape: 7.7137 - val_loss: 0.1906 - val_mse: 0.1906 - val_rmse: 0.4366 - val_mae: 0.2452 - val_mape: 7.7228 - lr: 1.0000e-04\n",
      "Epoch 500/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1898 - mse: 0.1898 - rmse: 0.4357 - mae: 0.2458 - mape: 7.7079\n",
      "Epoch 500: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2464 - mape: 7.7213 - val_loss: 0.1926 - val_mse: 0.1926 - val_rmse: 0.4389 - val_mae: 0.2474 - val_mape: 7.8498 - lr: 1.0000e-04\n",
      "Epoch 501/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.1915 - mse: 0.1915 - rmse: 0.4377 - mae: 0.2456 - mape: 7.7171\n",
      "Epoch 501: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1919 - mse: 0.1919 - rmse: 0.4380 - mae: 0.2454 - mape: 7.6806 - val_loss: 0.1911 - val_mse: 0.1911 - val_rmse: 0.4372 - val_mae: 0.2487 - val_mape: 7.8612 - lr: 1.0000e-04\n",
      "Epoch 502/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1937 - mse: 0.1937 - rmse: 0.4401 - mae: 0.2477 - mape: 7.7688\n",
      "Epoch 502: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4381 - mae: 0.2465 - mape: 7.7329 - val_loss: 0.1938 - val_mse: 0.1938 - val_rmse: 0.4402 - val_mae: 0.2476 - val_mape: 7.7607 - lr: 1.0000e-04\n",
      "Epoch 503/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2453 - mape: 7.6893\n",
      "Epoch 503: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2460 - mape: 7.7074 - val_loss: 0.1907 - val_mse: 0.1907 - val_rmse: 0.4367 - val_mae: 0.2458 - val_mape: 7.7562 - lr: 1.0000e-04\n",
      "Epoch 504/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.1895 - mse: 0.1895 - rmse: 0.4354 - mae: 0.2456 - mape: 7.6976\n",
      "Epoch 504: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4373 - mae: 0.2455 - mape: 7.6970 - val_loss: 0.1902 - val_mse: 0.1902 - val_rmse: 0.4361 - val_mae: 0.2444 - val_mape: 7.6224 - lr: 1.0000e-04\n",
      "Epoch 505/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2462 - mape: 7.7192\n",
      "Epoch 505: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1918 - mse: 0.1918 - rmse: 0.4380 - mae: 0.2463 - mape: 7.7222 - val_loss: 0.1909 - val_mse: 0.1909 - val_rmse: 0.4370 - val_mae: 0.2465 - val_mape: 7.6897 - lr: 1.0000e-04\n",
      "Epoch 506/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1909 - mse: 0.1909 - rmse: 0.4370 - mae: 0.2456 - mape: 7.6945\n",
      "Epoch 506: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1918 - mse: 0.1918 - rmse: 0.4379 - mae: 0.2463 - mape: 7.7112 - val_loss: 0.1904 - val_mse: 0.1904 - val_rmse: 0.4363 - val_mae: 0.2479 - val_mape: 7.7992 - lr: 1.0000e-04\n",
      "Epoch 507/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1906 - mse: 0.1906 - rmse: 0.4365 - mae: 0.2455 - mape: 7.6983\n",
      "Epoch 507: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2458 - mape: 7.6991 - val_loss: 0.1939 - val_mse: 0.1939 - val_rmse: 0.4404 - val_mae: 0.2472 - val_mape: 7.7468 - lr: 1.0000e-04\n",
      "Epoch 508/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2462 - mape: 7.7165\n",
      "Epoch 508: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2462 - mape: 7.7165 - val_loss: 0.1912 - val_mse: 0.1912 - val_rmse: 0.4373 - val_mae: 0.2472 - val_mape: 7.7120 - lr: 1.0000e-04\n",
      "Epoch 509/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1908 - mse: 0.1908 - rmse: 0.4369 - mae: 0.2451 - mape: 7.6787\n",
      "Epoch 509: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2455 - mape: 7.6837 - val_loss: 0.1907 - val_mse: 0.1907 - val_rmse: 0.4367 - val_mae: 0.2470 - val_mape: 7.8133 - lr: 1.0000e-04\n",
      "Epoch 510/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2460 - mape: 7.7224\n",
      "Epoch 510: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2460 - mape: 7.7224 - val_loss: 0.1907 - val_mse: 0.1907 - val_rmse: 0.4367 - val_mae: 0.2465 - val_mape: 7.7412 - lr: 1.0000e-04\n",
      "Epoch 511/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1931 - mse: 0.1931 - rmse: 0.4394 - mae: 0.2466 - mape: 7.7238\n",
      "Epoch 511: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1910 - mse: 0.1910 - rmse: 0.4371 - mae: 0.2458 - mape: 7.7043 - val_loss: 0.1912 - val_mse: 0.1912 - val_rmse: 0.4373 - val_mae: 0.2457 - val_mape: 7.7349 - lr: 1.0000e-04\n",
      "Epoch 512/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1926 - mse: 0.1926 - rmse: 0.4389 - mae: 0.2461 - mape: 7.6977\n",
      "Epoch 512: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4381 - mae: 0.2459 - mape: 7.7016 - val_loss: 0.1902 - val_mse: 0.1902 - val_rmse: 0.4362 - val_mae: 0.2457 - val_mape: 7.6891 - lr: 1.0000e-04\n",
      "Epoch 513/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1910 - mse: 0.1910 - rmse: 0.4370 - mae: 0.2453 - mape: 7.6914\n",
      "Epoch 513: val_loss did not improve from 0.18989\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1923 - mse: 0.1923 - rmse: 0.4385 - mae: 0.2460 - mape: 7.7085 - val_loss: 0.1902 - val_mse: 0.1902 - val_rmse: 0.4361 - val_mae: 0.2448 - val_mape: 7.6821 - lr: 1.0000e-04\n",
      "Epoch 514/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2469 - mape: 7.7347\n",
      "Epoch 514: val_loss improved from 0.18989 to 0.18969, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2461 - mape: 7.7037 - val_loss: 0.1897 - val_mse: 0.1897 - val_rmse: 0.4355 - val_mae: 0.2441 - val_mape: 7.6440 - lr: 1.0000e-04\n",
      "Epoch 515/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2456 - mape: 7.6935\n",
      "Epoch 515: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4379 - mae: 0.2464 - mape: 7.7191 - val_loss: 0.1911 - val_mse: 0.1911 - val_rmse: 0.4371 - val_mae: 0.2471 - val_mape: 7.7536 - lr: 1.0000e-04\n",
      "Epoch 516/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2451 - mape: 7.6759\n",
      "Epoch 516: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1918 - mse: 0.1918 - rmse: 0.4379 - mae: 0.2458 - mape: 7.6964 - val_loss: 0.1927 - val_mse: 0.1927 - val_rmse: 0.4389 - val_mae: 0.2448 - val_mape: 7.5678 - lr: 1.0000e-04\n",
      "Epoch 517/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1922 - mse: 0.1922 - rmse: 0.4384 - mae: 0.2463 - mape: 7.7078\n",
      "Epoch 517: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2463 - mape: 7.7073 - val_loss: 0.1906 - val_mse: 0.1906 - val_rmse: 0.4366 - val_mae: 0.2486 - val_mape: 7.8844 - lr: 1.0000e-04\n",
      "Epoch 518/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1896 - mse: 0.1896 - rmse: 0.4354 - mae: 0.2447 - mape: 7.6772\n",
      "Epoch 518: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1919 - mse: 0.1919 - rmse: 0.4380 - mae: 0.2459 - mape: 7.7047 - val_loss: 0.1934 - val_mse: 0.1934 - val_rmse: 0.4398 - val_mae: 0.2476 - val_mape: 7.7704 - lr: 1.0000e-04\n",
      "Epoch 519/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4346 - mae: 0.2439 - mape: 7.6533\n",
      "Epoch 519: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2455 - mape: 7.6984 - val_loss: 0.1933 - val_mse: 0.1933 - val_rmse: 0.4397 - val_mae: 0.2482 - val_mape: 7.8086 - lr: 1.0000e-04\n",
      "Epoch 520/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2450 - mape: 7.6785\n",
      "Epoch 520: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2455 - mape: 7.6934 - val_loss: 0.1927 - val_mse: 0.1927 - val_rmse: 0.4389 - val_mae: 0.2463 - val_mape: 7.6409 - lr: 1.0000e-04\n",
      "Epoch 521/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2457 - mape: 7.7075\n",
      "Epoch 521: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2455 - mape: 7.6945 - val_loss: 0.1900 - val_mse: 0.1900 - val_rmse: 0.4359 - val_mae: 0.2449 - val_mape: 7.7087 - lr: 1.0000e-04\n",
      "Epoch 522/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1923 - mse: 0.1923 - rmse: 0.4385 - mae: 0.2459 - mape: 7.7110\n",
      "Epoch 522: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4378 - mae: 0.2457 - mape: 7.7031 - val_loss: 0.1909 - val_mse: 0.1909 - val_rmse: 0.4369 - val_mae: 0.2461 - val_mape: 7.7145 - lr: 1.0000e-04\n",
      "Epoch 523/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1904 - mse: 0.1904 - rmse: 0.4364 - mae: 0.2446 - mape: 7.6661\n",
      "Epoch 523: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2450 - mape: 7.6719 - val_loss: 0.1914 - val_mse: 0.1914 - val_rmse: 0.4375 - val_mae: 0.2479 - val_mape: 7.8386 - lr: 1.0000e-04\n",
      "Epoch 524/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1918 - mse: 0.1918 - rmse: 0.4380 - mae: 0.2467 - mape: 7.7302\n",
      "Epoch 524: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1918 - mse: 0.1918 - rmse: 0.4380 - mae: 0.2464 - mape: 7.7216 - val_loss: 0.1923 - val_mse: 0.1923 - val_rmse: 0.4386 - val_mae: 0.2437 - val_mape: 7.5719 - lr: 1.0000e-04\n",
      "Epoch 525/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1924 - mse: 0.1924 - rmse: 0.4387 - mae: 0.2463 - mape: 7.7196\n",
      "Epoch 525: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2456 - mape: 7.7032 - val_loss: 0.1899 - val_mse: 0.1899 - val_rmse: 0.4358 - val_mae: 0.2426 - val_mape: 7.5403 - lr: 1.0000e-04\n",
      "Epoch 526/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1923 - mse: 0.1923 - rmse: 0.4386 - mae: 0.2464 - mape: 7.7227\n",
      "Epoch 526: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4378 - mae: 0.2461 - mape: 7.7110 - val_loss: 0.1901 - val_mse: 0.1901 - val_rmse: 0.4360 - val_mae: 0.2434 - val_mape: 7.5970 - lr: 1.0000e-04\n",
      "Epoch 527/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2450 - mape: 7.6778\n",
      "Epoch 527: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2448 - mape: 7.6676 - val_loss: 0.1902 - val_mse: 0.1902 - val_rmse: 0.4361 - val_mae: 0.2445 - val_mape: 7.6967 - lr: 1.0000e-04\n",
      "Epoch 528/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2459 - mape: 7.7070\n",
      "Epoch 528: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2459 - mape: 7.7070 - val_loss: 0.1904 - val_mse: 0.1904 - val_rmse: 0.4364 - val_mae: 0.2456 - val_mape: 7.6847 - lr: 1.0000e-04\n",
      "Epoch 529/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1930 - mse: 0.1930 - rmse: 0.4394 - mae: 0.2469 - mape: 7.7312\n",
      "Epoch 529: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1919 - mse: 0.1919 - rmse: 0.4380 - mae: 0.2461 - mape: 7.7109 - val_loss: 0.1908 - val_mse: 0.1908 - val_rmse: 0.4368 - val_mae: 0.2440 - val_mape: 7.5716 - lr: 1.0000e-04\n",
      "Epoch 530/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2452 - mape: 7.6678\n",
      "Epoch 530: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1919 - mse: 0.1919 - rmse: 0.4380 - mae: 0.2454 - mape: 7.6800 - val_loss: 0.1902 - val_mse: 0.1902 - val_rmse: 0.4361 - val_mae: 0.2463 - val_mape: 7.7560 - lr: 1.0000e-04\n",
      "Epoch 531/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2462 - mape: 7.7143\n",
      "Epoch 531: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2463 - mape: 7.7140 - val_loss: 0.1920 - val_mse: 0.1920 - val_rmse: 0.4382 - val_mae: 0.2459 - val_mape: 7.7160 - lr: 1.0000e-04\n",
      "Epoch 532/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2459 - mape: 7.7045\n",
      "Epoch 532: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2460 - mape: 7.7040 - val_loss: 0.1940 - val_mse: 0.1940 - val_rmse: 0.4405 - val_mae: 0.2464 - val_mape: 7.7370 - lr: 1.0000e-04\n",
      "Epoch 533/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2458 - mape: 7.6956\n",
      "Epoch 533: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1916 - mse: 0.1916 - rmse: 0.4378 - mae: 0.2456 - mape: 7.6940 - val_loss: 0.1906 - val_mse: 0.1906 - val_rmse: 0.4366 - val_mae: 0.2448 - val_mape: 7.6226 - lr: 1.0000e-04\n",
      "Epoch 534/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1922 - mse: 0.1922 - rmse: 0.4384 - mae: 0.2463 - mape: 7.7138\n",
      "Epoch 534: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2460 - mape: 7.7107 - val_loss: 0.1923 - val_mse: 0.1923 - val_rmse: 0.4385 - val_mae: 0.2457 - val_mape: 7.5866 - lr: 1.0000e-04\n",
      "Epoch 535/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1917 - mse: 0.1917 - rmse: 0.4378 - mae: 0.2456 - mape: 7.6869\n",
      "Epoch 535: val_loss did not improve from 0.18969\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1922 - mse: 0.1922 - rmse: 0.4384 - mae: 0.2456 - mape: 7.6803 - val_loss: 0.1901 - val_mse: 0.1901 - val_rmse: 0.4360 - val_mae: 0.2457 - val_mape: 7.6787 - lr: 1.0000e-04\n",
      "Epoch 536/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1917 - mse: 0.1917 - rmse: 0.4378 - mae: 0.2460 - mape: 7.7254\n",
      "Epoch 536: val_loss improved from 0.18969 to 0.18955, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2460 - mape: 7.7124 - val_loss: 0.1895 - val_mse: 0.1895 - val_rmse: 0.4354 - val_mae: 0.2439 - val_mape: 7.6346 - lr: 1.0000e-04\n",
      "Epoch 537/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2469 - mape: 7.7337\n",
      "Epoch 537: val_loss did not improve from 0.18955\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2458 - mape: 7.7006 - val_loss: 0.1921 - val_mse: 0.1921 - val_rmse: 0.4383 - val_mae: 0.2454 - val_mape: 7.6986 - lr: 1.0000e-04\n",
      "Epoch 538/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1940 - mse: 0.1940 - rmse: 0.4404 - mae: 0.2467 - mape: 7.7344\n",
      "Epoch 538: val_loss did not improve from 0.18955\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1919 - mse: 0.1919 - rmse: 0.4381 - mae: 0.2458 - mape: 7.7083 - val_loss: 0.1903 - val_mse: 0.1903 - val_rmse: 0.4362 - val_mae: 0.2456 - val_mape: 7.6797 - lr: 1.0000e-04\n",
      "Epoch 539/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2458 - mape: 7.7000\n",
      "Epoch 539: val_loss did not improve from 0.18955\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2458 - mape: 7.7000 - val_loss: 0.1912 - val_mse: 0.1912 - val_rmse: 0.4373 - val_mae: 0.2442 - val_mape: 7.5469 - lr: 1.0000e-04\n",
      "Epoch 540/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2452 - mape: 7.6889\n",
      "Epoch 540: val_loss did not improve from 0.18955\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4378 - mae: 0.2453 - mape: 7.6796 - val_loss: 0.1926 - val_mse: 0.1926 - val_rmse: 0.4389 - val_mae: 0.2475 - val_mape: 7.7768 - lr: 1.0000e-04\n",
      "Epoch 541/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2451 - mape: 7.6739\n",
      "Epoch 541: val_loss did not improve from 0.18955\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2451 - mape: 7.6715 - val_loss: 0.1926 - val_mse: 0.1926 - val_rmse: 0.4389 - val_mae: 0.2484 - val_mape: 7.8180 - lr: 1.0000e-04\n",
      "Epoch 542/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2459 - mape: 7.7123\n",
      "Epoch 542: val_loss did not improve from 0.18955\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2456 - mape: 7.7018 - val_loss: 0.1906 - val_mse: 0.1906 - val_rmse: 0.4366 - val_mae: 0.2460 - val_mape: 7.7468 - lr: 1.0000e-04\n",
      "Epoch 543/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1912 - mse: 0.1912 - rmse: 0.4372 - mae: 0.2455 - mape: 7.6885\n",
      "Epoch 543: val_loss did not improve from 0.18955\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1918 - mse: 0.1918 - rmse: 0.4379 - mae: 0.2461 - mape: 7.7090 - val_loss: 0.1896 - val_mse: 0.1896 - val_rmse: 0.4354 - val_mae: 0.2434 - val_mape: 7.6401 - lr: 1.0000e-04\n",
      "Epoch 544/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1910 - mse: 0.1910 - rmse: 0.4370 - mae: 0.2450 - mape: 7.6815\n",
      "Epoch 544: val_loss did not improve from 0.18955\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2454 - mape: 7.6982 - val_loss: 0.1905 - val_mse: 0.1905 - val_rmse: 0.4364 - val_mae: 0.2420 - val_mape: 7.5181 - lr: 1.0000e-04\n",
      "Epoch 545/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2449 - mape: 7.6665\n",
      "Epoch 545: val_loss did not improve from 0.18955\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4379 - mae: 0.2456 - mape: 7.6902 - val_loss: 0.1901 - val_mse: 0.1901 - val_rmse: 0.4360 - val_mae: 0.2447 - val_mape: 7.6630 - lr: 1.0000e-04\n",
      "Epoch 546/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2465 - mape: 7.7134\n",
      "Epoch 546: val_loss did not improve from 0.18955\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2458 - mape: 7.6899 - val_loss: 0.1910 - val_mse: 0.1910 - val_rmse: 0.4370 - val_mae: 0.2438 - val_mape: 7.6346 - lr: 1.0000e-04\n",
      "Epoch 547/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1924 - mse: 0.1924 - rmse: 0.4386 - mae: 0.2459 - mape: 7.7118\n",
      "Epoch 547: val_loss did not improve from 0.18955\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2458 - mape: 7.7013 - val_loss: 0.1898 - val_mse: 0.1898 - val_rmse: 0.4356 - val_mae: 0.2448 - val_mape: 7.7136 - lr: 1.0000e-04\n",
      "Epoch 548/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1923 - mse: 0.1923 - rmse: 0.4385 - mae: 0.2462 - mape: 7.6976\n",
      "Epoch 548: val_loss did not improve from 0.18955\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2460 - mape: 7.7126 - val_loss: 0.1913 - val_mse: 0.1913 - val_rmse: 0.4374 - val_mae: 0.2443 - val_mape: 7.6069 - lr: 1.0000e-04\n",
      "Epoch 549/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1916 - mse: 0.1916 - rmse: 0.4378 - mae: 0.2452 - mape: 7.6808\n",
      "Epoch 549: val_loss improved from 0.18955 to 0.18952, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1918 - mse: 0.1918 - rmse: 0.4380 - mae: 0.2452 - mape: 7.6768 - val_loss: 0.1895 - val_mse: 0.1895 - val_rmse: 0.4353 - val_mae: 0.2442 - val_mape: 7.6736 - lr: 1.0000e-04\n",
      "Epoch 550/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1924 - mse: 0.1924 - rmse: 0.4386 - mae: 0.2454 - mape: 7.6881\n",
      "Epoch 550: val_loss did not improve from 0.18952\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2453 - mape: 7.6860 - val_loss: 0.1897 - val_mse: 0.1897 - val_rmse: 0.4355 - val_mae: 0.2427 - val_mape: 7.6102 - lr: 1.0000e-04\n",
      "Epoch 551/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2450 - mape: 7.6751\n",
      "Epoch 551: val_loss did not improve from 0.18952\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4379 - mae: 0.2452 - mape: 7.6802 - val_loss: 0.1919 - val_mse: 0.1919 - val_rmse: 0.4380 - val_mae: 0.2468 - val_mape: 7.7836 - lr: 1.0000e-04\n",
      "Epoch 552/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1906 - mse: 0.1906 - rmse: 0.4365 - mae: 0.2444 - mape: 7.6638\n",
      "Epoch 552: val_loss improved from 0.18952 to 0.18940, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4370 - mae: 0.2448 - mape: 7.6756 - val_loss: 0.1894 - val_mse: 0.1894 - val_rmse: 0.4352 - val_mae: 0.2447 - val_mape: 7.6777 - lr: 1.0000e-04\n",
      "Epoch 553/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1924 - mse: 0.1924 - rmse: 0.4387 - mae: 0.2457 - mape: 7.7165\n",
      "Epoch 553: val_loss did not improve from 0.18940\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4379 - mae: 0.2459 - mape: 7.7170 - val_loss: 0.1925 - val_mse: 0.1925 - val_rmse: 0.4387 - val_mae: 0.2476 - val_mape: 7.7513 - lr: 1.0000e-04\n",
      "Epoch 554/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1906 - mse: 0.1906 - rmse: 0.4366 - mae: 0.2450 - mape: 7.6807\n",
      "Epoch 554: val_loss improved from 0.18940 to 0.18939, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2456 - mape: 7.6869 - val_loss: 0.1894 - val_mse: 0.1894 - val_rmse: 0.4352 - val_mae: 0.2429 - val_mape: 7.5734 - lr: 1.0000e-04\n",
      "Epoch 555/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1905 - mse: 0.1905 - rmse: 0.4364 - mae: 0.2450 - mape: 7.6867\n",
      "Epoch 555: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4377 - mae: 0.2462 - mape: 7.7175 - val_loss: 0.1903 - val_mse: 0.1903 - val_rmse: 0.4363 - val_mae: 0.2448 - val_mape: 7.6155 - lr: 1.0000e-04\n",
      "Epoch 556/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1904 - mse: 0.1904 - rmse: 0.4363 - mae: 0.2445 - mape: 7.6469\n",
      "Epoch 556: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1910 - mse: 0.1910 - rmse: 0.4371 - mae: 0.2450 - mape: 7.6719 - val_loss: 0.1897 - val_mse: 0.1897 - val_rmse: 0.4355 - val_mae: 0.2459 - val_mape: 7.7581 - lr: 1.0000e-04\n",
      "Epoch 557/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2455 - mape: 7.6863\n",
      "Epoch 557: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2453 - mape: 7.6772 - val_loss: 0.1898 - val_mse: 0.1898 - val_rmse: 0.4356 - val_mae: 0.2451 - val_mape: 7.7395 - lr: 1.0000e-04\n",
      "Epoch 558/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2452 - mape: 7.6867\n",
      "Epoch 558: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2449 - mape: 7.6800 - val_loss: 0.1895 - val_mse: 0.1895 - val_rmse: 0.4353 - val_mae: 0.2441 - val_mape: 7.6435 - lr: 1.0000e-04\n",
      "Epoch 559/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1911 - mse: 0.1911 - rmse: 0.4372 - mae: 0.2449 - mape: 7.6605\n",
      "Epoch 559: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2452 - mape: 7.6781 - val_loss: 0.1896 - val_mse: 0.1896 - val_rmse: 0.4355 - val_mae: 0.2433 - val_mape: 7.6167 - lr: 1.0000e-04\n",
      "Epoch 560/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1922 - mse: 0.1922 - rmse: 0.4384 - mae: 0.2457 - mape: 7.6897\n",
      "Epoch 560: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4372 - mae: 0.2450 - mape: 7.6780 - val_loss: 0.1901 - val_mse: 0.1901 - val_rmse: 0.4360 - val_mae: 0.2446 - val_mape: 7.7145 - lr: 1.0000e-04\n",
      "Epoch 561/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2444 - mape: 7.6571\n",
      "Epoch 561: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1910 - mse: 0.1910 - rmse: 0.4371 - mae: 0.2446 - mape: 7.6609 - val_loss: 0.1907 - val_mse: 0.1907 - val_rmse: 0.4367 - val_mae: 0.2473 - val_mape: 7.8161 - lr: 1.0000e-04\n",
      "Epoch 562/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2457 - mape: 7.7066\n",
      "Epoch 562: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4378 - mae: 0.2461 - mape: 7.7191 - val_loss: 0.1938 - val_mse: 0.1938 - val_rmse: 0.4402 - val_mae: 0.2477 - val_mape: 7.7790 - lr: 1.0000e-04\n",
      "Epoch 563/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1938 - mse: 0.1938 - rmse: 0.4403 - mae: 0.2466 - mape: 7.7222\n",
      "Epoch 563: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2454 - mape: 7.6921 - val_loss: 0.1915 - val_mse: 0.1915 - val_rmse: 0.4376 - val_mae: 0.2456 - val_mape: 7.6378 - lr: 1.0000e-04\n",
      "Epoch 564/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2445 - mape: 7.6557\n",
      "Epoch 564: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1910 - mse: 0.1910 - rmse: 0.4370 - mae: 0.2448 - mape: 7.6639 - val_loss: 0.1902 - val_mse: 0.1902 - val_rmse: 0.4361 - val_mae: 0.2455 - val_mape: 7.6978 - lr: 1.0000e-04\n",
      "Epoch 565/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1923 - mse: 0.1923 - rmse: 0.4385 - mae: 0.2454 - mape: 7.6872\n",
      "Epoch 565: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2453 - mape: 7.6809 - val_loss: 0.1912 - val_mse: 0.1912 - val_rmse: 0.4373 - val_mae: 0.2456 - val_mape: 7.7232 - lr: 1.0000e-04\n",
      "Epoch 566/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1909 - mse: 0.1909 - rmse: 0.4370 - mae: 0.2449 - mape: 7.6811\n",
      "Epoch 566: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2451 - mape: 7.6856 - val_loss: 0.1919 - val_mse: 0.1919 - val_rmse: 0.4380 - val_mae: 0.2460 - val_mape: 7.7677 - lr: 1.0000e-04\n",
      "Epoch 567/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1903 - mse: 0.1903 - rmse: 0.4363 - mae: 0.2454 - mape: 7.6904\n",
      "Epoch 567: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4370 - mae: 0.2457 - mape: 7.7064 - val_loss: 0.1915 - val_mse: 0.1915 - val_rmse: 0.4376 - val_mae: 0.2443 - val_mape: 7.5909 - lr: 1.0000e-04\n",
      "Epoch 568/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.1948 - mse: 0.1948 - rmse: 0.4414 - mae: 0.2473 - mape: 7.7390\n",
      "Epoch 568: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4372 - mae: 0.2453 - mape: 7.6863 - val_loss: 0.1912 - val_mse: 0.1912 - val_rmse: 0.4372 - val_mae: 0.2454 - val_mape: 7.7105 - lr: 1.0000e-04\n",
      "Epoch 569/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2452 - mape: 7.6890\n",
      "Epoch 569: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1910 - mse: 0.1910 - rmse: 0.4370 - mae: 0.2451 - mape: 7.6878 - val_loss: 0.1908 - val_mse: 0.1908 - val_rmse: 0.4368 - val_mae: 0.2422 - val_mape: 7.4964 - lr: 1.0000e-04\n",
      "Epoch 570/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1925 - mse: 0.1925 - rmse: 0.4387 - mae: 0.2463 - mape: 7.7021\n",
      "Epoch 570: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2457 - mape: 7.6926 - val_loss: 0.1895 - val_mse: 0.1895 - val_rmse: 0.4353 - val_mae: 0.2422 - val_mape: 7.5571 - lr: 1.0000e-04\n",
      "Epoch 571/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1942 - mse: 0.1942 - rmse: 0.4407 - mae: 0.2469 - mape: 7.7211\n",
      "Epoch 571: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2445 - mape: 7.6527 - val_loss: 0.1908 - val_mse: 0.1908 - val_rmse: 0.4368 - val_mae: 0.2456 - val_mape: 7.7630 - lr: 1.0000e-04\n",
      "Epoch 572/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2456 - mape: 7.7000\n",
      "Epoch 572: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4381 - mae: 0.2464 - mape: 7.7222 - val_loss: 0.1925 - val_mse: 0.1925 - val_rmse: 0.4388 - val_mae: 0.2474 - val_mape: 7.8477 - lr: 1.0000e-04\n",
      "Epoch 573/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1900 - mse: 0.1900 - rmse: 0.4359 - mae: 0.2448 - mape: 7.6786\n",
      "Epoch 573: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1918 - mse: 0.1918 - rmse: 0.4379 - mae: 0.2454 - mape: 7.6895 - val_loss: 0.1958 - val_mse: 0.1958 - val_rmse: 0.4425 - val_mae: 0.2480 - val_mape: 7.7163 - lr: 1.0000e-04\n",
      "Epoch 574/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1935 - mse: 0.1935 - rmse: 0.4399 - mae: 0.2460 - mape: 7.7059\n",
      "Epoch 574: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2452 - mape: 7.6869 - val_loss: 0.1897 - val_mse: 0.1897 - val_rmse: 0.4355 - val_mae: 0.2444 - val_mape: 7.7091 - lr: 1.0000e-04\n",
      "Epoch 575/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.1924 - mse: 0.1924 - rmse: 0.4387 - mae: 0.2455 - mape: 7.6925\n",
      "Epoch 575: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2445 - mape: 7.6680 - val_loss: 0.1896 - val_mse: 0.1896 - val_rmse: 0.4354 - val_mae: 0.2440 - val_mape: 7.6338 - lr: 1.0000e-04\n",
      "Epoch 576/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2447 - mape: 7.6595\n",
      "Epoch 576: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1918 - mse: 0.1918 - rmse: 0.4379 - mae: 0.2450 - mape: 7.6738 - val_loss: 0.1896 - val_mse: 0.1896 - val_rmse: 0.4354 - val_mae: 0.2437 - val_mape: 7.6235 - lr: 1.0000e-04\n",
      "Epoch 577/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2452 - mape: 7.6821\n",
      "Epoch 577: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4379 - mae: 0.2451 - mape: 7.6792 - val_loss: 0.1895 - val_mse: 0.1895 - val_rmse: 0.4353 - val_mae: 0.2451 - val_mape: 7.6552 - lr: 1.0000e-04\n",
      "Epoch 578/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.1932 - mse: 0.1932 - rmse: 0.4395 - mae: 0.2472 - mape: 7.7378\n",
      "Epoch 578: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1906 - mse: 0.1906 - rmse: 0.4366 - mae: 0.2448 - mape: 7.6770 - val_loss: 0.1914 - val_mse: 0.1914 - val_rmse: 0.4375 - val_mae: 0.2446 - val_mape: 7.5869 - lr: 1.0000e-04\n",
      "Epoch 579/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1910 - mse: 0.1910 - rmse: 0.4370 - mae: 0.2444 - mape: 7.6477\n",
      "Epoch 579: val_loss did not improve from 0.18939\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2449 - mape: 7.6619 - val_loss: 0.1906 - val_mse: 0.1906 - val_rmse: 0.4366 - val_mae: 0.2449 - val_mape: 7.7505 - lr: 1.0000e-04\n",
      "Epoch 580/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1906 - mse: 0.1906 - rmse: 0.4365 - mae: 0.2444 - mape: 7.6721\n",
      "Epoch 580: val_loss improved from 0.18939 to 0.18935, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2448 - mape: 7.6818 - val_loss: 0.1894 - val_mse: 0.1894 - val_rmse: 0.4351 - val_mae: 0.2424 - val_mape: 7.5774 - lr: 1.0000e-04\n",
      "Epoch 581/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2446 - mape: 7.6741\n",
      "Epoch 581: val_loss did not improve from 0.18935\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2448 - mape: 7.6680 - val_loss: 0.1910 - val_mse: 0.1910 - val_rmse: 0.4370 - val_mae: 0.2466 - val_mape: 7.8146 - lr: 1.0000e-04\n",
      "Epoch 582/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2452 - mape: 7.6822\n",
      "Epoch 582: val_loss did not improve from 0.18935\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2452 - mape: 7.6820 - val_loss: 0.1906 - val_mse: 0.1906 - val_rmse: 0.4366 - val_mae: 0.2481 - val_mape: 7.8484 - lr: 1.0000e-04\n",
      "Epoch 583/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1894 - mse: 0.1894 - rmse: 0.4352 - mae: 0.2441 - mape: 7.6445\n",
      "Epoch 583: val_loss improved from 0.18935 to 0.18917, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2449 - mape: 7.6813 - val_loss: 0.1892 - val_mse: 0.1892 - val_rmse: 0.4349 - val_mae: 0.2438 - val_mape: 7.6482 - lr: 1.0000e-04\n",
      "Epoch 584/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.1922 - mse: 0.1922 - rmse: 0.4385 - mae: 0.2461 - mape: 7.7075\n",
      "Epoch 584: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4372 - mae: 0.2455 - mape: 7.6862 - val_loss: 0.1915 - val_mse: 0.1915 - val_rmse: 0.4376 - val_mae: 0.2447 - val_mape: 7.6297 - lr: 1.0000e-04\n",
      "Epoch 585/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1906 - mse: 0.1906 - rmse: 0.4365 - mae: 0.2446 - mape: 7.6597\n",
      "Epoch 585: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2450 - mape: 7.6723 - val_loss: 0.1951 - val_mse: 0.1951 - val_rmse: 0.4417 - val_mae: 0.2514 - val_mape: 7.9102 - lr: 1.0000e-04\n",
      "Epoch 586/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2455 - mape: 7.7041\n",
      "Epoch 586: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2452 - mape: 7.6887 - val_loss: 0.1893 - val_mse: 0.1893 - val_rmse: 0.4351 - val_mae: 0.2421 - val_mape: 7.5735 - lr: 1.0000e-04\n",
      "Epoch 587/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2454 - mape: 7.6827\n",
      "Epoch 587: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2451 - mape: 7.6765 - val_loss: 0.1898 - val_mse: 0.1898 - val_rmse: 0.4356 - val_mae: 0.2421 - val_mape: 7.5147 - lr: 1.0000e-04\n",
      "Epoch 588/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1922 - mse: 0.1922 - rmse: 0.4384 - mae: 0.2453 - mape: 7.6783\n",
      "Epoch 588: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1906 - mse: 0.1906 - rmse: 0.4365 - mae: 0.2444 - mape: 7.6590 - val_loss: 0.1905 - val_mse: 0.1905 - val_rmse: 0.4365 - val_mae: 0.2430 - val_mape: 7.6328 - lr: 1.0000e-04\n",
      "Epoch 589/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2450 - mape: 7.6704\n",
      "Epoch 589: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2455 - mape: 7.6868 - val_loss: 0.1914 - val_mse: 0.1914 - val_rmse: 0.4375 - val_mae: 0.2452 - val_mape: 7.6527 - lr: 1.0000e-04\n",
      "Epoch 590/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2450 - mape: 7.6880\n",
      "Epoch 590: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1906 - mse: 0.1906 - rmse: 0.4365 - mae: 0.2449 - mape: 7.6849 - val_loss: 0.1900 - val_mse: 0.1900 - val_rmse: 0.4358 - val_mae: 0.2439 - val_mape: 7.5977 - lr: 1.0000e-04\n",
      "Epoch 591/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1910 - mse: 0.1910 - rmse: 0.4370 - mae: 0.2447 - mape: 7.6697\n",
      "Epoch 591: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4372 - mae: 0.2448 - mape: 7.6784 - val_loss: 0.1904 - val_mse: 0.1904 - val_rmse: 0.4364 - val_mae: 0.2463 - val_mape: 7.7161 - lr: 1.0000e-04\n",
      "Epoch 592/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1907 - mse: 0.1907 - rmse: 0.4366 - mae: 0.2446 - mape: 7.6696\n",
      "Epoch 592: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2450 - mape: 7.6817 - val_loss: 0.1901 - val_mse: 0.1901 - val_rmse: 0.4360 - val_mae: 0.2441 - val_mape: 7.6853 - lr: 1.0000e-04\n",
      "Epoch 593/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1925 - mse: 0.1925 - rmse: 0.4388 - mae: 0.2451 - mape: 7.6891\n",
      "Epoch 593: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1919 - mse: 0.1919 - rmse: 0.4380 - mae: 0.2449 - mape: 7.6768 - val_loss: 0.1903 - val_mse: 0.1903 - val_rmse: 0.4363 - val_mae: 0.2446 - val_mape: 7.6232 - lr: 1.0000e-04\n",
      "Epoch 594/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1893 - mse: 0.1893 - rmse: 0.4351 - mae: 0.2434 - mape: 7.6299\n",
      "Epoch 594: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2446 - mape: 7.6638 - val_loss: 0.1910 - val_mse: 0.1910 - val_rmse: 0.4370 - val_mae: 0.2475 - val_mape: 7.7636 - lr: 1.0000e-04\n",
      "Epoch 595/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1877 - mse: 0.1877 - rmse: 0.4333 - mae: 0.2440 - mape: 7.6475\n",
      "Epoch 595: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2452 - mape: 7.6816 - val_loss: 0.1898 - val_mse: 0.1898 - val_rmse: 0.4356 - val_mae: 0.2441 - val_mape: 7.6634 - lr: 1.0000e-04\n",
      "Epoch 596/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1912 - mse: 0.1912 - rmse: 0.4372 - mae: 0.2460 - mape: 7.7066\n",
      "Epoch 596: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1910 - mse: 0.1910 - rmse: 0.4370 - mae: 0.2453 - mape: 7.6836 - val_loss: 0.1903 - val_mse: 0.1903 - val_rmse: 0.4363 - val_mae: 0.2435 - val_mape: 7.6826 - lr: 1.0000e-04\n",
      "Epoch 597/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2459 - mape: 7.6957\n",
      "Epoch 597: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4378 - mae: 0.2456 - mape: 7.6870 - val_loss: 0.1894 - val_mse: 0.1894 - val_rmse: 0.4352 - val_mae: 0.2440 - val_mape: 7.6637 - lr: 1.0000e-04\n",
      "Epoch 598/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1904 - mse: 0.1904 - rmse: 0.4364 - mae: 0.2446 - mape: 7.6691\n",
      "Epoch 598: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4379 - mae: 0.2453 - mape: 7.6884 - val_loss: 0.1893 - val_mse: 0.1893 - val_rmse: 0.4351 - val_mae: 0.2438 - val_mape: 7.6823 - lr: 1.0000e-04\n",
      "Epoch 599/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2447 - mape: 7.6700\n",
      "Epoch 599: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4372 - mae: 0.2451 - mape: 7.6827 - val_loss: 0.1894 - val_mse: 0.1894 - val_rmse: 0.4353 - val_mae: 0.2428 - val_mape: 7.5857 - lr: 1.0000e-04\n",
      "Epoch 600/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1917 - mse: 0.1917 - rmse: 0.4378 - mae: 0.2456 - mape: 7.7099\n",
      "Epoch 600: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1910 - mse: 0.1910 - rmse: 0.4371 - mae: 0.2447 - mape: 7.6739 - val_loss: 0.1895 - val_mse: 0.1895 - val_rmse: 0.4353 - val_mae: 0.2414 - val_mape: 7.4955 - lr: 1.0000e-04\n",
      "Epoch 601/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1907 - mse: 0.1907 - rmse: 0.4366 - mae: 0.2450 - mape: 7.6729\n",
      "Epoch 601: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1906 - mse: 0.1906 - rmse: 0.4366 - mae: 0.2449 - mape: 7.6776 - val_loss: 0.1895 - val_mse: 0.1895 - val_rmse: 0.4353 - val_mae: 0.2414 - val_mape: 7.5273 - lr: 1.0000e-04\n",
      "Epoch 602/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2443 - mape: 7.6547\n",
      "Epoch 602: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2443 - mape: 7.6547 - val_loss: 0.1902 - val_mse: 0.1902 - val_rmse: 0.4362 - val_mae: 0.2420 - val_mape: 7.5058 - lr: 1.0000e-04\n",
      "Epoch 603/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.1927 - mse: 0.1927 - rmse: 0.4389 - mae: 0.2464 - mape: 7.7188\n",
      "Epoch 603: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4372 - mae: 0.2452 - mape: 7.6814 - val_loss: 0.1906 - val_mse: 0.1906 - val_rmse: 0.4366 - val_mae: 0.2428 - val_mape: 7.5333 - lr: 1.0000e-04\n",
      "Epoch 604/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2442 - mape: 7.6501\n",
      "Epoch 604: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4370 - mae: 0.2447 - mape: 7.6704 - val_loss: 0.1895 - val_mse: 0.1895 - val_rmse: 0.4353 - val_mae: 0.2442 - val_mape: 7.6427 - lr: 1.0000e-04\n",
      "Epoch 605/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1906 - mse: 0.1906 - rmse: 0.4365 - mae: 0.2454 - mape: 7.6840\n",
      "Epoch 605: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1910 - mse: 0.1910 - rmse: 0.4371 - mae: 0.2450 - mape: 7.6806 - val_loss: 0.1911 - val_mse: 0.1911 - val_rmse: 0.4371 - val_mae: 0.2425 - val_mape: 7.5724 - lr: 1.0000e-04\n",
      "Epoch 606/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2452 - mape: 7.6744\n",
      "Epoch 606: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2452 - mape: 7.6775 - val_loss: 0.1916 - val_mse: 0.1916 - val_rmse: 0.4377 - val_mae: 0.2430 - val_mape: 7.5120 - lr: 1.0000e-04\n",
      "Epoch 607/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2453 - mape: 7.6840\n",
      "Epoch 607: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2453 - mape: 7.6840 - val_loss: 0.1916 - val_mse: 0.1916 - val_rmse: 0.4377 - val_mae: 0.2448 - val_mape: 7.7249 - lr: 1.0000e-04\n",
      "Epoch 608/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2451 - mape: 7.6825\n",
      "Epoch 608: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2447 - mape: 7.6743 - val_loss: 0.1894 - val_mse: 0.1894 - val_rmse: 0.4352 - val_mae: 0.2410 - val_mape: 7.5140 - lr: 1.0000e-04\n",
      "Epoch 609/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1886 - mse: 0.1886 - rmse: 0.4342 - mae: 0.2430 - mape: 7.6109\n",
      "Epoch 609: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2444 - mape: 7.6536 - val_loss: 0.1899 - val_mse: 0.1899 - val_rmse: 0.4358 - val_mae: 0.2432 - val_mape: 7.5651 - lr: 1.0000e-04\n",
      "Epoch 610/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1916 - mse: 0.1916 - rmse: 0.4378 - mae: 0.2446 - mape: 7.6556\n",
      "Epoch 610: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2445 - mape: 7.6533 - val_loss: 0.1903 - val_mse: 0.1903 - val_rmse: 0.4362 - val_mae: 0.2491 - val_mape: 7.9002 - lr: 1.0000e-04\n",
      "Epoch 611/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2443 - mape: 7.6688\n",
      "Epoch 611: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2448 - mape: 7.6741 - val_loss: 0.1901 - val_mse: 0.1901 - val_rmse: 0.4360 - val_mae: 0.2430 - val_mape: 7.6073 - lr: 1.0000e-04\n",
      "Epoch 612/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1894 - mse: 0.1894 - rmse: 0.4352 - mae: 0.2440 - mape: 7.6470\n",
      "Epoch 612: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2445 - mape: 7.6575 - val_loss: 0.1899 - val_mse: 0.1899 - val_rmse: 0.4358 - val_mae: 0.2435 - val_mape: 7.6052 - lr: 1.0000e-04\n",
      "Epoch 613/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1905 - mse: 0.1905 - rmse: 0.4365 - mae: 0.2446 - mape: 7.6728\n",
      "Epoch 613: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2448 - mape: 7.6739 - val_loss: 0.1901 - val_mse: 0.1901 - val_rmse: 0.4360 - val_mae: 0.2438 - val_mape: 7.6032 - lr: 1.0000e-04\n",
      "Epoch 614/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2455 - mape: 7.6992\n",
      "Epoch 614: val_loss did not improve from 0.18917\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2450 - mape: 7.6833 - val_loss: 0.1900 - val_mse: 0.1900 - val_rmse: 0.4359 - val_mae: 0.2447 - val_mape: 7.6404 - lr: 1.0000e-04\n",
      "Epoch 615/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1906 - mse: 0.1906 - rmse: 0.4366 - mae: 0.2449 - mape: 7.6703\n",
      "Epoch 615: val_loss improved from 0.18917 to 0.18905, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2449 - mape: 7.6696 - val_loss: 0.1891 - val_mse: 0.1891 - val_rmse: 0.4348 - val_mae: 0.2431 - val_mape: 7.6378 - lr: 1.0000e-04\n",
      "Epoch 616/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2445 - mape: 7.6751\n",
      "Epoch 616: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2445 - mape: 7.6696 - val_loss: 0.1896 - val_mse: 0.1896 - val_rmse: 0.4354 - val_mae: 0.2431 - val_mape: 7.5716 - lr: 1.0000e-04\n",
      "Epoch 617/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2453 - mape: 7.6865\n",
      "Epoch 617: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2450 - mape: 7.6780 - val_loss: 0.1891 - val_mse: 0.1891 - val_rmse: 0.4348 - val_mae: 0.2438 - val_mape: 7.6346 - lr: 1.0000e-04\n",
      "Epoch 618/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2452 - mape: 7.6966\n",
      "Epoch 618: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2450 - mape: 7.6906 - val_loss: 0.1910 - val_mse: 0.1910 - val_rmse: 0.4370 - val_mae: 0.2436 - val_mape: 7.5639 - lr: 1.0000e-04\n",
      "Epoch 619/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1905 - mse: 0.1905 - rmse: 0.4365 - mae: 0.2441 - mape: 7.6394\n",
      "Epoch 619: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2442 - mape: 7.6423 - val_loss: 0.1901 - val_mse: 0.1901 - val_rmse: 0.4360 - val_mae: 0.2439 - val_mape: 7.7046 - lr: 1.0000e-04\n",
      "Epoch 620/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1922 - mse: 0.1922 - rmse: 0.4384 - mae: 0.2455 - mape: 7.6896\n",
      "Epoch 620: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4379 - mae: 0.2454 - mape: 7.6871 - val_loss: 0.1907 - val_mse: 0.1907 - val_rmse: 0.4367 - val_mae: 0.2469 - val_mape: 7.7501 - lr: 1.0000e-04\n",
      "Epoch 621/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1920 - mse: 0.1920 - rmse: 0.4381 - mae: 0.2456 - mape: 7.6874\n",
      "Epoch 621: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1910 - mse: 0.1910 - rmse: 0.4371 - mae: 0.2450 - mape: 7.6746 - val_loss: 0.1903 - val_mse: 0.1903 - val_rmse: 0.4363 - val_mae: 0.2411 - val_mape: 7.4804 - lr: 1.0000e-04\n",
      "Epoch 622/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1918 - mse: 0.1918 - rmse: 0.4379 - mae: 0.2447 - mape: 7.6676\n",
      "Epoch 622: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2446 - mape: 7.6643 - val_loss: 0.1893 - val_mse: 0.1893 - val_rmse: 0.4351 - val_mae: 0.2447 - val_mape: 7.6741 - lr: 1.0000e-04\n",
      "Epoch 623/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2456 - mape: 7.6901\n",
      "Epoch 623: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2453 - mape: 7.6863 - val_loss: 0.1899 - val_mse: 0.1899 - val_rmse: 0.4358 - val_mae: 0.2453 - val_mape: 7.7483 - lr: 1.0000e-04\n",
      "Epoch 624/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1906 - mse: 0.1906 - rmse: 0.4365 - mae: 0.2451 - mape: 7.6837\n",
      "Epoch 624: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2452 - mape: 7.6817 - val_loss: 0.1908 - val_mse: 0.1908 - val_rmse: 0.4368 - val_mae: 0.2422 - val_mape: 7.5119 - lr: 1.0000e-04\n",
      "Epoch 625/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1900 - mse: 0.1900 - rmse: 0.4359 - mae: 0.2446 - mape: 7.6492\n",
      "Epoch 625: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2448 - mape: 7.6628 - val_loss: 0.1896 - val_mse: 0.1896 - val_rmse: 0.4354 - val_mae: 0.2413 - val_mape: 7.5111 - lr: 1.0000e-04\n",
      "Epoch 626/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1910 - mse: 0.1910 - rmse: 0.4371 - mae: 0.2444 - mape: 7.6560\n",
      "Epoch 626: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1917 - mse: 0.1917 - rmse: 0.4378 - mae: 0.2444 - mape: 7.6586 - val_loss: 0.1896 - val_mse: 0.1896 - val_rmse: 0.4354 - val_mae: 0.2441 - val_mape: 7.6094 - lr: 1.0000e-04\n",
      "Epoch 627/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1917 - mse: 0.1917 - rmse: 0.4378 - mae: 0.2459 - mape: 7.7003\n",
      "Epoch 627: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2451 - mape: 7.6793 - val_loss: 0.1918 - val_mse: 0.1918 - val_rmse: 0.4379 - val_mae: 0.2469 - val_mape: 7.7236 - lr: 1.0000e-04\n",
      "Epoch 628/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1900 - mse: 0.1900 - rmse: 0.4359 - mae: 0.2443 - mape: 7.6543\n",
      "Epoch 628: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2449 - mape: 7.6766 - val_loss: 0.1893 - val_mse: 0.1893 - val_rmse: 0.4351 - val_mae: 0.2435 - val_mape: 7.6595 - lr: 1.0000e-04\n",
      "Epoch 629/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1904 - mse: 0.1904 - rmse: 0.4364 - mae: 0.2441 - mape: 7.6572\n",
      "Epoch 629: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4372 - mae: 0.2446 - mape: 7.6642 - val_loss: 0.1937 - val_mse: 0.1937 - val_rmse: 0.4401 - val_mae: 0.2480 - val_mape: 7.7242 - lr: 1.0000e-04\n",
      "Epoch 630/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1911 - mse: 0.1911 - rmse: 0.4372 - mae: 0.2451 - mape: 7.6915\n",
      "Epoch 630: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1906 - mse: 0.1906 - rmse: 0.4365 - mae: 0.2444 - mape: 7.6645 - val_loss: 0.1915 - val_mse: 0.1915 - val_rmse: 0.4376 - val_mae: 0.2447 - val_mape: 7.6700 - lr: 1.0000e-04\n",
      "Epoch 631/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2449 - mape: 7.6626\n",
      "Epoch 631: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2447 - mape: 7.6558 - val_loss: 0.1908 - val_mse: 0.1908 - val_rmse: 0.4368 - val_mae: 0.2451 - val_mape: 7.7820 - lr: 1.0000e-04\n",
      "Epoch 632/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2448 - mape: 7.6829\n",
      "Epoch 632: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4372 - mae: 0.2444 - mape: 7.6701 - val_loss: 0.1893 - val_mse: 0.1893 - val_rmse: 0.4351 - val_mae: 0.2412 - val_mape: 7.5098 - lr: 1.0000e-04\n",
      "Epoch 633/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1894 - mse: 0.1894 - rmse: 0.4352 - mae: 0.2437 - mape: 7.6468\n",
      "Epoch 633: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2448 - mape: 7.6706 - val_loss: 0.1902 - val_mse: 0.1902 - val_rmse: 0.4361 - val_mae: 0.2447 - val_mape: 7.6655 - lr: 1.0000e-04\n",
      "Epoch 634/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2440 - mape: 7.6358\n",
      "Epoch 634: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2443 - mape: 7.6553 - val_loss: 0.1899 - val_mse: 0.1899 - val_rmse: 0.4358 - val_mae: 0.2433 - val_mape: 7.6303 - lr: 1.0000e-04\n",
      "Epoch 635/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1895 - mse: 0.1895 - rmse: 0.4353 - mae: 0.2441 - mape: 7.6552\n",
      "Epoch 635: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2447 - mape: 7.6732 - val_loss: 0.1900 - val_mse: 0.1900 - val_rmse: 0.4359 - val_mae: 0.2443 - val_mape: 7.6744 - lr: 1.0000e-04\n",
      "Epoch 636/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1905 - mse: 0.1905 - rmse: 0.4365 - mae: 0.2448 - mape: 7.6732\n",
      "Epoch 636: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4381 - mae: 0.2456 - mape: 7.6913 - val_loss: 0.1917 - val_mse: 0.1917 - val_rmse: 0.4378 - val_mae: 0.2456 - val_mape: 7.6897 - lr: 1.0000e-04\n",
      "Epoch 637/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1904 - mse: 0.1904 - rmse: 0.4363 - mae: 0.2447 - mape: 7.6662\n",
      "Epoch 637: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1905 - mse: 0.1905 - rmse: 0.4365 - mae: 0.2442 - mape: 7.6463 - val_loss: 0.1899 - val_mse: 0.1899 - val_rmse: 0.4358 - val_mae: 0.2474 - val_mape: 7.8326 - lr: 1.0000e-04\n",
      "Epoch 638/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1911 - mse: 0.1911 - rmse: 0.4372 - mae: 0.2446 - mape: 7.6672\n",
      "Epoch 638: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1905 - mse: 0.1905 - rmse: 0.4364 - mae: 0.2444 - mape: 7.6573 - val_loss: 0.1901 - val_mse: 0.1901 - val_rmse: 0.4360 - val_mae: 0.2457 - val_mape: 7.7767 - lr: 1.0000e-04\n",
      "Epoch 639/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1918 - mse: 0.1918 - rmse: 0.4380 - mae: 0.2452 - mape: 7.6817\n",
      "Epoch 639: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2445 - mape: 7.6647 - val_loss: 0.1917 - val_mse: 0.1917 - val_rmse: 0.4379 - val_mae: 0.2428 - val_mape: 7.5220 - lr: 1.0000e-04\n",
      "Epoch 640/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2452 - mape: 7.6782\n",
      "Epoch 640: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1906 - mse: 0.1906 - rmse: 0.4366 - mae: 0.2442 - mape: 7.6543 - val_loss: 0.1899 - val_mse: 0.1899 - val_rmse: 0.4357 - val_mae: 0.2411 - val_mape: 7.4918 - lr: 1.0000e-04\n",
      "Epoch 641/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1904 - mse: 0.1904 - rmse: 0.4364 - mae: 0.2441 - mape: 7.6498\n",
      "Epoch 641: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2445 - mape: 7.6602 - val_loss: 0.1896 - val_mse: 0.1896 - val_rmse: 0.4354 - val_mae: 0.2436 - val_mape: 7.6130 - lr: 1.0000e-04\n",
      "Epoch 642/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1904 - mse: 0.1904 - rmse: 0.4363 - mae: 0.2442 - mape: 7.6578\n",
      "Epoch 642: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1903 - mse: 0.1903 - rmse: 0.4362 - mae: 0.2442 - mape: 7.6579 - val_loss: 0.1910 - val_mse: 0.1910 - val_rmse: 0.4370 - val_mae: 0.2446 - val_mape: 7.7113 - lr: 1.0000e-04\n",
      "Epoch 643/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2443 - mape: 7.6619\n",
      "Epoch 643: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2443 - mape: 7.6619 - val_loss: 0.1910 - val_mse: 0.1910 - val_rmse: 0.4371 - val_mae: 0.2444 - val_mape: 7.5986 - lr: 1.0000e-04\n",
      "Epoch 644/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1901 - mse: 0.1901 - rmse: 0.4360 - mae: 0.2443 - mape: 7.6616\n",
      "Epoch 644: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2447 - mape: 7.6526 - val_loss: 0.1910 - val_mse: 0.1910 - val_rmse: 0.4371 - val_mae: 0.2500 - val_mape: 7.9613 - lr: 1.0000e-04\n",
      "Epoch 645/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1931 - mse: 0.1931 - rmse: 0.4394 - mae: 0.2462 - mape: 7.7175\n",
      "Epoch 645: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2455 - mape: 7.6936 - val_loss: 0.1899 - val_mse: 0.1899 - val_rmse: 0.4358 - val_mae: 0.2431 - val_mape: 7.5823 - lr: 1.0000e-04\n",
      "Epoch 646/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2446 - mape: 7.6563\n",
      "Epoch 646: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1908 - mse: 0.1908 - rmse: 0.4369 - mae: 0.2439 - mape: 7.6481 - val_loss: 0.1891 - val_mse: 0.1891 - val_rmse: 0.4348 - val_mae: 0.2433 - val_mape: 7.6315 - lr: 1.0000e-04\n",
      "Epoch 647/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1918 - mse: 0.1918 - rmse: 0.4379 - mae: 0.2451 - mape: 7.6687\n",
      "Epoch 647: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1906 - mse: 0.1906 - rmse: 0.4366 - mae: 0.2446 - mape: 7.6572 - val_loss: 0.1918 - val_mse: 0.1918 - val_rmse: 0.4379 - val_mae: 0.2458 - val_mape: 7.6671 - lr: 1.0000e-04\n",
      "Epoch 648/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2445 - mape: 7.6506\n",
      "Epoch 648: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4371 - mae: 0.2442 - mape: 7.6479 - val_loss: 0.1897 - val_mse: 0.1897 - val_rmse: 0.4355 - val_mae: 0.2435 - val_mape: 7.5998 - lr: 1.0000e-04\n",
      "Epoch 649/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1914 - mse: 0.1914 - rmse: 0.4374 - mae: 0.2439 - mape: 7.6434\n",
      "Epoch 649: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2443 - mape: 7.6606 - val_loss: 0.1956 - val_mse: 0.1956 - val_rmse: 0.4422 - val_mae: 0.2463 - val_mape: 7.6308 - lr: 1.0000e-04\n",
      "Epoch 650/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2445 - mape: 7.6614\n",
      "Epoch 650: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1910 - mse: 0.1910 - rmse: 0.4371 - mae: 0.2445 - mape: 7.6601 - val_loss: 0.1904 - val_mse: 0.1904 - val_rmse: 0.4364 - val_mae: 0.2422 - val_mape: 7.4985 - lr: 1.0000e-04\n",
      "Epoch 651/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2445 - mape: 7.6609\n",
      "Epoch 651: val_loss did not improve from 0.18905\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2443 - mape: 7.6538 - val_loss: 0.1940 - val_mse: 0.1940 - val_rmse: 0.4404 - val_mae: 0.2447 - val_mape: 7.5355 - lr: 1.0000e-04\n",
      "Epoch 652/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2450 - mape: 7.6606\n",
      "Epoch 652: val_loss improved from 0.18905 to 0.18895, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2450 - mape: 7.6606 - val_loss: 0.1890 - val_mse: 0.1890 - val_rmse: 0.4347 - val_mae: 0.2427 - val_mape: 7.6116 - lr: 1.0000e-04\n",
      "Epoch 653/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4348 - mae: 0.2438 - mape: 7.6526\n",
      "Epoch 653: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1905 - mse: 0.1905 - rmse: 0.4365 - mae: 0.2440 - mape: 7.6457 - val_loss: 0.1890 - val_mse: 0.1890 - val_rmse: 0.4347 - val_mae: 0.2453 - val_mape: 7.6970 - lr: 1.0000e-04\n",
      "Epoch 654/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1901 - mse: 0.1901 - rmse: 0.4360 - mae: 0.2453 - mape: 7.7006\n",
      "Epoch 654: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2451 - mape: 7.6853 - val_loss: 0.1894 - val_mse: 0.1894 - val_rmse: 0.4352 - val_mae: 0.2433 - val_mape: 7.6318 - lr: 1.0000e-04\n",
      "Epoch 655/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2451 - mape: 7.6859\n",
      "Epoch 655: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4372 - mae: 0.2450 - mape: 7.6795 - val_loss: 0.1894 - val_mse: 0.1894 - val_rmse: 0.4352 - val_mae: 0.2418 - val_mape: 7.5605 - lr: 1.0000e-04\n",
      "Epoch 656/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2448 - mape: 7.6723\n",
      "Epoch 656: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1906 - mse: 0.1906 - rmse: 0.4366 - mae: 0.2445 - mape: 7.6596 - val_loss: 0.1897 - val_mse: 0.1897 - val_rmse: 0.4356 - val_mae: 0.2443 - val_mape: 7.6415 - lr: 1.0000e-04\n",
      "Epoch 657/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2449 - mape: 7.6708\n",
      "Epoch 657: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2450 - mape: 7.6739 - val_loss: 0.1891 - val_mse: 0.1891 - val_rmse: 0.4348 - val_mae: 0.2432 - val_mape: 7.6297 - lr: 1.0000e-04\n",
      "Epoch 658/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2440 - mape: 7.6402\n",
      "Epoch 658: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2439 - mape: 7.6376 - val_loss: 0.1892 - val_mse: 0.1892 - val_rmse: 0.4350 - val_mae: 0.2440 - val_mape: 7.6747 - lr: 1.0000e-04\n",
      "Epoch 659/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2446 - mape: 7.6621\n",
      "Epoch 659: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2446 - mape: 7.6580 - val_loss: 0.1893 - val_mse: 0.1893 - val_rmse: 0.4351 - val_mae: 0.2436 - val_mape: 7.6724 - lr: 1.0000e-04\n",
      "Epoch 660/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2449 - mape: 7.6753\n",
      "Epoch 660: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2441 - mape: 7.6456 - val_loss: 0.1894 - val_mse: 0.1894 - val_rmse: 0.4352 - val_mae: 0.2422 - val_mape: 7.6051 - lr: 1.0000e-04\n",
      "Epoch 661/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1910 - mse: 0.1910 - rmse: 0.4370 - mae: 0.2437 - mape: 7.6350\n",
      "Epoch 661: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1903 - mse: 0.1903 - rmse: 0.4363 - mae: 0.2436 - mape: 7.6410 - val_loss: 0.1914 - val_mse: 0.1914 - val_rmse: 0.4375 - val_mae: 0.2445 - val_mape: 7.6037 - lr: 1.0000e-04\n",
      "Epoch 662/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1901 - mse: 0.1901 - rmse: 0.4360 - mae: 0.2444 - mape: 7.6586\n",
      "Epoch 662: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1904 - mse: 0.1904 - rmse: 0.4363 - mae: 0.2443 - mape: 7.6459 - val_loss: 0.1909 - val_mse: 0.1909 - val_rmse: 0.4370 - val_mae: 0.2481 - val_mape: 7.8713 - lr: 1.0000e-04\n",
      "Epoch 663/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1901 - mse: 0.1901 - rmse: 0.4360 - mae: 0.2441 - mape: 7.6564\n",
      "Epoch 663: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4370 - mae: 0.2445 - mape: 7.6736 - val_loss: 0.1899 - val_mse: 0.1899 - val_rmse: 0.4358 - val_mae: 0.2433 - val_mape: 7.6115 - lr: 1.0000e-04\n",
      "Epoch 664/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1905 - mse: 0.1905 - rmse: 0.4365 - mae: 0.2445 - mape: 7.6614\n",
      "Epoch 664: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2447 - mape: 7.6681 - val_loss: 0.1904 - val_mse: 0.1904 - val_rmse: 0.4364 - val_mae: 0.2452 - val_mape: 7.7364 - lr: 1.0000e-04\n",
      "Epoch 665/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1909 - mse: 0.1909 - rmse: 0.4370 - mae: 0.2441 - mape: 7.6473\n",
      "Epoch 665: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2445 - mape: 7.6562 - val_loss: 0.1894 - val_mse: 0.1894 - val_rmse: 0.4352 - val_mae: 0.2469 - val_mape: 7.7977 - lr: 1.0000e-04\n",
      "Epoch 666/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1906 - mse: 0.1906 - rmse: 0.4366 - mae: 0.2443 - mape: 7.6658\n",
      "Epoch 666: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2445 - mape: 7.6665 - val_loss: 0.1901 - val_mse: 0.1901 - val_rmse: 0.4360 - val_mae: 0.2475 - val_mape: 7.7967 - lr: 1.0000e-04\n",
      "Epoch 667/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2458 - mape: 7.7034\n",
      "Epoch 667: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2445 - mape: 7.6598 - val_loss: 0.1927 - val_mse: 0.1927 - val_rmse: 0.4390 - val_mae: 0.2451 - val_mape: 7.7677 - lr: 1.0000e-04\n",
      "Epoch 668/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1917 - mse: 0.1917 - rmse: 0.4379 - mae: 0.2451 - mape: 7.6900\n",
      "Epoch 668: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2454 - mape: 7.6909 - val_loss: 0.1912 - val_mse: 0.1912 - val_rmse: 0.4372 - val_mae: 0.2469 - val_mape: 7.7751 - lr: 1.0000e-04\n",
      "Epoch 669/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1899 - mse: 0.1899 - rmse: 0.4358 - mae: 0.2448 - mape: 7.6727\n",
      "Epoch 669: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1905 - mse: 0.1905 - rmse: 0.4365 - mae: 0.2452 - mape: 7.6811 - val_loss: 0.1895 - val_mse: 0.1895 - val_rmse: 0.4354 - val_mae: 0.2425 - val_mape: 7.5859 - lr: 1.0000e-04\n",
      "Epoch 670/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1923 - mse: 0.1923 - rmse: 0.4386 - mae: 0.2457 - mape: 7.6891\n",
      "Epoch 670: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2451 - mape: 7.6834 - val_loss: 0.1900 - val_mse: 0.1900 - val_rmse: 0.4359 - val_mae: 0.2418 - val_mape: 7.4937 - lr: 1.0000e-04\n",
      "Epoch 671/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1897 - mse: 0.1897 - rmse: 0.4355 - mae: 0.2438 - mape: 7.6384\n",
      "Epoch 671: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2448 - mape: 7.6690 - val_loss: 0.1902 - val_mse: 0.1902 - val_rmse: 0.4362 - val_mae: 0.2441 - val_mape: 7.6047 - lr: 1.0000e-04\n",
      "Epoch 672/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1893 - mse: 0.1893 - rmse: 0.4351 - mae: 0.2447 - mape: 7.6683\n",
      "Epoch 672: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1902 - mse: 0.1902 - rmse: 0.4361 - mae: 0.2444 - mape: 7.6515 - val_loss: 0.1907 - val_mse: 0.1907 - val_rmse: 0.4367 - val_mae: 0.2427 - val_mape: 7.6338 - lr: 1.0000e-04\n",
      "Epoch 673/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2444 - mape: 7.6593\n",
      "Epoch 673: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1914 - mse: 0.1914 - rmse: 0.4375 - mae: 0.2444 - mape: 7.6593 - val_loss: 0.1921 - val_mse: 0.1921 - val_rmse: 0.4383 - val_mae: 0.2428 - val_mape: 7.5395 - lr: 1.0000e-04\n",
      "Epoch 674/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1897 - mse: 0.1897 - rmse: 0.4356 - mae: 0.2436 - mape: 7.6248\n",
      "Epoch 674: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1910 - mse: 0.1910 - rmse: 0.4370 - mae: 0.2443 - mape: 7.6525 - val_loss: 0.1894 - val_mse: 0.1894 - val_rmse: 0.4353 - val_mae: 0.2451 - val_mape: 7.6560 - lr: 1.0000e-04\n",
      "Epoch 675/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1916 - mse: 0.1916 - rmse: 0.4377 - mae: 0.2449 - mape: 7.6729\n",
      "Epoch 675: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1915 - mse: 0.1915 - rmse: 0.4377 - mae: 0.2449 - mape: 7.6727 - val_loss: 0.1894 - val_mse: 0.1894 - val_rmse: 0.4352 - val_mae: 0.2434 - val_mape: 7.6201 - lr: 1.0000e-04\n",
      "Epoch 676/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1911 - mse: 0.1911 - rmse: 0.4372 - mae: 0.2438 - mape: 7.6333\n",
      "Epoch 676: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2439 - mape: 7.6322 - val_loss: 0.1946 - val_mse: 0.1946 - val_rmse: 0.4412 - val_mae: 0.2471 - val_mape: 7.6575 - lr: 1.0000e-04\n",
      "Epoch 677/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2447 - mape: 7.6708\n",
      "Epoch 677: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1906 - mse: 0.1906 - rmse: 0.4366 - mae: 0.2447 - mape: 7.6675 - val_loss: 0.1961 - val_mse: 0.1961 - val_rmse: 0.4429 - val_mae: 0.2506 - val_mape: 7.9763 - lr: 1.0000e-04\n",
      "Epoch 678/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2434 - mape: 7.6460\n",
      "Epoch 678: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2443 - mape: 7.6555 - val_loss: 0.1928 - val_mse: 0.1928 - val_rmse: 0.4391 - val_mae: 0.2496 - val_mape: 7.9518 - lr: 1.0000e-04\n",
      "Epoch 679/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1900 - mse: 0.1900 - rmse: 0.4359 - mae: 0.2443 - mape: 7.6566\n",
      "Epoch 679: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1908 - mse: 0.1908 - rmse: 0.4369 - mae: 0.2443 - mape: 7.6574 - val_loss: 0.1923 - val_mse: 0.1923 - val_rmse: 0.4385 - val_mae: 0.2488 - val_mape: 7.8564 - lr: 1.0000e-04\n",
      "Epoch 680/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1920 - mse: 0.1920 - rmse: 0.4382 - mae: 0.2445 - mape: 7.6567\n",
      "Epoch 680: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2442 - mape: 7.6506 - val_loss: 0.1906 - val_mse: 0.1906 - val_rmse: 0.4366 - val_mae: 0.2459 - val_mape: 7.7101 - lr: 1.0000e-04\n",
      "Epoch 681/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1901 - mse: 0.1901 - rmse: 0.4360 - mae: 0.2432 - mape: 7.6149\n",
      "Epoch 681: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.1911 - mse: 0.1911 - rmse: 0.4372 - mae: 0.2444 - mape: 7.6572 - val_loss: 0.1893 - val_mse: 0.1893 - val_rmse: 0.4351 - val_mae: 0.2434 - val_mape: 7.6379 - lr: 1.0000e-04\n",
      "Epoch 682/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1917 - mse: 0.1917 - rmse: 0.4379 - mae: 0.2449 - mape: 7.6712\n",
      "Epoch 682: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2447 - mape: 7.6625 - val_loss: 0.1896 - val_mse: 0.1896 - val_rmse: 0.4354 - val_mae: 0.2448 - val_mape: 7.7012 - lr: 1.0000e-04\n",
      "Epoch 683/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2443 - mape: 7.6420\n",
      "Epoch 683: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.1906 - mse: 0.1906 - rmse: 0.4366 - mae: 0.2442 - mape: 7.6478 - val_loss: 0.1904 - val_mse: 0.1904 - val_rmse: 0.4363 - val_mae: 0.2431 - val_mape: 7.6524 - lr: 1.0000e-04\n",
      "Epoch 684/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1926 - mse: 0.1926 - rmse: 0.4389 - mae: 0.2456 - mape: 7.7055\n",
      "Epoch 684: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2448 - mape: 7.6809 - val_loss: 0.1896 - val_mse: 0.1896 - val_rmse: 0.4354 - val_mae: 0.2401 - val_mape: 7.4434 - lr: 1.0000e-04\n",
      "Epoch 685/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1880 - mse: 0.1880 - rmse: 0.4336 - mae: 0.2431 - mape: 7.6140\n",
      "Epoch 685: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2440 - mape: 7.6424 - val_loss: 0.1891 - val_mse: 0.1891 - val_rmse: 0.4348 - val_mae: 0.2421 - val_mape: 7.5874 - lr: 1.0000e-04\n",
      "Epoch 686/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1915 - mse: 0.1915 - rmse: 0.4376 - mae: 0.2448 - mape: 7.6765\n",
      "Epoch 686: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1910 - mse: 0.1910 - rmse: 0.4370 - mae: 0.2441 - mape: 7.6485 - val_loss: 0.1914 - val_mse: 0.1914 - val_rmse: 0.4375 - val_mae: 0.2444 - val_mape: 7.6655 - lr: 1.0000e-04\n",
      "Epoch 687/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2446 - mape: 7.6656\n",
      "Epoch 687: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2450 - mape: 7.6795 - val_loss: 0.1938 - val_mse: 0.1938 - val_rmse: 0.4403 - val_mae: 0.2453 - val_mape: 7.5283 - lr: 1.0000e-04\n",
      "Epoch 688/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1906 - mse: 0.1906 - rmse: 0.4366 - mae: 0.2439 - mape: 7.6338\n",
      "Epoch 688: val_loss did not improve from 0.18895\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1906 - mse: 0.1906 - rmse: 0.4366 - mae: 0.2438 - mape: 7.6316 - val_loss: 0.1892 - val_mse: 0.1892 - val_rmse: 0.4350 - val_mae: 0.2430 - val_mape: 7.6209 - lr: 1.0000e-04\n",
      "Epoch 689/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1909 - mse: 0.1909 - rmse: 0.4370 - mae: 0.2447 - mape: 7.6846\n",
      "Epoch 689: val_loss improved from 0.18895 to 0.18886, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2449 - mape: 7.6886 - val_loss: 0.1889 - val_mse: 0.1889 - val_rmse: 0.4346 - val_mae: 0.2414 - val_mape: 7.5303 - lr: 1.0000e-04\n",
      "Epoch 690/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1921 - mse: 0.1921 - rmse: 0.4383 - mae: 0.2450 - mape: 7.6672\n",
      "Epoch 690: val_loss did not improve from 0.18886\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2447 - mape: 7.6592 - val_loss: 0.1916 - val_mse: 0.1916 - val_rmse: 0.4377 - val_mae: 0.2430 - val_mape: 7.6817 - lr: 1.0000e-04\n",
      "Epoch 691/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1913 - mse: 0.1913 - rmse: 0.4374 - mae: 0.2440 - mape: 7.6302\n",
      "Epoch 691: val_loss did not improve from 0.18886\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1910 - mse: 0.1910 - rmse: 0.4370 - mae: 0.2441 - mape: 7.6425 - val_loss: 0.1900 - val_mse: 0.1900 - val_rmse: 0.4359 - val_mae: 0.2437 - val_mape: 7.6857 - lr: 1.0000e-04\n",
      "Epoch 692/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1927 - mse: 0.1927 - rmse: 0.4390 - mae: 0.2447 - mape: 7.6594\n",
      "Epoch 692: val_loss did not improve from 0.18886\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2441 - mape: 7.6500 - val_loss: 0.1896 - val_mse: 0.1896 - val_rmse: 0.4354 - val_mae: 0.2443 - val_mape: 7.6859 - lr: 1.0000e-04\n",
      "Epoch 693/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1881 - mse: 0.1881 - rmse: 0.4337 - mae: 0.2424 - mape: 7.6049\n",
      "Epoch 693: val_loss improved from 0.18886 to 0.18877, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4346 - mae: 0.2427 - mape: 7.6068 - val_loss: 0.1888 - val_mse: 0.1888 - val_rmse: 0.4345 - val_mae: 0.2425 - val_mape: 7.5870 - lr: 1.0000e-05\n",
      "Epoch 694/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2426 - mape: 7.5957\n",
      "Epoch 694: val_loss improved from 0.18877 to 0.18855, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1889 - mse: 0.1889 - rmse: 0.4346 - mae: 0.2427 - mape: 7.6072 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2423 - val_mape: 7.5929 - lr: 1.0000e-05\n",
      "Epoch 695/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1875 - mse: 0.1875 - rmse: 0.4330 - mae: 0.2418 - mape: 7.5835\n",
      "Epoch 695: val_loss improved from 0.18855 to 0.18854, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1889 - mse: 0.1889 - rmse: 0.4346 - mae: 0.2423 - mape: 7.5865 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2424 - val_mape: 7.5982 - lr: 1.0000e-05\n",
      "Epoch 696/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1889 - mse: 0.1889 - rmse: 0.4346 - mae: 0.2427 - mape: 7.6139\n",
      "Epoch 696: val_loss improved from 0.18854 to 0.18854, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4346 - mae: 0.2427 - mape: 7.6127 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2423 - val_mape: 7.5818 - lr: 1.0000e-05\n",
      "Epoch 697/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1895 - mse: 0.1895 - rmse: 0.4353 - mae: 0.2427 - mape: 7.5948\n",
      "Epoch 697: val_loss did not improve from 0.18854\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2423 - mape: 7.5839 - val_loss: 0.1886 - val_mse: 0.1886 - val_rmse: 0.4342 - val_mae: 0.2424 - val_mape: 7.6082 - lr: 1.0000e-05\n",
      "Epoch 698/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1901 - mse: 0.1901 - rmse: 0.4360 - mae: 0.2429 - mape: 7.6210\n",
      "Epoch 698: val_loss did not improve from 0.18854\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2425 - mape: 7.6097 - val_loss: 0.1886 - val_mse: 0.1886 - val_rmse: 0.4343 - val_mae: 0.2425 - val_mape: 7.5847 - lr: 1.0000e-05\n",
      "Epoch 699/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1876 - mse: 0.1876 - rmse: 0.4331 - mae: 0.2418 - mape: 7.5837\n",
      "Epoch 699: val_loss did not improve from 0.18854\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2424 - mape: 7.5915 - val_loss: 0.1890 - val_mse: 0.1890 - val_rmse: 0.4348 - val_mae: 0.2429 - val_mape: 7.6161 - lr: 1.0000e-05\n",
      "Epoch 700/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4347 - mae: 0.2424 - mape: 7.5900\n",
      "Epoch 700: val_loss did not improve from 0.18854\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1890 - mse: 0.1890 - rmse: 0.4347 - mae: 0.2424 - mape: 7.5900 - val_loss: 0.1886 - val_mse: 0.1886 - val_rmse: 0.4342 - val_mae: 0.2427 - val_mape: 7.6162 - lr: 1.0000e-05\n",
      "Epoch 701/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2433 - mape: 7.6250\n",
      "Epoch 701: val_loss did not improve from 0.18854\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2425 - mape: 7.6064 - val_loss: 0.1886 - val_mse: 0.1886 - val_rmse: 0.4343 - val_mae: 0.2423 - val_mape: 7.5836 - lr: 1.0000e-05\n",
      "Epoch 702/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1898 - mse: 0.1898 - rmse: 0.4357 - mae: 0.2430 - mape: 7.6228\n",
      "Epoch 702: val_loss improved from 0.18854 to 0.18852, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2426 - mape: 7.6028 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2419 - val_mape: 7.5691 - lr: 1.0000e-05\n",
      "Epoch 703/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1894 - mse: 0.1894 - rmse: 0.4352 - mae: 0.2431 - mape: 7.6076\n",
      "Epoch 703: val_loss improved from 0.18852 to 0.18850, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1889 - mse: 0.1889 - rmse: 0.4346 - mae: 0.2423 - mape: 7.5888 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2420 - val_mape: 7.5831 - lr: 1.0000e-05\n",
      "Epoch 704/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2422 - mape: 7.5825\n",
      "Epoch 704: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4345 - mae: 0.2421 - mape: 7.5819 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2425 - val_mape: 7.6202 - lr: 1.0000e-05\n",
      "Epoch 705/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4340 - mae: 0.2426 - mape: 7.6097\n",
      "Epoch 705: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2423 - mape: 7.5971 - val_loss: 0.1886 - val_mse: 0.1886 - val_rmse: 0.4342 - val_mae: 0.2421 - val_mape: 7.5869 - lr: 1.0000e-05\n",
      "Epoch 706/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1905 - mse: 0.1905 - rmse: 0.4364 - mae: 0.2433 - mape: 7.6199\n",
      "Epoch 706: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4345 - mae: 0.2426 - mape: 7.6154 - val_loss: 0.1886 - val_mse: 0.1886 - val_rmse: 0.4343 - val_mae: 0.2417 - val_mape: 7.5573 - lr: 1.0000e-05\n",
      "Epoch 707/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1864 - mse: 0.1864 - rmse: 0.4317 - mae: 0.2411 - mape: 7.5527\n",
      "Epoch 707: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2419 - mape: 7.5704 - val_loss: 0.1888 - val_mse: 0.1888 - val_rmse: 0.4346 - val_mae: 0.2429 - val_mape: 7.6305 - lr: 1.0000e-05\n",
      "Epoch 708/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2422 - mape: 7.5950\n",
      "Epoch 708: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1889 - mse: 0.1889 - rmse: 0.4346 - mae: 0.2424 - mape: 7.5992 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2424 - val_mape: 7.6087 - lr: 1.0000e-05\n",
      "Epoch 709/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1893 - mse: 0.1893 - rmse: 0.4350 - mae: 0.2428 - mape: 7.6161\n",
      "Epoch 709: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2425 - mape: 7.6082 - val_loss: 0.1886 - val_mse: 0.1886 - val_rmse: 0.4342 - val_mae: 0.2421 - val_mape: 7.5943 - lr: 1.0000e-05\n",
      "Epoch 710/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2422 - mape: 7.5857\n",
      "Epoch 710: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2422 - mape: 7.5888 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2421 - val_mape: 7.5931 - lr: 1.0000e-05\n",
      "Epoch 711/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1879 - mse: 0.1879 - rmse: 0.4335 - mae: 0.2423 - mape: 7.5862\n",
      "Epoch 711: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2427 - mape: 7.6131 - val_loss: 0.1886 - val_mse: 0.1886 - val_rmse: 0.4343 - val_mae: 0.2419 - val_mape: 7.5882 - lr: 1.0000e-05\n",
      "Epoch 712/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4348 - mae: 0.2418 - mape: 7.5806\n",
      "Epoch 712: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2419 - mape: 7.5826 - val_loss: 0.1888 - val_mse: 0.1888 - val_rmse: 0.4345 - val_mae: 0.2420 - val_mape: 7.5566 - lr: 1.0000e-05\n",
      "Epoch 713/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1862 - mse: 0.1862 - rmse: 0.4316 - mae: 0.2410 - mape: 7.5557\n",
      "Epoch 713: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2420 - mape: 7.5715 - val_loss: 0.1886 - val_mse: 0.1886 - val_rmse: 0.4342 - val_mae: 0.2426 - val_mape: 7.6209 - lr: 1.0000e-05\n",
      "Epoch 714/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1889 - mse: 0.1889 - rmse: 0.4346 - mae: 0.2425 - mape: 7.6099\n",
      "Epoch 714: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2425 - mape: 7.6126 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2419 - val_mape: 7.5718 - lr: 1.0000e-05\n",
      "Epoch 715/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.1912 - mse: 0.1912 - rmse: 0.4372 - mae: 0.2434 - mape: 7.6283\n",
      "Epoch 715: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2422 - mape: 7.5869 - val_loss: 0.1886 - val_mse: 0.1886 - val_rmse: 0.4343 - val_mae: 0.2424 - val_mape: 7.6070 - lr: 1.0000e-05\n",
      "Epoch 716/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1898 - mse: 0.1898 - rmse: 0.4357 - mae: 0.2431 - mape: 7.6117\n",
      "Epoch 716: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1889 - mse: 0.1889 - rmse: 0.4346 - mae: 0.2424 - mape: 7.5990 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2423 - val_mape: 7.5879 - lr: 1.0000e-05\n",
      "Epoch 717/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1897 - mse: 0.1897 - rmse: 0.4355 - mae: 0.2424 - mape: 7.5963\n",
      "Epoch 717: val_loss did not improve from 0.18850\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2422 - mape: 7.5904 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2424 - val_mape: 7.5898 - lr: 1.0000e-05\n",
      "Epoch 718/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2432 - mape: 7.6140\n",
      "Epoch 718: val_loss improved from 0.18850 to 0.18849, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2425 - mape: 7.5994 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4341 - val_mae: 0.2419 - val_mape: 7.5697 - lr: 1.0000e-05\n",
      "Epoch 719/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2420 - mape: 7.5840\n",
      "Epoch 719: val_loss did not improve from 0.18849\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2420 - mape: 7.5816 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2422 - val_mape: 7.5883 - lr: 1.0000e-05\n",
      "Epoch 720/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2424 - mape: 7.5951\n",
      "Epoch 720: val_loss did not improve from 0.18849\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2424 - mape: 7.5944 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2419 - val_mape: 7.5847 - lr: 1.0000e-05\n",
      "Epoch 721/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4348 - mae: 0.2425 - mape: 7.6021\n",
      "Epoch 721: val_loss improved from 0.18849 to 0.18847, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2423 - mape: 7.5987 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4341 - val_mae: 0.2418 - val_mape: 7.5765 - lr: 1.0000e-05\n",
      "Epoch 722/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1886 - mse: 0.1886 - rmse: 0.4343 - mae: 0.2423 - mape: 7.5913\n",
      "Epoch 722: val_loss did not improve from 0.18847\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2422 - mape: 7.5901 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2417 - val_mape: 7.5790 - lr: 1.0000e-05\n",
      "Epoch 723/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1892 - mse: 0.1892 - rmse: 0.4350 - mae: 0.2422 - mape: 7.5826\n",
      "Epoch 723: val_loss did not improve from 0.18847\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2420 - mape: 7.5803 - val_loss: 0.1887 - val_mse: 0.1887 - val_rmse: 0.4344 - val_mae: 0.2423 - val_mape: 7.6121 - lr: 1.0000e-05\n",
      "Epoch 724/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1880 - mse: 0.1880 - rmse: 0.4336 - mae: 0.2412 - mape: 7.5567\n",
      "Epoch 724: val_loss did not improve from 0.18847\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2420 - mape: 7.5853 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2425 - val_mape: 7.6147 - lr: 1.0000e-05\n",
      "Epoch 725/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1904 - mse: 0.1904 - rmse: 0.4364 - mae: 0.2433 - mape: 7.6165\n",
      "Epoch 725: val_loss improved from 0.18847 to 0.18846, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4346 - mae: 0.2425 - mape: 7.6045 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5796 - lr: 1.0000e-05\n",
      "Epoch 726/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1889 - mse: 0.1889 - rmse: 0.4346 - mae: 0.2425 - mape: 7.5860\n",
      "Epoch 726: val_loss did not improve from 0.18846\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2423 - mape: 7.5883 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4341 - val_mae: 0.2422 - val_mape: 7.5925 - lr: 1.0000e-05\n",
      "Epoch 727/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1893 - mse: 0.1893 - rmse: 0.4351 - mae: 0.2422 - mape: 7.5892\n",
      "Epoch 727: val_loss did not improve from 0.18846\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2423 - mape: 7.5946 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2420 - val_mape: 7.5843 - lr: 1.0000e-05\n",
      "Epoch 728/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1873 - mse: 0.1873 - rmse: 0.4328 - mae: 0.2415 - mape: 7.5608\n",
      "Epoch 728: val_loss did not improve from 0.18846\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1886 - mse: 0.1886 - rmse: 0.4343 - mae: 0.2421 - mape: 7.5882 - val_loss: 0.1889 - val_mse: 0.1889 - val_rmse: 0.4346 - val_mae: 0.2431 - val_mape: 7.6384 - lr: 1.0000e-05\n",
      "Epoch 729/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2427 - mape: 7.6085\n",
      "Epoch 729: val_loss did not improve from 0.18846\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2427 - mape: 7.6085 - val_loss: 0.1887 - val_mse: 0.1887 - val_rmse: 0.4343 - val_mae: 0.2419 - val_mape: 7.5604 - lr: 1.0000e-05\n",
      "Epoch 730/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2417 - mape: 7.5662\n",
      "Epoch 730: val_loss did not improve from 0.18846\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2420 - mape: 7.5759 - val_loss: 0.1886 - val_mse: 0.1886 - val_rmse: 0.4343 - val_mae: 0.2425 - val_mape: 7.6153 - lr: 1.0000e-05\n",
      "Epoch 731/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1881 - mse: 0.1881 - rmse: 0.4338 - mae: 0.2423 - mape: 7.5995\n",
      "Epoch 731: val_loss did not improve from 0.18846\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2423 - mape: 7.5976 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2423 - val_mape: 7.6012 - lr: 1.0000e-05\n",
      "Epoch 732/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1873 - mse: 0.1873 - rmse: 0.4328 - mae: 0.2417 - mape: 7.5884\n",
      "Epoch 732: val_loss did not improve from 0.18846\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2424 - mape: 7.6050 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4341 - val_mae: 0.2419 - val_mape: 7.5671 - lr: 1.0000e-05\n",
      "Epoch 733/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1895 - mse: 0.1895 - rmse: 0.4353 - mae: 0.2425 - mape: 7.5937\n",
      "Epoch 733: val_loss did not improve from 0.18846\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2422 - mape: 7.5887 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4342 - val_mae: 0.2421 - val_mape: 7.5868 - lr: 1.0000e-05\n",
      "Epoch 734/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1881 - mse: 0.1881 - rmse: 0.4338 - mae: 0.2424 - mape: 7.5963\n",
      "Epoch 734: val_loss did not improve from 0.18846\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2420 - mape: 7.5775 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4341 - val_mae: 0.2423 - val_mape: 7.6009 - lr: 1.0000e-05\n",
      "Epoch 735/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4340 - mae: 0.2425 - mape: 7.6079\n",
      "Epoch 735: val_loss improved from 0.18846 to 0.18845, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4342 - mae: 0.2423 - mape: 7.6027 - val_loss: 0.1885 - val_mse: 0.1885 - val_rmse: 0.4341 - val_mae: 0.2423 - val_mape: 7.5985 - lr: 1.0000e-06\n",
      "Epoch 736/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1877 - mse: 0.1877 - rmse: 0.4332 - mae: 0.2420 - mape: 7.5605\n",
      "Epoch 736: val_loss improved from 0.18845 to 0.18844, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4342 - mae: 0.2423 - mape: 7.5984 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2423 - val_mape: 7.5961 - lr: 1.0000e-06\n",
      "Epoch 737/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1886 - mse: 0.1886 - rmse: 0.4343 - mae: 0.2424 - mape: 7.5954\n",
      "Epoch 737: val_loss improved from 0.18844 to 0.18844, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2422 - mape: 7.5933 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2422 - val_mape: 7.5947 - lr: 1.0000e-06\n",
      "Epoch 738/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1886 - mse: 0.1886 - rmse: 0.4343 - mae: 0.2423 - mape: 7.5977\n",
      "Epoch 738: val_loss improved from 0.18844 to 0.18844, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2422 - mape: 7.5930 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2422 - val_mape: 7.5914 - lr: 1.0000e-06\n",
      "Epoch 739/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4348 - mae: 0.2424 - mape: 7.5970\n",
      "Epoch 739: val_loss did not improve from 0.18844\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2422 - mape: 7.5949 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5892 - lr: 1.0000e-06\n",
      "Epoch 740/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1877 - mse: 0.1877 - rmse: 0.4332 - mae: 0.2417 - mape: 7.5852\n",
      "Epoch 740: val_loss improved from 0.18844 to 0.18844, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2422 - mape: 7.5906 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5874 - lr: 1.0000e-06\n",
      "Epoch 741/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1881 - mse: 0.1881 - rmse: 0.4337 - mae: 0.2420 - mape: 7.5833\n",
      "Epoch 741: val_loss improved from 0.18844 to 0.18844, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5881 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5886 - lr: 1.0000e-06\n",
      "Epoch 742/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1899 - mse: 0.1899 - rmse: 0.4358 - mae: 0.2426 - mape: 7.5966\n",
      "Epoch 742: val_loss did not improve from 0.18844\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5889 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5886 - lr: 1.0000e-06\n",
      "Epoch 743/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1879 - mse: 0.1879 - rmse: 0.4334 - mae: 0.2418 - mape: 7.5833\n",
      "Epoch 743: val_loss improved from 0.18844 to 0.18844, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5886 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5880 - lr: 1.0000e-06\n",
      "Epoch 744/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1891 - mse: 0.1891 - rmse: 0.4348 - mae: 0.2423 - mape: 7.5819\n",
      "Epoch 744: val_loss improved from 0.18844 to 0.18844, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2422 - mape: 7.5910 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5878 - lr: 1.0000e-06\n",
      "Epoch 745/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1900 - mse: 0.1900 - rmse: 0.4359 - mae: 0.2428 - mape: 7.6058\n",
      "Epoch 745: val_loss improved from 0.18844 to 0.18844, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5868 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5879 - lr: 1.0000e-06\n",
      "Epoch 746/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1881 - mse: 0.1881 - rmse: 0.4337 - mae: 0.2421 - mape: 7.5877\n",
      "Epoch 746: val_loss improved from 0.18844 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5905 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5854 - lr: 1.0000e-06\n",
      "Epoch 747/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1896 - mse: 0.1896 - rmse: 0.4355 - mae: 0.2430 - mape: 7.6117\n",
      "Epoch 747: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5821 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5857 - lr: 1.0000e-06\n",
      "Epoch 748/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2421 - mape: 7.5838\n",
      "Epoch 748: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5863 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5866 - lr: 1.0000e-06\n",
      "Epoch 749/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1885 - mse: 0.1885 - rmse: 0.4342 - mae: 0.2418 - mape: 7.5788\n",
      "Epoch 749: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5849 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5860 - lr: 1.0000e-06\n",
      "Epoch 750/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4348 - mae: 0.2422 - mape: 7.5840\n",
      "Epoch 750: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5852 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5879 - lr: 1.0000e-06\n",
      "Epoch 751/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1885 - mse: 0.1885 - rmse: 0.4342 - mae: 0.2421 - mape: 7.5877\n",
      "Epoch 751: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5874 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5873 - lr: 1.0000e-06\n",
      "Epoch 752/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1857 - mse: 0.1857 - rmse: 0.4309 - mae: 0.2414 - mape: 7.5674\n",
      "Epoch 752: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5861 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5856 - lr: 1.0000e-06\n",
      "Epoch 753/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1881 - mse: 0.1881 - rmse: 0.4337 - mae: 0.2416 - mape: 7.5740\n",
      "Epoch 753: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5894 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5847 - lr: 1.0000e-06\n",
      "Epoch 754/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1901 - mse: 0.1901 - rmse: 0.4360 - mae: 0.2425 - mape: 7.5945\n",
      "Epoch 754: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5853 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5853 - lr: 1.0000e-06\n",
      "Epoch 755/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4347 - mae: 0.2424 - mape: 7.5976\n",
      "Epoch 755: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5868 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5846 - lr: 1.0000e-06\n",
      "Epoch 756/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1885 - mse: 0.1885 - rmse: 0.4342 - mae: 0.2421 - mape: 7.5762\n",
      "Epoch 756: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5859 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5867 - lr: 1.0000e-06\n",
      "Epoch 757/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1877 - mse: 0.1877 - rmse: 0.4332 - mae: 0.2419 - mape: 7.5799\n",
      "Epoch 757: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5866 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-06\n",
      "Epoch 758/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4348 - mae: 0.2423 - mape: 7.5993\n",
      "Epoch 758: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5832 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5827 - lr: 1.0000e-06\n",
      "Epoch 759/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2421 - mape: 7.5829\n",
      "Epoch 759: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5840 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5827 - lr: 1.0000e-06\n",
      "Epoch 760/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1893 - mse: 0.1893 - rmse: 0.4351 - mae: 0.2432 - mape: 7.6050\n",
      "Epoch 760: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5846 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5849 - lr: 1.0000e-06\n",
      "Epoch 761/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1908 - mse: 0.1908 - rmse: 0.4368 - mae: 0.2431 - mape: 7.5988\n",
      "Epoch 761: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5830 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5847 - lr: 1.0000e-06\n",
      "Epoch 762/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.1871 - mse: 0.1871 - rmse: 0.4325 - mae: 0.2415 - mape: 7.5650\n",
      "Epoch 762: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5848 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5848 - lr: 1.0000e-06\n",
      "Epoch 763/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1880 - mse: 0.1880 - rmse: 0.4336 - mae: 0.2417 - mape: 7.5707\n",
      "Epoch 763: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5837 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5847 - lr: 1.0000e-06\n",
      "Epoch 764/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1886 - mse: 0.1886 - rmse: 0.4342 - mae: 0.2420 - mape: 7.5897\n",
      "Epoch 764: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5865 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5851 - lr: 1.0000e-06\n",
      "Epoch 765/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1892 - mse: 0.1892 - rmse: 0.4349 - mae: 0.2424 - mape: 7.5967\n",
      "Epoch 765: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5859 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5827 - lr: 1.0000e-06\n",
      "Epoch 766/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2418 - mape: 7.5821\n",
      "Epoch 766: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5835 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5843 - lr: 1.0000e-06\n",
      "Epoch 767/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1912 - mse: 0.1912 - rmse: 0.4373 - mae: 0.2439 - mape: 7.6349\n",
      "Epoch 767: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5881 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5835 - lr: 1.0000e-06\n",
      "Epoch 768/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1889 - mse: 0.1889 - rmse: 0.4346 - mae: 0.2423 - mape: 7.5974\n",
      "Epoch 768: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5845 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5854 - lr: 1.0000e-06\n",
      "Epoch 769/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1861 - mse: 0.1861 - rmse: 0.4314 - mae: 0.2409 - mape: 7.5525\n",
      "Epoch 769: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5862 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5856 - lr: 1.0000e-06\n",
      "Epoch 770/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1885 - mse: 0.1885 - rmse: 0.4342 - mae: 0.2421 - mape: 7.5850\n",
      "Epoch 770: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5851 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2421 - val_mape: 7.5869 - lr: 1.0000e-06\n",
      "Epoch 771/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1889 - mse: 0.1889 - rmse: 0.4346 - mae: 0.2422 - mape: 7.5895\n",
      "Epoch 771: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5860 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5851 - lr: 1.0000e-06\n",
      "Epoch 772/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1885 - mse: 0.1885 - rmse: 0.4342 - mae: 0.2422 - mape: 7.5998\n",
      "Epoch 772: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5895 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5860 - lr: 1.0000e-06\n",
      "Epoch 773/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2425 - mape: 7.6038\n",
      "Epoch 773: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5867 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5856 - lr: 1.0000e-06\n",
      "Epoch 774/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1905 - mse: 0.1905 - rmse: 0.4365 - mae: 0.2429 - mape: 7.6012\n",
      "Epoch 774: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5862 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5848 - lr: 1.0000e-06\n",
      "Epoch 775/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1891 - mse: 0.1891 - rmse: 0.4349 - mae: 0.2424 - mape: 7.5951\n",
      "Epoch 775: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5830 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5838 - lr: 1.0000e-06\n",
      "Epoch 776/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1895 - mse: 0.1895 - rmse: 0.4353 - mae: 0.2422 - mape: 7.6034\n",
      "Epoch 776: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5841 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5846 - lr: 1.0000e-06\n",
      "Epoch 777/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4340 - mae: 0.2419 - mape: 7.5822\n",
      "Epoch 777: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5846 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5845 - lr: 1.0000e-07\n",
      "Epoch 778/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1879 - mse: 0.1879 - rmse: 0.4334 - mae: 0.2421 - mape: 7.5875\n",
      "Epoch 778: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5841 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5845 - lr: 1.0000e-07\n",
      "Epoch 779/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1877 - mse: 0.1877 - rmse: 0.4333 - mae: 0.2415 - mape: 7.5720\n",
      "Epoch 779: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5847 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5844 - lr: 1.0000e-07\n",
      "Epoch 780/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2421 - mape: 7.5847\n",
      "Epoch 780: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5849 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5846 - lr: 1.0000e-07\n",
      "Epoch 781/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5844\n",
      "Epoch 781: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5844 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5845 - lr: 1.0000e-07\n",
      "Epoch 782/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1876 - mse: 0.1876 - rmse: 0.4331 - mae: 0.2421 - mape: 7.5894\n",
      "Epoch 782: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5848 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5844 - lr: 1.0000e-07\n",
      "Epoch 783/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.1879 - mse: 0.1879 - rmse: 0.4335 - mae: 0.2413 - mape: 7.5752\n",
      "Epoch 783: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5845 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5845 - lr: 1.0000e-07\n",
      "Epoch 784/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1896 - mse: 0.1896 - rmse: 0.4355 - mae: 0.2426 - mape: 7.6009\n",
      "Epoch 784: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5844 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5845 - lr: 1.0000e-07\n",
      "Epoch 785/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2421 - mape: 7.5869\n",
      "Epoch 785: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5848 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5848 - lr: 1.0000e-07\n",
      "Epoch 786/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1867 - mse: 0.1867 - rmse: 0.4321 - mae: 0.2415 - mape: 7.5698\n",
      "Epoch 786: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5850 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5848 - lr: 1.0000e-07\n",
      "Epoch 787/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1899 - mse: 0.1899 - rmse: 0.4358 - mae: 0.2425 - mape: 7.5936\n",
      "Epoch 787: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5852 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5848 - lr: 1.0000e-07\n",
      "Epoch 788/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1882 - mse: 0.1882 - rmse: 0.4339 - mae: 0.2418 - mape: 7.5787\n",
      "Epoch 788: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5846 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5847 - lr: 1.0000e-07\n",
      "Epoch 789/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1873 - mse: 0.1873 - rmse: 0.4328 - mae: 0.2418 - mape: 7.5661\n",
      "Epoch 789: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5847 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5847 - lr: 1.0000e-07\n",
      "Epoch 790/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1895 - mse: 0.1895 - rmse: 0.4353 - mae: 0.2424 - mape: 7.5940\n",
      "Epoch 790: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5850 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5845 - lr: 1.0000e-07\n",
      "Epoch 791/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1896 - mse: 0.1896 - rmse: 0.4354 - mae: 0.2423 - mape: 7.5903\n",
      "Epoch 791: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5846 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5845 - lr: 1.0000e-07\n",
      "Epoch 792/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.1876 - mse: 0.1876 - rmse: 0.4331 - mae: 0.2419 - mape: 7.5850\n",
      "Epoch 792: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5844 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5845 - lr: 1.0000e-07\n",
      "Epoch 793/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1886 - mse: 0.1886 - rmse: 0.4343 - mae: 0.2419 - mape: 7.5769\n",
      "Epoch 793: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5845 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5847 - lr: 1.0000e-07\n",
      "Epoch 794/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1891 - mse: 0.1891 - rmse: 0.4349 - mae: 0.2421 - mape: 7.5808\n",
      "Epoch 794: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5846 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5844 - lr: 1.0000e-07\n",
      "Epoch 795/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1873 - mse: 0.1873 - rmse: 0.4328 - mae: 0.2413 - mape: 7.5765\n",
      "Epoch 795: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5841 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5844 - lr: 1.0000e-07\n",
      "Epoch 796/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2422 - mape: 7.5885\n",
      "Epoch 796: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5844 - lr: 1.0000e-07\n",
      "Epoch 797/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4340 - mae: 0.2419 - mape: 7.5830\n",
      "Epoch 797: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5843 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5843 - lr: 1.0000e-07\n",
      "Epoch 798/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1901 - mse: 0.1901 - rmse: 0.4360 - mae: 0.2429 - mape: 7.5986\n",
      "Epoch 798: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5843 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5844 - lr: 1.0000e-07\n",
      "Epoch 799/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1886 - mse: 0.1886 - rmse: 0.4343 - mae: 0.2419 - mape: 7.5792\n",
      "Epoch 799: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5845 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5843 - lr: 1.0000e-07\n",
      "Epoch 800/2000\n",
      "288/318 [==========================>...] - ETA: 0s - loss: 0.1868 - mse: 0.1868 - rmse: 0.4321 - mae: 0.2412 - mape: 7.5709\n",
      "Epoch 800: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5846 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5844 - lr: 1.0000e-07\n",
      "Epoch 801/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1875 - mse: 0.1875 - rmse: 0.4330 - mae: 0.2419 - mape: 7.5879\n",
      "Epoch 801: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5843 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5844 - lr: 1.0000e-07\n",
      "Epoch 802/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1879 - mse: 0.1879 - rmse: 0.4334 - mae: 0.2417 - mape: 7.5718\n",
      "Epoch 802: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5846 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5845 - lr: 1.0000e-07\n",
      "Epoch 803/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1907 - mse: 0.1907 - rmse: 0.4367 - mae: 0.2432 - mape: 7.6216\n",
      "Epoch 803: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5846 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5845 - lr: 1.0000e-07\n",
      "Epoch 804/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1879 - mse: 0.1879 - rmse: 0.4335 - mae: 0.2420 - mape: 7.5787\n",
      "Epoch 804: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5845 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5844 - lr: 1.0000e-07\n",
      "Epoch 805/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4340 - mae: 0.2420 - mape: 7.5878\n",
      "Epoch 805: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5844 - lr: 1.0000e-07\n",
      "Epoch 806/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5850\n",
      "Epoch 806: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5844 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5841 - lr: 1.0000e-07\n",
      "Epoch 807/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1889 - mse: 0.1889 - rmse: 0.4346 - mae: 0.2428 - mape: 7.6098\n",
      "Epoch 807: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5844 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-07\n",
      "Epoch 808/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1891 - mse: 0.1891 - rmse: 0.4349 - mae: 0.2425 - mape: 7.5969\n",
      "Epoch 808: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5843 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-07\n",
      "Epoch 809/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4340 - mae: 0.2419 - mape: 7.5887\n",
      "Epoch 809: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5840 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-07\n",
      "Epoch 810/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1891 - mse: 0.1891 - rmse: 0.4349 - mae: 0.2422 - mape: 7.5895\n",
      "Epoch 810: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5843 - lr: 1.0000e-07\n",
      "Epoch 811/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1862 - mse: 0.1862 - rmse: 0.4316 - mae: 0.2404 - mape: 7.5538\n",
      "Epoch 811: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-07\n",
      "Epoch 812/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.1885 - mse: 0.1885 - rmse: 0.4341 - mae: 0.2417 - mape: 7.5781\n",
      "Epoch 812: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5844 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5844 - lr: 1.0000e-07\n",
      "Epoch 813/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2422 - mape: 7.5877\n",
      "Epoch 813: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5843 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5843 - lr: 1.0000e-07\n",
      "Epoch 814/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2423 - mape: 7.5854\n",
      "Epoch 814: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5843 - lr: 1.0000e-07\n",
      "Epoch 815/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1879 - mse: 0.1879 - rmse: 0.4334 - mae: 0.2416 - mape: 7.5770\n",
      "Epoch 815: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5844 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5843 - lr: 1.0000e-07\n",
      "Epoch 816/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5844\n",
      "Epoch 816: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5844 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-07\n",
      "Epoch 817/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1898 - mse: 0.1898 - rmse: 0.4357 - mae: 0.2429 - mape: 7.6025\n",
      "Epoch 817: val_loss improved from 0.18843 to 0.18843, saving model to model_weights/20221125-124022_mlp_seg_best_weights.hdf5\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 818/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1883 - mse: 0.1883 - rmse: 0.4339 - mae: 0.2419 - mape: 7.5813\n",
      "Epoch 818: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 819/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1885 - mse: 0.1885 - rmse: 0.4342 - mae: 0.2421 - mape: 7.5865\n",
      "Epoch 819: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 820/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1883 - mse: 0.1883 - rmse: 0.4339 - mae: 0.2421 - mape: 7.5940\n",
      "Epoch 820: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 821/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1889 - mse: 0.1889 - rmse: 0.4347 - mae: 0.2426 - mape: 7.5960\n",
      "Epoch 821: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 822/2000\n",
      "287/318 [==========================>...] - ETA: 0s - loss: 0.1896 - mse: 0.1896 - rmse: 0.4354 - mae: 0.2423 - mape: 7.5960\n",
      "Epoch 822: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 823/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1881 - mse: 0.1881 - rmse: 0.4337 - mae: 0.2419 - mape: 7.5809\n",
      "Epoch 823: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 824/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1894 - mse: 0.1894 - rmse: 0.4352 - mae: 0.2425 - mape: 7.5994\n",
      "Epoch 824: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 825/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2422 - mape: 7.5907\n",
      "Epoch 825: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 826/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1853 - mse: 0.1853 - rmse: 0.4304 - mae: 0.2410 - mape: 7.5577\n",
      "Epoch 826: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 827/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1899 - mse: 0.1899 - rmse: 0.4358 - mae: 0.2422 - mape: 7.5848\n",
      "Epoch 827: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 828/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2422 - mape: 7.5911\n",
      "Epoch 828: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 829/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2422 - mape: 7.5879\n",
      "Epoch 829: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 830/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1882 - mse: 0.1882 - rmse: 0.4338 - mae: 0.2418 - mape: 7.5782\n",
      "Epoch 830: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 831/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842\n",
      "Epoch 831: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 832/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1894 - mse: 0.1894 - rmse: 0.4352 - mae: 0.2424 - mape: 7.5895\n",
      "Epoch 832: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 833/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842\n",
      "Epoch 833: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 834/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1880 - mse: 0.1880 - rmse: 0.4336 - mae: 0.2418 - mape: 7.5803\n",
      "Epoch 834: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 835/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2422 - mape: 7.5871\n",
      "Epoch 835: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 836/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842\n",
      "Epoch 836: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 837/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1894 - mse: 0.1894 - rmse: 0.4352 - mae: 0.2429 - mape: 7.6021\n",
      "Epoch 837: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 838/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1872 - mse: 0.1872 - rmse: 0.4326 - mae: 0.2421 - mape: 7.5845\n",
      "Epoch 838: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 839/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1882 - mse: 0.1882 - rmse: 0.4338 - mae: 0.2421 - mape: 7.5965\n",
      "Epoch 839: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 840/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1883 - mse: 0.1883 - rmse: 0.4339 - mae: 0.2420 - mape: 7.5722\n",
      "Epoch 840: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 841/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1880 - mse: 0.1880 - rmse: 0.4335 - mae: 0.2423 - mape: 7.5997\n",
      "Epoch 841: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 842/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1882 - mse: 0.1882 - rmse: 0.4338 - mae: 0.2417 - mape: 7.5787\n",
      "Epoch 842: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 843/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4348 - mae: 0.2423 - mape: 7.5938\n",
      "Epoch 843: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 844/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.1909 - mse: 0.1909 - rmse: 0.4370 - mae: 0.2429 - mape: 7.5988\n",
      "Epoch 844: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 845/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1867 - mse: 0.1867 - rmse: 0.4321 - mae: 0.2415 - mape: 7.5676\n",
      "Epoch 845: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 846/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1883 - mse: 0.1883 - rmse: 0.4339 - mae: 0.2418 - mape: 7.5704\n",
      "Epoch 846: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 847/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1882 - mse: 0.1882 - rmse: 0.4338 - mae: 0.2419 - mape: 7.5811\n",
      "Epoch 847: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 848/2000\n",
      "293/318 [==========================>...] - ETA: 0s - loss: 0.1899 - mse: 0.1899 - rmse: 0.4358 - mae: 0.2429 - mape: 7.6088\n",
      "Epoch 848: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 849/2000\n",
      "304/318 [===========================>..] - ETA: 0s - loss: 0.1879 - mse: 0.1879 - rmse: 0.4334 - mae: 0.2420 - mape: 7.5846\n",
      "Epoch 849: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 850/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1887 - mse: 0.1887 - rmse: 0.4343 - mae: 0.2422 - mape: 7.5843\n",
      "Epoch 850: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 851/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4347 - mae: 0.2423 - mape: 7.5882\n",
      "Epoch 851: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 852/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1886 - mse: 0.1886 - rmse: 0.4342 - mae: 0.2418 - mape: 7.5722\n",
      "Epoch 852: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 853/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1885 - mse: 0.1885 - rmse: 0.4342 - mae: 0.2421 - mape: 7.5860\n",
      "Epoch 853: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 854/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1886 - mse: 0.1886 - rmse: 0.4343 - mae: 0.2414 - mape: 7.5607\n",
      "Epoch 854: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 855/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2422 - mape: 7.5867\n",
      "Epoch 855: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 856/2000\n",
      "316/318 [============================>.] - ETA: 0s - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2422 - mape: 7.5871\n",
      "Epoch 856: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-08\n",
      "Epoch 857/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1878 - mse: 0.1878 - rmse: 0.4333 - mae: 0.2419 - mape: 7.5810\n",
      "Epoch 857: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 858/2000\n",
      "296/318 [==========================>...] - ETA: 0s - loss: 0.1909 - mse: 0.1909 - rmse: 0.4369 - mae: 0.2438 - mape: 7.6241\n",
      "Epoch 858: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 859/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1896 - mse: 0.1896 - rmse: 0.4354 - mae: 0.2426 - mape: 7.5916\n",
      "Epoch 859: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 860/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1882 - mse: 0.1882 - rmse: 0.4338 - mae: 0.2421 - mape: 7.5934\n",
      "Epoch 860: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 861/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1880 - mse: 0.1880 - rmse: 0.4336 - mae: 0.2417 - mape: 7.5783\n",
      "Epoch 861: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 862/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1882 - mse: 0.1882 - rmse: 0.4338 - mae: 0.2416 - mape: 7.5717\n",
      "Epoch 862: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 863/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1883 - mse: 0.1883 - rmse: 0.4339 - mae: 0.2422 - mape: 7.5908\n",
      "Epoch 863: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 864/2000\n",
      "315/318 [============================>.] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2421 - mape: 7.5887\n",
      "Epoch 864: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 865/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1876 - mse: 0.1876 - rmse: 0.4332 - mae: 0.2418 - mape: 7.5858\n",
      "Epoch 865: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 866/2000\n",
      "308/318 [============================>.] - ETA: 0s - loss: 0.1899 - mse: 0.1899 - rmse: 0.4357 - mae: 0.2425 - mape: 7.5992\n",
      "Epoch 866: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 867/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1866 - mse: 0.1866 - rmse: 0.4320 - mae: 0.2411 - mape: 7.5550\n",
      "Epoch 867: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 868/2000\n",
      "317/318 [============================>.] - ETA: 0s - loss: 0.1885 - mse: 0.1885 - rmse: 0.4342 - mae: 0.2421 - mape: 7.5859\n",
      "Epoch 868: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 869/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1887 - mse: 0.1887 - rmse: 0.4343 - mae: 0.2422 - mape: 7.5899\n",
      "Epoch 869: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 870/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1868 - mse: 0.1868 - rmse: 0.4322 - mae: 0.2412 - mape: 7.5564\n",
      "Epoch 870: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 871/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1876 - mse: 0.1876 - rmse: 0.4331 - mae: 0.2419 - mape: 7.5653\n",
      "Epoch 871: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 872/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1885 - mse: 0.1885 - rmse: 0.4342 - mae: 0.2420 - mape: 7.5853\n",
      "Epoch 872: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 873/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4347 - mae: 0.2425 - mape: 7.6010\n",
      "Epoch 873: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 874/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1894 - mse: 0.1894 - rmse: 0.4352 - mae: 0.2425 - mape: 7.5951\n",
      "Epoch 874: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 875/2000\n",
      "311/318 [============================>.] - ETA: 0s - loss: 0.1872 - mse: 0.1872 - rmse: 0.4327 - mae: 0.2415 - mape: 7.5754\n",
      "Epoch 875: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 876/2000\n",
      "299/318 [===========================>..] - ETA: 0s - loss: 0.1879 - mse: 0.1879 - rmse: 0.4334 - mae: 0.2419 - mape: 7.5641\n",
      "Epoch 876: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 877/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5756\n",
      "Epoch 877: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 878/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2421 - mape: 7.5887\n",
      "Epoch 878: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 879/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1871 - mse: 0.1871 - rmse: 0.4326 - mae: 0.2420 - mape: 7.5789\n",
      "Epoch 879: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 880/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1869 - mse: 0.1869 - rmse: 0.4323 - mae: 0.2419 - mape: 7.5834\n",
      "Epoch 880: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 881/2000\n",
      "310/318 [============================>.] - ETA: 0s - loss: 0.1897 - mse: 0.1897 - rmse: 0.4355 - mae: 0.2426 - mape: 7.5958\n",
      "Epoch 881: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 882/2000\n",
      "292/318 [==========================>...] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4347 - mae: 0.2426 - mape: 7.5912\n",
      "Epoch 882: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 883/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4348 - mae: 0.2424 - mape: 7.5960\n",
      "Epoch 883: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 884/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1877 - mse: 0.1877 - rmse: 0.4332 - mae: 0.2417 - mape: 7.5736\n",
      "Epoch 884: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 885/2000\n",
      "309/318 [============================>.] - ETA: 0s - loss: 0.1878 - mse: 0.1878 - rmse: 0.4333 - mae: 0.2414 - mape: 7.5682\n",
      "Epoch 885: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 886/2000\n",
      "297/318 [===========================>..] - ETA: 0s - loss: 0.1875 - mse: 0.1875 - rmse: 0.4330 - mae: 0.2412 - mape: 7.5582\n",
      "Epoch 886: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 887/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1878 - mse: 0.1878 - rmse: 0.4334 - mae: 0.2418 - mape: 7.5635\n",
      "Epoch 887: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 888/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1902 - mse: 0.1902 - rmse: 0.4361 - mae: 0.2426 - mape: 7.5864\n",
      "Epoch 888: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 889/2000\n",
      "318/318 [==============================] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842\n",
      "Epoch 889: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 890/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1892 - mse: 0.1892 - rmse: 0.4349 - mae: 0.2424 - mape: 7.5969\n",
      "Epoch 890: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 891/2000\n",
      "313/318 [============================>.] - ETA: 0s - loss: 0.1893 - mse: 0.1893 - rmse: 0.4351 - mae: 0.2424 - mape: 7.5949\n",
      "Epoch 891: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 892/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1890 - mse: 0.1890 - rmse: 0.4348 - mae: 0.2423 - mape: 7.5908\n",
      "Epoch 892: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 893/2000\n",
      "289/318 [==========================>...] - ETA: 0s - loss: 0.1862 - mse: 0.1862 - rmse: 0.4315 - mae: 0.2411 - mape: 7.5735\n",
      "Epoch 893: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 894/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1882 - mse: 0.1882 - rmse: 0.4338 - mae: 0.2427 - mape: 7.5989\n",
      "Epoch 894: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 895/2000\n",
      "305/318 [===========================>..] - ETA: 0s - loss: 0.1876 - mse: 0.1876 - rmse: 0.4331 - mae: 0.2416 - mape: 7.5813\n",
      "Epoch 895: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 896/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1882 - mse: 0.1882 - rmse: 0.4338 - mae: 0.2416 - mape: 7.5762\n",
      "Epoch 896: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-09\n",
      "Epoch 897/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1876 - mse: 0.1876 - rmse: 0.4332 - mae: 0.2417 - mape: 7.5671\n",
      "Epoch 897: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 898/2000\n",
      "290/318 [==========================>...] - ETA: 0s - loss: 0.1893 - mse: 0.1893 - rmse: 0.4351 - mae: 0.2426 - mape: 7.5981\n",
      "Epoch 898: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 899/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1874 - mse: 0.1874 - rmse: 0.4329 - mae: 0.2420 - mape: 7.5823\n",
      "Epoch 899: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 900/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1897 - mse: 0.1897 - rmse: 0.4355 - mae: 0.2427 - mape: 7.6024\n",
      "Epoch 900: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 901/2000\n",
      "312/318 [============================>.] - ETA: 0s - loss: 0.1884 - mse: 0.1884 - rmse: 0.4340 - mae: 0.2416 - mape: 7.5699\n",
      "Epoch 901: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 4ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 902/2000\n",
      "302/318 [===========================>..] - ETA: 0s - loss: 0.1894 - mse: 0.1894 - rmse: 0.4352 - mae: 0.2434 - mape: 7.6272\n",
      "Epoch 902: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 903/2000\n",
      "314/318 [============================>.] - ETA: 0s - loss: 0.1888 - mse: 0.1888 - rmse: 0.4345 - mae: 0.2423 - mape: 7.5874\n",
      "Epoch 903: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 904/2000\n",
      "307/318 [===========================>..] - ETA: 0s - loss: 0.1878 - mse: 0.1878 - rmse: 0.4334 - mae: 0.2416 - mape: 7.5749\n",
      "Epoch 904: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 905/2000\n",
      "300/318 [===========================>..] - ETA: 0s - loss: 0.1873 - mse: 0.1873 - rmse: 0.4328 - mae: 0.2410 - mape: 7.5501\n",
      "Epoch 905: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 906/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1898 - mse: 0.1898 - rmse: 0.4357 - mae: 0.2423 - mape: 7.5852\n",
      "Epoch 906: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 907/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1872 - mse: 0.1872 - rmse: 0.4327 - mae: 0.2412 - mape: 7.5698\n",
      "Epoch 907: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 908/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1882 - mse: 0.1882 - rmse: 0.4338 - mae: 0.2421 - mape: 7.5893\n",
      "Epoch 908: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 909/2000\n",
      "291/318 [==========================>...] - ETA: 0s - loss: 0.1843 - mse: 0.1843 - rmse: 0.4293 - mae: 0.2406 - mape: 7.5559\n",
      "Epoch 909: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 910/2000\n",
      "295/318 [==========================>...] - ETA: 0s - loss: 0.1896 - mse: 0.1896 - rmse: 0.4354 - mae: 0.2423 - mape: 7.5777\n",
      "Epoch 910: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 911/2000\n",
      "306/318 [===========================>..] - ETA: 0s - loss: 0.1864 - mse: 0.1864 - rmse: 0.4318 - mae: 0.2407 - mape: 7.5517\n",
      "Epoch 911: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 912/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1902 - mse: 0.1902 - rmse: 0.4362 - mae: 0.2430 - mape: 7.6093\n",
      "Epoch 912: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 913/2000\n",
      "303/318 [===========================>..] - ETA: 0s - loss: 0.1886 - mse: 0.1886 - rmse: 0.4342 - mae: 0.2418 - mape: 7.5780\n",
      "Epoch 913: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 914/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1887 - mse: 0.1887 - rmse: 0.4344 - mae: 0.2429 - mape: 7.6020\n",
      "Epoch 914: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 915/2000\n",
      "298/318 [===========================>..] - ETA: 0s - loss: 0.1882 - mse: 0.1882 - rmse: 0.4338 - mae: 0.2424 - mape: 7.6019\n",
      "Epoch 915: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 916/2000\n",
      "294/318 [==========================>...] - ETA: 0s - loss: 0.1875 - mse: 0.1875 - rmse: 0.4330 - mae: 0.2417 - mape: 7.5771\n",
      "Epoch 916: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 917/2000\n",
      "301/318 [===========================>..] - ETA: 0s - loss: 0.1882 - mse: 0.1882 - rmse: 0.4338 - mae: 0.2414 - mape: 7.5644\n",
      "Epoch 917: val_loss did not improve from 0.18843\n",
      "318/318 [==============================] - 1s 3ms/step - loss: 0.1884 - mse: 0.1884 - rmse: 0.4341 - mae: 0.2420 - mape: 7.5842 - val_loss: 0.1884 - val_mse: 0.1884 - val_rmse: 0.4341 - val_mae: 0.2420 - val_mape: 7.5842 - lr: 1.0000e-10\n",
      "Epoch 917: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff404f1fa30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 2000, batch_size = 64, validation_data = (X_val, y_val), callbacks=[checkpoint_callback, es_callback, tf.keras.callbacks.ReduceLROnPlateau(patience=40)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20221125-124022\n"
     ]
    }
   ],
   "source": [
    "print(date_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models.load_model(f'model_weights/{date_actual}_mlp_seg_best_weights.hdf5')\n",
    "#best_model = models.load_model(f'model_weights/20221123-225504_mlp_seg_best_weights.hdf5')\n",
    "#best_model = models.load_model(f'model_weights/MLPRegressor_Seg_V3_1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 1s 1ms/step - loss: 0.2258 - mse: 0.2258 - rmse: 0.4752 - mae: 0.2791 - mape: 9.7301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22580108046531677,\n",
       " 0.22580108046531677,\n",
       " 0.4751853048801422,\n",
       " 0.2790627181529999,\n",
       " 9.730066299438477]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 651us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.422092566707832\n",
      "mse:  0.2258010653621972\n",
      "rmse:  0.47518529581858615\n",
      "mae:  0.2790626532858474\n",
      "mape:  0.0973006752812812\n",
      "Error estandar:  0.4650789919813469\n"
     ]
    }
   ],
   "source": [
    "print(\"R^2: \", r2_score(y_test, y_pred))\n",
    "print(\"mse: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"rmse: \", mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"mae: \", mean_absolute_error(y_test, y_pred))\n",
    "print(\"mape: \", mean_absolute_percentage_error(y_test, y_pred))\n",
    "print(\"Error estandar: \", stde(y_test.squeeze(),\n",
    "      y_pred.squeeze(), ddof=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x640 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residuals = y_test - y_pred\n",
    "residuals_std = residuals/residuals.std()\n",
    "\n",
    "y_real_stage = np.array([i[0] for i in y_test])\n",
    "residual_stage = np.array([i[0] for i in residuals])\n",
    "\n",
    "#y_real_discharge = np.array([i[-1] for i in y_test])\n",
    "#residual_discharge = np.array([i[-1] for i in residuals])\n",
    "\n",
    "\n",
    "figure, ax = plt.subplots(ncols=2, figsize=(20, 8), dpi=80)\n",
    "\n",
    "ax[0].scatter(y_real_stage, residual_stage / residual_stage.std(), label=\"stage residuals\")\n",
    "#ax[1].scatter(y_real_discharge, residual_discharge / residual_discharge.std(), label=\"discharge residuals\")\n",
    "ax[0].axhline(y=0.0, color='r', linestyle='-')\n",
    "ax[1].axhline(y=0.0, color='r', linestyle='-')\n",
    "\n",
    "ax[0].set_title(\"Stage residuals\")\n",
    "ax[1].set_title(\"Discharge residuals\")\n",
    "\n",
    "ax[1].set_xlabel(\"Fitted values\")\n",
    "ax[0].set_xlabel(\"Fitted values\")\n",
    "ax[1].set_ylabel(\"Standarized residuals\")\n",
    "ax[0].set_ylabel(\"Standarized residuals\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import normal_ad\n",
    "\n",
    "#figure = sm.qqplot(residual_stage / residual_stage.std(), line ='45', label='stage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = sm.qqplot(residual_stage / residual_stage.std(), line='45', label='discharge')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(residual_stage / residual_stage.std(), density=True, bins = 60)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Residual stage');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plt.hist(residual_discharge / residual_discharge.std(), density=True, bins = 60)\\nplt.ylabel('Count')\\nplt.xlabel('Residual discharge');\\nplt.show()\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"plt.hist(residual_discharge / residual_discharge.std(), density=True, bins = 60)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Residual discharge');\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.0\n",
      "Hay evidencia de que los residuos no provienen de una distribución normal.\n"
     ]
    }
   ],
   "source": [
    "stat, pval = normal_ad(residual_stage / residual_stage.std())\n",
    "print(\"p-value:\", pval)\n",
    "\n",
    "if pval < 0.05:\n",
    "    print(\"Hay evidencia de que los residuos no provienen de una distribución normal.\")\n",
    "else:\n",
    "    print(\"No hay evidencia para rechazar la hipótesis de que los residuos vienen de una distribución normal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x640 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure, ax = plt.subplots(ncols=2, figsize=(20, 8), dpi=80)\n",
    "\n",
    "ax[0].plot(np.arange(len(y_test)), y_test, label=\"Stage real\")\n",
    "ax[0].plot(np.arange(len(y_test)), y_pred, label=\"Stage pred\")\n",
    "\n",
    "ax[0].set_title(\"Stage predictions\")\n",
    "ax[1].set_title(\"Discharge predictions\")\n",
    "\n",
    "ax[1].set_ylabel(\"Values\")\n",
    "ax[0].set_ylabel(\"Values\")\n",
    "ax[1].set_xlabel(\"Time\")\n",
    "ax[0].set_xlabel(\"Time\")\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('AI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71f8a22563664e7fd593e913f560669104737b8ff064ffb20c7dfa1c5db14acf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
